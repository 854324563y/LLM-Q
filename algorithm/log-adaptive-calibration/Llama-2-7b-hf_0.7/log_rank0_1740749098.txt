[2025-02-28 13:24:58 root] (main_calib_config2.py 284): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-adaptive-calibration/Llama-2-7b-hf_0.7', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.7.pkl', scale_calibration=True, compensation_calibration=True)
[2025-02-28 13:25:05 root] (main_calib_config2.py 351): INFO === start quantization ===
[2025-02-28 13:25:05 root] (main_calib_config2.py 357): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-28 13:25:05 root] (abq_llm_calib_config2.py 82): INFO Starting ...
[2025-02-28 13:25:05 root] (abq_llm_calib_config2.py 89): INFO Loaded quant_map from log-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.7.pkl
[2025-02-28 13:25:07 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 0 ===
[2025-02-28 13:25:09 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 13:25:40 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 0 loss:0.005262426566332579 norm:0.009158051572740078 max memory_allocated 22562.10693359375 
[2025-02-28 13:26:11 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 1 loss:0.002964269369840622 norm:0.0057612257078289986 max memory_allocated 22562.10693359375 
[2025-02-28 13:26:42 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 2 loss:0.0024327693972736597 norm:0.004205039702355862 max memory_allocated 22562.10693359375 
[2025-02-28 13:27:13 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 3 loss:0.0022583012469112873 norm:0.0034695675130933523 max memory_allocated 22562.10693359375 
[2025-02-28 13:27:43 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 4 loss:0.002077077515423298 norm:0.002878628671169281 max memory_allocated 22562.10693359375 
[2025-02-28 13:28:14 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 5 loss:0.0019635932985693216 norm:0.0024868508335202932 max memory_allocated 22562.10693359375 
[2025-02-28 13:28:45 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 6 loss:0.0019143096869811416 norm:0.0022795742843300104 max memory_allocated 22562.10693359375 
[2025-02-28 13:29:16 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 7 loss:0.0019130553118884563 norm:0.0022166194394230843 max memory_allocated 22562.10693359375 
[2025-02-28 13:29:47 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 8 loss:0.001867997576482594 norm:0.0019863781053572893 max memory_allocated 22562.10693359375 
[2025-02-28 13:30:18 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 9 loss:0.001815847703255713 norm:0.0017747021047398448 max memory_allocated 22562.10693359375 
[2025-02-28 13:30:48 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 10 loss:0.0018100404413416982 norm:0.0016182789113372564 max memory_allocated 22562.10693359375 
[2025-02-28 13:31:19 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 11 loss:0.0017697017174214125 norm:0.0014662932371720672 max memory_allocated 22562.10693359375 
[2025-02-28 13:31:50 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 12 loss:0.0017669182270765305 norm:0.001307384460233152 max memory_allocated 22562.10693359375 
[2025-02-28 13:32:21 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 13 loss:0.0017600208520889282 norm:0.0012060310691595078 max memory_allocated 22562.10693359375 
[2025-02-28 13:32:52 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 14 loss:0.001723159453831613 norm:0.0010501445503905416 max memory_allocated 22562.10693359375 
[2025-02-28 13:33:22 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 15 loss:0.001738554099574685 norm:0.0010105816181749105 max memory_allocated 22562.10693359375 
[2025-02-28 13:33:53 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 16 loss:0.001728001399897039 norm:0.001020102296024561 max memory_allocated 22562.10693359375 
[2025-02-28 13:34:24 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 17 loss:0.0017591449432075024 norm:0.0010406236397102475 max memory_allocated 22562.10693359375 
[2025-02-28 13:34:55 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 18 loss:0.0016903514042496681 norm:0.0008006806601770222 max memory_allocated 22562.10693359375 
[2025-02-28 13:35:26 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 19 loss:0.0017155250534415245 norm:0.0008301441557705402 max memory_allocated 22562.10693359375 
[2025-02-28 13:35:34 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 1 ===
[2025-02-28 13:35:36 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 13:36:07 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 0 loss:0.030567951500415802 norm:0.02398398518562317 max memory_allocated 22562.27880859375 
[2025-02-28 13:36:38 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 1 loss:0.019079744815826416 norm:0.012422974221408367 max memory_allocated 22562.27880859375 
[2025-02-28 13:37:09 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 2 loss:0.014725416898727417 norm:0.01156050805002451 max memory_allocated 22562.27880859375 
[2025-02-28 13:37:40 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 3 loss:0.012370355427265167 norm:0.007285718806087971 max memory_allocated 22562.27880859375 
[2025-02-28 13:38:10 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 4 loss:0.011859656311571598 norm:0.007059440482407808 max memory_allocated 22562.27880859375 
[2025-02-28 13:38:41 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 5 loss:0.011557064950466156 norm:0.006864167749881744 max memory_allocated 22562.27880859375 
[2025-02-28 13:39:12 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 6 loss:0.011352606117725372 norm:0.006520802620798349 max memory_allocated 22562.27880859375 
[2025-02-28 13:39:43 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 7 loss:0.011520951986312866 norm:0.006827595643699169 max memory_allocated 22562.27880859375 
[2025-02-28 13:40:14 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 8 loss:0.011872449889779091 norm:0.007485050242394209 max memory_allocated 22562.27880859375 
[2025-02-28 13:40:44 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 9 loss:0.011435333639383316 norm:0.0072892531752586365 max memory_allocated 22562.27880859375 
[2025-02-28 13:41:15 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 10 loss:0.0119329197332263 norm:0.009029521606862545 max memory_allocated 22562.27880859375 
[2025-02-28 13:41:46 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 11 loss:0.011553938500583172 norm:0.009245387278497219 max memory_allocated 22562.27880859375 
[2025-02-28 13:42:17 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 12 loss:0.011658046394586563 norm:0.009071705862879753 max memory_allocated 22562.27880859375 
[2025-02-28 13:42:48 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 13 loss:0.012038521468639374 norm:0.01046887319535017 max memory_allocated 22562.27880859375 
[2025-02-28 13:43:18 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 14 loss:0.011640701442956924 norm:0.009867011569440365 max memory_allocated 22562.27880859375 
[2025-02-28 13:43:49 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 15 loss:0.012140966020524502 norm:0.009655547328293324 max memory_allocated 22562.27880859375 
[2025-02-28 13:44:20 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 16 loss:0.011748716235160828 norm:0.009465864859521389 max memory_allocated 22562.27880859375 
[2025-02-28 13:44:51 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 17 loss:0.011828337796032429 norm:0.009112130850553513 max memory_allocated 22562.27880859375 
[2025-02-28 13:45:22 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 18 loss:0.01222060713917017 norm:0.010066710412502289 max memory_allocated 22562.27880859375 
[2025-02-28 13:45:52 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 19 loss:0.01236223429441452 norm:0.010552075691521168 max memory_allocated 22562.27880859375 
[2025-02-28 13:46:01 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 2 ===
[2025-02-28 13:46:03 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 13:46:34 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 0 loss:0.024132508784532547 norm:0.0069329747930169106 max memory_allocated 22562.45068359375 
[2025-02-28 13:47:05 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 1 loss:0.017718449234962463 norm:0.005073003936558962 max memory_allocated 22562.45068359375 
[2025-02-28 13:47:35 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 2 loss:0.01524725928902626 norm:0.003787764348089695 max memory_allocated 22562.45068359375 
[2025-02-28 13:48:06 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 3 loss:0.014474055729806423 norm:0.003100807312875986 max memory_allocated 22562.45068359375 
[2025-02-28 13:48:37 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 4 loss:0.013900610618293285 norm:0.002596750622615218 max memory_allocated 22562.45068359375 
[2025-02-28 13:49:08 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 5 loss:0.013472928665578365 norm:0.002127384999766946 max memory_allocated 22562.45068359375 
[2025-02-28 13:49:39 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 6 loss:0.013235250487923622 norm:0.0018680560169741511 max memory_allocated 22562.45068359375 
[2025-02-28 13:50:10 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 7 loss:0.013135705143213272 norm:0.0016892109997570515 max memory_allocated 22562.45068359375 
[2025-02-28 13:50:40 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 8 loss:0.013073456473648548 norm:0.0014927380252629519 max memory_allocated 22562.45068359375 
[2025-02-28 13:51:11 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 9 loss:0.013025385327637196 norm:0.0013694817898795009 max memory_allocated 22562.45068359375 
[2025-02-28 13:51:42 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 10 loss:0.013009898364543915 norm:0.0013400563038885593 max memory_allocated 22562.45068359375 
[2025-02-28 13:52:13 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 11 loss:0.012983988970518112 norm:0.0012983231572434306 max memory_allocated 22562.45068359375 
[2025-02-28 13:52:44 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 12 loss:0.012991800904273987 norm:0.001273955451324582 max memory_allocated 22562.45068359375 
[2025-02-28 13:53:14 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 13 loss:0.012996343895792961 norm:0.001263224519789219 max memory_allocated 22562.45068359375 
[2025-02-28 13:53:45 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 14 loss:0.012974909506738186 norm:0.0011903282720595598 max memory_allocated 22562.45068359375 
[2025-02-28 13:54:16 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 15 loss:0.012989918701350689 norm:0.001143626170232892 max memory_allocated 22562.45068359375 
[2025-02-28 13:54:47 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 16 loss:0.012983318418264389 norm:0.0011233222903683782 max memory_allocated 22562.45068359375 
[2025-02-28 13:55:18 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 17 loss:0.012971987016499043 norm:0.0010854543652385473 max memory_allocated 22562.45068359375 
[2025-02-28 13:55:48 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 18 loss:0.012970897369086742 norm:0.001056393957696855 max memory_allocated 22562.45068359375 
[2025-02-28 13:56:19 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 19 loss:0.012994403950870037 norm:0.001041947747580707 max memory_allocated 22562.45068359375 
[2025-02-28 13:56:27 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 3 ===
[2025-02-28 13:57:01 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 0 loss:0.028011895716190338 norm:0.0012629259144887328 max memory_allocated 22562.50732421875 
[2025-02-28 13:57:31 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 1 loss:0.021646860986948013 norm:0.0005686872755177319 max memory_allocated 22562.50732421875 
[2025-02-28 13:58:02 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 2 loss:0.018620487302541733 norm:0.00032981636468321085 max memory_allocated 22562.50732421875 
[2025-02-28 13:58:33 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 3 loss:0.017705503851175308 norm:0.00019307785260025412 max memory_allocated 22562.50732421875 
[2025-02-28 13:59:03 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 4 loss:0.01707480102777481 norm:0.00016048797988332808 max memory_allocated 22562.50732421875 
[2025-02-28 13:59:34 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 5 loss:0.01676492765545845 norm:0.000147846934851259 max memory_allocated 22562.50732421875 
[2025-02-28 14:00:05 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 6 loss:0.016654353588819504 norm:0.00012981238251086324 max memory_allocated 22562.50732421875 
[2025-02-28 14:00:35 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 7 loss:0.016606969758868217 norm:0.0001123907495639287 max memory_allocated 22562.50732421875 
[2025-02-28 14:01:06 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 8 loss:0.01657196506857872 norm:0.00011342843936290592 max memory_allocated 22562.50732421875 
[2025-02-28 14:01:37 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 9 loss:0.01655380055308342 norm:0.00011693818669300526 max memory_allocated 22562.50732421875 
[2025-02-28 14:02:07 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 10 loss:0.01656382717192173 norm:0.0001211072740261443 max memory_allocated 22562.50732421875 
[2025-02-28 14:02:38 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 11 loss:0.016530847176909447 norm:0.00010873858991544694 max memory_allocated 22562.50732421875 
[2025-02-28 14:03:09 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 12 loss:0.016516147181391716 norm:0.00048429492744617164 max memory_allocated 22562.50732421875 
[2025-02-28 14:03:39 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 13 loss:0.016494393348693848 norm:0.00010722849401645362 max memory_allocated 22562.50732421875 
[2025-02-28 14:04:10 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 14 loss:0.016485530883073807 norm:0.00010048333206214011 max memory_allocated 22562.50732421875 
[2025-02-28 14:04:41 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 15 loss:0.016479086130857468 norm:0.00010444838699186221 max memory_allocated 22562.50732421875 
[2025-02-28 14:05:11 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 16 loss:0.01647322252392769 norm:0.00010242918506264687 max memory_allocated 22562.50732421875 
[2025-02-28 14:05:42 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 17 loss:0.01646607741713524 norm:0.00010062067303806543 max memory_allocated 22562.50732421875 
[2025-02-28 14:06:13 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 18 loss:0.01646130532026291 norm:0.00010212630877504125 max memory_allocated 22562.50732421875 
[2025-02-28 14:06:43 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 19 loss:0.016456685960292816 norm:9.975896682590246e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:06:52 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 4 ===
[2025-02-28 14:07:25 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 0 loss:0.027935102581977844 norm:0.0010077012702822685 max memory_allocated 22562.67919921875 
[2025-02-28 14:07:55 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 1 loss:0.023326728492975235 norm:0.00040878838626667857 max memory_allocated 22562.67919921875 
[2025-02-28 14:08:26 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 2 loss:0.021407008171081543 norm:0.00024629957624711096 max memory_allocated 22562.67919921875 
[2025-02-28 14:08:57 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 3 loss:0.020235000178217888 norm:0.0001739042199915275 max memory_allocated 22562.67919921875 
[2025-02-28 14:09:28 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 4 loss:0.01951972208917141 norm:0.00014431538875214756 max memory_allocated 22562.67919921875 
[2025-02-28 14:09:58 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 5 loss:0.01921319030225277 norm:0.00014787993859499693 max memory_allocated 22562.67919921875 
[2025-02-28 14:10:29 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 6 loss:0.01905004493892193 norm:0.00013072844012640417 max memory_allocated 22562.67919921875 
[2025-02-28 14:11:00 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 7 loss:0.019002709537744522 norm:0.0001338980655418709 max memory_allocated 22562.67919921875 
[2025-02-28 14:11:30 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 8 loss:0.019071094691753387 norm:0.00015526737843174487 max memory_allocated 22562.67919921875 
[2025-02-28 14:12:01 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 9 loss:0.019011827185750008 norm:0.00014470324094872922 max memory_allocated 22562.67919921875 
[2025-02-28 14:12:32 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 10 loss:0.018990524113178253 norm:0.00015072688984218985 max memory_allocated 22562.67919921875 
[2025-02-28 14:13:02 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 11 loss:0.018953245133161545 norm:0.00014525096048600972 max memory_allocated 22562.67919921875 
[2025-02-28 14:13:33 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 12 loss:0.01892806775867939 norm:0.00014827522682026029 max memory_allocated 22562.67919921875 
[2025-02-28 14:14:04 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 13 loss:0.01890270784497261 norm:0.00014499221288133413 max memory_allocated 22562.67919921875 
[2025-02-28 14:14:34 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 14 loss:0.018887009471654892 norm:0.00015419907867908478 max memory_allocated 22562.67919921875 
[2025-02-28 14:15:05 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 15 loss:0.018893849104642868 norm:0.0001552462053950876 max memory_allocated 22562.67919921875 
[2025-02-28 14:15:36 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 16 loss:0.018891073763370514 norm:0.00014926506264600903 max memory_allocated 22562.67919921875 
[2025-02-28 14:16:06 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 17 loss:0.018871314823627472 norm:0.00014648101932834834 max memory_allocated 22562.67919921875 
[2025-02-28 14:16:37 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 18 loss:0.01885763183236122 norm:0.00014625272888224572 max memory_allocated 22562.67919921875 
[2025-02-28 14:17:08 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 19 loss:0.01885046623647213 norm:0.00014793217997066677 max memory_allocated 22562.67919921875 
[2025-02-28 14:17:18 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 5 ===
[2025-02-28 14:17:58 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 0 loss:0.034809865057468414 norm:0.000977291725575924 max memory_allocated 22562.85107421875 
[2025-02-28 14:18:28 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 1 loss:0.02844052202999592 norm:0.00046336970990523696 max memory_allocated 22562.85107421875 
[2025-02-28 14:18:59 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 2 loss:0.0247080959379673 norm:0.0002690635737963021 max memory_allocated 22562.85107421875 
[2025-02-28 14:19:30 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 3 loss:0.023530418053269386 norm:0.00017136847600340843 max memory_allocated 22562.85107421875 
[2025-02-28 14:20:00 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 4 loss:0.022876637056469917 norm:0.00013750976359006017 max memory_allocated 22562.85107421875 
[2025-02-28 14:20:31 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 5 loss:0.022605758160352707 norm:0.00011643668403849006 max memory_allocated 22562.85107421875 
[2025-02-28 14:21:02 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 6 loss:0.022489052265882492 norm:0.00010104929970111698 max memory_allocated 22562.85107421875 
[2025-02-28 14:21:32 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 7 loss:0.02242148481309414 norm:0.00010328854841645807 max memory_allocated 22562.85107421875 
[2025-02-28 14:22:03 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 8 loss:0.02236122451722622 norm:9.98916948447004e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:22:34 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 9 loss:0.022318702191114426 norm:9.752731421031058e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:23:04 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 10 loss:0.022290900349617004 norm:9.67473752098158e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:23:35 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 11 loss:0.022276191040873528 norm:0.00010214794747298583 max memory_allocated 22562.85107421875 
[2025-02-28 14:24:06 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 12 loss:0.02225363627076149 norm:0.0001013353030430153 max memory_allocated 22562.85107421875 
[2025-02-28 14:24:37 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 13 loss:0.022234972566366196 norm:9.960302850231528e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:25:07 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 14 loss:0.02221854031085968 norm:0.0001034797533066012 max memory_allocated 22562.85107421875 
[2025-02-28 14:25:38 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 15 loss:0.022210143506526947 norm:0.00010044539521913975 max memory_allocated 22562.85107421875 
[2025-02-28 14:26:09 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 16 loss:0.02221948839724064 norm:0.00010826744983205572 max memory_allocated 22562.85107421875 
[2025-02-28 14:26:39 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 17 loss:0.02220742031931877 norm:9.883070015348494e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:27:10 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 18 loss:0.022202057763934135 norm:9.799435792956501e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:27:41 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 19 loss:0.02219559997320175 norm:9.521262109046802e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:27:49 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 6 ===
[2025-02-28 14:28:22 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 0 loss:0.03483850508928299 norm:0.001834897673688829 max memory_allocated 22563.02294921875 
[2025-02-28 14:28:53 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 1 loss:0.03024531528353691 norm:0.0008218567818403244 max memory_allocated 22563.02294921875 
[2025-02-28 14:29:23 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 2 loss:0.027783630415797234 norm:0.00043842775630764663 max memory_allocated 22563.02294921875 
[2025-02-28 14:29:54 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 3 loss:0.02612251229584217 norm:0.0003014395188074559 max memory_allocated 22563.02294921875 
[2025-02-28 14:30:25 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 4 loss:0.025311565026640892 norm:0.00022690879995934665 max memory_allocated 22563.02294921875 
[2025-02-28 14:30:55 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 5 loss:0.024949591606855392 norm:0.00019134143076371402 max memory_allocated 22563.02294921875 
[2025-02-28 14:31:26 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 6 loss:0.024816635996103287 norm:0.00016454960859846324 max memory_allocated 22563.02294921875 
[2025-02-28 14:31:57 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 7 loss:0.024777229875326157 norm:0.0001534916809760034 max memory_allocated 22563.02294921875 
[2025-02-28 14:32:27 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 8 loss:0.024722261354327202 norm:0.00015174242435023189 max memory_allocated 22563.02294921875 
[2025-02-28 14:32:58 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 9 loss:0.024699848145246506 norm:0.0001357709988951683 max memory_allocated 22563.02294921875 
[2025-02-28 14:33:29 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 10 loss:0.024675289168953896 norm:0.00013712966756429523 max memory_allocated 22563.02294921875 
[2025-02-28 14:33:59 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 11 loss:0.024648819118738174 norm:0.00013755151303485036 max memory_allocated 22563.02294921875 
[2025-02-28 14:34:30 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 12 loss:0.024641821160912514 norm:0.00013465828669723123 max memory_allocated 22563.02294921875 
[2025-02-28 14:35:01 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 13 loss:0.02464897930622101 norm:0.0001327221398241818 max memory_allocated 22563.02294921875 
[2025-02-28 14:35:31 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 14 loss:0.024630462750792503 norm:0.00012802350101992488 max memory_allocated 22563.02294921875 
[2025-02-28 14:36:02 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 15 loss:0.0246246550232172 norm:0.00013023836072534323 max memory_allocated 22563.02294921875 
[2025-02-28 14:36:33 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 16 loss:0.024625549092888832 norm:0.00013147438585292548 max memory_allocated 22563.02294921875 
[2025-02-28 14:37:03 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 17 loss:0.024604016914963722 norm:0.00012740821694023907 max memory_allocated 22563.02294921875 
[2025-02-28 14:37:34 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 18 loss:0.024604234844446182 norm:0.00012858971604146063 max memory_allocated 22563.02294921875 
[2025-02-28 14:38:05 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 19 loss:0.02460220642387867 norm:0.00012743698607664555 max memory_allocated 22563.02294921875 
[2025-02-28 14:38:13 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 7 ===
[2025-02-28 14:38:46 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 0 loss:0.040882378816604614 norm:0.002752686385065317 max memory_allocated 22563.19482421875 
[2025-02-28 14:39:17 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 1 loss:0.0344400517642498 norm:0.0010545557597652078 max memory_allocated 22563.19482421875 
[2025-02-28 14:39:47 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 2 loss:0.031377919018268585 norm:0.0005412460886873305 max memory_allocated 22563.19482421875 
[2025-02-28 14:40:18 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 3 loss:0.029462214559316635 norm:0.00033651915146037936 max memory_allocated 22563.19482421875 
[2025-02-28 14:40:49 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 4 loss:0.028591211885213852 norm:0.00025939138140529394 max memory_allocated 22563.19482421875 
[2025-02-28 14:41:19 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 5 loss:0.028215518221259117 norm:0.00022447605442721397 max memory_allocated 22563.19482421875 
[2025-02-28 14:41:50 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 6 loss:0.028055327013134956 norm:0.0006131676491349936 max memory_allocated 22563.19482421875 
[2025-02-28 14:42:21 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 7 loss:0.02796732448041439 norm:0.00018198529141955078 max memory_allocated 22563.19482421875 
[2025-02-28 14:42:52 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 8 loss:0.027939554303884506 norm:0.0001761874882504344 max memory_allocated 22563.19482421875 
[2025-02-28 14:43:22 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 9 loss:0.027935223653912544 norm:0.00016448687529191375 max memory_allocated 22563.19482421875 
[2025-02-28 14:43:53 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 10 loss:0.027901606634259224 norm:0.000160578332724981 max memory_allocated 22563.19482421875 
[2025-02-28 14:44:24 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 11 loss:0.02785409241914749 norm:0.00014233548427000642 max memory_allocated 22563.19482421875 
[2025-02-28 14:44:54 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 12 loss:0.027828874066472054 norm:0.00013993607717566192 max memory_allocated 22563.19482421875 
[2025-02-28 14:45:25 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 13 loss:0.02779509872198105 norm:0.00015138568414840847 max memory_allocated 22563.19482421875 
[2025-02-28 14:45:56 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 14 loss:0.02778608538210392 norm:0.00014084727445151657 max memory_allocated 22563.19482421875 
[2025-02-28 14:46:26 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 15 loss:0.027772007510066032 norm:0.00012971967225894332 max memory_allocated 22563.19482421875 
[2025-02-28 14:46:57 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 16 loss:0.027769578620791435 norm:0.00012889018398709595 max memory_allocated 22563.19482421875 
[2025-02-28 14:47:28 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 17 loss:0.027772946283221245 norm:0.00012652248551603407 max memory_allocated 22563.19482421875 
[2025-02-28 14:47:58 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 18 loss:0.02776002138853073 norm:0.00012327627337072045 max memory_allocated 22563.19482421875 
[2025-02-28 14:48:29 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 19 loss:0.027742760255932808 norm:0.00012318685185164213 max memory_allocated 22563.19482421875 
[2025-02-28 14:48:37 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 8 ===
[2025-02-28 14:49:11 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 0 loss:0.0411289744079113 norm:0.0009474807884544134 max memory_allocated 22563.36669921875 
[2025-02-28 14:49:41 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 1 loss:0.03644333407282829 norm:0.00049571756972 max memory_allocated 22563.36669921875 
[2025-02-28 14:50:12 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 2 loss:0.033947404474020004 norm:0.00031356493127532303 max memory_allocated 22563.36669921875 
[2025-02-28 14:50:43 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 3 loss:0.032486945390701294 norm:0.00023943981796037406 max memory_allocated 22563.36669921875 
[2025-02-28 14:51:13 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 4 loss:0.031731873750686646 norm:0.00020571573986671865 max memory_allocated 22563.36669921875 
[2025-02-28 14:51:44 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 5 loss:0.03140498325228691 norm:0.00020264892373234034 max memory_allocated 22563.36669921875 
[2025-02-28 14:52:15 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 6 loss:0.03122399002313614 norm:0.0001687846815912053 max memory_allocated 22563.36669921875 
[2025-02-28 14:52:45 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 7 loss:0.0310831181704998 norm:0.00016496580792590976 max memory_allocated 22563.36669921875 
[2025-02-28 14:53:16 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 8 loss:0.03102620132267475 norm:0.00017463239782955498 max memory_allocated 22563.36669921875 
[2025-02-28 14:53:47 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 9 loss:0.03100588172674179 norm:0.0001839266624301672 max memory_allocated 22563.36669921875 
[2025-02-28 14:54:18 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 10 loss:0.030949130654335022 norm:0.0001761346065904945 max memory_allocated 22563.36669921875 
[2025-02-28 14:54:48 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 11 loss:0.0308701004832983 norm:0.0001630904880585149 max memory_allocated 22563.36669921875 
[2025-02-28 14:55:19 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 12 loss:0.03085950016975403 norm:0.0001671947247814387 max memory_allocated 22563.36669921875 
[2025-02-28 14:55:50 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 13 loss:0.030852530151605606 norm:0.00017281269538216293 max memory_allocated 22563.36669921875 
[2025-02-28 14:56:20 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 14 loss:0.030863074585795403 norm:0.00016737004625611007 max memory_allocated 22563.36669921875 
[2025-02-28 14:56:51 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 15 loss:0.030841581523418427 norm:0.00016616527864243835 max memory_allocated 22563.36669921875 
[2025-02-28 14:57:22 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 16 loss:0.030841713771224022 norm:0.0001599186798557639 max memory_allocated 22563.36669921875 
[2025-02-28 14:57:52 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 17 loss:0.03083156980574131 norm:0.00017287753871642053 max memory_allocated 22563.36669921875 
[2025-02-28 14:58:23 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 18 loss:0.03080163151025772 norm:0.00016280166164506227 max memory_allocated 22563.36669921875 
[2025-02-28 14:58:54 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 19 loss:0.030787713825702667 norm:0.00016302957374136895 max memory_allocated 22563.36669921875 
[2025-02-28 14:59:02 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 9 ===
[2025-02-28 14:59:35 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 0 loss:0.042608778923749924 norm:0.0008530630730092525 max memory_allocated 22563.53857421875 
[2025-02-28 15:00:06 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 1 loss:0.03810769319534302 norm:0.000377384276362136 max memory_allocated 22563.53857421875 
[2025-02-28 15:00:37 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 2 loss:0.03583505004644394 norm:0.00023778993636369705 max memory_allocated 22563.53857421875 
[2025-02-28 15:01:07 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 3 loss:0.0345444418489933 norm:0.00018551315588410944 max memory_allocated 22563.53857421875 
[2025-02-28 15:01:38 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 4 loss:0.033841390162706375 norm:0.00017466535791754723 max memory_allocated 22563.53857421875 
[2025-02-28 15:02:09 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 5 loss:0.03351825103163719 norm:0.00016564260295126587 max memory_allocated 22563.53857421875 
[2025-02-28 15:02:40 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 6 loss:0.03336488455533981 norm:0.00014587098849005997 max memory_allocated 22563.53857421875 
[2025-02-28 15:03:10 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 7 loss:0.033269189298152924 norm:0.00013887691602576524 max memory_allocated 22563.53857421875 
[2025-02-28 15:03:41 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 8 loss:0.03321482241153717 norm:0.00013263725850265473 max memory_allocated 22563.53857421875 
[2025-02-28 15:04:12 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 9 loss:0.03318708389997482 norm:0.00014303516945801675 max memory_allocated 22563.53857421875 
[2025-02-28 15:04:42 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 10 loss:0.03312249854207039 norm:0.00012806570157408714 max memory_allocated 22563.53857421875 
[2025-02-28 15:05:13 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 11 loss:0.0330887995660305 norm:0.00012730459275189787 max memory_allocated 22563.53857421875 
[2025-02-28 15:05:44 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 12 loss:0.03305928781628609 norm:0.0001272233494091779 max memory_allocated 22563.53857421875 
[2025-02-28 15:06:14 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 13 loss:0.0330902561545372 norm:0.00013033686263952404 max memory_allocated 22563.53857421875 
[2025-02-28 15:06:45 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 14 loss:0.03310751914978027 norm:0.00012338775559328496 max memory_allocated 22563.53857421875 
[2025-02-28 15:07:16 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 15 loss:0.0330846793949604 norm:0.00012082862667739391 max memory_allocated 22563.53857421875 
[2025-02-28 15:07:47 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 16 loss:0.033073313534259796 norm:0.00012435980897862464 max memory_allocated 22563.53857421875 
[2025-02-28 15:08:17 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 17 loss:0.03307674825191498 norm:0.00011853168689412996 max memory_allocated 22563.53857421875 
[2025-02-28 15:08:48 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 18 loss:0.033060409128665924 norm:0.00012353109195828438 max memory_allocated 22563.53857421875 
[2025-02-28 15:09:19 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 19 loss:0.033047180622816086 norm:0.00012277337373234332 max memory_allocated 22563.53857421875 
[2025-02-28 15:09:27 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 10 ===
[2025-02-28 15:10:00 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 0 loss:0.043077267706394196 norm:0.0005250353715382516 max memory_allocated 22563.71044921875 
[2025-02-28 15:10:31 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 1 loss:0.03983362764120102 norm:0.00027934907120652497 max memory_allocated 22563.71044921875 
[2025-02-28 15:11:01 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 2 loss:0.037718214094638824 norm:0.00018819271645043045 max memory_allocated 22563.71044921875 
[2025-02-28 15:11:32 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 3 loss:0.036531586199998856 norm:0.00016932559083215892 max memory_allocated 22563.71044921875 
[2025-02-28 15:12:03 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 4 loss:0.03587605804204941 norm:0.00015063256432767957 max memory_allocated 22563.71044921875 
[2025-02-28 15:12:33 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 5 loss:0.035522282123565674 norm:0.0001443470100639388 max memory_allocated 22563.71044921875 
[2025-02-28 15:13:04 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 6 loss:0.035379406064748764 norm:0.000143895682413131 max memory_allocated 22563.71044921875 
[2025-02-28 15:13:35 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 7 loss:0.03524637594819069 norm:0.0001310296356678009 max memory_allocated 22563.71044921875 
[2025-02-28 15:14:06 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 8 loss:0.03518233075737953 norm:0.00013169382873456925 max memory_allocated 22563.71044921875 
[2025-02-28 15:14:36 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 9 loss:0.035136859863996506 norm:0.000123747275210917 max memory_allocated 22563.71044921875 
[2025-02-28 15:15:07 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 10 loss:0.035120002925395966 norm:0.00013032974675297737 max memory_allocated 22563.71044921875 
[2025-02-28 15:15:38 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 11 loss:0.03510887920856476 norm:0.00012370453623589128 max memory_allocated 22563.71044921875 
[2025-02-28 15:16:08 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 12 loss:0.03508543223142624 norm:0.00011473234189907089 max memory_allocated 22563.71044921875 
[2025-02-28 15:16:39 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 13 loss:0.03514822572469711 norm:0.0001258357660844922 max memory_allocated 22563.71044921875 
[2025-02-28 15:17:10 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 14 loss:0.03514987975358963 norm:0.00011757499305531383 max memory_allocated 22563.71044921875 
[2025-02-28 15:17:40 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 15 loss:0.03512591868638992 norm:0.00011917286610696465 max memory_allocated 22563.71044921875 
[2025-02-28 15:18:11 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 16 loss:0.03512299433350563 norm:0.00012439316196832806 max memory_allocated 22563.71044921875 
[2025-02-28 15:18:42 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 17 loss:0.03511081635951996 norm:0.0001247977779712528 max memory_allocated 22563.71044921875 
[2025-02-28 15:19:13 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 18 loss:0.03510187566280365 norm:0.00012376518861856312 max memory_allocated 22563.71044921875 
[2025-02-28 15:19:43 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 19 loss:0.035092178732156754 norm:0.00013308992492966354 max memory_allocated 22563.71044921875 
[2025-02-28 15:19:51 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 11 ===
[2025-02-28 15:20:25 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 0 loss:0.0457099974155426 norm:0.0006811405764892697 max memory_allocated 22563.88232421875 
[2025-02-28 15:20:55 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 1 loss:0.041835080832242966 norm:0.00035272035165689886 max memory_allocated 22563.88232421875 
[2025-02-28 15:21:26 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 2 loss:0.03958474472165108 norm:0.00024067223421297967 max memory_allocated 22563.88232421875 
[2025-02-28 15:21:57 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 3 loss:0.03830467909574509 norm:0.0002039421524386853 max memory_allocated 22563.88232421875 
[2025-02-28 15:22:28 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 4 loss:0.03760778531432152 norm:0.0001785690983524546 max memory_allocated 22563.88232421875 
[2025-02-28 15:22:58 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 5 loss:0.037197813391685486 norm:0.00016334855172317475 max memory_allocated 22563.88232421875 
[2025-02-28 15:23:29 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 6 loss:0.037026744335889816 norm:0.00016021516057662666 max memory_allocated 22563.88232421875 
[2025-02-28 15:24:00 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 7 loss:0.03691231459379196 norm:0.00015089966473169625 max memory_allocated 22563.88232421875 
[2025-02-28 15:24:30 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 8 loss:0.03685670718550682 norm:0.0001444644876755774 max memory_allocated 22563.88232421875 
[2025-02-28 15:25:01 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 9 loss:0.03678756207227707 norm:0.00014287196972873062 max memory_allocated 22563.88232421875 
[2025-02-28 15:25:32 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 10 loss:0.036730073392391205 norm:0.00014033372281119227 max memory_allocated 22563.88232421875 
[2025-02-28 15:26:02 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 11 loss:0.03669773414731026 norm:0.0001339926675427705 max memory_allocated 22563.88232421875 
[2025-02-28 15:26:33 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 12 loss:0.036634162068367004 norm:0.0001228963228641078 max memory_allocated 22563.88232421875 
[2025-02-28 15:27:04 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 13 loss:0.03662693127989769 norm:0.00012287254503462464 max memory_allocated 22563.88232421875 
[2025-02-28 15:27:34 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 14 loss:0.03663308918476105 norm:0.00012402315041981637 max memory_allocated 22563.88232421875 
[2025-02-28 15:28:05 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 15 loss:0.03661875054240227 norm:0.00011589296627789736 max memory_allocated 22563.88232421875 
[2025-02-28 15:28:36 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 16 loss:0.03662000596523285 norm:0.00012069931835867465 max memory_allocated 22563.88232421875 
[2025-02-28 15:29:07 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 17 loss:0.03661429136991501 norm:0.00012017072003800422 max memory_allocated 22563.88232421875 
[2025-02-28 15:29:37 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 18 loss:0.036595750600099564 norm:0.00011712531704688445 max memory_allocated 22563.88232421875 
[2025-02-28 15:30:08 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 19 loss:0.03658398240804672 norm:0.00011968878243351355 max memory_allocated 22563.88232421875 
[2025-02-28 15:30:18 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 12 ===
[2025-02-28 15:30:51 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 0 loss:0.045956771820783615 norm:0.0005705447401851416 max memory_allocated 22564.05419921875 
[2025-02-28 15:31:22 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 1 loss:0.04269174486398697 norm:0.0003066629287786782 max memory_allocated 22564.05419921875 
[2025-02-28 15:31:53 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 2 loss:0.04066223278641701 norm:0.00021205384109634906 max memory_allocated 22564.05419921875 
[2025-02-28 15:32:24 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 3 loss:0.039468731731176376 norm:0.00017604298773221672 max memory_allocated 22564.05419921875 
[2025-02-28 15:32:54 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 4 loss:0.0388747975230217 norm:0.00015841156709939241 max memory_allocated 22564.05419921875 
[2025-02-28 15:33:25 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 5 loss:0.03848422318696976 norm:0.00014290690887719393 max memory_allocated 22564.05419921875 
[2025-02-28 15:33:56 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 6 loss:0.0383150652050972 norm:0.0001329873048234731 max memory_allocated 22564.05419921875 
[2025-02-28 15:34:26 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 7 loss:0.038238056004047394 norm:0.0001278594572795555 max memory_allocated 22564.05419921875 
[2025-02-28 15:34:57 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 8 loss:0.038169946521520615 norm:0.0001256099931197241 max memory_allocated 22564.05419921875 
[2025-02-28 15:35:28 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 9 loss:0.03812145069241524 norm:0.00013043300714343786 max memory_allocated 22564.05419921875 
[2025-02-28 15:35:58 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 10 loss:0.03812054172158241 norm:0.0001382566406391561 max memory_allocated 22564.05419921875 
[2025-02-28 15:36:29 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 11 loss:0.03808024525642395 norm:0.0001202743878820911 max memory_allocated 22564.05419921875 
[2025-02-28 15:37:00 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 12 loss:0.03802565112709999 norm:0.00011215083213755861 max memory_allocated 22564.05419921875 
[2025-02-28 15:37:31 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 13 loss:0.03798574581742287 norm:0.00011862959945574403 max memory_allocated 22564.05419921875 
[2025-02-28 15:38:01 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 14 loss:0.037968847900629044 norm:0.00011388341954443604 max memory_allocated 22564.05419921875 
[2025-02-28 15:38:32 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 15 loss:0.03796638920903206 norm:0.00011927072773687541 max memory_allocated 22564.05419921875 
[2025-02-28 15:39:03 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 16 loss:0.03796837851405144 norm:0.0001214767326018773 max memory_allocated 22564.05419921875 
[2025-02-28 15:39:33 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 17 loss:0.037958040833473206 norm:0.00011663195618893951 max memory_allocated 22564.05419921875 
[2025-02-28 15:40:04 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 18 loss:0.03796394169330597 norm:0.00011849054135382175 max memory_allocated 22564.05419921875 
[2025-02-28 15:40:35 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 19 loss:0.037959881126880646 norm:0.00011428412835812196 max memory_allocated 22564.05419921875 
[2025-02-28 15:40:43 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 13 ===
[2025-02-28 15:41:16 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 0 loss:0.04502620920538902 norm:0.0007220706902444363 max memory_allocated 22564.22607421875 
[2025-02-28 15:41:47 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 1 loss:0.042153701186180115 norm:0.0003620533098001033 max memory_allocated 22564.22607421875 
[2025-02-28 15:42:18 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 2 loss:0.04023749381303787 norm:0.00023626000620424747 max memory_allocated 22564.22607421875 
[2025-02-28 15:42:48 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 3 loss:0.039054106920957565 norm:0.00018030194041784853 max memory_allocated 22564.22607421875 
[2025-02-28 15:43:19 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 4 loss:0.03846992924809456 norm:0.00015301104576792568 max memory_allocated 22564.22607421875 
[2025-02-28 15:43:50 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 5 loss:0.038137037307024 norm:0.00013790342200081795 max memory_allocated 22564.22607421875 
[2025-02-28 15:44:20 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 6 loss:0.03796153515577316 norm:0.00012909754877910018 max memory_allocated 22564.22607421875 
[2025-02-28 15:44:51 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 7 loss:0.037845246493816376 norm:0.0001224370498675853 max memory_allocated 22564.22607421875 
[2025-02-28 15:45:22 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 8 loss:0.03780434653162956 norm:0.00012352445628494024 max memory_allocated 22564.22607421875 
[2025-02-28 15:45:52 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 9 loss:0.03773976117372513 norm:0.00011453116167103872 max memory_allocated 22564.22607421875 
[2025-02-28 15:46:23 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 10 loss:0.03770510107278824 norm:0.00011179873399669304 max memory_allocated 22564.22607421875 
[2025-02-28 15:46:54 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 11 loss:0.037697866559028625 norm:0.00011694076238200068 max memory_allocated 22564.22607421875 
[2025-02-28 15:47:25 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 12 loss:0.037679143249988556 norm:0.00011443372932262719 max memory_allocated 22564.22607421875 
[2025-02-28 15:47:55 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 13 loss:0.037660397589206696 norm:0.00011378830822650343 max memory_allocated 22564.22607421875 
[2025-02-28 15:48:26 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 14 loss:0.03763791173696518 norm:0.00011724334763130173 max memory_allocated 22564.22607421875 
[2025-02-28 15:48:57 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 15 loss:0.03761385381221771 norm:0.00011194135004188865 max memory_allocated 22564.22607421875 
[2025-02-28 15:49:27 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 16 loss:0.037608593702316284 norm:0.00011226704373257235 max memory_allocated 22564.22607421875 
[2025-02-28 15:49:58 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 17 loss:0.03762153908610344 norm:0.00012274476466700435 max memory_allocated 22564.22607421875 
[2025-02-28 15:50:29 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 18 loss:0.03761574253439903 norm:0.00011447387805674225 max memory_allocated 22564.22607421875 
[2025-02-28 15:50:59 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 19 loss:0.03760109096765518 norm:0.00011974002700299025 max memory_allocated 22564.22607421875 
[2025-02-28 15:51:08 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 14 ===
[2025-02-28 15:51:41 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 0 loss:0.04531863331794739 norm:0.0005364110693335533 max memory_allocated 22564.39794921875 
[2025-02-28 15:52:12 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 1 loss:0.04260668903589249 norm:0.00028997898334637284 max memory_allocated 22564.39794921875 
[2025-02-28 15:52:42 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 2 loss:0.04083156958222389 norm:0.0002026724541792646 max memory_allocated 22564.39794921875 
[2025-02-28 15:53:13 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 3 loss:0.039716076105833054 norm:0.00016213460185099393 max memory_allocated 22564.39794921875 
[2025-02-28 15:53:44 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 4 loss:0.03912549465894699 norm:0.00013976305490359664 max memory_allocated 22564.39794921875 
[2025-02-28 15:54:15 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 5 loss:0.03878624364733696 norm:0.00012629092088900506 max memory_allocated 22564.39794921875 
[2025-02-28 15:54:45 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 6 loss:0.038594361394643784 norm:0.00011343823280185461 max memory_allocated 22564.39794921875 
[2025-02-28 15:55:16 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 7 loss:0.03850061073899269 norm:0.00011209698277525604 max memory_allocated 22564.39794921875 
[2025-02-28 15:55:47 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 8 loss:0.03843286260962486 norm:0.00011628884385572746 max memory_allocated 22564.39794921875 
[2025-02-28 15:56:17 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 9 loss:0.03836377337574959 norm:0.00010789422958623618 max memory_allocated 22564.39794921875 
[2025-02-28 15:56:48 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 10 loss:0.03830743208527565 norm:9.972591942641884e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:57:19 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 11 loss:0.038283806294202805 norm:9.249598224414513e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:57:50 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 12 loss:0.03827735036611557 norm:9.81749253696762e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:58:20 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 13 loss:0.03825562074780464 norm:9.414624946657568e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:58:51 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 14 loss:0.03824029490351677 norm:9.382347343489528e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:59:22 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 15 loss:0.038222163915634155 norm:9.132914419751614e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:59:52 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 16 loss:0.03822064772248268 norm:8.555518434150144e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:00:23 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 17 loss:0.03822037950158119 norm:8.554707164876163e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:00:54 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 18 loss:0.03821069002151489 norm:8.449420420220122e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:01:25 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 19 loss:0.03820169344544411 norm:8.369905117433518e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:01:33 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 15 ===
[2025-02-28 16:02:06 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 0 loss:0.05267675220966339 norm:0.0008758138283155859 max memory_allocated 22564.56982421875 
[2025-02-28 16:02:37 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 1 loss:0.04639148339629173 norm:0.00038851305725984275 max memory_allocated 22564.56982421875 
[2025-02-28 16:03:07 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 2 loss:0.042282212525606155 norm:0.0002432582841720432 max memory_allocated 22564.56982421875 
[2025-02-28 16:03:38 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 3 loss:0.041162751615047455 norm:0.00017379109340254217 max memory_allocated 22564.56982421875 
[2025-02-28 16:04:09 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 4 loss:0.040478918701410294 norm:0.000145422964124009 max memory_allocated 22564.56982421875 
[2025-02-28 16:04:40 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 5 loss:0.04008268937468529 norm:0.00013380958989728242 max memory_allocated 22564.56982421875 
[2025-02-28 16:05:10 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 6 loss:0.039873670786619186 norm:0.00012260201037861407 max memory_allocated 22564.56982421875 
[2025-02-28 16:05:41 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 7 loss:0.039732739329338074 norm:0.0001111961028072983 max memory_allocated 22564.56982421875 
[2025-02-28 16:06:12 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 8 loss:0.03962686285376549 norm:0.00010379979357821867 max memory_allocated 22564.56982421875 
[2025-02-28 16:06:42 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 9 loss:0.039544668048620224 norm:0.00010192408080911264 max memory_allocated 22564.56982421875 
[2025-02-28 16:07:13 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 10 loss:0.03946056589484215 norm:9.786157897906378e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:07:44 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 11 loss:0.03939163312315941 norm:9.671678708400577e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:08:14 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 12 loss:0.03933868929743767 norm:9.237565973307937e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:08:45 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 13 loss:0.03928380459547043 norm:8.531865751137957e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:09:16 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 14 loss:0.039241380989551544 norm:8.602038724347949e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:09:46 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 15 loss:0.03920837491750717 norm:8.566398173570633e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:10:17 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 16 loss:0.039181530475616455 norm:8.223805343732238e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:10:48 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 17 loss:0.0391545295715332 norm:7.783624459989369e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:11:18 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 18 loss:0.03914330527186394 norm:8.169349166564643e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:11:49 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 19 loss:0.03913283348083496 norm:8.036896906560287e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:11:57 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 16 ===
[2025-02-28 16:12:31 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 0 loss:0.05436836555600166 norm:0.0010874152649194002 max memory_allocated 22564.74169921875 
[2025-02-28 16:13:01 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 1 loss:0.047696419060230255 norm:0.00043869687942788005 max memory_allocated 22564.74169921875 
[2025-02-28 16:13:32 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 2 loss:0.04329372197389603 norm:0.00024982308968901634 max memory_allocated 22564.74169921875 
[2025-02-28 16:14:03 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 3 loss:0.04217983037233353 norm:0.00018915347754955292 max memory_allocated 22564.74169921875 
[2025-02-28 16:14:33 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 4 loss:0.041452668607234955 norm:0.00016013794811442494 max memory_allocated 22564.74169921875 
[2025-02-28 16:15:04 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 5 loss:0.041002899408340454 norm:0.0001413091376889497 max memory_allocated 22564.74169921875 
[2025-02-28 16:15:35 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 6 loss:0.040776971727609634 norm:0.0001311663509113714 max memory_allocated 22564.74169921875 
[2025-02-28 16:16:05 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 7 loss:0.04061653092503548 norm:0.00012153158604633063 max memory_allocated 22564.74169921875 
[2025-02-28 16:16:36 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 8 loss:0.04050023853778839 norm:0.00011565269232960418 max memory_allocated 22564.74169921875 
[2025-02-28 16:17:07 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 9 loss:0.04039556905627251 norm:0.00010730822395998985 max memory_allocated 22564.74169921875 
[2025-02-28 16:17:37 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 10 loss:0.04030650109052658 norm:0.00010292154911439866 max memory_allocated 22564.74169921875 
[2025-02-28 16:18:08 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 11 loss:0.04022814705967903 norm:9.668667917139828e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:18:39 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 12 loss:0.04017180576920509 norm:9.697974746813998e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:19:09 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 13 loss:0.0401136539876461 norm:9.543837222736329e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:19:40 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 14 loss:0.04006101191043854 norm:9.359304385725409e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:20:11 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 15 loss:0.04001786559820175 norm:8.515948138665408e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:20:41 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 16 loss:0.039987243711948395 norm:8.344433445017785e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:21:12 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 17 loss:0.03994724527001381 norm:8.738433825783432e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:21:43 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 18 loss:0.03991885855793953 norm:8.367016562260687e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:22:14 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 19 loss:0.039892274886369705 norm:7.650023326277733e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:22:26 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 17 ===
[2025-02-28 16:23:01 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 0 loss:0.04502519592642784 norm:0.0005105719901621342 max memory_allocated 22564.91357421875 
[2025-02-28 16:23:31 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 1 loss:0.043034013360738754 norm:0.00019071043061558157 max memory_allocated 22564.91357421875 
[2025-02-28 16:24:02 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 2 loss:0.04192080721259117 norm:0.00013859837781637907 max memory_allocated 22564.91357421875 
[2025-02-28 16:24:33 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 3 loss:0.04095439612865448 norm:0.00012744030391331762 max memory_allocated 22564.91357421875 
[2025-02-28 16:25:03 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 4 loss:0.040329255163669586 norm:0.00010959814972011372 max memory_allocated 22564.91357421875 
[2025-02-28 16:25:34 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 5 loss:0.03999955952167511 norm:8.993064693640918e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:26:05 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 6 loss:0.03988500311970711 norm:9.087350917980075e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:26:36 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 7 loss:0.039773646742105484 norm:9.015926480060443e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:27:06 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 8 loss:0.03970404714345932 norm:8.338219777215272e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:27:37 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 9 loss:0.039681218564510345 norm:8.108730253297836e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:28:08 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 10 loss:0.03964950889348984 norm:7.326472405111417e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:28:38 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 11 loss:0.03960953652858734 norm:7.460212509613484e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:29:09 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 12 loss:0.03958316147327423 norm:7.026563253020868e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:29:40 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 13 loss:0.03954869136214256 norm:6.899111031088978e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:30:11 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 14 loss:0.03955445811152458 norm:7.184644346125424e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:30:41 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 15 loss:0.039548080414533615 norm:6.764879071852192e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:31:12 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 16 loss:0.03953954204916954 norm:6.698003562632948e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:31:43 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 17 loss:0.039542056620121 norm:7.093721069395542e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:32:13 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 18 loss:0.03954029828310013 norm:7.294830720638856e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:32:44 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 19 loss:0.039541952311992645 norm:7.00476230122149e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:32:52 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 18 ===
[2025-02-28 16:33:25 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 0 loss:0.04948705434799194 norm:0.0015112621476873755 max memory_allocated 22565.08544921875 
[2025-02-28 16:33:56 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 1 loss:0.046709924936294556 norm:0.0006289617740549147 max memory_allocated 22565.08544921875 
[2025-02-28 16:34:27 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 2 loss:0.04510561376810074 norm:0.0003599954361561686 max memory_allocated 22565.08544921875 
[2025-02-28 16:34:58 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 3 loss:0.04392602667212486 norm:0.00024754105834290385 max memory_allocated 22565.08544921875 
[2025-02-28 16:35:28 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 4 loss:0.04314003884792328 norm:0.00020521126862149686 max memory_allocated 22565.08544921875 
[2025-02-28 16:35:59 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 5 loss:0.04273369163274765 norm:0.00017299936735071242 max memory_allocated 22565.08544921875 
[2025-02-28 16:36:30 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 6 loss:0.04256192222237587 norm:0.00015686529513914138 max memory_allocated 22565.08544921875 
[2025-02-28 16:37:00 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 7 loss:0.04246453195810318 norm:0.00014181069855112582 max memory_allocated 22565.08544921875 
[2025-02-28 16:37:31 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 8 loss:0.04238839074969292 norm:0.0001241708523593843 max memory_allocated 22565.08544921875 
[2025-02-28 16:38:02 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 9 loss:0.042339399456977844 norm:0.00011961173731833696 max memory_allocated 22565.08544921875 
[2025-02-28 16:38:32 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 10 loss:0.04228513687849045 norm:0.00011162472947034985 max memory_allocated 22565.08544921875 
[2025-02-28 16:39:03 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 11 loss:0.04225544258952141 norm:0.00010679176193661988 max memory_allocated 22565.08544921875 
[2025-02-28 16:39:34 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 12 loss:0.042224541306495667 norm:0.00010079010826302692 max memory_allocated 22565.08544921875 
[2025-02-28 16:40:05 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 13 loss:0.04220135509967804 norm:9.570235852152109e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:40:35 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 14 loss:0.0421646311879158 norm:9.464612958254293e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:41:06 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 15 loss:0.04214147850871086 norm:9.184944065054879e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:41:37 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 16 loss:0.04211561009287834 norm:8.433528273599222e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:42:07 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 17 loss:0.04209627956151962 norm:7.895530143287033e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:42:38 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 18 loss:0.04208570718765259 norm:7.978044595802203e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:43:09 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 19 loss:0.04206746816635132 norm:7.874394941609353e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:43:18 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 19 ===
[2025-02-28 16:43:52 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 0 loss:0.051341064274311066 norm:0.0006965911015868187 max memory_allocated 22565.25732421875 
[2025-02-28 16:44:22 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 1 loss:0.049098268151283264 norm:0.00032176022068597376 max memory_allocated 22565.25732421875 
[2025-02-28 16:44:53 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 2 loss:0.047860510647296906 norm:0.00020722496265079826 max memory_allocated 22565.25732421875 
[2025-02-28 16:45:24 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 3 loss:0.04690001159906387 norm:0.0001483288360759616 max memory_allocated 22565.25732421875 
[2025-02-28 16:45:54 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 4 loss:0.04618113487958908 norm:0.00012298027286306024 max memory_allocated 22565.25732421875 
[2025-02-28 16:46:25 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 5 loss:0.04581281170248985 norm:0.00010833678970811889 max memory_allocated 22565.25732421875 
[2025-02-28 16:46:56 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 6 loss:0.04568668082356453 norm:9.86023442237638e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:47:26 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 7 loss:0.04559843987226486 norm:8.919812535168603e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:47:57 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 8 loss:0.04555613920092583 norm:8.51961740409024e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:48:28 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 9 loss:0.04549860209226608 norm:8.158355922205374e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:48:59 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 10 loss:0.045456282794475555 norm:8.331851131515577e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:49:29 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 11 loss:0.04543190822005272 norm:7.766748603899032e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:50:00 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 12 loss:0.045403845608234406 norm:7.017997995717451e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:50:31 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 13 loss:0.045387450605630875 norm:6.671456503681839e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:51:01 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 14 loss:0.04536125436425209 norm:6.654712342424318e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:51:32 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 15 loss:0.045361410826444626 norm:7.188077142927796e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:52:03 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 16 loss:0.04534459114074707 norm:7.063246448524296e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:52:33 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 17 loss:0.04532920569181442 norm:6.978283636271954e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:53:04 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 18 loss:0.045315664261579514 norm:6.461019802372903e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:53:35 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 19 loss:0.04530015215277672 norm:6.0802856751251966e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:53:43 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 20 ===
[2025-02-28 16:54:16 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 0 loss:0.057013511657714844 norm:0.0009592840215191245 max memory_allocated 22565.42919921875 
[2025-02-28 16:54:47 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 1 loss:0.054032109677791595 norm:0.00034717327798716724 max memory_allocated 22565.42919921875 
[2025-02-28 16:55:18 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 2 loss:0.05251238867640495 norm:0.00021976567222736776 max memory_allocated 22565.42919921875 
[2025-02-28 16:55:49 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 3 loss:0.05142821744084358 norm:0.0001564530684845522 max memory_allocated 22565.42919921875 
[2025-02-28 16:56:19 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 4 loss:0.05066535994410515 norm:0.00014102003478910774 max memory_allocated 22565.42919921875 
[2025-02-28 16:56:50 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 5 loss:0.0502549484372139 norm:0.00012441286526154727 max memory_allocated 22565.42919921875 
[2025-02-28 16:57:21 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 6 loss:0.050064533948898315 norm:0.00011412338062655181 max memory_allocated 22565.42919921875 
[2025-02-28 16:57:51 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 7 loss:0.04996813088655472 norm:0.00011597437696764246 max memory_allocated 22565.42919921875 
[2025-02-28 16:58:22 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 8 loss:0.04988695681095123 norm:0.00010120950901182368 max memory_allocated 22565.42919921875 
[2025-02-28 16:58:53 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 9 loss:0.04984794184565544 norm:0.00010657630627974868 max memory_allocated 22565.42919921875 
[2025-02-28 16:59:23 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 10 loss:0.04981392249464989 norm:9.769714961294085e-05 max memory_allocated 22565.42919921875 
[2025-02-28 16:59:54 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 11 loss:0.04975993186235428 norm:9.823752043303102e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:00:25 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 12 loss:0.049721427261829376 norm:9.498393046669662e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:00:56 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 13 loss:0.04968986287713051 norm:8.968648035079241e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:01:26 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 14 loss:0.04965617507696152 norm:8.463763515464962e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:01:57 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 15 loss:0.04963329806923866 norm:8.480341784888878e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:02:28 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 16 loss:0.04961007833480835 norm:8.777975745033473e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:02:58 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 17 loss:0.04959874600172043 norm:8.719302422832698e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:03:29 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 18 loss:0.04960007965564728 norm:9.136011067312211e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:04:00 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 19 loss:0.049583081156015396 norm:8.958792022895068e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:04:08 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 21 ===
[2025-02-28 17:04:41 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 0 loss:0.05993737652897835 norm:0.0004517739580478519 max memory_allocated 22565.60107421875 
[2025-02-28 17:05:12 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 1 loss:0.05831429362297058 norm:0.00020357368339318782 max memory_allocated 22565.60107421875 
[2025-02-28 17:05:43 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 2 loss:0.05736338347196579 norm:0.0001467116380808875 max memory_allocated 22565.60107421875 
[2025-02-28 17:06:13 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 3 loss:0.05650464445352554 norm:0.00012623617658391595 max memory_allocated 22565.60107421875 
[2025-02-28 17:06:44 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 4 loss:0.05573183298110962 norm:9.869100176729262e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:07:15 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 5 loss:0.05534762516617775 norm:9.369481267640367e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:07:45 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 6 loss:0.055226974189281464 norm:9.321897232439369e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:08:16 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 7 loss:0.05516631156206131 norm:8.50051364977844e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:08:47 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 8 loss:0.05511942878365517 norm:7.826912769814953e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:09:18 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 9 loss:0.055081065744161606 norm:7.675604138057679e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:09:48 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 10 loss:0.05505317449569702 norm:8.33610538393259e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:10:19 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 11 loss:0.055046021938323975 norm:7.851864211261272e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:10:50 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 12 loss:0.05500631779432297 norm:7.316192204598337e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:11:20 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 13 loss:0.05499154329299927 norm:6.934653356438503e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:11:51 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 14 loss:0.05498753860592842 norm:7.193208148237318e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:12:22 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 15 loss:0.05496915429830551 norm:6.839894194854423e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:12:52 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 16 loss:0.054951705038547516 norm:6.753452908014879e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:13:23 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 17 loss:0.05492974445223808 norm:6.474916881415993e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:13:54 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 18 loss:0.05492473021149635 norm:6.47452543489635e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:14:25 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 19 loss:0.05491984635591507 norm:6.200851203175262e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:14:33 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 22 ===
[2025-02-28 17:15:06 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 0 loss:0.07036591321229935 norm:0.0014191461959853768 max memory_allocated 22565.77294921875 
[2025-02-28 17:15:37 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 1 loss:0.06727853417396545 norm:0.0005170726217329502 max memory_allocated 22565.77294921875 
[2025-02-28 17:16:07 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 2 loss:0.0657326728105545 norm:0.00030037041869945824 max memory_allocated 22565.77294921875 
[2025-02-28 17:16:38 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 3 loss:0.06449685245752335 norm:0.00024859857512637973 max memory_allocated 22565.77294921875 
[2025-02-28 17:17:09 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 4 loss:0.06362568587064743 norm:0.00020739619503729045 max memory_allocated 22565.77294921875 
[2025-02-28 17:17:40 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 5 loss:0.06322990357875824 norm:0.00018398706743028015 max memory_allocated 22565.77294921875 
[2025-02-28 17:18:10 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 6 loss:0.06310116499662399 norm:0.00016759481513872743 max memory_allocated 22565.77294921875 
[2025-02-28 17:18:41 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 7 loss:0.06301455944776535 norm:0.00015809608157724142 max memory_allocated 22565.77294921875 
[2025-02-28 17:19:12 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 8 loss:0.06293868273496628 norm:0.0001486265828134492 max memory_allocated 22565.77294921875 
[2025-02-28 17:19:42 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 9 loss:0.06289049983024597 norm:0.00013741423026658595 max memory_allocated 22565.77294921875 
[2025-02-28 17:20:13 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 10 loss:0.06283319741487503 norm:0.00013419885362964123 max memory_allocated 22565.77294921875 
[2025-02-28 17:20:44 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 11 loss:0.0627908855676651 norm:0.0001253129739779979 max memory_allocated 22565.77294921875 
[2025-02-28 17:21:14 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 12 loss:0.06273794174194336 norm:0.0001179343307740055 max memory_allocated 22565.77294921875 
[2025-02-28 17:21:45 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 13 loss:0.06270400434732437 norm:0.0001118018408305943 max memory_allocated 22565.77294921875 
[2025-02-28 17:22:16 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 14 loss:0.0626840591430664 norm:0.00010670300252968445 max memory_allocated 22565.77294921875 
[2025-02-28 17:22:47 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 15 loss:0.06265930831432343 norm:0.00010587846918497235 max memory_allocated 22565.77294921875 
[2025-02-28 17:23:17 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 16 loss:0.06264745444059372 norm:0.0001020965792122297 max memory_allocated 22565.77294921875 
[2025-02-28 17:23:48 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 17 loss:0.0626290813088417 norm:9.774244972504675e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:24:19 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 18 loss:0.06260102987289429 norm:0.0001012780558085069 max memory_allocated 22565.77294921875 
[2025-02-28 17:24:49 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 19 loss:0.06257537007331848 norm:9.394378867000341e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:24:58 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 23 ===
[2025-02-28 17:25:31 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 0 loss:0.07879166305065155 norm:0.0015898332931101322 max memory_allocated 22565.94482421875 
[2025-02-28 17:26:01 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 1 loss:0.0760066881775856 norm:0.0007165801944211125 max memory_allocated 22565.94482421875 
[2025-02-28 17:26:32 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 2 loss:0.07425873726606369 norm:0.0003747188311535865 max memory_allocated 22565.94482421875 
[2025-02-28 17:27:03 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 3 loss:0.07293327152729034 norm:0.00028435702552087605 max memory_allocated 22565.94482421875 
[2025-02-28 17:27:34 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 4 loss:0.07197681069374084 norm:0.00022388804063666612 max memory_allocated 22565.94482421875 
[2025-02-28 17:28:04 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 5 loss:0.07164475321769714 norm:0.000189487065654248 max memory_allocated 22565.94482421875 
[2025-02-28 17:28:35 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 6 loss:0.07152128219604492 norm:0.0001674931845627725 max memory_allocated 22565.94482421875 
[2025-02-28 17:29:06 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 7 loss:0.07142564654350281 norm:0.00015623553190380335 max memory_allocated 22565.94482421875 
[2025-02-28 17:29:36 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 8 loss:0.07137656211853027 norm:0.00013602874241769314 max memory_allocated 22565.94482421875 
[2025-02-28 17:30:07 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 9 loss:0.07132481038570404 norm:0.00012105557107133791 max memory_allocated 22565.94482421875 
[2025-02-28 17:30:38 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 10 loss:0.07127118855714798 norm:0.0001211668859468773 max memory_allocated 22565.94482421875 
[2025-02-28 17:31:08 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 11 loss:0.0712311714887619 norm:0.00011579328565858305 max memory_allocated 22565.94482421875 
[2025-02-28 17:31:39 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 12 loss:0.07119624316692352 norm:0.00010765578190330416 max memory_allocated 22565.94482421875 
[2025-02-28 17:32:10 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 13 loss:0.0711631327867508 norm:0.00010197592928307131 max memory_allocated 22565.94482421875 
[2025-02-28 17:32:41 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 14 loss:0.07114719599485397 norm:9.544382919557393e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:33:11 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 15 loss:0.07112876325845718 norm:9.35071730054915e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:33:42 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 16 loss:0.07111436873674393 norm:9.047414641827345e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:34:13 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 17 loss:0.07109859585762024 norm:8.502552373101935e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:34:43 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 18 loss:0.07108414173126221 norm:8.009841985767707e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:35:14 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 19 loss:0.07107390463352203 norm:7.820945756975561e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:35:22 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 24 ===
[2025-02-28 17:35:56 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 0 loss:0.08932200819253922 norm:0.0020459189545363188 max memory_allocated 22566.11669921875 
[2025-02-28 17:36:26 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 1 loss:0.08570875972509384 norm:0.0006137569434940815 max memory_allocated 22566.11669921875 
[2025-02-28 17:36:57 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 2 loss:0.08398928493261337 norm:0.0003903728793375194 max memory_allocated 22566.11669921875 
[2025-02-28 17:37:28 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 3 loss:0.08255180716514587 norm:0.0003023724420927465 max memory_allocated 22566.11669921875 
[2025-02-28 17:37:58 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 4 loss:0.08153305947780609 norm:0.0002537539112381637 max memory_allocated 22566.11669921875 
[2025-02-28 17:38:29 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 5 loss:0.0812218114733696 norm:0.00021493411622941494 max memory_allocated 22566.11669921875 
[2025-02-28 17:39:00 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 6 loss:0.08109253644943237 norm:0.00020012004824820906 max memory_allocated 22566.11669921875 
[2025-02-28 17:39:31 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 7 loss:0.08103089779615402 norm:0.0001834520953707397 max memory_allocated 22566.11669921875 
[2025-02-28 17:40:01 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 8 loss:0.0809379294514656 norm:0.00016688191681168973 max memory_allocated 22566.11669921875 
[2025-02-28 17:40:32 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 9 loss:0.08087822049856186 norm:0.00016868892998900265 max memory_allocated 22566.11669921875 
[2025-02-28 17:41:03 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 10 loss:0.08081982284784317 norm:0.00016212505579460412 max memory_allocated 22566.11669921875 
[2025-02-28 17:41:33 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 11 loss:0.08078756928443909 norm:0.00015160559269133955 max memory_allocated 22566.11669921875 
[2025-02-28 17:42:04 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 12 loss:0.08075393736362457 norm:0.0001466160174459219 max memory_allocated 22566.11669921875 
[2025-02-28 17:42:35 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 13 loss:0.08072321116924286 norm:0.0001408842799719423 max memory_allocated 22566.11669921875 
[2025-02-28 17:43:06 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 14 loss:0.08068788051605225 norm:0.00014065802679397166 max memory_allocated 22566.11669921875 
[2025-02-28 17:43:36 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 15 loss:0.08065786212682724 norm:0.0001325917983194813 max memory_allocated 22566.11669921875 
[2025-02-28 17:44:07 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 16 loss:0.08062507957220078 norm:0.00012770039029419422 max memory_allocated 22566.11669921875 
[2025-02-28 17:44:38 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 17 loss:0.08061439543962479 norm:0.00012545724166557193 max memory_allocated 22566.11669921875 
[2025-02-28 17:45:08 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 18 loss:0.0805925726890564 norm:0.00012091349344700575 max memory_allocated 22566.11669921875 
[2025-02-28 17:45:39 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 19 loss:0.08057498186826706 norm:0.00011705981160048395 max memory_allocated 22566.11669921875 
[2025-02-28 17:45:47 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 25 ===
[2025-02-28 17:46:21 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 0 loss:0.10132981836795807 norm:0.001031643943861127 max memory_allocated 22566.28857421875 
[2025-02-28 17:46:51 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 1 loss:0.098358653485775 norm:0.0005145908216945827 max memory_allocated 22566.28857421875 
[2025-02-28 17:47:22 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 2 loss:0.09621617197990417 norm:0.00031957394094206393 max memory_allocated 22566.28857421875 
[2025-02-28 17:47:53 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 3 loss:0.09463772922754288 norm:0.0002351672446820885 max memory_allocated 22566.28857421875 
[2025-02-28 17:48:23 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 4 loss:0.09368640184402466 norm:0.00021460751304402947 max memory_allocated 22566.28857421875 
[2025-02-28 17:48:54 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 5 loss:0.09339118003845215 norm:0.000198354508029297 max memory_allocated 22566.28857421875 
[2025-02-28 17:49:25 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 6 loss:0.09327589720487595 norm:0.0001712266239337623 max memory_allocated 22566.28857421875 
[2025-02-28 17:49:55 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 7 loss:0.09315987676382065 norm:0.0001621319679543376 max memory_allocated 22566.28857421875 
[2025-02-28 17:50:26 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 8 loss:0.09308522194623947 norm:0.00015400668780785054 max memory_allocated 22566.28857421875 
[2025-02-28 17:50:57 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 9 loss:0.0930248498916626 norm:0.00014243379700928926 max memory_allocated 22566.28857421875 
[2025-02-28 17:51:27 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 10 loss:0.0929408147931099 norm:0.00013768064673058689 max memory_allocated 22566.28857421875 
[2025-02-28 17:51:58 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 11 loss:0.09291716665029526 norm:0.00013160378148313612 max memory_allocated 22566.28857421875 
[2025-02-28 17:52:29 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 12 loss:0.09286150336265564 norm:0.00012345648428890854 max memory_allocated 22566.28857421875 
[2025-02-28 17:53:00 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 13 loss:0.092807337641716 norm:0.00012066921044606715 max memory_allocated 22566.28857421875 
[2025-02-28 17:53:30 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 14 loss:0.09278881549835205 norm:0.00011519438703544438 max memory_allocated 22566.28857421875 
[2025-02-28 17:54:01 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 15 loss:0.09275693446397781 norm:0.00011051582987420261 max memory_allocated 22566.28857421875 
[2025-02-28 17:54:32 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 16 loss:0.09271293878555298 norm:0.00011644118058029562 max memory_allocated 22566.28857421875 
[2025-02-28 17:55:02 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 17 loss:0.09270530939102173 norm:0.00011318206816213205 max memory_allocated 22566.28857421875 
[2025-02-28 17:55:33 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 18 loss:0.09266436100006104 norm:0.00010171228495892137 max memory_allocated 22566.28857421875 
[2025-02-28 17:56:04 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 19 loss:0.09267198294401169 norm:0.00010529642895562574 max memory_allocated 22566.28857421875 
[2025-02-28 17:56:12 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 26 ===
[2025-02-28 17:56:45 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 0 loss:0.11466121673583984 norm:0.0010271159699186683 max memory_allocated 22566.46044921875 
[2025-02-28 17:57:16 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 1 loss:0.11130805313587189 norm:0.000508053635712713 max memory_allocated 22566.46044921875 
[2025-02-28 17:57:47 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 2 loss:0.1091584712266922 norm:0.00033352093305438757 max memory_allocated 22566.46044921875 
[2025-02-28 17:58:17 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 3 loss:0.10741584748029709 norm:0.0002526929310988635 max memory_allocated 22566.46044921875 
[2025-02-28 17:58:48 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 4 loss:0.10648030787706375 norm:0.00021135412680450827 max memory_allocated 22566.46044921875 
[2025-02-28 17:59:19 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 5 loss:0.10615842789411545 norm:0.0001814694405766204 max memory_allocated 22566.46044921875 
[2025-02-28 17:59:49 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 6 loss:0.10599952191114426 norm:0.0001646241726120934 max memory_allocated 22566.46044921875 
[2025-02-28 18:00:20 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 7 loss:0.10586419701576233 norm:0.00015592122508678585 max memory_allocated 22566.46044921875 
[2025-02-28 18:00:51 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 8 loss:0.10574547946453094 norm:0.0001454241864848882 max memory_allocated 22566.46044921875 
[2025-02-28 18:01:22 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 9 loss:0.1056918054819107 norm:0.00014027312863618135 max memory_allocated 22566.46044921875 
[2025-02-28 18:01:52 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 10 loss:0.10564105212688446 norm:0.00013798510190099478 max memory_allocated 22566.46044921875 
[2025-02-28 18:02:23 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 11 loss:0.10554975271224976 norm:0.000124752739793621 max memory_allocated 22566.46044921875 
[2025-02-28 18:02:54 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 12 loss:0.10550811141729355 norm:0.000122955214465037 max memory_allocated 22566.46044921875 
[2025-02-28 18:03:24 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 13 loss:0.10548534989356995 norm:0.00011967278260271996 max memory_allocated 22566.46044921875 
[2025-02-28 18:03:55 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 14 loss:0.10546234250068665 norm:0.00011905387509614229 max memory_allocated 22566.46044921875 
[2025-02-28 18:04:26 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 15 loss:0.10541001707315445 norm:0.00012263985991012305 max memory_allocated 22566.46044921875 
[2025-02-28 18:04:56 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 16 loss:0.10540702939033508 norm:0.0001238426921190694 max memory_allocated 22566.46044921875 
[2025-02-28 18:05:27 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 17 loss:0.10537484288215637 norm:0.00012431759387254715 max memory_allocated 22566.46044921875 
[2025-02-28 18:05:58 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 18 loss:0.10536838322877884 norm:0.00011797463230323046 max memory_allocated 22566.46044921875 
[2025-02-28 18:06:28 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 19 loss:0.10534488409757614 norm:0.00011599265417316929 max memory_allocated 22566.46044921875 
[2025-02-28 18:06:37 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 27 ===
[2025-02-28 18:07:10 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 0 loss:0.13125620782375336 norm:0.0021095017436891794 max memory_allocated 22566.63232421875 
[2025-02-28 18:07:41 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 1 loss:0.12719181180000305 norm:0.0008430043235421181 max memory_allocated 22566.63232421875 
[2025-02-28 18:08:11 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 2 loss:0.12457339465618134 norm:0.0004049734852742404 max memory_allocated 22566.63232421875 
[2025-02-28 18:08:42 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 3 loss:0.12266965210437775 norm:0.00028479183674789965 max memory_allocated 22566.63232421875 
[2025-02-28 18:09:13 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 4 loss:0.12182920426130295 norm:0.0002461089752614498 max memory_allocated 22566.63232421875 
[2025-02-28 18:09:43 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 5 loss:0.12161819636821747 norm:0.00020786281675100327 max memory_allocated 22566.63232421875 
[2025-02-28 18:10:14 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 6 loss:0.12150542438030243 norm:0.00020079014939256012 max memory_allocated 22566.63232421875 
[2025-02-28 18:10:45 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 7 loss:0.12140513956546783 norm:0.0001846589002525434 max memory_allocated 22566.63232421875 
[2025-02-28 18:11:15 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 8 loss:0.12133749574422836 norm:0.0001718079438433051 max memory_allocated 22566.63232421875 
[2025-02-28 18:11:46 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 9 loss:0.12127958983182907 norm:0.00016617693472653627 max memory_allocated 22566.63232421875 
[2025-02-28 18:12:17 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 10 loss:0.12123601138591766 norm:0.00015844448353163898 max memory_allocated 22566.63232421875 
[2025-02-28 18:12:48 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 11 loss:0.12119083106517792 norm:0.00014639011351391673 max memory_allocated 22566.63232421875 
[2025-02-28 18:13:18 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 12 loss:0.12114255875349045 norm:0.00013802370813209563 max memory_allocated 22566.63232421875 
[2025-02-28 18:13:49 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 13 loss:0.12111726403236389 norm:0.00013691428466700017 max memory_allocated 22566.63232421875 
[2025-02-28 18:14:20 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 14 loss:0.121079221367836 norm:0.00013085256796330214 max memory_allocated 22566.63232421875 
[2025-02-28 18:14:50 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 15 loss:0.12106291204690933 norm:0.00012546939251478761 max memory_allocated 22566.63232421875 
[2025-02-28 18:15:21 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 16 loss:0.12105116993188858 norm:0.000124843543744646 max memory_allocated 22566.63232421875 
[2025-02-28 18:15:52 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 17 loss:0.12102824449539185 norm:0.00011863333929795772 max memory_allocated 22566.63232421875 
[2025-02-28 18:16:22 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 18 loss:0.12100531160831451 norm:0.00011717373854480684 max memory_allocated 22566.63232421875 
[2025-02-28 18:16:53 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 19 loss:0.1209847554564476 norm:0.00011543920845724642 max memory_allocated 22566.63232421875 
[2025-02-28 18:17:01 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 28 ===
[2025-02-28 18:17:04 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 18:17:35 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 0 loss:0.14934290945529938 norm:0.00518328882753849 max memory_allocated 22566.91943359375 
[2025-02-28 18:18:06 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 1 loss:0.14620569348335266 norm:0.004101124592125416 max memory_allocated 22566.91943359375 
[2025-02-28 18:18:37 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 2 loss:0.14372843503952026 norm:0.003257563104853034 max memory_allocated 22566.91943359375 
[2025-02-28 18:19:07 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 3 loss:0.14161932468414307 norm:0.0026637702248990536 max memory_allocated 22566.91943359375 
[2025-02-28 18:19:38 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 4 loss:0.1408022940158844 norm:0.0022113437298685312 max memory_allocated 22566.91943359375 
[2025-02-28 18:20:09 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 5 loss:0.1404876559972763 norm:0.0018619035836309195 max memory_allocated 22566.91943359375 
[2025-02-28 18:20:40 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 6 loss:0.14032815396785736 norm:0.0016739964485168457 max memory_allocated 22566.91943359375 
[2025-02-28 18:21:11 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 7 loss:0.1402449905872345 norm:0.0016493507428094745 max memory_allocated 22566.91943359375 
[2025-02-28 18:21:42 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 8 loss:0.14016792178153992 norm:0.001640204805880785 max memory_allocated 22566.91943359375 
[2025-02-28 18:22:13 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 9 loss:0.1401146948337555 norm:0.001528585096821189 max memory_allocated 22566.91943359375 
[2025-02-28 18:22:43 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 10 loss:0.13999870419502258 norm:0.0014584865421056747 max memory_allocated 22566.91943359375 
[2025-02-28 18:23:14 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 11 loss:0.13999375700950623 norm:0.0014715415891259909 max memory_allocated 22566.91943359375 
[2025-02-28 18:23:45 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 12 loss:0.1399562805891037 norm:0.0014643260510638356 max memory_allocated 22566.91943359375 
[2025-02-28 18:24:16 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 13 loss:0.13993453979492188 norm:0.0014009246369823813 max memory_allocated 22566.91943359375 
[2025-02-28 18:24:47 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 14 loss:0.1399383693933487 norm:0.0014448135625571012 max memory_allocated 22566.91943359375 
[2025-02-28 18:25:18 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 15 loss:0.13989901542663574 norm:0.0013880711048841476 max memory_allocated 22566.91943359375 
[2025-02-28 18:25:48 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 16 loss:0.1399017870426178 norm:0.0013681653654202819 max memory_allocated 22566.91943359375 
[2025-02-28 18:26:19 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 17 loss:0.13986200094223022 norm:0.001357155735604465 max memory_allocated 22566.91943359375 
[2025-02-28 18:26:50 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 18 loss:0.13986419141292572 norm:0.0012447594199329615 max memory_allocated 22566.91943359375 
[2025-02-28 18:27:21 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 19 loss:0.13981083035469055 norm:0.0012917645508423448 max memory_allocated 22566.91943359375 
[2025-02-28 18:27:32 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 29 ===
[2025-02-28 18:27:35 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 18:28:05 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 0 loss:0.18222177028656006 norm:0.0099721048027277 max memory_allocated 22567.09130859375 
[2025-02-28 18:28:36 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 1 loss:0.17396488785743713 norm:0.007516009733080864 max memory_allocated 22567.09130859375 
[2025-02-28 18:29:07 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 2 loss:0.16890007257461548 norm:0.005108652636408806 max memory_allocated 22567.09130859375 
[2025-02-28 18:29:38 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 3 loss:0.16593687236309052 norm:0.0040502906776964664 max memory_allocated 22567.09130859375 
[2025-02-28 18:30:09 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 4 loss:0.16480356454849243 norm:0.003424663795158267 max memory_allocated 22567.09130859375 
[2025-02-28 18:30:40 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 5 loss:0.16433751583099365 norm:0.0029301061294972897 max memory_allocated 22567.09130859375 
[2025-02-28 18:31:10 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 6 loss:0.16405341029167175 norm:0.002494130050763488 max memory_allocated 22567.09130859375 
[2025-02-28 18:31:41 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 7 loss:0.1638721525669098 norm:0.0022331364452838898 max memory_allocated 22567.09130859375 
[2025-02-28 18:32:12 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 8 loss:0.16378040611743927 norm:0.0020875565242022276 max memory_allocated 22567.09130859375 
[2025-02-28 18:32:43 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 9 loss:0.16365493834018707 norm:0.00199726689606905 max memory_allocated 22567.09130859375 
[2025-02-28 18:33:14 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 10 loss:0.1636306643486023 norm:0.0019042198546230793 max memory_allocated 22567.09130859375 
[2025-02-28 18:33:44 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 11 loss:0.16355867683887482 norm:0.0017631647642701864 max memory_allocated 22567.09130859375 
[2025-02-28 18:34:15 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 12 loss:0.16347603499889374 norm:0.0018793635535985231 max memory_allocated 22567.09130859375 
[2025-02-28 18:34:46 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 13 loss:0.163420170545578 norm:0.0016295305686071515 max memory_allocated 22567.09130859375 
[2025-02-28 18:35:17 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 14 loss:0.16343334317207336 norm:0.001871386542916298 max memory_allocated 22567.09130859375 
[2025-02-28 18:35:48 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 15 loss:0.16342753171920776 norm:0.0016747871413826942 max memory_allocated 22567.09130859375 
[2025-02-28 18:36:19 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 16 loss:0.1633569896221161 norm:0.0018166243098676205 max memory_allocated 22567.09130859375 
[2025-02-28 18:36:50 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 17 loss:0.1633325070142746 norm:0.0016228713793680072 max memory_allocated 22567.09130859375 
[2025-02-28 18:37:20 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 18 loss:0.16336244344711304 norm:0.0017021234380081296 max memory_allocated 22567.09130859375 
[2025-02-28 18:37:51 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 19 loss:0.16337613761425018 norm:0.0015766402939334512 max memory_allocated 22567.09130859375 
[2025-02-28 18:38:00 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 30 ===
[2025-02-28 18:38:02 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 18:38:33 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 0 loss:1.0215933322906494 norm:0.14479373395442963 max memory_allocated 22567.26318359375 
[2025-02-28 18:39:04 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 1 loss:0.7145704030990601 norm:0.12956415116786957 max memory_allocated 22567.26318359375 
[2025-02-28 18:39:35 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 2 loss:0.4296368658542633 norm:0.058969512581825256 max memory_allocated 22567.26318359375 
[2025-02-28 18:40:06 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 3 loss:0.33895060420036316 norm:0.03732161968946457 max memory_allocated 22567.26318359375 
[2025-02-28 18:40:36 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 4 loss:0.3032103180885315 norm:0.03544176369905472 max memory_allocated 22567.26318359375 
[2025-02-28 18:41:07 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 5 loss:0.2781803607940674 norm:0.03282230719923973 max memory_allocated 22567.26318359375 
[2025-02-28 18:41:38 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 6 loss:0.2626839876174927 norm:0.03566752374172211 max memory_allocated 22567.26318359375 
[2025-02-28 18:42:09 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 7 loss:0.2543925344944 norm:0.03413243964314461 max memory_allocated 22567.26318359375 
[2025-02-28 18:42:40 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 8 loss:0.248101145029068 norm:0.030707407742738724 max memory_allocated 22567.26318359375 
[2025-02-28 18:43:11 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 9 loss:0.24342578649520874 norm:0.028976451605558395 max memory_allocated 22567.26318359375 
[2025-02-28 18:43:41 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 10 loss:0.23893509805202484 norm:0.027090705931186676 max memory_allocated 22567.26318359375 
[2025-02-28 18:44:12 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 11 loss:0.2364451140165329 norm:0.025576548650860786 max memory_allocated 22567.26318359375 
[2025-02-28 18:44:43 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 12 loss:0.23391279578208923 norm:0.02330414205789566 max memory_allocated 22567.26318359375 
[2025-02-28 18:45:14 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 13 loss:0.23244047164916992 norm:0.022610638290643692 max memory_allocated 22567.26318359375 
[2025-02-28 18:45:45 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 14 loss:0.23120570182800293 norm:0.020903220400214195 max memory_allocated 22567.26318359375 
[2025-02-28 18:46:16 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 15 loss:0.23074038326740265 norm:0.019846323877573013 max memory_allocated 22567.26318359375 
[2025-02-28 18:46:46 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 16 loss:0.22992904484272003 norm:0.018668610602617264 max memory_allocated 22567.26318359375 
[2025-02-28 18:47:17 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 17 loss:0.23065686225891113 norm:0.02089172974228859 max memory_allocated 22567.26318359375 
[2025-02-28 18:47:48 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 18 loss:0.23066264390945435 norm:0.021063512191176414 max memory_allocated 22567.26318359375 
[2025-02-28 18:48:19 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 19 loss:0.2305249124765396 norm:0.02072395570576191 max memory_allocated 22567.26318359375 
[2025-02-28 18:48:27 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 31 ===
[2025-02-28 18:48:30 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 18:49:01 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 0 loss:0.46311646699905396 norm:0.03138931468129158 max memory_allocated 22567.43505859375 
[2025-02-28 18:49:31 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 1 loss:0.4337913393974304 norm:0.022765707224607468 max memory_allocated 22567.43505859375 
[2025-02-28 18:50:02 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 2 loss:0.4171718657016754 norm:0.01798931509256363 max memory_allocated 22567.43505859375 
[2025-02-28 18:50:33 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 3 loss:0.40856242179870605 norm:0.015775397419929504 max memory_allocated 22567.43505859375 
[2025-02-28 18:51:04 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 4 loss:0.40379399061203003 norm:0.014906419441103935 max memory_allocated 22567.43505859375 
[2025-02-28 18:51:35 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 5 loss:0.40006303787231445 norm:0.014123925007879734 max memory_allocated 22567.43505859375 
[2025-02-28 18:52:06 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 6 loss:0.3976958692073822 norm:0.013696103356778622 max memory_allocated 22567.43505859375 
[2025-02-28 18:52:36 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 7 loss:0.39557889103889465 norm:0.01348100695759058 max memory_allocated 22567.43505859375 
[2025-02-28 18:53:07 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 8 loss:0.39414700865745544 norm:0.013449905440211296 max memory_allocated 22567.43505859375 
[2025-02-28 18:53:38 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 9 loss:0.39330366253852844 norm:0.013436630368232727 max memory_allocated 22567.43505859375 
[2025-02-28 18:54:09 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 10 loss:0.39286312460899353 norm:0.013531885109841824 max memory_allocated 22567.43505859375 
[2025-02-28 18:54:40 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 11 loss:0.39242950081825256 norm:0.013201707974076271 max memory_allocated 22567.43505859375 
[2025-02-28 18:55:11 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 12 loss:0.39236462116241455 norm:0.013914958573877811 max memory_allocated 22567.43505859375 
[2025-02-28 18:55:42 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 13 loss:0.39165663719177246 norm:0.01319938339293003 max memory_allocated 22567.43505859375 
[2025-02-28 18:56:12 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 14 loss:0.39152535796165466 norm:0.01370569784194231 max memory_allocated 22567.43505859375 
[2025-02-28 18:56:43 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 15 loss:0.39197075366973877 norm:0.014831218868494034 max memory_allocated 22567.43505859375 
[2025-02-28 18:57:14 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 16 loss:0.39133501052856445 norm:0.013544394634664059 max memory_allocated 22567.43505859375 
[2025-02-28 18:57:45 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 17 loss:0.3910351097583771 norm:0.013871305622160435 max memory_allocated 22567.43505859375 
[2025-02-28 18:58:16 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 18 loss:0.39035701751708984 norm:0.01344181876629591 max memory_allocated 22567.43505859375 
[2025-02-28 18:58:47 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 19 loss:0.3896114230155945 norm:0.011771566234529018 max memory_allocated 22567.43505859375 
[2025-02-28 18:58:55 root] (main_calib_config2.py 380): INFO 20030.41897916794
[2025-02-28 18:58:59 root] (main_calib_config2.py 115): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-28 19:00:03 root] (main_calib_config2.py 159): INFO wikitext2 : 5.693970680236816
[2025-02-28 19:00:03 root] (main_calib_config2.py 115): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-28 19:01:40 root] (main_calib_config2.py 159): INFO c4 : 7.266587257385254
[2025-02-28 20:44:04 root] (main_calib_config2.py 170): INFO {'wikitext2': 5.693970680236816, 'c4': 7.266587257385254, 'results': {'hellaswag': {'acc': 0.5530770762796255, 'acc_stderr': 0.004961587574275621, 'acc_norm': 0.7151961760605458, 'acc_norm_stderr': 0.004503985839041978}, 'winogrande': {'acc': 0.6519337016574586, 'acc_stderr': 0.013388004531086068}, 'piqa': {'acc': 0.7742110990206746, 'acc_stderr': 0.009754980670917315, 'acc_norm': 0.764961915125136, 'acc_norm_stderr': 0.009893146688805315}, 'boolq': {'acc': 0.6847094801223241, 'acc_stderr': 0.008126455592662887}, 'arc_challenge': {'acc': 0.3796928327645051, 'acc_stderr': 0.014182119866974874, 'acc_norm': 0.39078498293515357, 'acc_norm_stderr': 0.014258563880513777}, 'arc_easy': {'acc': 0.6696127946127947, 'acc_stderr': 0.009651430216428187, 'acc_norm': 0.5252525252525253, 'acc_norm_stderr': 0.010246690042583849}}, 'versions': {'hellaswag': 0, 'winogrande': 0, 'piqa': 0, 'boolq': 1, 'arc_challenge': 0, 'arc_easy': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
