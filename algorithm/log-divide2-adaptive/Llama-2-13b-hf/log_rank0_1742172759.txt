[2025-03-17 00:52:39 root] (main_quant_config.py 118): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-divide2-adaptive/Llama-2-13b-hf', blocks_pkl='./log-divide2/Llama-2-13b-hf-w4a4/Llama-2-13b-hf_blocks.pkl', calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, reload=False, parallel=False, size_bound_factor=1.0, bitops_bound_factor=0.4, weight_quant_params={'per_channel_axes': [0], 'symmetric': False, 'dynamic_method': 'per_channel', 'group_size': None, 'lwc': False, 'disable_zero_point': False}, act_quant_params={'per_channel_axes': [], 'symmetric': False, 'dynamic_method': 'per_token'})
[2025-03-17 00:52:47 root] (main_quant_config.py 141): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-17 00:52:47 root] (main_quant_config.py 152): INFO [(0, 1), (1, 2), (2, 3), (3, 4), (4, 6), (6, 8), (8, 10), (10, 12), (12, 14), (14, 16), (16, 18), (18, 20), (20, 22), (22, 24), (24, 26), (26, 28), (28, 30), (30, 32), (32, 34), (34, 36), (36, 37), (37, 38), (38, 39), (39, 40)]
[2025-03-17 00:52:50 root] (block_wise_quant_config_search.py 307): INFO searching quant config for block 0 with indices (0, 1)
[2025-03-17 00:52:59 root] (block_wise_quant_config_search.py 431): INFO 0.self_attn.q_proj 0.self_attn.q_proj w4a4 w4a4 loss: 1.8132694094674662e-05
[2025-03-17 00:52:59 root] (block_wise_quant_config_search.py 431): INFO 0.self_attn.q_proj 0.self_attn.q_proj w4a4 w4a8 loss: 0
[2025-03-17 00:52:59 root] (block_wise_quant_config_search.py 431): INFO 0.self_attn.q_proj 0.self_attn.q_proj w4a4 w8a8 loss: 0
[2025-03-17 00:53:01 root] (block_wise_quant_config_search.py 431): INFO 0.self_attn.q_proj 0.self_attn.q_proj w4a8 w4a8 loss: 1.4052977803657996e-06
[2025-03-17 00:53:01 root] (block_wise_quant_config_search.py 431): INFO 0.self_attn.q_proj 0.self_attn.q_proj w4a8 w8a8 loss: 0
[2025-03-17 00:53:03 root] (block_wise_quant_config_search.py 431): INFO 0.self_attn.q_proj 0.self_attn.q_proj w8a8 w8a8 loss: 1.3069292208456318e-06
[2025-03-17 00:53:05 root] (block_wise_quant_config_search.py 431): INFO 0.self_attn.q_proj 0.self_attn.k_proj w4a4 w4a4 loss: 0.00012749386951327324
