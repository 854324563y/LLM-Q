[2025-03-22 01:18:40 root] (main_calibration_a2.py 274): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-loss/llama-7b-hf-w4a4-cos', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=1, loss_type='cos')
[2025-03-22 01:22:58 root] (main_calibration_a2.py 341): INFO === start quantization ===
[2025-03-22 01:22:58 root] (main_calibration_a2.py 347): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-22 01:22:58 root] (abq_llm_calibration_a2.py 62): INFO Starting ...
[2025-03-22 01:23:01 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 01:23:05 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 01:23:38 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 0 loss:0.06893332302570343 norm:0.04123208671808243 max memory_allocated 22559.10693359375 
[2025-03-22 01:24:11 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 1 loss:0.04480491951107979 norm:0.021365748718380928 max memory_allocated 22559.10693359375 
[2025-03-22 01:24:44 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 2 loss:0.03655882924795151 norm:0.016117632389068604 max memory_allocated 22559.10693359375 
[2025-03-22 01:25:17 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 3 loss:0.033206358551979065 norm:0.014269853010773659 max memory_allocated 22559.10693359375 
[2025-03-22 01:25:49 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 4 loss:0.031629472970962524 norm:0.012257739901542664 max memory_allocated 22559.10693359375 
[2025-03-22 01:26:22 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 5 loss:0.03056197240948677 norm:0.010686200112104416 max memory_allocated 22559.10693359375 
[2025-03-22 01:26:55 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 6 loss:0.029777878895401955 norm:0.008996558375656605 max memory_allocated 22559.10693359375 
[2025-03-22 01:27:28 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 7 loss:0.029472211375832558 norm:0.008168581873178482 max memory_allocated 22559.10693359375 
[2025-03-22 01:28:01 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 8 loss:0.029019251465797424 norm:0.006810240913182497 max memory_allocated 22559.10693359375 
[2025-03-22 01:28:34 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 9 loss:0.028811538591980934 norm:0.006246790289878845 max memory_allocated 22559.10693359375 
[2025-03-22 01:29:07 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 10 loss:0.02855491265654564 norm:0.005522855091840029 max memory_allocated 22559.10693359375 
[2025-03-22 01:29:39 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 11 loss:0.028615884482860565 norm:0.004933488089591265 max memory_allocated 22559.10693359375 
[2025-03-22 01:30:12 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 12 loss:0.028415370732545853 norm:0.004574825521558523 max memory_allocated 22559.10693359375 
[2025-03-22 01:30:45 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 13 loss:0.02827809378504753 norm:0.004159029573202133 max memory_allocated 22559.10693359375 
[2025-03-22 01:31:18 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 14 loss:0.02811320126056671 norm:0.003909701947122812 max memory_allocated 22559.10693359375 
[2025-03-22 01:31:51 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 15 loss:0.028147410601377487 norm:0.003691757097840309 max memory_allocated 22559.10693359375 
[2025-03-22 01:32:24 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 16 loss:0.028135493397712708 norm:0.003565022489055991 max memory_allocated 22559.10693359375 
[2025-03-22 01:32:57 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 17 loss:0.02819054014980793 norm:0.0034034340642392635 max memory_allocated 22559.10693359375 
[2025-03-22 01:33:30 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 18 loss:0.028003599494695663 norm:0.003357037901878357 max memory_allocated 22559.10693359375 
[2025-03-22 01:34:03 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 19 loss:0.028045112267136574 norm:0.0033166517969220877 max memory_allocated 22559.10693359375 
[2025-03-22 01:34:12 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 01:34:15 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 01:34:48 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 0 loss:0.24869641661643982 norm:0.13361582159996033 max memory_allocated 22559.27880859375 
[2025-03-22 01:35:21 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 1 loss:0.14050738513469696 norm:0.03936528414487839 max memory_allocated 22559.27880859375 
[2025-03-22 01:35:54 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 2 loss:0.10722813010215759 norm:0.02004375495016575 max memory_allocated 22559.27880859375 
[2025-03-22 01:36:27 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 3 loss:0.09357740730047226 norm:0.017509792000055313 max memory_allocated 22559.27880859375 
[2025-03-22 01:37:00 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 4 loss:0.08622711151838303 norm:0.014573466032743454 max memory_allocated 22559.27880859375 
[2025-03-22 01:37:33 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 5 loss:0.08193759620189667 norm:0.012764261104166508 max memory_allocated 22559.27880859375 
[2025-03-22 01:38:06 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 6 loss:0.07921536266803741 norm:0.011499108746647835 max memory_allocated 22559.27880859375 
[2025-03-22 01:38:39 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 7 loss:0.07748760282993317 norm:0.010440569370985031 max memory_allocated 22559.27880859375 
[2025-03-22 01:39:12 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 8 loss:0.07612739503383636 norm:0.009445286355912685 max memory_allocated 22559.27880859375 
[2025-03-22 01:39:45 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 9 loss:0.0751103013753891 norm:0.008476292714476585 max memory_allocated 22559.27880859375 
[2025-03-22 01:40:17 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 10 loss:0.07437008619308472 norm:0.007692168466746807 max memory_allocated 22559.27880859375 
[2025-03-22 01:40:50 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 11 loss:0.07386759668588638 norm:0.0070070005021989346 max memory_allocated 22559.27880859375 
[2025-03-22 01:41:23 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 12 loss:0.0734853520989418 norm:0.006653786636888981 max memory_allocated 22559.27880859375 
[2025-03-22 01:41:56 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 13 loss:0.0731285810470581 norm:0.006277927663177252 max memory_allocated 22559.27880859375 
[2025-03-22 01:42:29 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 14 loss:0.07286735624074936 norm:0.005901350174099207 max memory_allocated 22559.27880859375 
[2025-03-22 01:43:02 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 15 loss:0.0725662037730217 norm:0.005642339587211609 max memory_allocated 22559.27880859375 
[2025-03-22 01:43:35 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 16 loss:0.07234664261341095 norm:0.005662674084305763 max memory_allocated 22559.27880859375 
[2025-03-22 01:44:08 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 17 loss:0.07216822355985641 norm:0.005518559366464615 max memory_allocated 22559.27880859375 
[2025-03-22 01:44:41 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 18 loss:0.07195185124874115 norm:0.005343611817806959 max memory_allocated 22559.27880859375 
[2025-03-22 01:45:14 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 19 loss:0.07185009866952896 norm:0.005161483772099018 max memory_allocated 22559.27880859375 
[2025-03-22 01:45:23 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 01:45:26 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 01:45:59 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 0 loss:0.16859208047389984 norm:0.036160908639431 max memory_allocated 22559.45068359375 
[2025-03-22 01:46:32 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 1 loss:0.13130436837673187 norm:0.020708320662379265 max memory_allocated 22559.45068359375 
[2025-03-22 01:47:05 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 2 loss:0.1166391670703888 norm:0.013139713555574417 max memory_allocated 22559.45068359375 
[2025-03-22 01:47:38 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 3 loss:0.11089227348566055 norm:0.008903486654162407 max memory_allocated 22559.45068359375 
[2025-03-22 01:48:12 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 4 loss:0.1058243066072464 norm:0.0115749416872859 max memory_allocated 22559.45068359375 
[2025-03-22 01:48:45 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 5 loss:0.10387465357780457 norm:0.011200033128261566 max memory_allocated 22559.45068359375 
[2025-03-22 01:49:18 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 6 loss:0.10259256511926651 norm:0.010465153492987156 max memory_allocated 22559.45068359375 
[2025-03-22 01:49:51 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 7 loss:0.10166889429092407 norm:0.009642465971410275 max memory_allocated 22559.45068359375 
[2025-03-22 01:50:24 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 8 loss:0.10083594918251038 norm:0.00857453141361475 max memory_allocated 22559.45068359375 
[2025-03-22 01:50:57 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 9 loss:0.10025164484977722 norm:0.007587837520986795 max memory_allocated 22559.45068359375 
[2025-03-22 01:51:30 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 10 loss:0.09970011562108994 norm:0.006661686114966869 max memory_allocated 22559.45068359375 
[2025-03-22 01:52:03 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 11 loss:0.09942133724689484 norm:0.005902478471398354 max memory_allocated 22559.45068359375 
[2025-03-22 01:52:37 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 12 loss:0.09930598735809326 norm:0.005509436130523682 max memory_allocated 22559.45068359375 
[2025-03-22 01:53:10 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 13 loss:0.09911691397428513 norm:0.005175197962671518 max memory_allocated 22559.45068359375 
[2025-03-22 01:53:43 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 14 loss:0.0990370512008667 norm:0.0050443136133253574 max memory_allocated 22559.45068359375 
[2025-03-22 01:54:16 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 15 loss:0.09900420159101486 norm:0.004884032532572746 max memory_allocated 22559.45068359375 
[2025-03-22 01:54:49 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 16 loss:0.09894368797540665 norm:0.004802999552339315 max memory_allocated 22559.45068359375 
[2025-03-22 01:55:22 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 17 loss:0.09891533851623535 norm:0.004739581607282162 max memory_allocated 22559.45068359375 
[2025-03-22 01:55:55 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 18 loss:0.09888573735952377 norm:0.004653620533645153 max memory_allocated 22559.45068359375 
[2025-03-22 01:56:29 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 19 loss:0.09881879389286041 norm:0.004500706680119038 max memory_allocated 22559.45068359375 
[2025-03-22 01:56:38 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 01:57:13 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 0 loss:0.7271644473075867 norm:0.11560672521591187 max memory_allocated 22559.50732421875 
[2025-03-22 01:57:46 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 1 loss:0.6748501062393188 norm:0.0781451165676117 max memory_allocated 22559.50732421875 
[2025-03-22 01:58:19 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 2 loss:0.6162760257720947 norm:0.04929979145526886 max memory_allocated 22559.50732421875 
[2025-03-22 01:58:52 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 3 loss:0.5638458728790283 norm:0.030584877356886864 max memory_allocated 22559.50732421875 
[2025-03-22 01:59:25 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 4 loss:0.5355246067047119 norm:0.021898070350289345 max memory_allocated 22559.50732421875 
[2025-03-22 01:59:58 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 5 loss:0.5153234004974365 norm:0.017557194456458092 max memory_allocated 22559.50732421875 
[2025-03-22 02:00:31 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 6 loss:0.5011017322540283 norm:0.014830273576080799 max memory_allocated 22559.50732421875 
[2025-03-22 02:01:04 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 7 loss:0.4913947880268097 norm:0.012644472531974316 max memory_allocated 22559.50732421875 
[2025-03-22 02:01:37 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 8 loss:0.48304274678230286 norm:0.01119498535990715 max memory_allocated 22559.50732421875 
[2025-03-22 02:02:10 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 9 loss:0.47607749700546265 norm:0.009657916612923145 max memory_allocated 22559.50732421875 
[2025-03-22 02:02:43 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 10 loss:0.4677385687828064 norm:0.00862625241279602 max memory_allocated 22559.50732421875 
[2025-03-22 02:03:16 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 11 loss:0.46357661485671997 norm:0.007914254441857338 max memory_allocated 22559.50732421875 
[2025-03-22 02:03:49 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 12 loss:0.4593829810619354 norm:0.0069643291644752026 max memory_allocated 22559.50732421875 
[2025-03-22 02:04:22 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 13 loss:0.4554062783718109 norm:0.006166729144752026 max memory_allocated 22559.50732421875 
[2025-03-22 02:04:55 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 14 loss:0.4497150182723999 norm:0.0055167037062346935 max memory_allocated 22559.50732421875 
[2025-03-22 02:05:28 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 15 loss:0.4469759166240692 norm:0.005238380748778582 max memory_allocated 22559.50732421875 
[2025-03-22 02:06:01 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 16 loss:0.444198340177536 norm:0.004913222044706345 max memory_allocated 22559.50732421875 
[2025-03-22 02:06:34 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 17 loss:0.44024255871772766 norm:0.0042334371246397495 max memory_allocated 22559.50732421875 
[2025-03-22 02:07:07 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 18 loss:0.43853050470352173 norm:0.004254911094903946 max memory_allocated 22559.50732421875 
[2025-03-22 02:07:40 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 19 loss:0.43698424100875854 norm:0.003867164719849825 max memory_allocated 22559.50732421875 
[2025-03-22 02:07:49 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 02:08:25 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 0 loss:0.7205533385276794 norm:0.1281348615884781 max memory_allocated 22559.67919921875 
[2025-03-22 02:08:58 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 1 loss:0.6033869981765747 norm:0.04015403985977173 max memory_allocated 22559.67919921875 
[2025-03-22 02:09:31 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 2 loss:0.5241557359695435 norm:0.030009176582098007 max memory_allocated 22559.67919921875 
[2025-03-22 02:10:04 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 3 loss:0.4791468381881714 norm:0.018718328326940536 max memory_allocated 22559.67919921875 
[2025-03-22 02:10:37 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 4 loss:0.45085805654525757 norm:0.012403715401887894 max memory_allocated 22559.67919921875 
[2025-03-22 02:11:10 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 5 loss:0.4339568614959717 norm:0.0105906892567873 max memory_allocated 22559.67919921875 
[2025-03-22 02:11:43 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 6 loss:0.42372265458106995 norm:0.009697052650153637 max memory_allocated 22559.67919921875 
[2025-03-22 02:12:16 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 7 loss:0.4165460169315338 norm:0.007426800671964884 max memory_allocated 22559.67919921875 
[2025-03-22 02:12:49 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 8 loss:0.4107917845249176 norm:0.007237851154059172 max memory_allocated 22559.67919921875 
[2025-03-22 02:13:22 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 9 loss:0.40596505999565125 norm:0.006551332306116819 max memory_allocated 22559.67919921875 
[2025-03-22 02:13:55 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 10 loss:0.4017570912837982 norm:0.005635300185531378 max memory_allocated 22559.67919921875 
[2025-03-22 02:14:28 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 11 loss:0.3986572027206421 norm:0.005750394891947508 max memory_allocated 22559.67919921875 
[2025-03-22 02:15:01 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 12 loss:0.3963679075241089 norm:0.00553236436098814 max memory_allocated 22559.67919921875 
[2025-03-22 02:15:34 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 13 loss:0.3948083221912384 norm:0.0054289172403514385 max memory_allocated 22559.67919921875 
[2025-03-22 02:16:07 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 14 loss:0.3941594660282135 norm:0.005448419600725174 max memory_allocated 22559.67919921875 
[2025-03-22 02:16:40 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 15 loss:0.39282363653182983 norm:0.005859668366611004 max memory_allocated 22559.67919921875 
[2025-03-22 02:17:13 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 16 loss:0.39182719588279724 norm:0.005557188764214516 max memory_allocated 22559.67919921875 
[2025-03-22 02:17:46 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 17 loss:0.39058688282966614 norm:0.005878236144781113 max memory_allocated 22559.67919921875 
[2025-03-22 02:18:19 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 18 loss:0.38998696208000183 norm:0.005339343566447496 max memory_allocated 22559.67919921875 
[2025-03-22 02:18:52 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 19 loss:0.3892088234424591 norm:0.00565864285454154 max memory_allocated 22559.67919921875 
[2025-03-22 02:19:01 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 02:19:38 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 0 loss:0.6070239543914795 norm:0.09534148871898651 max memory_allocated 22559.85107421875 
[2025-03-22 02:20:11 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 1 loss:0.5385404229164124 norm:0.03740261495113373 max memory_allocated 22559.85107421875 
[2025-03-22 02:20:44 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 2 loss:0.4917930066585541 norm:0.020657740533351898 max memory_allocated 22559.85107421875 
[2025-03-22 02:21:17 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 3 loss:0.462885320186615 norm:0.012951207347214222 max memory_allocated 22559.85107421875 
[2025-03-22 02:21:50 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 4 loss:0.44497132301330566 norm:0.009096714667975903 max memory_allocated 22559.85107421875 
[2025-03-22 02:22:23 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 5 loss:0.4334169030189514 norm:0.007086208090186119 max memory_allocated 22559.85107421875 
[2025-03-22 02:22:56 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 6 loss:0.4261128306388855 norm:0.005833569914102554 max memory_allocated 22559.85107421875 
[2025-03-22 02:23:29 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 7 loss:0.42006802558898926 norm:0.004985762760043144 max memory_allocated 22559.85107421875 
[2025-03-22 02:24:02 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 8 loss:0.41580888628959656 norm:0.004366155713796616 max memory_allocated 22559.85107421875 
[2025-03-22 02:24:35 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 9 loss:0.41187363862991333 norm:0.003856840543448925 max memory_allocated 22559.85107421875 
[2025-03-22 02:25:08 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 10 loss:0.4095562696456909 norm:0.003615899244323373 max memory_allocated 22559.85107421875 
[2025-03-22 02:25:41 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 11 loss:0.4062613248825073 norm:0.003434303216636181 max memory_allocated 22559.85107421875 
[2025-03-22 02:26:14 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 12 loss:0.4031294882297516 norm:0.00317849637940526 max memory_allocated 22559.85107421875 
[2025-03-22 02:26:47 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 13 loss:0.40149593353271484 norm:0.002951011760160327 max memory_allocated 22559.85107421875 
[2025-03-22 02:27:20 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 14 loss:0.399786114692688 norm:0.0027653358411043882 max memory_allocated 22559.85107421875 
[2025-03-22 02:27:53 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 15 loss:0.3979830741882324 norm:0.0025954849552363157 max memory_allocated 22559.85107421875 
[2025-03-22 02:28:26 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 16 loss:0.397146612405777 norm:0.0024873102083802223 max memory_allocated 22559.85107421875 
[2025-03-22 02:28:59 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 17 loss:0.39607736468315125 norm:0.0024330320302397013 max memory_allocated 22559.85107421875 
[2025-03-22 02:29:32 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 18 loss:0.3937879800796509 norm:0.0023114834912121296 max memory_allocated 22559.85107421875 
[2025-03-22 02:30:05 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 19 loss:0.3933335542678833 norm:0.0023465342819690704 max memory_allocated 22559.85107421875 
[2025-03-22 02:30:14 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 02:30:50 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 0 loss:0.6454975605010986 norm:0.13219518959522247 max memory_allocated 22560.02294921875 
[2025-03-22 02:31:23 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 1 loss:0.5748723149299622 norm:0.04045427590608597 max memory_allocated 22560.02294921875 
[2025-03-22 02:31:56 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 2 loss:0.5383021831512451 norm:0.019503137096762657 max memory_allocated 22560.02294921875 
[2025-03-22 02:32:29 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 3 loss:0.5175434350967407 norm:0.012453167699277401 max memory_allocated 22560.02294921875 
[2025-03-22 02:33:02 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 4 loss:0.5027300715446472 norm:0.008588403463363647 max memory_allocated 22560.02294921875 
[2025-03-22 02:33:35 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 5 loss:0.49301430583000183 norm:0.006084364373236895 max memory_allocated 22560.02294921875 
[2025-03-22 02:34:08 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 6 loss:0.486434668302536 norm:0.004724983125925064 max memory_allocated 22560.02294921875 
[2025-03-22 02:34:41 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 7 loss:0.48193955421447754 norm:0.0042038168758153915 max memory_allocated 22560.02294921875 
[2025-03-22 02:35:14 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 8 loss:0.4786423146724701 norm:0.00383232394233346 max memory_allocated 22560.02294921875 
[2025-03-22 02:35:47 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 9 loss:0.47544723749160767 norm:0.0033580069430172443 max memory_allocated 22560.02294921875 
[2025-03-22 02:36:20 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 10 loss:0.47303709387779236 norm:0.003040266688913107 max memory_allocated 22560.02294921875 
[2025-03-22 02:36:53 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 11 loss:0.47101590037345886 norm:0.002969886641949415 max memory_allocated 22560.02294921875 
[2025-03-22 02:37:26 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 12 loss:0.4696774482727051 norm:0.0028231050819158554 max memory_allocated 22560.02294921875 
[2025-03-22 02:37:59 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 13 loss:0.4679187536239624 norm:0.0026472024619579315 max memory_allocated 22560.02294921875 
[2025-03-22 02:38:32 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 14 loss:0.4670490622520447 norm:0.0024851614143699408 max memory_allocated 22560.02294921875 
[2025-03-22 02:39:05 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 15 loss:0.4662511944770813 norm:0.002522103488445282 max memory_allocated 22560.02294921875 
[2025-03-22 02:39:38 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 16 loss:0.46467283368110657 norm:0.002345792017877102 max memory_allocated 22560.02294921875 
[2025-03-22 02:40:11 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 17 loss:0.4637572765350342 norm:0.002296985127031803 max memory_allocated 22560.02294921875 
[2025-03-22 02:40:44 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 18 loss:0.46290162205696106 norm:0.002155069960281253 max memory_allocated 22560.02294921875 
[2025-03-22 02:41:17 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 19 loss:0.4623420238494873 norm:0.0021528834477066994 max memory_allocated 22560.02294921875 
[2025-03-22 02:41:27 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 02:42:03 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 0 loss:0.7202113270759583 norm:0.1333618313074112 max memory_allocated 22560.19482421875 
[2025-03-22 02:42:36 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 1 loss:0.6169043183326721 norm:0.034246690571308136 max memory_allocated 22560.19482421875 
[2025-03-22 02:43:08 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 2 loss:0.5751480460166931 norm:0.01289677806198597 max memory_allocated 22560.19482421875 
[2025-03-22 02:43:42 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 3 loss:0.5551229119300842 norm:0.008725045248866081 max memory_allocated 22560.19482421875 
[2025-03-22 02:44:14 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 4 loss:0.5428414940834045 norm:0.006711538415402174 max memory_allocated 22560.19482421875 
[2025-03-22 02:44:48 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 5 loss:0.5337113738059998 norm:0.005263142287731171 max memory_allocated 22560.19482421875 
[2025-03-22 02:45:20 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 6 loss:0.526261031627655 norm:0.004198596812784672 max memory_allocated 22560.19482421875 
[2025-03-22 02:45:53 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 7 loss:0.5206462740898132 norm:0.0037027497310191393 max memory_allocated 22560.19482421875 
[2025-03-22 02:46:26 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 8 loss:0.5167227387428284 norm:0.0031863872427493334 max memory_allocated 22560.19482421875 
[2025-03-22 02:46:59 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 9 loss:0.514128565788269 norm:0.0029972977936267853 max memory_allocated 22560.19482421875 
[2025-03-22 02:47:32 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 10 loss:0.5111528635025024 norm:0.002970993285998702 max memory_allocated 22560.19482421875 
[2025-03-22 02:48:06 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 11 loss:0.5084339380264282 norm:0.0027901616413146257 max memory_allocated 22560.19482421875 
[2025-03-22 02:48:39 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 12 loss:0.5064465999603271 norm:0.0027713319286704063 max memory_allocated 22560.19482421875 
[2025-03-22 02:49:12 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 13 loss:0.5046246647834778 norm:0.002857745159417391 max memory_allocated 22560.19482421875 
[2025-03-22 02:49:45 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 14 loss:0.503919780254364 norm:0.002695947652682662 max memory_allocated 22560.19482421875 
[2025-03-22 02:50:18 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 15 loss:0.5022129416465759 norm:0.0024397375527769327 max memory_allocated 22560.19482421875 
[2025-03-22 02:50:51 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 16 loss:0.5010179281234741 norm:0.002379130572080612 max memory_allocated 22560.19482421875 
[2025-03-22 02:51:24 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 17 loss:0.499449223279953 norm:0.002367326756939292 max memory_allocated 22560.19482421875 
[2025-03-22 02:51:57 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 18 loss:0.49827462434768677 norm:0.00238215085119009 max memory_allocated 22560.19482421875 
[2025-03-22 02:52:30 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 19 loss:0.4977613687515259 norm:0.0023472507018595934 max memory_allocated 22560.19482421875 
[2025-03-22 02:52:39 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 02:53:15 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 0 loss:0.6242834329605103 norm:0.04282643646001816 max memory_allocated 22560.36669921875 
[2025-03-22 02:53:48 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 1 loss:0.5851998329162598 norm:0.01124352216720581 max memory_allocated 22560.36669921875 
[2025-03-22 02:54:21 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 2 loss:0.560631275177002 norm:0.005894562695175409 max memory_allocated 22560.36669921875 
[2025-03-22 02:54:54 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 3 loss:0.5470854043960571 norm:0.004588136449456215 max memory_allocated 22560.36669921875 
[2025-03-22 02:55:27 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 4 loss:0.5389217138290405 norm:0.0035488256253302097 max memory_allocated 22560.36669921875 
[2025-03-22 02:56:00 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 5 loss:0.5325992107391357 norm:0.0029434082098305225 max memory_allocated 22560.36669921875 
[2025-03-22 02:56:33 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 6 loss:0.5290502309799194 norm:0.0026062747929245234 max memory_allocated 22560.36669921875 
[2025-03-22 02:57:06 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 7 loss:0.5265759825706482 norm:0.0023382632061839104 max memory_allocated 22560.36669921875 
[2025-03-22 02:57:39 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 8 loss:0.5242424607276917 norm:0.0020473403856158257 max memory_allocated 22560.36669921875 
[2025-03-22 02:58:12 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 9 loss:0.5214346051216125 norm:0.002068080473691225 max memory_allocated 22560.36669921875 
[2025-03-22 02:58:45 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 10 loss:0.520548403263092 norm:0.0020757163874804974 max memory_allocated 22560.36669921875 
[2025-03-22 02:59:18 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 11 loss:0.5191531181335449 norm:0.0019325746688991785 max memory_allocated 22560.36669921875 
[2025-03-22 02:59:51 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 12 loss:0.5176644325256348 norm:0.0018508787034079432 max memory_allocated 22560.36669921875 
[2025-03-22 03:00:24 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 13 loss:0.5158914923667908 norm:0.0017946918960660696 max memory_allocated 22560.36669921875 
[2025-03-22 03:00:57 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 14 loss:0.5144879221916199 norm:0.0016824824269860983 max memory_allocated 22560.36669921875 
[2025-03-22 03:01:30 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 15 loss:0.5137033462524414 norm:0.0016030105762183666 max memory_allocated 22560.36669921875 
[2025-03-22 03:02:03 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 16 loss:0.512940526008606 norm:0.001620585098862648 max memory_allocated 22560.36669921875 
[2025-03-22 03:02:36 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 17 loss:0.5121859908103943 norm:0.0015825716545805335 max memory_allocated 22560.36669921875 
[2025-03-22 03:03:09 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 18 loss:0.5122070908546448 norm:0.0016506132669746876 max memory_allocated 22560.36669921875 
[2025-03-22 03:03:42 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 19 loss:0.511664092540741 norm:0.0015755483182147145 max memory_allocated 22560.36669921875 
[2025-03-22 03:03:51 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 03:04:27 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 0 loss:0.6339972019195557 norm:0.05684478580951691 max memory_allocated 22560.53857421875 
[2025-03-22 03:05:00 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 1 loss:0.5976898670196533 norm:0.015337756834924221 max memory_allocated 22560.53857421875 
[2025-03-22 03:05:33 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 2 loss:0.5795817375183105 norm:0.0069223856553435326 max memory_allocated 22560.53857421875 
[2025-03-22 03:06:06 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 3 loss:0.568205714225769 norm:0.00507681630551815 max memory_allocated 22560.53857421875 
[2025-03-22 03:06:39 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 4 loss:0.5609305500984192 norm:0.0037644999101758003 max memory_allocated 22560.53857421875 
[2025-03-22 03:07:12 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 5 loss:0.556455135345459 norm:0.003345049452036619 max memory_allocated 22560.53857421875 
[2025-03-22 03:07:45 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 6 loss:0.5526771545410156 norm:0.0027923425659537315 max memory_allocated 22560.53857421875 
[2025-03-22 03:08:18 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 7 loss:0.5495785474777222 norm:0.0024310818407684565 max memory_allocated 22560.53857421875 
[2025-03-22 03:08:51 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 8 loss:0.5470886826515198 norm:0.0021653014700859785 max memory_allocated 22560.53857421875 
[2025-03-22 03:09:24 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 9 loss:0.5448929667472839 norm:0.001848537940531969 max memory_allocated 22560.53857421875 
[2025-03-22 03:09:57 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 10 loss:0.5432487726211548 norm:0.001767597859725356 max memory_allocated 22560.53857421875 
[2025-03-22 03:10:30 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 11 loss:0.5418349504470825 norm:0.0016705080633983016 max memory_allocated 22560.53857421875 
[2025-03-22 03:11:03 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 12 loss:0.5406256914138794 norm:0.0016288317274302244 max memory_allocated 22560.53857421875 
[2025-03-22 03:11:36 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 13 loss:0.5395404696464539 norm:0.0014939133543521166 max memory_allocated 22560.53857421875 
[2025-03-22 03:12:09 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 14 loss:0.5389737486839294 norm:0.0014447898138314486 max memory_allocated 22560.53857421875 
[2025-03-22 03:12:42 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 15 loss:0.5382580757141113 norm:0.0013819688465446234 max memory_allocated 22560.53857421875 
[2025-03-22 03:13:15 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 16 loss:0.5373629331588745 norm:0.0013511069118976593 max memory_allocated 22560.53857421875 
[2025-03-22 03:13:48 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 17 loss:0.5367151498794556 norm:0.0013173357583582401 max memory_allocated 22560.53857421875 
[2025-03-22 03:14:21 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 18 loss:0.536333441734314 norm:0.001243077334947884 max memory_allocated 22560.53857421875 
[2025-03-22 03:14:54 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 19 loss:0.53579181432724 norm:0.0012369019677862525 max memory_allocated 22560.53857421875 
[2025-03-22 03:15:03 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 03:15:39 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 0 loss:0.6096282005310059 norm:0.02647879719734192 max memory_allocated 22560.71044921875 
[2025-03-22 03:16:12 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 1 loss:0.5884323120117188 norm:0.010416904464364052 max memory_allocated 22560.71044921875 
[2025-03-22 03:16:45 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 2 loss:0.5729466676712036 norm:0.004477829672396183 max memory_allocated 22560.71044921875 
[2025-03-22 03:17:18 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 3 loss:0.563611626625061 norm:0.003233369905501604 max memory_allocated 22560.71044921875 
[2025-03-22 03:17:51 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 4 loss:0.5574860572814941 norm:0.002602771855890751 max memory_allocated 22560.71044921875 
[2025-03-22 03:18:24 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 5 loss:0.5529738664627075 norm:0.002044602297246456 max memory_allocated 22560.71044921875 
[2025-03-22 03:18:57 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 6 loss:0.5500814914703369 norm:0.0017850815784186125 max memory_allocated 22560.71044921875 
[2025-03-22 03:19:30 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 7 loss:0.5473554134368896 norm:0.0015634925803169608 max memory_allocated 22560.71044921875 
[2025-03-22 03:20:03 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 8 loss:0.5456410050392151 norm:0.0014589783968403935 max memory_allocated 22560.71044921875 
[2025-03-22 03:20:36 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 9 loss:0.5440788865089417 norm:0.0012991094263270497 max memory_allocated 22560.71044921875 
[2025-03-22 03:21:09 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 10 loss:0.5428098440170288 norm:0.0012594574363902211 max memory_allocated 22560.71044921875 
[2025-03-22 03:21:42 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 11 loss:0.5415911674499512 norm:0.001169153256341815 max memory_allocated 22560.71044921875 
[2025-03-22 03:22:15 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 12 loss:0.540679931640625 norm:0.0012138343881815672 max memory_allocated 22560.71044921875 
[2025-03-22 03:22:48 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 13 loss:0.5401768684387207 norm:0.001213070354424417 max memory_allocated 22560.71044921875 
[2025-03-22 03:23:21 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 14 loss:0.5393731594085693 norm:0.0011487507726997137 max memory_allocated 22560.71044921875 
[2025-03-22 03:23:54 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 15 loss:0.5384598970413208 norm:0.0010638607200235128 max memory_allocated 22560.71044921875 
[2025-03-22 03:24:27 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 16 loss:0.5379024744033813 norm:0.0010430413531139493 max memory_allocated 22560.71044921875 
[2025-03-22 03:25:00 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 17 loss:0.5374683141708374 norm:0.0010655942605808377 max memory_allocated 22560.71044921875 
[2025-03-22 03:25:33 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 18 loss:0.5371775031089783 norm:0.0010609669843688607 max memory_allocated 22560.71044921875 
[2025-03-22 03:26:06 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 19 loss:0.5368360877037048 norm:0.0010244610020890832 max memory_allocated 22560.71044921875 
[2025-03-22 03:26:16 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 03:26:52 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 0 loss:0.5963315963745117 norm:0.022121146321296692 max memory_allocated 22560.88232421875 
[2025-03-22 03:27:24 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 1 loss:0.5779169201850891 norm:0.008708879351615906 max memory_allocated 22560.88232421875 
[2025-03-22 03:27:57 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 2 loss:0.5637677907943726 norm:0.004131611902266741 max memory_allocated 22560.88232421875 
[2025-03-22 03:28:30 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 3 loss:0.5559520125389099 norm:0.002883101347833872 max memory_allocated 22560.88232421875 
[2025-03-22 03:29:03 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 4 loss:0.5511507391929626 norm:0.002308102324604988 max memory_allocated 22560.88232421875 
[2025-03-22 03:29:36 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 5 loss:0.5479874610900879 norm:0.0021153127308934927 max memory_allocated 22560.88232421875 
[2025-03-22 03:30:09 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 6 loss:0.5453208684921265 norm:0.0019195080967620015 max memory_allocated 22560.88232421875 
[2025-03-22 03:30:42 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 7 loss:0.5436099767684937 norm:0.0017435149056836963 max memory_allocated 22560.88232421875 
[2025-03-22 03:31:15 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 8 loss:0.5420650839805603 norm:0.0016855724388733506 max memory_allocated 22560.88232421875 
[2025-03-22 03:31:48 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 9 loss:0.5406806468963623 norm:0.0015645420644432306 max memory_allocated 22560.88232421875 
[2025-03-22 03:32:22 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 10 loss:0.5397971868515015 norm:0.001538357581011951 max memory_allocated 22560.88232421875 
[2025-03-22 03:32:55 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 11 loss:0.5389838218688965 norm:0.0014960106927901506 max memory_allocated 22560.88232421875 
[2025-03-22 03:33:28 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 12 loss:0.5380651354789734 norm:0.0014180789003148675 max memory_allocated 22560.88232421875 
[2025-03-22 03:34:01 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 13 loss:0.5372670888900757 norm:0.0013267871690914035 max memory_allocated 22560.88232421875 
[2025-03-22 03:34:34 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 14 loss:0.5365941524505615 norm:0.001255257986485958 max memory_allocated 22560.88232421875 
[2025-03-22 03:35:07 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 15 loss:0.5360866189002991 norm:0.0012406983878463507 max memory_allocated 22560.88232421875 
[2025-03-22 03:35:40 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 16 loss:0.5356289148330688 norm:0.0011996979592368007 max memory_allocated 22560.88232421875 
[2025-03-22 03:36:13 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 17 loss:0.5351614356040955 norm:0.0011360942153260112 max memory_allocated 22560.88232421875 
[2025-03-22 03:36:46 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 18 loss:0.534755527973175 norm:0.0011138905538246036 max memory_allocated 22560.88232421875 
[2025-03-22 03:37:19 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 19 loss:0.5344765782356262 norm:0.001134512829594314 max memory_allocated 22560.88232421875 
[2025-03-22 03:37:28 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 03:38:04 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 0 loss:0.5612900257110596 norm:0.019499152898788452 max memory_allocated 22561.05419921875 
[2025-03-22 03:38:37 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 1 loss:0.5459729433059692 norm:0.007285119034349918 max memory_allocated 22561.05419921875 
[2025-03-22 03:39:10 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 2 loss:0.5364220142364502 norm:0.003824950661510229 max memory_allocated 22561.05419921875 
[2025-03-22 03:39:43 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 3 loss:0.5304265022277832 norm:0.0027867264579981565 max memory_allocated 22561.05419921875 
[2025-03-22 03:40:16 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 4 loss:0.5265780687332153 norm:0.0021099247969686985 max memory_allocated 22561.05419921875 
[2025-03-22 03:40:49 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 5 loss:0.5237082839012146 norm:0.0018274043686687946 max memory_allocated 22561.05419921875 
[2025-03-22 03:41:22 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 6 loss:0.5211508274078369 norm:0.0016309713246300817 max memory_allocated 22561.05419921875 
[2025-03-22 03:41:55 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 7 loss:0.5192254185676575 norm:0.0014288707170635462 max memory_allocated 22561.05419921875 
[2025-03-22 03:42:28 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 8 loss:0.5177557468414307 norm:0.001339321373961866 max memory_allocated 22561.05419921875 
[2025-03-22 03:43:01 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 9 loss:0.5164591670036316 norm:0.0012175894808024168 max memory_allocated 22561.05419921875 
[2025-03-22 03:43:34 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 10 loss:0.5153242349624634 norm:0.0011200420558452606 max memory_allocated 22561.05419921875 
[2025-03-22 03:44:07 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 11 loss:0.5142911672592163 norm:0.0010662369895726442 max memory_allocated 22561.05419921875 
[2025-03-22 03:44:40 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 12 loss:0.5135178565979004 norm:0.001037508831359446 max memory_allocated 22561.05419921875 
[2025-03-22 03:45:13 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 13 loss:0.5129222869873047 norm:0.0009998454479500651 max memory_allocated 22561.05419921875 
[2025-03-22 03:45:46 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 14 loss:0.5124008655548096 norm:0.000970340333878994 max memory_allocated 22561.05419921875 
[2025-03-22 03:46:19 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 15 loss:0.5119034051895142 norm:0.0009217418846674263 max memory_allocated 22561.05419921875 
[2025-03-22 03:46:52 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 16 loss:0.5114859938621521 norm:0.0009264309192076325 max memory_allocated 22561.05419921875 
[2025-03-22 03:47:25 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 17 loss:0.5110312700271606 norm:0.000897844962310046 max memory_allocated 22561.05419921875 
[2025-03-22 03:47:58 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 18 loss:0.510723888874054 norm:0.0008812224259600043 max memory_allocated 22561.05419921875 
[2025-03-22 03:48:31 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 19 loss:0.5104230046272278 norm:0.0008771417196840048 max memory_allocated 22561.05419921875 
[2025-03-22 03:48:41 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 03:49:16 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 0 loss:0.5409562587738037 norm:0.021236073225736618 max memory_allocated 22561.22607421875 
[2025-03-22 03:49:49 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 1 loss:0.5255014896392822 norm:0.007856530137360096 max memory_allocated 22561.22607421875 
[2025-03-22 03:50:22 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 2 loss:0.5170937180519104 norm:0.004019211977720261 max memory_allocated 22561.22607421875 
[2025-03-22 03:50:55 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 3 loss:0.5114231705665588 norm:0.002884188899770379 max memory_allocated 22561.22607421875 
[2025-03-22 03:51:28 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 4 loss:0.5077704191207886 norm:0.0025752687361091375 max memory_allocated 22561.22607421875 
[2025-03-22 03:52:01 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 5 loss:0.5052235126495361 norm:0.0022041050251573324 max memory_allocated 22561.22607421875 
[2025-03-22 03:52:34 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 6 loss:0.5030015110969543 norm:0.0017803959781304002 max memory_allocated 22561.22607421875 
[2025-03-22 03:53:07 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 7 loss:0.5012789964675903 norm:0.001734619727358222 max memory_allocated 22561.22607421875 
[2025-03-22 03:53:40 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 8 loss:0.4998921751976013 norm:0.0015276530757546425 max memory_allocated 22561.22607421875 
[2025-03-22 03:54:13 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 9 loss:0.4986571669578552 norm:0.0013622712576761842 max memory_allocated 22561.22607421875 
[2025-03-22 03:54:46 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 10 loss:0.4976082444190979 norm:0.0012564451899379492 max memory_allocated 22561.22607421875 
[2025-03-22 03:55:19 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 11 loss:0.49687087535858154 norm:0.0011610820656642318 max memory_allocated 22561.22607421875 
[2025-03-22 03:55:53 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 12 loss:0.49608948826789856 norm:0.0011035788338631392 max memory_allocated 22561.22607421875 
[2025-03-22 03:56:26 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 13 loss:0.49542179703712463 norm:0.0010399007005617023 max memory_allocated 22561.22607421875 
[2025-03-22 03:56:59 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 14 loss:0.4948817491531372 norm:0.0009698017966002226 max memory_allocated 22561.22607421875 
[2025-03-22 03:57:32 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 15 loss:0.49442166090011597 norm:0.0009337298106402159 max memory_allocated 22561.22607421875 
[2025-03-22 03:58:05 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 16 loss:0.4939398765563965 norm:0.0009249320137314498 max memory_allocated 22561.22607421875 
[2025-03-22 03:58:38 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 17 loss:0.4934619963169098 norm:0.0008749966509640217 max memory_allocated 22561.22607421875 
[2025-03-22 03:59:11 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 18 loss:0.49308574199676514 norm:0.0008540450944565237 max memory_allocated 22561.22607421875 
[2025-03-22 03:59:44 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 19 loss:0.4929327070713043 norm:0.0008815201581455767 max memory_allocated 22561.22607421875 
[2025-03-22 03:59:53 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 04:00:29 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 0 loss:0.5344310998916626 norm:0.04234565794467926 max memory_allocated 22561.39794921875 
[2025-03-22 04:01:02 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 1 loss:0.5101687908172607 norm:0.013631781563162804 max memory_allocated 22561.39794921875 
[2025-03-22 04:01:35 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 2 loss:0.49897006154060364 norm:0.006732418201863766 max memory_allocated 22561.39794921875 
[2025-03-22 04:02:08 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 3 loss:0.49202585220336914 norm:0.004711673129349947 max memory_allocated 22561.39794921875 
[2025-03-22 04:02:41 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 4 loss:0.4879056215286255 norm:0.0037700035609304905 max memory_allocated 22561.39794921875 
[2025-03-22 04:03:14 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 5 loss:0.48468446731567383 norm:0.003326384350657463 max memory_allocated 22561.39794921875 
[2025-03-22 04:03:47 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 6 loss:0.48213839530944824 norm:0.0028424167539924383 max memory_allocated 22561.39794921875 
[2025-03-22 04:04:20 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 7 loss:0.4802006483078003 norm:0.0024081701412796974 max memory_allocated 22561.39794921875 
[2025-03-22 04:04:53 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 8 loss:0.47864094376564026 norm:0.002101293532177806 max memory_allocated 22561.39794921875 
[2025-03-22 04:05:26 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 9 loss:0.47709041833877563 norm:0.0017319457838311791 max memory_allocated 22561.39794921875 
[2025-03-22 04:05:59 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 10 loss:0.47557035088539124 norm:0.001378209562972188 max memory_allocated 22561.39794921875 
[2025-03-22 04:06:32 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 11 loss:0.47451892495155334 norm:0.0012410653289407492 max memory_allocated 22561.39794921875 
[2025-03-22 04:07:05 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 12 loss:0.4737341105937958 norm:0.0011566270841285586 max memory_allocated 22561.39794921875 
[2025-03-22 04:07:38 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 13 loss:0.473034143447876 norm:0.0010918518528342247 max memory_allocated 22561.39794921875 
[2025-03-22 04:08:11 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 14 loss:0.47242406010627747 norm:0.0010229034814983606 max memory_allocated 22561.39794921875 
[2025-03-22 04:08:44 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 15 loss:0.47180435061454773 norm:0.0009801741689443588 max memory_allocated 22561.39794921875 
[2025-03-22 04:09:17 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 16 loss:0.47137826681137085 norm:0.0009442794835194945 max memory_allocated 22561.39794921875 
[2025-03-22 04:09:50 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 17 loss:0.4710232615470886 norm:0.0008943325956352055 max memory_allocated 22561.39794921875 
[2025-03-22 04:10:23 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 18 loss:0.4707387685775757 norm:0.0008845159318298101 max memory_allocated 22561.39794921875 
[2025-03-22 04:10:56 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 19 loss:0.47051364183425903 norm:0.0008559269481338561 max memory_allocated 22561.39794921875 
[2025-03-22 04:11:05 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 04:11:42 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 0 loss:0.48069795966148376 norm:0.011862303130328655 max memory_allocated 22561.56982421875 
[2025-03-22 04:12:15 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 1 loss:0.4714237451553345 norm:0.005674379412084818 max memory_allocated 22561.56982421875 
[2025-03-22 04:12:48 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 2 loss:0.46378394961357117 norm:0.003500000573694706 max memory_allocated 22561.56982421875 
[2025-03-22 04:13:21 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 3 loss:0.4588519036769867 norm:0.002558580134063959 max memory_allocated 22561.56982421875 
[2025-03-22 04:13:54 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 4 loss:0.4554072916507721 norm:0.0019695088267326355 max memory_allocated 22561.56982421875 
[2025-03-22 04:14:27 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 5 loss:0.4532826840877533 norm:0.0017662819009274244 max memory_allocated 22561.56982421875 
[2025-03-22 04:15:00 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 6 loss:0.4515874981880188 norm:0.0014794713351875544 max memory_allocated 22561.56982421875 
[2025-03-22 04:15:33 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 7 loss:0.4503442049026489 norm:0.0013429614482447505 max memory_allocated 22561.56982421875 
[2025-03-22 04:16:06 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 8 loss:0.4492465853691101 norm:0.0011234343983232975 max memory_allocated 22561.56982421875 
[2025-03-22 04:16:39 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 9 loss:0.44823718070983887 norm:0.0010003589559346437 max memory_allocated 22561.56982421875 
[2025-03-22 04:17:12 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 10 loss:0.44749540090560913 norm:0.000968620297499001 max memory_allocated 22561.56982421875 
[2025-03-22 04:17:45 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 11 loss:0.44695302844047546 norm:0.0009424600866623223 max memory_allocated 22561.56982421875 
[2025-03-22 04:18:18 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 12 loss:0.4465084671974182 norm:0.0008967395406216383 max memory_allocated 22561.56982421875 
[2025-03-22 04:18:51 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 13 loss:0.4460509717464447 norm:0.000814728089608252 max memory_allocated 22561.56982421875 
[2025-03-22 04:19:24 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 14 loss:0.44566580653190613 norm:0.0008015967323444784 max memory_allocated 22561.56982421875 
[2025-03-22 04:19:57 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 15 loss:0.44521239399909973 norm:0.0007581003592349589 max memory_allocated 22561.56982421875 
[2025-03-22 04:20:30 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 16 loss:0.4448935389518738 norm:0.0007572300964966416 max memory_allocated 22561.56982421875 
[2025-03-22 04:21:03 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 17 loss:0.4447399377822876 norm:0.000764105177950114 max memory_allocated 22561.56982421875 
[2025-03-22 04:21:36 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 18 loss:0.4446846842765808 norm:0.0007643960416316986 max memory_allocated 22561.56982421875 
[2025-03-22 04:22:09 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 19 loss:0.44438236951828003 norm:0.0007438684115186334 max memory_allocated 22561.56982421875 
[2025-03-22 04:22:19 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 04:22:54 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 0 loss:0.4672788679599762 norm:0.021044589579105377 max memory_allocated 22561.74169921875 
[2025-03-22 04:23:27 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 1 loss:0.4564601480960846 norm:0.009226721711456776 max memory_allocated 22561.74169921875 
[2025-03-22 04:24:00 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 2 loss:0.4489772319793701 norm:0.00523675512522459 max memory_allocated 22561.74169921875 
[2025-03-22 04:24:33 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 3 loss:0.4438464343547821 norm:0.0032598921097815037 max memory_allocated 22561.74169921875 
[2025-03-22 04:25:06 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 4 loss:0.4406675398349762 norm:0.002596089616417885 max memory_allocated 22561.74169921875 
[2025-03-22 04:25:39 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 5 loss:0.4384090006351471 norm:0.0021969645749777555 max memory_allocated 22561.74169921875 
[2025-03-22 04:26:12 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 6 loss:0.43690577149391174 norm:0.0018931212835013866 max memory_allocated 22561.74169921875 
[2025-03-22 04:26:45 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 7 loss:0.4354410469532013 norm:0.0016399940941482782 max memory_allocated 22561.74169921875 
[2025-03-22 04:27:18 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 8 loss:0.4344062805175781 norm:0.0014514271169900894 max memory_allocated 22561.74169921875 
[2025-03-22 04:27:52 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 9 loss:0.43347904086112976 norm:0.001322961994446814 max memory_allocated 22561.74169921875 
[2025-03-22 04:28:25 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 10 loss:0.43265050649642944 norm:0.0011957383248955011 max memory_allocated 22561.74169921875 
[2025-03-22 04:28:58 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 11 loss:0.4319463074207306 norm:0.0010977728525176644 max memory_allocated 22561.74169921875 
[2025-03-22 04:29:31 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 12 loss:0.43134406208992004 norm:0.0009896063711494207 max memory_allocated 22561.74169921875 
[2025-03-22 04:30:04 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 13 loss:0.43085765838623047 norm:0.0009915978880599141 max memory_allocated 22561.74169921875 
[2025-03-22 04:30:37 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 14 loss:0.43050116300582886 norm:0.0009531723335385323 max memory_allocated 22561.74169921875 
[2025-03-22 04:31:10 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 15 loss:0.4301503598690033 norm:0.0009269117726944387 max memory_allocated 22561.74169921875 
[2025-03-22 04:31:43 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 16 loss:0.4297577738761902 norm:0.0009102417388930917 max memory_allocated 22561.74169921875 
[2025-03-22 04:32:16 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 17 loss:0.4293937087059021 norm:0.0008365745889022946 max memory_allocated 22561.74169921875 
[2025-03-22 04:32:49 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 18 loss:0.4290963113307953 norm:0.0008168956846930087 max memory_allocated 22561.74169921875 
[2025-03-22 04:33:22 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 19 loss:0.42879965901374817 norm:0.000756759662181139 max memory_allocated 22561.74169921875 
[2025-03-22 04:33:31 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 04:34:07 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 0 loss:0.437416672706604 norm:0.022212764248251915 max memory_allocated 22561.91357421875 
[2025-03-22 04:34:40 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 1 loss:0.42434677481651306 norm:0.010181826539337635 max memory_allocated 22561.91357421875 
[2025-03-22 04:35:13 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 2 loss:0.4149549901485443 norm:0.005073335021734238 max memory_allocated 22561.91357421875 
[2025-03-22 04:35:46 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 3 loss:0.4088076949119568 norm:0.0030116718262434006 max memory_allocated 22561.91357421875 
[2025-03-22 04:36:19 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 4 loss:0.40542319416999817 norm:0.0024167087394744158 max memory_allocated 22561.91357421875 
[2025-03-22 04:36:52 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 5 loss:0.40313786268234253 norm:0.0019741575233638287 max memory_allocated 22561.91357421875 
[2025-03-22 04:37:25 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 6 loss:0.4016250967979431 norm:0.0017743426142260432 max memory_allocated 22561.91357421875 
[2025-03-22 04:37:58 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 7 loss:0.40027445554733276 norm:0.0015580210601910949 max memory_allocated 22561.91357421875 
[2025-03-22 04:38:31 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 8 loss:0.39916202425956726 norm:0.0013632348272949457 max memory_allocated 22561.91357421875 
[2025-03-22 04:39:04 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 9 loss:0.39827683568000793 norm:0.0012261518277227879 max memory_allocated 22561.91357421875 
[2025-03-22 04:39:37 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 10 loss:0.3975975811481476 norm:0.0011297863675281405 max memory_allocated 22561.91357421875 
[2025-03-22 04:40:10 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 11 loss:0.39701321721076965 norm:0.001055256580002606 max memory_allocated 22561.91357421875 
[2025-03-22 04:40:43 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 12 loss:0.39645615220069885 norm:0.0009635415626689792 max memory_allocated 22561.91357421875 
[2025-03-22 04:41:16 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 13 loss:0.3959859311580658 norm:0.0009170427219942212 max memory_allocated 22561.91357421875 
[2025-03-22 04:41:49 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 14 loss:0.39558058977127075 norm:0.0008741207420825958 max memory_allocated 22561.91357421875 
[2025-03-22 04:42:22 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 15 loss:0.3952139616012573 norm:0.0008591380901634693 max memory_allocated 22561.91357421875 
[2025-03-22 04:42:55 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 16 loss:0.39483198523521423 norm:0.000816687592305243 max memory_allocated 22561.91357421875 
[2025-03-22 04:43:28 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 17 loss:0.394597589969635 norm:0.0007849489338696003 max memory_allocated 22561.91357421875 
[2025-03-22 04:44:01 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 18 loss:0.39427343010902405 norm:0.0007354339468292892 max memory_allocated 22561.91357421875 
[2025-03-22 04:44:34 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 19 loss:0.3940029740333557 norm:0.000710669148247689 max memory_allocated 22561.91357421875 
[2025-03-22 04:44:44 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 04:45:19 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 0 loss:0.401605486869812 norm:0.010476896539330482 max memory_allocated 22562.08544921875 
[2025-03-22 04:45:52 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 1 loss:0.3953123390674591 norm:0.004876435734331608 max memory_allocated 22562.08544921875 
[2025-03-22 04:46:25 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 2 loss:0.39002346992492676 norm:0.002894692588597536 max memory_allocated 22562.08544921875 
[2025-03-22 04:46:58 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 3 loss:0.3867756724357605 norm:0.002162439050152898 max memory_allocated 22562.08544921875 
[2025-03-22 04:47:31 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 4 loss:0.3850647807121277 norm:0.0018481953302398324 max memory_allocated 22562.08544921875 
[2025-03-22 04:48:04 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 5 loss:0.3835945129394531 norm:0.0015644183149561286 max memory_allocated 22562.08544921875 
[2025-03-22 04:48:37 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 6 loss:0.3823721706867218 norm:0.001325939316302538 max memory_allocated 22562.08544921875 
[2025-03-22 04:49:10 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 7 loss:0.38163313269615173 norm:0.0011942220153287053 max memory_allocated 22562.08544921875 
[2025-03-22 04:49:43 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 8 loss:0.38087472319602966 norm:0.0010596507927402854 max memory_allocated 22562.08544921875 
[2025-03-22 04:50:16 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 9 loss:0.3803092837333679 norm:0.0009620895143598318 max memory_allocated 22562.08544921875 
[2025-03-22 04:50:50 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 10 loss:0.3797558546066284 norm:0.0008766065584495664 max memory_allocated 22562.08544921875 
[2025-03-22 04:51:23 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 11 loss:0.3792935311794281 norm:0.000834220671094954 max memory_allocated 22562.08544921875 
[2025-03-22 04:51:56 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 12 loss:0.3788103759288788 norm:0.0007655287627130747 max memory_allocated 22562.08544921875 
[2025-03-22 04:52:29 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 13 loss:0.37843602895736694 norm:0.0007385379285551608 max memory_allocated 22562.08544921875 
[2025-03-22 04:53:02 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 14 loss:0.3781062364578247 norm:0.0006873063975945115 max memory_allocated 22562.08544921875 
[2025-03-22 04:53:35 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 15 loss:0.3778456747531891 norm:0.0006623858935199678 max memory_allocated 22562.08544921875 
[2025-03-22 04:54:08 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 16 loss:0.37762030959129333 norm:0.0006195221212692559 max memory_allocated 22562.08544921875 
[2025-03-22 04:54:41 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 17 loss:0.37743210792541504 norm:0.0006047192146070302 max memory_allocated 22562.08544921875 
[2025-03-22 04:55:14 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 18 loss:0.3773311674594879 norm:0.0005929172039031982 max memory_allocated 22562.08544921875 
[2025-03-22 04:55:47 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 19 loss:0.3771417737007141 norm:0.0005676706205122173 max memory_allocated 22562.08544921875 
[2025-03-22 04:55:56 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 04:56:32 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 0 loss:0.38153010606765747 norm:0.013036683201789856 max memory_allocated 22562.25732421875 
[2025-03-22 04:57:05 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 1 loss:0.3741530179977417 norm:0.005906180012971163 max memory_allocated 22562.25732421875 
[2025-03-22 04:57:38 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 2 loss:0.3690680265426636 norm:0.0036060400307178497 max memory_allocated 22562.25732421875 
[2025-03-22 04:58:11 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 3 loss:0.3658781945705414 norm:0.0024241057690232992 max memory_allocated 22562.25732421875 
[2025-03-22 04:58:44 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 4 loss:0.36373794078826904 norm:0.0019627087749540806 max memory_allocated 22562.25732421875 
[2025-03-22 04:59:17 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 5 loss:0.36204802989959717 norm:0.0016407743096351624 max memory_allocated 22562.25732421875 
[2025-03-22 04:59:50 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 6 loss:0.36081641912460327 norm:0.0014080781256780028 max memory_allocated 22562.25732421875 
[2025-03-22 05:00:23 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 7 loss:0.3597871661186218 norm:0.001235493691638112 max memory_allocated 22562.25732421875 
[2025-03-22 05:00:56 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 8 loss:0.35906529426574707 norm:0.0011229532537981868 max memory_allocated 22562.25732421875 
[2025-03-22 05:01:29 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 9 loss:0.35844194889068604 norm:0.001010119216516614 max memory_allocated 22562.25732421875 
[2025-03-22 05:02:02 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 10 loss:0.35788577795028687 norm:0.0009255189215764403 max memory_allocated 22562.25732421875 
[2025-03-22 05:02:35 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 11 loss:0.3574870824813843 norm:0.0008732724818401039 max memory_allocated 22562.25732421875 
[2025-03-22 05:03:08 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 12 loss:0.35709819197654724 norm:0.0008150426438078284 max memory_allocated 22562.25732421875 
[2025-03-22 05:03:41 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 13 loss:0.35680967569351196 norm:0.0007574409828521311 max memory_allocated 22562.25732421875 
[2025-03-22 05:04:14 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 14 loss:0.3564528822898865 norm:0.0007054436719045043 max memory_allocated 22562.25732421875 
[2025-03-22 05:04:47 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 15 loss:0.3561220169067383 norm:0.000669317611027509 max memory_allocated 22562.25732421875 
[2025-03-22 05:05:20 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 16 loss:0.35581904649734497 norm:0.0006293017650023103 max memory_allocated 22562.25732421875 
[2025-03-22 05:05:53 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 17 loss:0.3555563986301422 norm:0.0006086291396059096 max memory_allocated 22562.25732421875 
[2025-03-22 05:06:26 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 18 loss:0.3554094731807709 norm:0.000587917456869036 max memory_allocated 22562.25732421875 
[2025-03-22 05:06:59 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 19 loss:0.3552582263946533 norm:0.0005737354513257742 max memory_allocated 22562.25732421875 
[2025-03-22 05:07:09 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 05:07:45 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 0 loss:0.3763697147369385 norm:0.019038435071706772 max memory_allocated 22562.42919921875 
[2025-03-22 05:08:18 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 1 loss:0.36738577485084534 norm:0.009256919845938683 max memory_allocated 22562.42919921875 
[2025-03-22 05:08:51 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 2 loss:0.35988664627075195 norm:0.004739140626043081 max memory_allocated 22562.42919921875 
[2025-03-22 05:09:24 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 3 loss:0.35588061809539795 norm:0.003182533197104931 max memory_allocated 22562.42919921875 
[2025-03-22 05:09:57 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 4 loss:0.3537861704826355 norm:0.002558989217504859 max memory_allocated 22562.42919921875 
[2025-03-22 05:10:30 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 5 loss:0.35237982869148254 norm:0.00220269663259387 max memory_allocated 22562.42919921875 
[2025-03-22 05:11:03 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 6 loss:0.3511785864830017 norm:0.0018919251160696149 max memory_allocated 22562.42919921875 
[2025-03-22 05:11:36 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 7 loss:0.35024574398994446 norm:0.0016315225511789322 max memory_allocated 22562.42919921875 
[2025-03-22 05:12:09 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 8 loss:0.34946152567863464 norm:0.0014291622210294008 max memory_allocated 22562.42919921875 
[2025-03-22 05:12:42 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 9 loss:0.3487805128097534 norm:0.0012681528460234404 max memory_allocated 22562.42919921875 
[2025-03-22 05:13:15 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 10 loss:0.3482353389263153 norm:0.0011500695254653692 max memory_allocated 22562.42919921875 
[2025-03-22 05:13:48 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 11 loss:0.3476853668689728 norm:0.001043012598529458 max memory_allocated 22562.42919921875 
[2025-03-22 05:14:21 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 12 loss:0.3472355902194977 norm:0.0009680563816800714 max memory_allocated 22562.42919921875 
[2025-03-22 05:14:54 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 13 loss:0.3468642234802246 norm:0.0009104745695367455 max memory_allocated 22562.42919921875 
[2025-03-22 05:15:27 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 14 loss:0.34644815325737 norm:0.0008400126826018095 max memory_allocated 22562.42919921875 
[2025-03-22 05:16:00 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 15 loss:0.34620288014411926 norm:0.0007797128055244684 max memory_allocated 22562.42919921875 
[2025-03-22 05:16:33 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 16 loss:0.3459545373916626 norm:0.000738886883482337 max memory_allocated 22562.42919921875 
[2025-03-22 05:17:06 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 17 loss:0.34567636251449585 norm:0.0007006117375567555 max memory_allocated 22562.42919921875 
[2025-03-22 05:17:39 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 18 loss:0.34550124406814575 norm:0.0006685336120426655 max memory_allocated 22562.42919921875 
[2025-03-22 05:18:12 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 19 loss:0.3452962636947632 norm:0.0006263382965698838 max memory_allocated 22562.42919921875 
[2025-03-22 05:18:21 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 05:18:57 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 0 loss:0.3474658131599426 norm:0.00810276810079813 max memory_allocated 22562.60107421875 
[2025-03-22 05:19:30 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 1 loss:0.3445201814174652 norm:0.005134181585162878 max memory_allocated 22562.60107421875 
[2025-03-22 05:20:03 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 2 loss:0.33947810530662537 norm:0.003240174613893032 max memory_allocated 22562.60107421875 
[2025-03-22 05:20:36 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 3 loss:0.3369367718696594 norm:0.0026856467593461275 max memory_allocated 22562.60107421875 
[2025-03-22 05:21:09 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 4 loss:0.33545947074890137 norm:0.00226771691814065 max memory_allocated 22562.60107421875 
[2025-03-22 05:21:42 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 5 loss:0.3342244327068329 norm:0.0018768530571833253 max memory_allocated 22562.60107421875 
[2025-03-22 05:22:15 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 6 loss:0.333368182182312 norm:0.0015926173655316234 max memory_allocated 22562.60107421875 
[2025-03-22 05:22:48 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 7 loss:0.3326264023780823 norm:0.001383836381137371 max memory_allocated 22562.60107421875 
[2025-03-22 05:23:21 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 8 loss:0.3320944309234619 norm:0.0012552172411233187 max memory_allocated 22562.60107421875 
[2025-03-22 05:23:54 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 9 loss:0.33153069019317627 norm:0.0011133067309856415 max memory_allocated 22562.60107421875 
[2025-03-22 05:24:27 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 10 loss:0.3310762047767639 norm:0.001023963326588273 max memory_allocated 22562.60107421875 
[2025-03-22 05:25:00 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 11 loss:0.33062732219696045 norm:0.0009244079701602459 max memory_allocated 22562.60107421875 
[2025-03-22 05:25:33 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 12 loss:0.3302041292190552 norm:0.0008233610424213111 max memory_allocated 22562.60107421875 
[2025-03-22 05:26:06 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 13 loss:0.3298015594482422 norm:0.0007557362550869584 max memory_allocated 22562.60107421875 
[2025-03-22 05:26:39 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 14 loss:0.32936280965805054 norm:0.0007088272250257432 max memory_allocated 22562.60107421875 
[2025-03-22 05:27:12 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 15 loss:0.3291460871696472 norm:0.0006737676449120045 max memory_allocated 22562.60107421875 
[2025-03-22 05:27:45 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 16 loss:0.3289684057235718 norm:0.0006444145692512393 max memory_allocated 22562.60107421875 
[2025-03-22 05:28:18 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 17 loss:0.32871466875076294 norm:0.0005908945458941162 max memory_allocated 22562.60107421875 
[2025-03-22 05:28:52 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 18 loss:0.3284544348716736 norm:0.0005723914946429431 max memory_allocated 22562.60107421875 
[2025-03-22 05:29:25 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 19 loss:0.3281685411930084 norm:0.0005782948574051261 max memory_allocated 22562.60107421875 
[2025-03-22 05:29:34 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 05:30:10 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 0 loss:0.34594589471817017 norm:0.00757986307144165 max memory_allocated 22562.77294921875 
[2025-03-22 05:30:43 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 1 loss:0.3421439826488495 norm:0.004773770924657583 max memory_allocated 22562.77294921875 
[2025-03-22 05:31:15 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 2 loss:0.3386496305465698 norm:0.0032563102431595325 max memory_allocated 22562.77294921875 
[2025-03-22 05:31:48 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 3 loss:0.3361126184463501 norm:0.002478049835190177 max memory_allocated 22562.77294921875 
[2025-03-22 05:32:22 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 4 loss:0.3345816731452942 norm:0.002046573208644986 max memory_allocated 22562.77294921875 
[2025-03-22 05:32:55 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 5 loss:0.3334689140319824 norm:0.0017259417800232768 max memory_allocated 22562.77294921875 
[2025-03-22 05:33:28 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 6 loss:0.33262938261032104 norm:0.0014528902247548103 max memory_allocated 22562.77294921875 
[2025-03-22 05:34:01 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 7 loss:0.331839382648468 norm:0.0012454316020011902 max memory_allocated 22562.77294921875 
[2025-03-22 05:34:34 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 8 loss:0.33115556836128235 norm:0.001081547001376748 max memory_allocated 22562.77294921875 
[2025-03-22 05:35:07 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 9 loss:0.3305014371871948 norm:0.0009462598245590925 max memory_allocated 22562.77294921875 
[2025-03-22 05:35:40 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 10 loss:0.32993313670158386 norm:0.0008462958503514528 max memory_allocated 22562.77294921875 
[2025-03-22 05:36:13 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 11 loss:0.32946473360061646 norm:0.0007593366899527609 max memory_allocated 22562.77294921875 
[2025-03-22 05:36:46 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 12 loss:0.3290810286998749 norm:0.0006942639593034983 max memory_allocated 22562.77294921875 
[2025-03-22 05:37:19 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 13 loss:0.3287150263786316 norm:0.0006371478666551411 max memory_allocated 22562.77294921875 
[2025-03-22 05:37:52 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 14 loss:0.32836687564849854 norm:0.0005867269937880337 max memory_allocated 22562.77294921875 
[2025-03-22 05:38:25 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 15 loss:0.32807186245918274 norm:0.0005406372365541756 max memory_allocated 22562.77294921875 
[2025-03-22 05:38:58 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 16 loss:0.3278166949748993 norm:0.0005004852428101003 max memory_allocated 22562.77294921875 
[2025-03-22 05:39:31 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 17 loss:0.3276405334472656 norm:0.0004652177740354091 max memory_allocated 22562.77294921875 
[2025-03-22 05:40:04 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 18 loss:0.3274412751197815 norm:0.00045014231000095606 max memory_allocated 22562.77294921875 
[2025-03-22 05:40:37 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 19 loss:0.3272689878940582 norm:0.0004317525017540902 max memory_allocated 22562.77294921875 
[2025-03-22 05:40:46 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 05:41:22 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 0 loss:0.32134580612182617 norm:0.004102449864149094 max memory_allocated 22562.94482421875 
[2025-03-22 05:41:55 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 1 loss:0.3201088309288025 norm:0.0029678225982934237 max memory_allocated 22562.94482421875 
[2025-03-22 05:42:28 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 2 loss:0.31794437766075134 norm:0.002198323141783476 max memory_allocated 22562.94482421875 
[2025-03-22 05:43:01 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 3 loss:0.3157665729522705 norm:0.0017053945921361446 max memory_allocated 22562.94482421875 
[2025-03-22 05:43:34 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 4 loss:0.31427913904190063 norm:0.0013328585773706436 max memory_allocated 22562.94482421875 
[2025-03-22 05:44:07 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 5 loss:0.3133675456047058 norm:0.0011012753238901496 max memory_allocated 22562.94482421875 
[2025-03-22 05:44:40 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 6 loss:0.3125685453414917 norm:0.0009067183127626777 max memory_allocated 22562.94482421875 
[2025-03-22 05:45:13 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 7 loss:0.3118574321269989 norm:0.0007919553900137544 max memory_allocated 22562.94482421875 
[2025-03-22 05:45:46 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 8 loss:0.31112343072891235 norm:0.000693434034474194 max memory_allocated 22562.94482421875 
[2025-03-22 05:46:19 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 9 loss:0.31053364276885986 norm:0.0006179305491968989 max memory_allocated 22562.94482421875 
[2025-03-22 05:46:52 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 10 loss:0.3100927770137787 norm:0.0005550161586143076 max memory_allocated 22562.94482421875 
[2025-03-22 05:47:25 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 11 loss:0.30970320105552673 norm:0.0005003865226171911 max memory_allocated 22562.94482421875 
[2025-03-22 05:47:58 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 12 loss:0.3093455135822296 norm:0.00046018825378268957 max memory_allocated 22562.94482421875 
[2025-03-22 05:48:31 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 13 loss:0.3090744912624359 norm:0.0004279363784007728 max memory_allocated 22562.94482421875 
[2025-03-22 05:49:04 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 14 loss:0.30879324674606323 norm:0.000398810429032892 max memory_allocated 22562.94482421875 
[2025-03-22 05:49:37 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 15 loss:0.3085654377937317 norm:0.0003875942202284932 max memory_allocated 22562.94482421875 
[2025-03-22 05:50:10 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 16 loss:0.30834680795669556 norm:0.000376452982891351 max memory_allocated 22562.94482421875 
[2025-03-22 05:50:43 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 17 loss:0.30817973613739014 norm:0.0003588674298953265 max memory_allocated 22562.94482421875 
[2025-03-22 05:51:16 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 18 loss:0.308011531829834 norm:0.00034693224006332457 max memory_allocated 22562.94482421875 
[2025-03-22 05:51:49 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 19 loss:0.30784550309181213 norm:0.00033507676562294364 max memory_allocated 22562.94482421875 
[2025-03-22 05:51:59 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 05:52:34 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 0 loss:0.31707003712654114 norm:0.004387191031128168 max memory_allocated 22563.11669921875 
[2025-03-22 05:53:07 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 1 loss:0.3158629536628723 norm:0.0033930258359760046 max memory_allocated 22563.11669921875 
[2025-03-22 05:53:40 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 2 loss:0.31407344341278076 norm:0.0026730261743068695 max memory_allocated 22563.11669921875 
[2025-03-22 05:54:13 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 3 loss:0.3123033046722412 norm:0.002198610920459032 max memory_allocated 22563.11669921875 
[2025-03-22 05:54:46 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 4 loss:0.31100326776504517 norm:0.0017919086385518312 max memory_allocated 22563.11669921875 
[2025-03-22 05:55:19 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 5 loss:0.30987775325775146 norm:0.001504367683082819 max memory_allocated 22563.11669921875 
[2025-03-22 05:55:52 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 6 loss:0.3090997040271759 norm:0.0012408765032887459 max memory_allocated 22563.11669921875 
[2025-03-22 05:56:25 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 7 loss:0.3081600069999695 norm:0.0010548681020736694 max memory_allocated 22563.11669921875 
[2025-03-22 05:56:58 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 8 loss:0.3074988126754761 norm:0.00091249227989465 max memory_allocated 22563.11669921875 
[2025-03-22 05:57:31 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 9 loss:0.30681824684143066 norm:0.000785761745646596 max memory_allocated 22563.11669921875 
[2025-03-22 05:58:04 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 10 loss:0.30629268288612366 norm:0.0006868090713396668 max memory_allocated 22563.11669921875 
[2025-03-22 05:58:37 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 11 loss:0.3058375120162964 norm:0.0006166605744510889 max memory_allocated 22563.11669921875 
[2025-03-22 05:59:10 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 12 loss:0.30540451407432556 norm:0.0005532072391360998 max memory_allocated 22563.11669921875 
[2025-03-22 05:59:44 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 13 loss:0.30504176020622253 norm:0.0005014710477553308 max memory_allocated 22563.11669921875 
[2025-03-22 06:00:17 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 14 loss:0.30472397804260254 norm:0.00046266906429082155 max memory_allocated 22563.11669921875 
[2025-03-22 06:00:50 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 15 loss:0.3044514060020447 norm:0.0004262266156729311 max memory_allocated 22563.11669921875 
[2025-03-22 06:01:23 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 16 loss:0.3041805624961853 norm:0.0003969772660639137 max memory_allocated 22563.11669921875 
[2025-03-22 06:01:56 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 17 loss:0.303958535194397 norm:0.0003697394276969135 max memory_allocated 22563.11669921875 
[2025-03-22 06:02:29 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 18 loss:0.30373823642730713 norm:0.00034637757926248014 max memory_allocated 22563.11669921875 
[2025-03-22 06:03:02 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 19 loss:0.30354270339012146 norm:0.0003225139225833118 max memory_allocated 22563.11669921875 
[2025-03-22 06:03:11 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 06:03:47 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 0 loss:0.3168926239013672 norm:0.004194978624582291 max memory_allocated 22563.28857421875 
[2025-03-22 06:04:20 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 1 loss:0.3160075843334198 norm:0.003193652257323265 max memory_allocated 22563.28857421875 
[2025-03-22 06:04:53 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 2 loss:0.3143855035305023 norm:0.002416519680991769 max memory_allocated 22563.28857421875 
[2025-03-22 06:05:26 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 3 loss:0.3125877380371094 norm:0.0018417659448459744 max memory_allocated 22563.28857421875 
[2025-03-22 06:05:59 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 4 loss:0.31133097410202026 norm:0.0014696009457111359 max memory_allocated 22563.28857421875 
[2025-03-22 06:06:32 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 5 loss:0.31051185727119446 norm:0.0012517053401097655 max memory_allocated 22563.28857421875 
[2025-03-22 06:07:05 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 6 loss:0.30974483489990234 norm:0.0010483446530997753 max memory_allocated 22563.28857421875 
[2025-03-22 06:07:38 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 7 loss:0.3090859055519104 norm:0.0008874842897057533 max memory_allocated 22563.28857421875 
[2025-03-22 06:08:11 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 8 loss:0.3084789216518402 norm:0.0007765003829263151 max memory_allocated 22563.28857421875 
[2025-03-22 06:08:44 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 9 loss:0.3078922629356384 norm:0.0006754116038791835 max memory_allocated 22563.28857421875 
[2025-03-22 06:09:17 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 10 loss:0.3073869049549103 norm:0.0005988639313727617 max memory_allocated 22563.28857421875 
[2025-03-22 06:09:50 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 11 loss:0.30696994066238403 norm:0.0005281604826450348 max memory_allocated 22563.28857421875 
[2025-03-22 06:10:23 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 12 loss:0.30659401416778564 norm:0.00047046123654581606 max memory_allocated 22563.28857421875 
[2025-03-22 06:10:56 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 13 loss:0.3062478303909302 norm:0.00042606599163264036 max memory_allocated 22563.28857421875 
[2025-03-22 06:11:29 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 14 loss:0.30597949028015137 norm:0.000387923966627568 max memory_allocated 22563.28857421875 
[2025-03-22 06:12:02 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 15 loss:0.3057050406932831 norm:0.0003614474553614855 max memory_allocated 22563.28857421875 
[2025-03-22 06:12:35 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 16 loss:0.3054734766483307 norm:0.00034317662357352674 max memory_allocated 22563.28857421875 
[2025-03-22 06:13:08 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 17 loss:0.30522868037223816 norm:0.0003301637480035424 max memory_allocated 22563.28857421875 
[2025-03-22 06:13:41 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 18 loss:0.30503618717193604 norm:0.00031069654505699873 max memory_allocated 22563.28857421875 
[2025-03-22 06:14:14 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 19 loss:0.3048703670501709 norm:0.00030353112379089 max memory_allocated 22563.28857421875 
[2025-03-22 06:14:24 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 06:15:00 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 0 loss:0.3227774500846863 norm:0.00474152946844697 max memory_allocated 22563.46044921875 
[2025-03-22 06:15:33 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 1 loss:0.3215296268463135 norm:0.00357789802365005 max memory_allocated 22563.46044921875 
[2025-03-22 06:16:06 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 2 loss:0.319932758808136 norm:0.00280404114164412 max memory_allocated 22563.46044921875 
[2025-03-22 06:16:39 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 3 loss:0.3179515600204468 norm:0.002171832835301757 max memory_allocated 22563.46044921875 
[2025-03-22 06:17:12 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 4 loss:0.3165969252586365 norm:0.0017325765220448375 max memory_allocated 22563.46044921875 
[2025-03-22 06:17:45 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 5 loss:0.315533846616745 norm:0.0014194874092936516 max memory_allocated 22563.46044921875 
[2025-03-22 06:18:18 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 6 loss:0.3146061301231384 norm:0.0011821285588666797 max memory_allocated 22563.46044921875 
[2025-03-22 06:18:51 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 7 loss:0.3137766420841217 norm:0.001008255174383521 max memory_allocated 22563.46044921875 
[2025-03-22 06:19:24 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 8 loss:0.31301847100257874 norm:0.0008665529312565923 max memory_allocated 22563.46044921875 
[2025-03-22 06:19:57 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 9 loss:0.3123849928379059 norm:0.0007374154520221055 max memory_allocated 22563.46044921875 
[2025-03-22 06:20:30 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 10 loss:0.3117603659629822 norm:0.0006297937943600118 max memory_allocated 22563.46044921875 
[2025-03-22 06:21:03 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 11 loss:0.3112080693244934 norm:0.0005554647650569677 max memory_allocated 22563.46044921875 
[2025-03-22 06:21:36 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 12 loss:0.31073805689811707 norm:0.0004887875984422863 max memory_allocated 22563.46044921875 
[2025-03-22 06:22:09 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 13 loss:0.3103407621383667 norm:0.000443796074250713 max memory_allocated 22563.46044921875 
[2025-03-22 06:22:42 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 14 loss:0.3099900186061859 norm:0.00040278478991240263 max memory_allocated 22563.46044921875 
[2025-03-22 06:23:15 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 15 loss:0.3096756935119629 norm:0.000364745530532673 max memory_allocated 22563.46044921875 
[2025-03-22 06:23:48 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 16 loss:0.3094062805175781 norm:0.00033830522443167865 max memory_allocated 22563.46044921875 
[2025-03-22 06:24:21 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 17 loss:0.30917614698410034 norm:0.00032207186450250447 max memory_allocated 22563.46044921875 
[2025-03-22 06:24:54 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 18 loss:0.30900251865386963 norm:0.000307922950014472 max memory_allocated 22563.46044921875 
[2025-03-22 06:25:27 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 19 loss:0.30882686376571655 norm:0.000293418561341241 max memory_allocated 22563.46044921875 
[2025-03-22 06:25:36 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 06:26:12 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 0 loss:0.33293014764785767 norm:0.006015127059072256 max memory_allocated 22563.63232421875 
[2025-03-22 06:26:45 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 1 loss:0.3316481113433838 norm:0.004452715162187815 max memory_allocated 22563.63232421875 
[2025-03-22 06:27:18 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 2 loss:0.3298167586326599 norm:0.0034204781986773014 max memory_allocated 22563.63232421875 
[2025-03-22 06:27:51 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 3 loss:0.327345073223114 norm:0.0027000773698091507 max memory_allocated 22563.63232421875 
[2025-03-22 06:28:24 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 4 loss:0.3249467611312866 norm:0.0021422035060822964 max memory_allocated 22563.63232421875 
[2025-03-22 06:28:57 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 5 loss:0.3232617676258087 norm:0.001754052471369505 max memory_allocated 22563.63232421875 
[2025-03-22 06:29:30 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 6 loss:0.32175013422966003 norm:0.0014541532145813107 max memory_allocated 22563.63232421875 
[2025-03-22 06:30:03 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 7 loss:0.3206639289855957 norm:0.0012311728205531836 max memory_allocated 22563.63232421875 
[2025-03-22 06:30:36 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 8 loss:0.31968897581100464 norm:0.0010701926657930017 max memory_allocated 22563.63232421875 
[2025-03-22 06:31:09 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 9 loss:0.3188585340976715 norm:0.0009416664252057672 max memory_allocated 22563.63232421875 
[2025-03-22 06:31:42 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 10 loss:0.31816375255584717 norm:0.0008334912708960474 max memory_allocated 22563.63232421875 
[2025-03-22 06:32:15 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 11 loss:0.31756114959716797 norm:0.0007207971648313105 max memory_allocated 22563.63232421875 
[2025-03-22 06:32:48 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 12 loss:0.317111998796463 norm:0.0006532418774440885 max memory_allocated 22563.63232421875 
[2025-03-22 06:33:21 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 13 loss:0.31666070222854614 norm:0.0005884259007871151 max memory_allocated 22563.63232421875 
[2025-03-22 06:33:54 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 14 loss:0.31623825430870056 norm:0.0005504839355126023 max memory_allocated 22563.63232421875 
[2025-03-22 06:34:27 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 15 loss:0.31587883830070496 norm:0.0005132652586326003 max memory_allocated 22563.63232421875 
[2025-03-22 06:35:00 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 16 loss:0.31556418538093567 norm:0.0004772469983436167 max memory_allocated 22563.63232421875 
[2025-03-22 06:35:33 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 17 loss:0.31522542238235474 norm:0.0004434009606484324 max memory_allocated 22563.63232421875 
[2025-03-22 06:36:06 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 18 loss:0.3149538040161133 norm:0.0004262231523171067 max memory_allocated 22563.63232421875 
[2025-03-22 06:36:39 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 19 loss:0.3146933317184448 norm:0.0004010751435998827 max memory_allocated 22563.63232421875 
[2025-03-22 06:36:49 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 06:36:51 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 06:37:25 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 0 loss:0.3207733929157257 norm:0.007268848363310099 max memory_allocated 22563.91943359375 
[2025-03-22 06:37:58 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 1 loss:0.31867653131484985 norm:0.007231241557747126 max memory_allocated 22563.91943359375 
[2025-03-22 06:38:31 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 2 loss:0.31716611981391907 norm:0.006293515674769878 max memory_allocated 22563.91943359375 
[2025-03-22 06:39:04 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 3 loss:0.31539425253868103 norm:0.005398490931838751 max memory_allocated 22563.91943359375 
[2025-03-22 06:39:37 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 4 loss:0.3137846887111664 norm:0.004696184303611517 max memory_allocated 22563.91943359375 
[2025-03-22 06:40:10 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 5 loss:0.3124862015247345 norm:0.004256450571119785 max memory_allocated 22563.91943359375 
[2025-03-22 06:40:43 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 6 loss:0.31143224239349365 norm:0.0039071752689778805 max memory_allocated 22563.91943359375 
[2025-03-22 06:41:16 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 7 loss:0.31065282225608826 norm:0.003649204270914197 max memory_allocated 22563.91943359375 
[2025-03-22 06:41:50 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 8 loss:0.3099004626274109 norm:0.003547899890691042 max memory_allocated 22563.91943359375 
[2025-03-22 06:42:23 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 9 loss:0.309212863445282 norm:0.0033442173153162003 max memory_allocated 22563.91943359375 
[2025-03-22 06:42:56 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 10 loss:0.3085419535636902 norm:0.003160054562613368 max memory_allocated 22563.91943359375 
[2025-03-22 06:43:29 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 11 loss:0.30797964334487915 norm:0.0030027958564460278 max memory_allocated 22563.91943359375 
[2025-03-22 06:44:02 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 12 loss:0.30752548575401306 norm:0.0029380256310105324 max memory_allocated 22563.91943359375 
[2025-03-22 06:44:35 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 13 loss:0.30712348222732544 norm:0.0028239069506525993 max memory_allocated 22563.91943359375 
[2025-03-22 06:45:08 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 14 loss:0.3067495822906494 norm:0.0028037638403475285 max memory_allocated 22563.91943359375 
[2025-03-22 06:45:42 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 15 loss:0.30643218755722046 norm:0.0027264540549367666 max memory_allocated 22563.91943359375 
[2025-03-22 06:46:15 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 16 loss:0.30611225962638855 norm:0.002701118588447571 max memory_allocated 22563.91943359375 
[2025-03-22 06:46:48 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 17 loss:0.3058393597602844 norm:0.002600979059934616 max memory_allocated 22563.91943359375 
[2025-03-22 06:47:21 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 18 loss:0.30559444427490234 norm:0.002583705820143223 max memory_allocated 22563.91943359375 
[2025-03-22 06:47:54 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 19 loss:0.3053748607635498 norm:0.0025148559361696243 max memory_allocated 22563.91943359375 
[2025-03-22 06:48:04 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 06:48:07 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 06:48:40 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 0 loss:0.3204458951950073 norm:0.00890640914440155 max memory_allocated 22564.09130859375 
[2025-03-22 06:49:13 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 1 loss:0.31766045093536377 norm:0.007583172060549259 max memory_allocated 22564.09130859375 
[2025-03-22 06:49:46 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 2 loss:0.31513819098472595 norm:0.006231988780200481 max memory_allocated 22564.09130859375 
[2025-03-22 06:50:19 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 3 loss:0.31258317828178406 norm:0.005162401590496302 max memory_allocated 22564.09130859375 
[2025-03-22 06:50:52 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 4 loss:0.3104163110256195 norm:0.004357668571174145 max memory_allocated 22564.09130859375 
[2025-03-22 06:51:25 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 5 loss:0.3088681697845459 norm:0.003843315877020359 max memory_allocated 22564.09130859375 
[2025-03-22 06:51:58 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 6 loss:0.30767905712127686 norm:0.0033658090978860855 max memory_allocated 22564.09130859375 
[2025-03-22 06:52:31 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 7 loss:0.3067205548286438 norm:0.0031502130441367626 max memory_allocated 22564.09130859375 
[2025-03-22 06:53:05 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 8 loss:0.30597665905952454 norm:0.0030046366155147552 max memory_allocated 22564.09130859375 
[2025-03-22 06:53:38 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 9 loss:0.305247038602829 norm:0.0029655955731868744 max memory_allocated 22564.09130859375 
[2025-03-22 06:54:11 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 10 loss:0.30452513694763184 norm:0.0026127472519874573 max memory_allocated 22564.09130859375 
[2025-03-22 06:54:44 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 11 loss:0.3039224147796631 norm:0.002489740028977394 max memory_allocated 22564.09130859375 
[2025-03-22 06:55:17 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 12 loss:0.3034539520740509 norm:0.0023522968403995037 max memory_allocated 22564.09130859375 
[2025-03-22 06:55:50 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 13 loss:0.303022563457489 norm:0.0023627979680895805 max memory_allocated 22564.09130859375 
[2025-03-22 06:56:24 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 14 loss:0.30258211493492126 norm:0.0021673135925084352 max memory_allocated 22564.09130859375 
[2025-03-22 06:56:57 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 15 loss:0.30224597454071045 norm:0.0021547162905335426 max memory_allocated 22564.09130859375 
[2025-03-22 06:57:30 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 16 loss:0.3019644021987915 norm:0.002125842496752739 max memory_allocated 22564.09130859375 
[2025-03-22 06:58:03 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 17 loss:0.30172887444496155 norm:0.0021877638064324856 max memory_allocated 22564.09130859375 
[2025-03-22 06:58:36 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 18 loss:0.3014105558395386 norm:0.0020889125298708677 max memory_allocated 22564.09130859375 
[2025-03-22 06:59:09 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 19 loss:0.3011445701122284 norm:0.002009281888604164 max memory_allocated 22564.09130859375 
[2025-03-22 06:59:19 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 06:59:21 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 06:59:54 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 0 loss:0.3440278172492981 norm:0.011366390623152256 max memory_allocated 22564.26318359375 
[2025-03-22 07:00:28 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 1 loss:0.33784249424934387 norm:0.009717506356537342 max memory_allocated 22564.26318359375 
[2025-03-22 07:01:01 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 2 loss:0.33287563920021057 norm:0.008046821691095829 max memory_allocated 22564.26318359375 
[2025-03-22 07:01:34 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 3 loss:0.3280906677246094 norm:0.006916912272572517 max memory_allocated 22564.26318359375 
[2025-03-22 07:02:07 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 4 loss:0.3244095742702484 norm:0.005997948348522186 max memory_allocated 22564.26318359375 
[2025-03-22 07:02:40 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 5 loss:0.32208478450775146 norm:0.005245222244411707 max memory_allocated 22564.26318359375 
[2025-03-22 07:03:13 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 6 loss:0.32055553793907166 norm:0.004704420920461416 max memory_allocated 22564.26318359375 
[2025-03-22 07:03:46 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 7 loss:0.3194097578525543 norm:0.0045014359056949615 max memory_allocated 22564.26318359375 
[2025-03-22 07:04:19 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 8 loss:0.318616658449173 norm:0.004721308592706919 max memory_allocated 22564.26318359375 
[2025-03-22 07:04:52 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 9 loss:0.317918062210083 norm:0.004706294275820255 max memory_allocated 22564.26318359375 
[2025-03-22 07:05:25 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 10 loss:0.3172183036804199 norm:0.004512186627835035 max memory_allocated 22564.26318359375 
[2025-03-22 07:05:59 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 11 loss:0.3167761564254761 norm:0.0045645348727703094 max memory_allocated 22564.26318359375 
[2025-03-22 07:06:32 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 12 loss:0.31599488854408264 norm:0.0042903851717710495 max memory_allocated 22564.26318359375 
[2025-03-22 07:07:05 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 13 loss:0.31525254249572754 norm:0.0036886294838041067 max memory_allocated 22564.26318359375 
[2025-03-22 07:07:38 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 14 loss:0.31509554386138916 norm:0.003948194906115532 max memory_allocated 22564.26318359375 
[2025-03-22 07:08:11 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 15 loss:0.3150131404399872 norm:0.004212288651615381 max memory_allocated 22564.26318359375 
[2025-03-22 07:08:44 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 16 loss:0.31456243991851807 norm:0.0039938692934811115 max memory_allocated 22564.26318359375 
[2025-03-22 07:09:18 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 17 loss:0.31399765610694885 norm:0.0034568975679576397 max memory_allocated 22564.26318359375 
[2025-03-22 07:09:51 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 18 loss:0.31380221247673035 norm:0.0035086385905742645 max memory_allocated 22564.26318359375 
[2025-03-22 07:10:24 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 19 loss:0.31381863355636597 norm:0.0037315969821065664 max memory_allocated 22564.26318359375 
[2025-03-22 07:10:33 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 07:10:36 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 07:11:09 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 0 loss:0.32244420051574707 norm:0.014121637679636478 max memory_allocated 22564.43505859375 
[2025-03-22 07:11:42 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 1 loss:0.3102932870388031 norm:0.011888371780514717 max memory_allocated 22564.43505859375 
[2025-03-22 07:12:15 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 2 loss:0.3017480969429016 norm:0.010676656849682331 max memory_allocated 22564.43505859375 
[2025-03-22 07:12:48 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 3 loss:0.2944168746471405 norm:0.010376245714724064 max memory_allocated 22564.43505859375 
[2025-03-22 07:13:21 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 4 loss:0.2903139293193817 norm:0.009621864184737206 max memory_allocated 22564.43505859375 
[2025-03-22 07:13:54 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 5 loss:0.2878063917160034 norm:0.00932428240776062 max memory_allocated 22564.43505859375 
[2025-03-22 07:14:27 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 6 loss:0.2859402000904083 norm:0.008785485289990902 max memory_allocated 22564.43505859375 
[2025-03-22 07:15:00 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 7 loss:0.28431546688079834 norm:0.0084089869633317 max memory_allocated 22564.43505859375 
[2025-03-22 07:15:33 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 8 loss:0.2830382287502289 norm:0.007971026003360748 max memory_allocated 22564.43505859375 
[2025-03-22 07:16:06 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 9 loss:0.28186532855033875 norm:0.007674900349229574 max memory_allocated 22564.43505859375 
[2025-03-22 07:16:40 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 10 loss:0.28097105026245117 norm:0.007377662695944309 max memory_allocated 22564.43505859375 
[2025-03-22 07:17:13 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 11 loss:0.2803778648376465 norm:0.007442518137395382 max memory_allocated 22564.43505859375 
[2025-03-22 07:17:46 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 12 loss:0.27951300144195557 norm:0.007838443852961063 max memory_allocated 22564.43505859375 
[2025-03-22 07:18:19 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 13 loss:0.27873653173446655 norm:0.007554410956799984 max memory_allocated 22564.43505859375 
[2025-03-22 07:18:52 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 14 loss:0.2785358130931854 norm:0.007923362776637077 max memory_allocated 22564.43505859375 
[2025-03-22 07:19:25 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 15 loss:0.2779400944709778 norm:0.007693895138800144 max memory_allocated 22564.43505859375 
[2025-03-22 07:19:58 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 16 loss:0.27761900424957275 norm:0.00763295590877533 max memory_allocated 22564.43505859375 
[2025-03-22 07:20:32 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 17 loss:0.27739304304122925 norm:0.007644123397767544 max memory_allocated 22564.43505859375 
[2025-03-22 07:21:06 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 18 loss:0.2768910229206085 norm:0.007197328843176365 max memory_allocated 22564.43505859375 
[2025-03-22 07:21:39 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 19 loss:0.27647650241851807 norm:0.006944452878087759 max memory_allocated 22564.43505859375 
[2025-03-22 07:21:49 root] (main_calibration_a2.py 370): INFO 21531.419478416443
[2025-03-22 07:21:54 root] (main_calibration_a2.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-22 07:23:05 root] (main_calibration_a2.py 158): INFO wikitext2 : 20.827001571655273
[2025-03-22 07:23:05 root] (main_calibration_a2.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-22 07:24:54 root] (main_calibration_a2.py 158): INFO c4 : 30.32278823852539
[2025-03-22 07:25:04 datasets.load] (load.py 1272): WARNING Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/piqa/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011 (last modified on Tue Feb 18 03:13:08 2025) since it couldn't be found locally at piqa., or remotely on the Hugging Face Hub.
[2025-03-22 09:03:59 root] (main_calibration_a2.py 169): INFO {'wikitext2': 20.827001571655273, 'c4': 30.32278823852539, 'results': {'piqa': {'acc': 0.6664853101196954, 'acc_stderr': 0.01100013959218457, 'acc_norm': 0.6621327529923831, 'acc_norm_stderr': 0.011035474307853841}, 'winogrande': {'acc': 0.5232833464877664, 'acc_stderr': 0.014037241309573643}, 'hellaswag': {'acc': 0.3959370643298148, 'acc_stderr': 0.004880515431323156, 'acc_norm': 0.5095598486357299, 'acc_norm_stderr': 0.004988869288786874}, 'arc_easy': {'acc': 0.4936868686868687, 'acc_stderr': 0.010258965668044436, 'acc_norm': 0.4212962962962963, 'acc_norm_stderr': 0.010131882498193141}, 'arc_challenge': {'acc': 0.29266211604095566, 'acc_stderr': 0.013295916103619417, 'acc_norm': 0.3148464163822526, 'acc_norm_stderr': 0.013572657703084948}, 'boolq': {'acc': 0.5801223241590214, 'acc_stderr': 0.00863204550478175}}, 'versions': {'piqa': 0, 'winogrande': 0, 'hellaswag': 0, 'arc_easy': 0, 'arc_challenge': 0, 'boolq': 1}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 09:03:59 root] (main_calibration_a2.py 172): INFO 29.27,49.37,58.01,39.59,66.65,52.33
