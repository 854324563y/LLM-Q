[2025-03-22 01:18:31 root] (main_calibration_a2.py 274): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-loss/llama-7b-hf-w4a4-mse', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=1, loss_type='mse')
[2025-03-22 01:22:58 root] (main_calibration_a2.py 341): INFO === start quantization ===
[2025-03-22 01:22:58 root] (main_calibration_a2.py 347): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-22 01:22:58 root] (abq_llm_calibration_a2.py 62): INFO Starting ...
[2025-03-22 01:23:01 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 01:23:05 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 01:23:37 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 0 loss:0.0003899899893440306 norm:0.00021027700859121978 max memory_allocated 22559.10595703125 
[2025-03-22 01:24:10 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 1 loss:0.00026812858413904905 norm:0.0001414209691574797 max memory_allocated 22559.10595703125 
[2025-03-22 01:24:42 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 2 loss:0.00023000575311016291 norm:0.00010563885007286444 max memory_allocated 22559.10595703125 
[2025-03-22 01:25:15 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 3 loss:0.00021100559388287365 norm:9.010245412355289e-05 max memory_allocated 22559.10595703125 
[2025-03-22 01:25:48 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 4 loss:0.00020242150640115142 norm:8.280160545837134e-05 max memory_allocated 22559.10595703125 
[2025-03-22 01:26:20 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 5 loss:0.00019624562992248684 norm:7.237712270580232e-05 max memory_allocated 22559.10595703125 
[2025-03-22 01:26:53 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 6 loss:0.00019391532987356186 norm:6.541718175867572e-05 max memory_allocated 22559.10595703125 
[2025-03-22 01:27:25 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 7 loss:0.0001921438961289823 norm:6.0168724303366616e-05 max memory_allocated 22559.10595703125 
[2025-03-22 01:27:58 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 8 loss:0.00018868570623453707 norm:5.4238855227595195e-05 max memory_allocated 22559.10595703125 
[2025-03-22 01:28:30 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 9 loss:0.0001865188532974571 norm:4.9594662414165214e-05 max memory_allocated 22559.10595703125 
[2025-03-22 01:29:03 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 10 loss:0.00018362834816798568 norm:4.571156750898808e-05 max memory_allocated 22559.10595703125 
[2025-03-22 01:29:35 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 11 loss:0.00018189739785157144 norm:4.226796590955928e-05 max memory_allocated 22559.10595703125 
[2025-03-22 01:30:08 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 12 loss:0.00018087336502503604 norm:3.957225271733478e-05 max memory_allocated 22559.10595703125 
[2025-03-22 01:30:41 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 13 loss:0.00018022386939264834 norm:3.598886542022228e-05 max memory_allocated 22559.10595703125 
[2025-03-22 01:31:13 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 14 loss:0.00017879123333841562 norm:3.2965261198114604e-05 max memory_allocated 22559.10595703125 
[2025-03-22 01:31:46 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 15 loss:0.00017830017895903438 norm:3.0839128157822415e-05 max memory_allocated 22559.10595703125 
[2025-03-22 01:32:18 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 16 loss:0.00017775921151041985 norm:2.8965212550247088e-05 max memory_allocated 22559.10595703125 
[2025-03-22 01:32:51 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 17 loss:0.00017777424363885075 norm:2.717798633966595e-05 max memory_allocated 22559.10595703125 
[2025-03-22 01:33:23 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 18 loss:0.00017800992645788938 norm:2.549125383666251e-05 max memory_allocated 22559.10595703125 
[2025-03-22 01:33:56 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 19 loss:0.0001793661795090884 norm:2.4039252821239643e-05 max memory_allocated 22559.10595703125 
[2025-03-22 01:34:05 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 01:34:08 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 01:34:40 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 0 loss:0.0024163469206541777 norm:0.0011083876015618443 max memory_allocated 22559.27783203125 
[2025-03-22 01:35:13 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 1 loss:0.0015796732623130083 norm:0.000366290973033756 max memory_allocated 22559.27783203125 
[2025-03-22 01:35:46 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 2 loss:0.0012681230437010527 norm:0.0002047727903118357 max memory_allocated 22559.27783203125 
[2025-03-22 01:36:18 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 3 loss:0.0011464200215414166 norm:0.0001745912741171196 max memory_allocated 22559.27783203125 
[2025-03-22 01:36:51 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 4 loss:0.001075102831237018 norm:0.00014160614227876067 max memory_allocated 22559.27783203125 
[2025-03-22 01:37:23 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 5 loss:0.001037081703543663 norm:0.00012683880049735308 max memory_allocated 22559.27783203125 
[2025-03-22 01:37:56 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 6 loss:0.0010127690620720387 norm:0.00011620036093518138 max memory_allocated 22559.27783203125 
[2025-03-22 01:38:29 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 7 loss:0.0009955872083082795 norm:0.00010727694461820647 max memory_allocated 22559.27783203125 
[2025-03-22 01:39:01 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 8 loss:0.0009848736226558685 norm:9.916070121107623e-05 max memory_allocated 22559.27783203125 
[2025-03-22 01:39:34 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 9 loss:0.0009765417780727148 norm:8.963337313616648e-05 max memory_allocated 22559.27783203125 
[2025-03-22 01:40:06 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 10 loss:0.0009701944654807448 norm:8.054272620938718e-05 max memory_allocated 22559.27783203125 
[2025-03-22 01:40:39 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 11 loss:0.000964276259765029 norm:7.300746801774949e-05 max memory_allocated 22559.27783203125 
[2025-03-22 01:41:12 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 12 loss:0.0009595690062269568 norm:6.757875235052779e-05 max memory_allocated 22559.27783203125 
[2025-03-22 01:41:44 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 13 loss:0.0009569288813509047 norm:6.22539155301638e-05 max memory_allocated 22559.27783203125 
[2025-03-22 01:42:17 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 14 loss:0.0009543841588310897 norm:5.784691893495619e-05 max memory_allocated 22559.27783203125 
[2025-03-22 01:42:50 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 15 loss:0.0009529877570457757 norm:5.618244904326275e-05 max memory_allocated 22559.27783203125 
[2025-03-22 01:43:22 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 16 loss:0.0009510496165603399 norm:5.3461080824490637e-05 max memory_allocated 22559.27783203125 
[2025-03-22 01:43:55 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 17 loss:0.0009505602065473795 norm:5.0271111831534654e-05 max memory_allocated 22559.27783203125 
[2025-03-22 01:44:27 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 18 loss:0.0009489998919889331 norm:4.938383062835783e-05 max memory_allocated 22559.27783203125 
[2025-03-22 01:45:00 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 19 loss:0.0009490425582043827 norm:4.864832226303406e-05 max memory_allocated 22559.27783203125 
[2025-03-22 01:45:09 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 01:45:12 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 01:45:45 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 0 loss:0.18559454381465912 norm:0.047714103013277054 max memory_allocated 22559.44970703125 
[2025-03-22 01:46:17 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 1 loss:0.06377293914556503 norm:0.023700714111328125 max memory_allocated 22559.44970703125 
[2025-03-22 01:46:50 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 2 loss:0.031613871455192566 norm:0.015300629660487175 max memory_allocated 22559.44970703125 
[2025-03-22 01:47:23 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 3 loss:0.023919885978102684 norm:0.012538302689790726 max memory_allocated 22559.44970703125 
[2025-03-22 01:47:55 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 4 loss:0.021420679986476898 norm:0.01093776524066925 max memory_allocated 22559.44970703125 
[2025-03-22 01:48:28 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 5 loss:0.020179029554128647 norm:0.010664735920727253 max memory_allocated 22559.44970703125 
[2025-03-22 01:49:01 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 6 loss:0.01873531937599182 norm:0.009688571095466614 max memory_allocated 22559.44970703125 
[2025-03-22 01:49:33 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 7 loss:0.017292356118559837 norm:0.008983450941741467 max memory_allocated 22559.44970703125 
[2025-03-22 01:50:06 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 8 loss:0.016162484884262085 norm:0.007516082376241684 max memory_allocated 22559.44970703125 
[2025-03-22 01:50:39 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 9 loss:0.015824034810066223 norm:0.007259450852870941 max memory_allocated 22559.44970703125 
[2025-03-22 01:51:12 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 10 loss:0.015875250101089478 norm:0.007260309532284737 max memory_allocated 22559.44970703125 
[2025-03-22 01:51:44 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 11 loss:0.014803420752286911 norm:0.005855874624103308 max memory_allocated 22559.44970703125 
[2025-03-22 01:52:17 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 12 loss:0.014431887306272984 norm:0.005954205058515072 max memory_allocated 22559.44970703125 
[2025-03-22 01:52:50 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 13 loss:0.014318511821329594 norm:0.005561725702136755 max memory_allocated 22559.44970703125 
[2025-03-22 01:53:23 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 14 loss:0.014381029643118382 norm:0.005759155843406916 max memory_allocated 22559.44970703125 
[2025-03-22 01:53:55 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 15 loss:0.014159445650875568 norm:0.005653635133057833 max memory_allocated 22559.44970703125 
[2025-03-22 01:54:28 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 16 loss:0.014072312042117119 norm:0.005741145461797714 max memory_allocated 22559.44970703125 
[2025-03-22 01:55:01 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 17 loss:0.013716783374547958 norm:0.005084414966404438 max memory_allocated 22559.44970703125 
[2025-03-22 01:55:34 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 18 loss:0.013546418398618698 norm:0.004775509238243103 max memory_allocated 22559.44970703125 
[2025-03-22 01:56:06 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 19 loss:0.013980783522129059 norm:0.005209199618548155 max memory_allocated 22559.44970703125 
[2025-03-22 01:56:15 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 01:56:51 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 0 loss:0.014187813736498356 norm:0.0012039883295074105 max memory_allocated 22559.50634765625 
[2025-03-22 01:57:23 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 1 loss:0.013363317586481571 norm:0.0003798783291131258 max memory_allocated 22559.50634765625 
[2025-03-22 01:57:56 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 2 loss:0.012849821709096432 norm:0.00015910575166344643 max memory_allocated 22559.50634765625 
[2025-03-22 01:58:29 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 3 loss:0.012612501159310341 norm:0.00010077218757942319 max memory_allocated 22559.50634765625 
[2025-03-22 01:59:01 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 4 loss:0.012476685456931591 norm:7.100656512193382e-05 max memory_allocated 22559.50634765625 
[2025-03-22 01:59:34 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 5 loss:0.012396291829645634 norm:6.041368760634214e-05 max memory_allocated 22559.50634765625 
[2025-03-22 02:00:06 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 6 loss:0.012353077530860901 norm:5.42459383723326e-05 max memory_allocated 22559.50634765625 
[2025-03-22 02:00:39 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 7 loss:0.012314781546592712 norm:4.9264999688602984e-05 max memory_allocated 22559.50634765625 
[2025-03-22 02:01:12 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 8 loss:0.012290898710489273 norm:4.8635356506565586e-05 max memory_allocated 22559.50634765625 
[2025-03-22 02:01:44 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 9 loss:0.012262634932994843 norm:4.5333756133913994e-05 max memory_allocated 22559.50634765625 
[2025-03-22 02:02:17 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 10 loss:0.012226110324263573 norm:4.1817158489720896e-05 max memory_allocated 22559.50634765625 
[2025-03-22 02:02:50 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 11 loss:0.012200124561786652 norm:3.922388350474648e-05 max memory_allocated 22559.50634765625 
[2025-03-22 02:03:22 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 12 loss:0.012175334617495537 norm:3.5772933188127354e-05 max memory_allocated 22559.50634765625 
[2025-03-22 02:03:55 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 13 loss:0.012160519137978554 norm:3.5245713661424816e-05 max memory_allocated 22559.50634765625 
[2025-03-22 02:04:28 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 14 loss:0.012151917442679405 norm:3.492063842713833e-05 max memory_allocated 22559.50634765625 
[2025-03-22 02:05:00 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 15 loss:0.012152479030191898 norm:3.386752723599784e-05 max memory_allocated 22559.50634765625 
[2025-03-22 02:05:33 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 16 loss:0.012152319774031639 norm:3.201845538569614e-05 max memory_allocated 22559.50634765625 
[2025-03-22 02:06:06 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 17 loss:0.012151593342423439 norm:3.183622902724892e-05 max memory_allocated 22559.50634765625 
[2025-03-22 02:06:38 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 18 loss:0.012143037281930447 norm:3.1879353628028184e-05 max memory_allocated 22559.50634765625 
[2025-03-22 02:07:11 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 19 loss:0.012152140960097313 norm:3.348394966451451e-05 max memory_allocated 22559.50634765625 
[2025-03-22 02:07:20 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 02:07:55 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 0 loss:0.01920006051659584 norm:0.0017083053244277835 max memory_allocated 22559.67822265625 
[2025-03-22 02:08:28 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 1 loss:0.01795046404004097 norm:0.0006914311088621616 max memory_allocated 22559.67822265625 
[2025-03-22 02:09:01 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 2 loss:0.017007291316986084 norm:0.00030745609547011554 max memory_allocated 22559.67822265625 
[2025-03-22 02:09:33 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 3 loss:0.016229264438152313 norm:0.00020139914704486728 max memory_allocated 22559.67822265625 
[2025-03-22 02:10:06 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 4 loss:0.015753595158457756 norm:0.00021169925457797945 max memory_allocated 22559.67822265625 
[2025-03-22 02:10:39 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 5 loss:0.01550430990755558 norm:0.00015885091852396727 max memory_allocated 22559.67822265625 
[2025-03-22 02:11:11 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 6 loss:0.01535939984023571 norm:0.00012306209828238934 max memory_allocated 22559.67822265625 
[2025-03-22 02:11:44 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 7 loss:0.015246627852320671 norm:9.381488780491054e-05 max memory_allocated 22559.67822265625 
[2025-03-22 02:12:16 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 8 loss:0.015156598761677742 norm:7.235220255097374e-05 max memory_allocated 22559.67822265625 
[2025-03-22 02:12:49 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 9 loss:0.015110846608877182 norm:6.22469960944727e-05 max memory_allocated 22559.67822265625 
[2025-03-22 02:13:22 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 10 loss:0.01509814988821745 norm:5.9384699852671474e-05 max memory_allocated 22559.67822265625 
[2025-03-22 02:13:54 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 11 loss:0.015070509165525436 norm:5.381487790145911e-05 max memory_allocated 22559.67822265625 
[2025-03-22 02:14:27 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 12 loss:0.015071626752614975 norm:5.5621112551307306e-05 max memory_allocated 22559.67822265625 
[2025-03-22 02:15:00 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 13 loss:0.015046640299260616 norm:4.948419154970907e-05 max memory_allocated 22559.67822265625 
[2025-03-22 02:15:32 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 14 loss:0.01506059430539608 norm:5.203405817155726e-05 max memory_allocated 22559.67822265625 
[2025-03-22 02:16:05 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 15 loss:0.015063184313476086 norm:5.313558358466253e-05 max memory_allocated 22559.67822265625 
[2025-03-22 02:16:38 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 16 loss:0.015040548518300056 norm:4.877037281403318e-05 max memory_allocated 22559.67822265625 
[2025-03-22 02:17:10 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 17 loss:0.015032539144158363 norm:4.8660338507033885e-05 max memory_allocated 22559.67822265625 
[2025-03-22 02:17:43 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 18 loss:0.015031445771455765 norm:4.9027377826860175e-05 max memory_allocated 22559.67822265625 
[2025-03-22 02:18:16 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 19 loss:0.01503005251288414 norm:4.6629113057861105e-05 max memory_allocated 22559.67822265625 
[2025-03-22 02:18:25 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 02:19:00 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 0 loss:0.02660261280834675 norm:0.003957212436944246 max memory_allocated 22559.85009765625 
[2025-03-22 02:19:33 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 1 loss:0.024310382083058357 norm:0.0014904175186529756 max memory_allocated 22559.85009765625 
[2025-03-22 02:20:06 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 2 loss:0.022224340587854385 norm:0.0004918563063256443 max memory_allocated 22559.85009765625 
[2025-03-22 02:20:38 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 3 loss:0.02127295732498169 norm:0.0002379373472649604 max memory_allocated 22559.85009765625 
[2025-03-22 02:21:11 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 4 loss:0.0208204984664917 norm:0.00018162481137551367 max memory_allocated 22559.85009765625 
[2025-03-22 02:21:44 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 5 loss:0.020572151988744736 norm:0.0001484991516917944 max memory_allocated 22559.85009765625 
[2025-03-22 02:22:16 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 6 loss:0.02042914181947708 norm:0.00012486136984080076 max memory_allocated 22559.85009765625 
[2025-03-22 02:22:49 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 7 loss:0.02036423794925213 norm:0.00011469141463749111 max memory_allocated 22559.85009765625 
[2025-03-22 02:23:22 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 8 loss:0.02030213549733162 norm:9.445798059459776e-05 max memory_allocated 22559.85009765625 
[2025-03-22 02:23:54 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 9 loss:0.020273705944418907 norm:8.920262916944921e-05 max memory_allocated 22559.85009765625 
[2025-03-22 02:24:27 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 10 loss:0.020242255181074142 norm:8.638361759949476e-05 max memory_allocated 22559.85009765625 
[2025-03-22 02:25:00 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 11 loss:0.02021338790655136 norm:7.973207539180294e-05 max memory_allocated 22559.85009765625 
[2025-03-22 02:25:32 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 12 loss:0.020172573626041412 norm:7.795832061674446e-05 max memory_allocated 22559.85009765625 
[2025-03-22 02:26:05 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 13 loss:0.020148223266005516 norm:7.603121048305184e-05 max memory_allocated 22559.85009765625 
[2025-03-22 02:26:38 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 14 loss:0.020105712115764618 norm:7.290941721294075e-05 max memory_allocated 22559.85009765625 
[2025-03-22 02:27:10 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 15 loss:0.020085014402866364 norm:7.293662201846018e-05 max memory_allocated 22559.85009765625 
[2025-03-22 02:27:43 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 16 loss:0.020042773336172104 norm:6.862505688332021e-05 max memory_allocated 22559.85009765625 
[2025-03-22 02:28:16 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 17 loss:0.020038578659296036 norm:6.666200351901352e-05 max memory_allocated 22559.85009765625 
[2025-03-22 02:28:48 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 18 loss:0.020022060722112656 norm:6.323457637336105e-05 max memory_allocated 22559.85009765625 
[2025-03-22 02:29:21 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 19 loss:0.02001931332051754 norm:6.270469020819291e-05 max memory_allocated 22559.85009765625 
[2025-03-22 02:29:30 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 02:30:06 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 0 loss:0.035050563514232635 norm:0.0044338032603263855 max memory_allocated 22560.02197265625 
[2025-03-22 02:30:38 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 1 loss:0.03279339522123337 norm:0.0020858505740761757 max memory_allocated 22560.02197265625 
[2025-03-22 02:31:11 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 2 loss:0.029875105246901512 norm:0.0006312277400866151 max memory_allocated 22560.02197265625 
[2025-03-22 02:31:44 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 3 loss:0.028365733101963997 norm:0.00025211722822859883 max memory_allocated 22560.02197265625 
[2025-03-22 02:32:16 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 4 loss:0.027737388387322426 norm:0.0001781460305210203 max memory_allocated 22560.02197265625 
[2025-03-22 02:32:49 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 5 loss:0.027498647570610046 norm:0.0001530020235804841 max memory_allocated 22560.02197265625 
[2025-03-22 02:33:22 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 6 loss:0.027393857017159462 norm:0.00014653566177003086 max memory_allocated 22560.02197265625 
[2025-03-22 02:33:54 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 7 loss:0.02729484811425209 norm:0.0001327321951976046 max memory_allocated 22560.02197265625 
[2025-03-22 02:34:27 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 8 loss:0.027221936732530594 norm:0.0001248731423402205 max memory_allocated 22560.02197265625 
[2025-03-22 02:35:00 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 9 loss:0.027169208973646164 norm:0.00011819196515716612 max memory_allocated 22560.02197265625 
[2025-03-22 02:35:32 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 10 loss:0.02710750885307789 norm:0.00011296033335383981 max memory_allocated 22560.02197265625 
[2025-03-22 02:36:05 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 11 loss:0.02704508975148201 norm:0.00010280400601914153 max memory_allocated 22560.02197265625 
[2025-03-22 02:36:38 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 12 loss:0.027016766369342804 norm:0.00010262879368383437 max memory_allocated 22560.02197265625 
[2025-03-22 02:37:10 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 13 loss:0.02700687386095524 norm:9.771438635652885e-05 max memory_allocated 22560.02197265625 
[2025-03-22 02:37:43 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 14 loss:0.02697443962097168 norm:9.873984527075663e-05 max memory_allocated 22560.02197265625 
[2025-03-22 02:38:16 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 15 loss:0.026927251368761063 norm:9.450403740629554e-05 max memory_allocated 22560.02197265625 
[2025-03-22 02:38:49 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 16 loss:0.026869378983974457 norm:8.439511293545365e-05 max memory_allocated 22560.02197265625 
[2025-03-22 02:39:21 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 17 loss:0.026844121515750885 norm:8.419508230872452e-05 max memory_allocated 22560.02197265625 
[2025-03-22 02:39:54 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 18 loss:0.026821332052350044 norm:8.522671123500913e-05 max memory_allocated 22560.02197265625 
[2025-03-22 02:40:27 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 19 loss:0.026819031685590744 norm:8.147204061970115e-05 max memory_allocated 22560.02197265625 
[2025-03-22 02:40:36 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 02:41:11 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 0 loss:0.04696788638830185 norm:0.00376433739438653 max memory_allocated 22560.19384765625 
[2025-03-22 02:41:44 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 1 loss:0.04302782192826271 norm:0.0016182259423658252 max memory_allocated 22560.19384765625 
[2025-03-22 02:42:17 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 2 loss:0.03915676102042198 norm:0.0005594381364062428 max memory_allocated 22560.19384765625 
[2025-03-22 02:42:49 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 3 loss:0.037405598908662796 norm:0.0002619161386974156 max memory_allocated 22560.19384765625 
[2025-03-22 02:43:22 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 4 loss:0.0365840345621109 norm:0.00018205858941655606 max memory_allocated 22560.19384765625 
[2025-03-22 02:43:55 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 5 loss:0.03627970814704895 norm:0.0001548517175251618 max memory_allocated 22560.19384765625 
[2025-03-22 02:44:27 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 6 loss:0.03611864894628525 norm:0.00013883996871300042 max memory_allocated 22560.19384765625 
[2025-03-22 02:45:00 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 7 loss:0.03600838780403137 norm:0.00013190381287131459 max memory_allocated 22560.19384765625 
[2025-03-22 02:45:33 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 8 loss:0.03591330721974373 norm:0.00012784557475242764 max memory_allocated 22560.19384765625 
[2025-03-22 02:46:05 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 9 loss:0.03584299236536026 norm:0.00012548577797133476 max memory_allocated 22560.19384765625 
[2025-03-22 02:46:38 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 10 loss:0.03574109449982643 norm:0.00011337726027704775 max memory_allocated 22560.19384765625 
[2025-03-22 02:47:11 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 11 loss:0.03568565100431442 norm:0.0001055399770848453 max memory_allocated 22560.19384765625 
[2025-03-22 02:47:44 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 12 loss:0.035656772553920746 norm:0.00010784275946207345 max memory_allocated 22560.19384765625 
[2025-03-22 02:48:16 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 13 loss:0.0356130376458168 norm:0.00010468278924236074 max memory_allocated 22560.19384765625 
[2025-03-22 02:48:49 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 14 loss:0.03560243546962738 norm:0.00010604463022900745 max memory_allocated 22560.19384765625 
[2025-03-22 02:49:22 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 15 loss:0.0356237068772316 norm:0.00010835824650712311 max memory_allocated 22560.19384765625 
[2025-03-22 02:49:54 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 16 loss:0.03557183966040611 norm:0.00010281530558131635 max memory_allocated 22560.19384765625 
[2025-03-22 02:50:27 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 17 loss:0.035521794110536575 norm:0.00010841563198482618 max memory_allocated 22560.19384765625 
[2025-03-22 02:51:00 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 18 loss:0.03548828884959221 norm:0.00010567980643827468 max memory_allocated 22560.19384765625 
[2025-03-22 02:51:32 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 19 loss:0.03546353429555893 norm:9.880645666271448e-05 max memory_allocated 22560.19384765625 
[2025-03-22 02:51:42 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 02:52:17 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 0 loss:0.05791422724723816 norm:0.003309240797534585 max memory_allocated 22560.36572265625 
[2025-03-22 02:52:50 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 1 loss:0.05332653969526291 norm:0.0014346539974212646 max memory_allocated 22560.36572265625 
[2025-03-22 02:53:22 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 2 loss:0.04884633421897888 norm:0.00047391935368068516 max memory_allocated 22560.36572265625 
[2025-03-22 02:53:55 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 3 loss:0.04687080904841423 norm:0.0002331682335352525 max memory_allocated 22560.36572265625 
[2025-03-22 02:54:28 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 4 loss:0.04604596644639969 norm:0.00017907960864249617 max memory_allocated 22560.36572265625 
[2025-03-22 02:55:00 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 5 loss:0.04566754028201103 norm:0.00015102232282515615 max memory_allocated 22560.36572265625 
[2025-03-22 02:55:33 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 6 loss:0.04541376233100891 norm:0.00013619127275887877 max memory_allocated 22560.36572265625 
[2025-03-22 02:56:06 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 7 loss:0.04525855556130409 norm:0.0001289800275117159 max memory_allocated 22560.36572265625 
[2025-03-22 02:56:39 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 8 loss:0.045157983899116516 norm:0.00013093870074953884 max memory_allocated 22560.36572265625 
[2025-03-22 02:57:11 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 9 loss:0.044986192137002945 norm:0.00012342844274826348 max memory_allocated 22560.36572265625 
[2025-03-22 02:57:44 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 10 loss:0.04488910362124443 norm:0.00011903137055924162 max memory_allocated 22560.36572265625 
[2025-03-22 02:58:17 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 11 loss:0.04475957900285721 norm:0.00011375034955563024 max memory_allocated 22560.36572265625 
[2025-03-22 02:58:49 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 12 loss:0.0446736142039299 norm:0.00011385122343199328 max memory_allocated 22560.36572265625 
[2025-03-22 02:59:22 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 13 loss:0.04457201808691025 norm:0.00010367843060521409 max memory_allocated 22560.36572265625 
[2025-03-22 02:59:55 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 14 loss:0.044536277651786804 norm:0.00010262856085319072 max memory_allocated 22560.36572265625 
[2025-03-22 03:00:28 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 15 loss:0.04445149749517441 norm:9.707597928354517e-05 max memory_allocated 22560.36572265625 
[2025-03-22 03:01:00 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 16 loss:0.04442478343844414 norm:9.611726272851229e-05 max memory_allocated 22560.36572265625 
[2025-03-22 03:01:33 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 17 loss:0.044370971620082855 norm:9.600289195077494e-05 max memory_allocated 22560.36572265625 
[2025-03-22 03:02:06 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 18 loss:0.0443536676466465 norm:9.733845945447683e-05 max memory_allocated 22560.36572265625 
[2025-03-22 03:02:38 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 19 loss:0.04432964324951172 norm:9.665897232480347e-05 max memory_allocated 22560.36572265625 
[2025-03-22 03:02:48 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 03:03:23 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 0 loss:0.07516201585531235 norm:0.0055738212540745735 max memory_allocated 22560.53759765625 
[2025-03-22 03:03:56 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 1 loss:0.06967101246118546 norm:0.0027574130799621344 max memory_allocated 22560.53759765625 
[2025-03-22 03:04:28 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 2 loss:0.062592513859272 norm:0.0009095625719055533 max memory_allocated 22560.53759765625 
[2025-03-22 03:05:01 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 3 loss:0.05894597992300987 norm:0.000344453495927155 max memory_allocated 22560.53759765625 
[2025-03-22 03:05:34 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 4 loss:0.0578581802546978 norm:0.0002454617351759225 max memory_allocated 22560.53759765625 
[2025-03-22 03:06:06 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 5 loss:0.05738994479179382 norm:0.00021301922970451415 max memory_allocated 22560.53759765625 
[2025-03-22 03:06:39 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 6 loss:0.057100847363471985 norm:0.00019049063848797232 max memory_allocated 22560.53759765625 
[2025-03-22 03:07:12 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 7 loss:0.05688347667455673 norm:0.00017678547010291368 max memory_allocated 22560.53759765625 
[2025-03-22 03:07:45 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 8 loss:0.05669287592172623 norm:0.00015570383402518928 max memory_allocated 22560.53759765625 
[2025-03-22 03:08:17 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 9 loss:0.056530021131038666 norm:0.00014698016457259655 max memory_allocated 22560.53759765625 
[2025-03-22 03:08:50 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 10 loss:0.05643557012081146 norm:0.0001419525797246024 max memory_allocated 22560.53759765625 
[2025-03-22 03:09:23 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 11 loss:0.056274332106113434 norm:0.00013246372691355646 max memory_allocated 22560.53759765625 
[2025-03-22 03:09:55 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 12 loss:0.05611928552389145 norm:0.00013156664499547333 max memory_allocated 22560.53759765625 
[2025-03-22 03:10:28 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 13 loss:0.05605713278055191 norm:0.00012397383397910744 max memory_allocated 22560.53759765625 
[2025-03-22 03:11:01 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 14 loss:0.05597534775733948 norm:0.0001184225402539596 max memory_allocated 22560.53759765625 
[2025-03-22 03:11:33 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 15 loss:0.055904701352119446 norm:0.00011620610166573897 max memory_allocated 22560.53759765625 
[2025-03-22 03:12:06 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 16 loss:0.0558478944003582 norm:0.00012037339911330491 max memory_allocated 22560.53759765625 
[2025-03-22 03:12:39 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 17 loss:0.05578891932964325 norm:0.00012586291995830834 max memory_allocated 22560.53759765625 
[2025-03-22 03:13:11 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 18 loss:0.05571597069501877 norm:0.0001246712781721726 max memory_allocated 22560.53759765625 
[2025-03-22 03:13:44 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 19 loss:0.055632591247558594 norm:0.00012036571570206434 max memory_allocated 22560.53759765625 
[2025-03-22 03:13:53 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 03:14:29 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 0 loss:0.0861629992723465 norm:0.003396675456315279 max memory_allocated 22560.70947265625 
[2025-03-22 03:15:02 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 1 loss:0.08122590184211731 norm:0.001662924187257886 max memory_allocated 22560.70947265625 
[2025-03-22 03:15:34 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 2 loss:0.07547855377197266 norm:0.0006253113388083875 max memory_allocated 22560.70947265625 
[2025-03-22 03:16:07 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 3 loss:0.07262295484542847 norm:0.00028684837161563337 max memory_allocated 22560.70947265625 
[2025-03-22 03:16:40 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 4 loss:0.07170270383358002 norm:0.00023660709848627448 max memory_allocated 22560.70947265625 
[2025-03-22 03:17:12 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 5 loss:0.07121603190898895 norm:0.00020476014469750226 max memory_allocated 22560.70947265625 
[2025-03-22 03:17:45 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 6 loss:0.07087305188179016 norm:0.00018246877880301327 max memory_allocated 22560.70947265625 
[2025-03-22 03:18:18 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 7 loss:0.07064651697874069 norm:0.00016962195513769984 max memory_allocated 22560.70947265625 
[2025-03-22 03:18:50 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 8 loss:0.07043304294347763 norm:0.00016124469402711838 max memory_allocated 22560.70947265625 
[2025-03-22 03:19:23 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 9 loss:0.07024550437927246 norm:0.0001536114141345024 max memory_allocated 22560.70947265625 
[2025-03-22 03:19:56 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 10 loss:0.07009690999984741 norm:0.00014962820569053292 max memory_allocated 22560.70947265625 
[2025-03-22 03:20:29 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 11 loss:0.06996752321720123 norm:0.00014487133012153208 max memory_allocated 22560.70947265625 
[2025-03-22 03:21:01 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 12 loss:0.06984546780586243 norm:0.00014325267693493515 max memory_allocated 22560.70947265625 
[2025-03-22 03:21:34 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 13 loss:0.0696982741355896 norm:0.00013982527889311314 max memory_allocated 22560.70947265625 
[2025-03-22 03:22:07 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 14 loss:0.0695846825838089 norm:0.0001341859606327489 max memory_allocated 22560.70947265625 
[2025-03-22 03:22:40 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 15 loss:0.06947462260723114 norm:0.0001276504190173 max memory_allocated 22560.70947265625 
[2025-03-22 03:23:12 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 16 loss:0.06937889009714127 norm:0.00012562991469167173 max memory_allocated 22560.70947265625 
[2025-03-22 03:23:45 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 17 loss:0.06931700557470322 norm:0.00012638622138183564 max memory_allocated 22560.70947265625 
[2025-03-22 03:24:18 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 18 loss:0.06924571841955185 norm:0.00012951814278494567 max memory_allocated 22560.70947265625 
[2025-03-22 03:24:50 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 19 loss:0.06917687505483627 norm:0.00012863338633906096 max memory_allocated 22560.70947265625 
[2025-03-22 03:25:00 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 03:25:35 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 0 loss:0.10846556723117828 norm:0.006554456427693367 max memory_allocated 22560.88134765625 
[2025-03-22 03:26:08 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 1 loss:0.1022418662905693 norm:0.003273625625297427 max memory_allocated 22560.88134765625 
[2025-03-22 03:26:41 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 2 loss:0.09415794909000397 norm:0.0011940967524424195 max memory_allocated 22560.88134765625 
[2025-03-22 03:27:13 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 3 loss:0.08957168459892273 norm:0.00047662731958553195 max memory_allocated 22560.88134765625 
[2025-03-22 03:27:46 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 4 loss:0.08824242651462555 norm:0.0003336499794386327 max memory_allocated 22560.88134765625 
[2025-03-22 03:28:19 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 5 loss:0.08754026144742966 norm:0.000288173439912498 max memory_allocated 22560.88134765625 
[2025-03-22 03:28:51 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 6 loss:0.08710736781358719 norm:0.00026415346655994654 max memory_allocated 22560.88134765625 
[2025-03-22 03:29:24 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 7 loss:0.08672211319208145 norm:0.00024123488401528448 max memory_allocated 22560.88134765625 
[2025-03-22 03:29:57 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 8 loss:0.08643869310617447 norm:0.00022961385548114777 max memory_allocated 22560.88134765625 
[2025-03-22 03:30:30 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 9 loss:0.08618248254060745 norm:0.00021257107437122613 max memory_allocated 22560.88134765625 
[2025-03-22 03:31:02 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 10 loss:0.08594720810651779 norm:0.00020155431411694735 max memory_allocated 22560.88134765625 
[2025-03-22 03:31:35 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 11 loss:0.08571784198284149 norm:0.00019199751841370016 max memory_allocated 22560.88134765625 
[2025-03-22 03:32:08 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 12 loss:0.08554833382368088 norm:0.00019671997870318592 max memory_allocated 22560.88134765625 
[2025-03-22 03:32:40 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 13 loss:0.0853748619556427 norm:0.00019000696192961186 max memory_allocated 22560.88134765625 
[2025-03-22 03:33:13 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 14 loss:0.08524349331855774 norm:0.00018671159341465682 max memory_allocated 22560.88134765625 
[2025-03-22 03:33:46 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 15 loss:0.08511755615472794 norm:0.00018315573106519878 max memory_allocated 22560.88134765625 
[2025-03-22 03:34:18 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 16 loss:0.0850321426987648 norm:0.00018509080109652132 max memory_allocated 22560.88134765625 
[2025-03-22 03:34:51 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 17 loss:0.08491961658000946 norm:0.00018390505283605307 max memory_allocated 22560.88134765625 
[2025-03-22 03:35:24 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 18 loss:0.08482353389263153 norm:0.0001891579304356128 max memory_allocated 22560.88134765625 
[2025-03-22 03:35:57 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 19 loss:0.08473959565162659 norm:0.00018083768372889608 max memory_allocated 22560.88134765625 
[2025-03-22 03:36:06 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 03:36:42 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 0 loss:0.12779080867767334 norm:0.007587539032101631 max memory_allocated 22561.05322265625 
[2025-03-22 03:37:14 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 1 loss:0.12163001298904419 norm:0.003936885856091976 max memory_allocated 22561.05322265625 
[2025-03-22 03:37:47 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 2 loss:0.11470596492290497 norm:0.0020745701622217894 max memory_allocated 22561.05322265625 
[2025-03-22 03:38:20 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 3 loss:0.10954765975475311 norm:0.0010752815287560225 max memory_allocated 22561.05322265625 
[2025-03-22 03:38:53 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 4 loss:0.10791701078414917 norm:0.0007701871218159795 max memory_allocated 22561.05322265625 
[2025-03-22 03:39:25 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 5 loss:0.10696809738874435 norm:0.0006297901854850352 max memory_allocated 22561.05322265625 
[2025-03-22 03:39:58 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 6 loss:0.10624466836452484 norm:0.0005323291406966746 max memory_allocated 22561.05322265625 
[2025-03-22 03:40:31 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 7 loss:0.10571834444999695 norm:0.0004798770823981613 max memory_allocated 22561.05322265625 
[2025-03-22 03:41:03 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 8 loss:0.10524113476276398 norm:0.00042125280015170574 max memory_allocated 22561.05322265625 
[2025-03-22 03:41:36 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 9 loss:0.10484565794467926 norm:0.00040170253487303853 max memory_allocated 22561.05322265625 
[2025-03-22 03:42:09 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 10 loss:0.10449924319982529 norm:0.00037138673360459507 max memory_allocated 22561.05322265625 
[2025-03-22 03:42:41 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 11 loss:0.10414082556962967 norm:0.0003397664113435894 max memory_allocated 22561.05322265625 
[2025-03-22 03:43:14 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 12 loss:0.10387858003377914 norm:0.00031482396298088133 max memory_allocated 22561.05322265625 
[2025-03-22 03:43:47 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 13 loss:0.10361464321613312 norm:0.00028913209098391235 max memory_allocated 22561.05322265625 
[2025-03-22 03:44:20 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 14 loss:0.10346394777297974 norm:0.0002912846102844924 max memory_allocated 22561.05322265625 
[2025-03-22 03:44:52 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 15 loss:0.1032668948173523 norm:0.0002748785773292184 max memory_allocated 22561.05322265625 
[2025-03-22 03:45:25 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 16 loss:0.10314495861530304 norm:0.00026561564300209284 max memory_allocated 22561.05322265625 
[2025-03-22 03:45:58 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 17 loss:0.10303342342376709 norm:0.000256986211752519 max memory_allocated 22561.05322265625 
[2025-03-22 03:46:30 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 18 loss:0.10294653475284576 norm:0.00024721541558392346 max memory_allocated 22561.05322265625 
[2025-03-22 03:47:03 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 19 loss:0.10286245495080948 norm:0.00023339188192039728 max memory_allocated 22561.05322265625 
[2025-03-22 03:47:12 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 03:47:48 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 0 loss:0.14783522486686707 norm:0.005232800729572773 max memory_allocated 22561.22509765625 
[2025-03-22 03:48:20 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 1 loss:0.1405685395002365 norm:0.0025662227999418974 max memory_allocated 22561.22509765625 
[2025-03-22 03:48:53 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 2 loss:0.13321824371814728 norm:0.001244759070686996 max memory_allocated 22561.22509765625 
[2025-03-22 03:49:26 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 3 loss:0.12890000641345978 norm:0.0006081708124838769 max memory_allocated 22561.22509765625 
[2025-03-22 03:49:59 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 4 loss:0.12715333700180054 norm:0.0004147884901612997 max memory_allocated 22561.22509765625 
[2025-03-22 03:50:31 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 5 loss:0.12624454498291016 norm:0.00035563320852816105 max memory_allocated 22561.22509765625 
[2025-03-22 03:51:04 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 6 loss:0.1255684196949005 norm:0.0003138817264698446 max memory_allocated 22561.22509765625 
[2025-03-22 03:51:37 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 7 loss:0.12508787214756012 norm:0.0003050421364605427 max memory_allocated 22561.22509765625 
[2025-03-22 03:52:09 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 8 loss:0.12464417517185211 norm:0.00028807067428715527 max memory_allocated 22561.22509765625 
[2025-03-22 03:52:42 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 9 loss:0.12428785115480423 norm:0.00028088432736694813 max memory_allocated 22561.22509765625 
[2025-03-22 03:53:15 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 10 loss:0.12406154721975327 norm:0.0002822556416504085 max memory_allocated 22561.22509765625 
[2025-03-22 03:53:47 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 11 loss:0.123829185962677 norm:0.0002880736137740314 max memory_allocated 22561.22509765625 
[2025-03-22 03:54:20 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 12 loss:0.12353096157312393 norm:0.0002735527232289314 max memory_allocated 22561.22509765625 
[2025-03-22 03:54:53 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 13 loss:0.12333221733570099 norm:0.00026875751791521907 max memory_allocated 22561.22509765625 
[2025-03-22 03:55:26 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 14 loss:0.1231083944439888 norm:0.00025457891752012074 max memory_allocated 22561.22509765625 
[2025-03-22 03:55:58 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 15 loss:0.12297278642654419 norm:0.00025058333994820714 max memory_allocated 22561.22509765625 
[2025-03-22 03:56:31 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 16 loss:0.12281901389360428 norm:0.0002447836159262806 max memory_allocated 22561.22509765625 
[2025-03-22 03:57:04 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 17 loss:0.12272680550813675 norm:0.00024578627198934555 max memory_allocated 22561.22509765625 
[2025-03-22 03:57:36 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 18 loss:0.1226753443479538 norm:0.00024094399122986943 max memory_allocated 22561.22509765625 
[2025-03-22 03:58:09 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 19 loss:0.12259545177221298 norm:0.00023441229132004082 max memory_allocated 22561.22509765625 
[2025-03-22 03:58:18 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 03:58:54 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 0 loss:0.19062115252017975 norm:0.019753647968173027 max memory_allocated 22561.39697265625 
[2025-03-22 03:59:27 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 1 loss:0.18031057715415955 norm:0.010291524231433868 max memory_allocated 22561.39697265625 
[2025-03-22 03:59:59 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 2 loss:0.16660210490226746 norm:0.004550680983811617 max memory_allocated 22561.39697265625 
[2025-03-22 04:00:32 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 3 loss:0.15917284786701202 norm:0.0022288148757070303 max memory_allocated 22561.39697265625 
[2025-03-22 04:01:04 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 4 loss:0.1565304696559906 norm:0.0016960520297288895 max memory_allocated 22561.39697265625 
[2025-03-22 04:01:37 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 5 loss:0.1549905687570572 norm:0.0013527290429919958 max memory_allocated 22561.39697265625 
[2025-03-22 04:02:10 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 6 loss:0.15390963852405548 norm:0.0011782862711697817 max memory_allocated 22561.39697265625 
[2025-03-22 04:02:42 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 7 loss:0.1531219482421875 norm:0.0010612657060846686 max memory_allocated 22561.39697265625 
[2025-03-22 04:03:15 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 8 loss:0.15238966047763824 norm:0.0009222091175615788 max memory_allocated 22561.39697265625 
[2025-03-22 04:03:48 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 9 loss:0.15167491137981415 norm:0.000764236378017813 max memory_allocated 22561.39697265625 
[2025-03-22 04:04:21 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 10 loss:0.15112335979938507 norm:0.0007043678197078407 max memory_allocated 22561.39697265625 
[2025-03-22 04:04:53 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 11 loss:0.15048693120479584 norm:0.0005072170752100646 max memory_allocated 22561.39697265625 
[2025-03-22 04:05:26 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 12 loss:0.14998246729373932 norm:0.00036663084756582975 max memory_allocated 22561.39697265625 
[2025-03-22 04:05:59 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 13 loss:0.1496473103761673 norm:0.00035932633909396827 max memory_allocated 22561.39697265625 
[2025-03-22 04:06:31 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 14 loss:0.149378702044487 norm:0.00035005749668926 max memory_allocated 22561.39697265625 
[2025-03-22 04:07:04 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 15 loss:0.1491704285144806 norm:0.00034248369047418237 max memory_allocated 22561.39697265625 
[2025-03-22 04:07:37 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 16 loss:0.14904175698757172 norm:0.0003382798167876899 max memory_allocated 22561.39697265625 
[2025-03-22 04:08:09 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 17 loss:0.14889824390411377 norm:0.0003232712624594569 max memory_allocated 22561.39697265625 
[2025-03-22 04:08:42 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 18 loss:0.1488199084997177 norm:0.0003247806162107736 max memory_allocated 22561.39697265625 
[2025-03-22 04:09:15 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 19 loss:0.1487809419631958 norm:0.00032296296558342874 max memory_allocated 22561.39697265625 
[2025-03-22 04:09:24 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 04:10:00 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 0 loss:0.221480131149292 norm:0.015440134331583977 max memory_allocated 22561.56884765625 
[2025-03-22 04:10:32 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 1 loss:0.21157492697238922 norm:0.007548234891146421 max memory_allocated 22561.56884765625 
[2025-03-22 04:11:05 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 2 loss:0.20047378540039062 norm:0.0032719208393245935 max memory_allocated 22561.56884765625 
[2025-03-22 04:11:38 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 3 loss:0.19414792954921722 norm:0.0016256250673905015 max memory_allocated 22561.56884765625 
[2025-03-22 04:12:10 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 4 loss:0.19153422117233276 norm:0.0010076573817059398 max memory_allocated 22561.56884765625 
[2025-03-22 04:12:43 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 5 loss:0.1899409294128418 norm:0.0007550548762083054 max memory_allocated 22561.56884765625 
[2025-03-22 04:13:16 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 6 loss:0.1887717992067337 norm:0.0006576452287845314 max memory_allocated 22561.56884765625 
[2025-03-22 04:13:48 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 7 loss:0.18780048191547394 norm:0.0005284586804918945 max memory_allocated 22561.56884765625 
[2025-03-22 04:14:21 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 8 loss:0.18701677024364471 norm:0.00044350986718200147 max memory_allocated 22561.56884765625 
[2025-03-22 04:14:54 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 9 loss:0.18640199303627014 norm:0.00042927684262394905 max memory_allocated 22561.56884765625 
[2025-03-22 04:15:26 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 10 loss:0.18589836359024048 norm:0.0004190462059341371 max memory_allocated 22561.56884765625 
[2025-03-22 04:15:59 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 11 loss:0.18543267250061035 norm:0.00042311075958423316 max memory_allocated 22561.56884765625 
[2025-03-22 04:16:32 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 12 loss:0.1851239651441574 norm:0.0004216354573145509 max memory_allocated 22561.56884765625 
[2025-03-22 04:17:04 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 13 loss:0.18488332629203796 norm:0.00040498885209672153 max memory_allocated 22561.56884765625 
[2025-03-22 04:17:37 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 14 loss:0.18462610244750977 norm:0.000395988899981603 max memory_allocated 22561.56884765625 
[2025-03-22 04:18:10 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 15 loss:0.18440815806388855 norm:0.00038452641456387937 max memory_allocated 22561.56884765625 
[2025-03-22 04:18:43 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 16 loss:0.1842607855796814 norm:0.0003791190101765096 max memory_allocated 22561.56884765625 
[2025-03-22 04:19:15 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 17 loss:0.1840888410806656 norm:0.00037073373096063733 max memory_allocated 22561.56884765625 
[2025-03-22 04:19:48 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 18 loss:0.18396355211734772 norm:0.0003644611861091107 max memory_allocated 22561.56884765625 
[2025-03-22 04:20:21 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 19 loss:0.18385303020477295 norm:0.0003628042177297175 max memory_allocated 22561.56884765625 
[2025-03-22 04:20:30 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 04:21:05 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 0 loss:0.28981977701187134 norm:0.01917128637433052 max memory_allocated 22561.74072265625 
[2025-03-22 04:21:38 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 1 loss:0.27572101354599 norm:0.010369793511927128 max memory_allocated 22561.74072265625 
[2025-03-22 04:22:11 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 2 loss:0.25464850664138794 norm:0.004437601659446955 max memory_allocated 22561.74072265625 
[2025-03-22 04:22:43 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 3 loss:0.2443465143442154 norm:0.0021887635812163353 max memory_allocated 22561.74072265625 
[2025-03-22 04:23:16 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 4 loss:0.24067190289497375 norm:0.0012084292247891426 max memory_allocated 22561.74072265625 
[2025-03-22 04:23:49 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 5 loss:0.238733172416687 norm:0.0009851183276623487 max memory_allocated 22561.74072265625 
[2025-03-22 04:24:21 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 6 loss:0.23741409182548523 norm:0.0009487891220487654 max memory_allocated 22561.74072265625 
[2025-03-22 04:24:54 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 7 loss:0.23617663979530334 norm:0.00072343775536865 max memory_allocated 22561.74072265625 
[2025-03-22 04:25:27 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 8 loss:0.23511430621147156 norm:0.000579562212806195 max memory_allocated 22561.74072265625 
[2025-03-22 04:25:59 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 9 loss:0.23438678681850433 norm:0.0005677564768120646 max memory_allocated 22561.74072265625 
[2025-03-22 04:26:32 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 10 loss:0.23381727933883667 norm:0.0005600738804787397 max memory_allocated 22561.74072265625 
[2025-03-22 04:27:05 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 11 loss:0.23329754173755646 norm:0.0005439106607809663 max memory_allocated 22561.74072265625 
[2025-03-22 04:27:38 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 12 loss:0.23280170559883118 norm:0.0005400365917012095 max memory_allocated 22561.74072265625 
[2025-03-22 04:28:10 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 13 loss:0.23242147266864777 norm:0.0005276158335618675 max memory_allocated 22561.74072265625 
[2025-03-22 04:28:43 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 14 loss:0.23212586343288422 norm:0.0005129885394126177 max memory_allocated 22561.74072265625 
[2025-03-22 04:29:16 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 15 loss:0.23190955817699432 norm:0.0005110874772071838 max memory_allocated 22561.74072265625 
[2025-03-22 04:29:48 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 16 loss:0.2315804362297058 norm:0.0005006908322684467 max memory_allocated 22561.74072265625 
[2025-03-22 04:30:21 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 17 loss:0.2314292937517166 norm:0.00048474909272044897 max memory_allocated 22561.74072265625 
[2025-03-22 04:30:54 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 18 loss:0.23131823539733887 norm:0.00047866281238384545 max memory_allocated 22561.74072265625 
[2025-03-22 04:31:26 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 19 loss:0.2311907708644867 norm:0.0004693724913522601 max memory_allocated 22561.74072265625 
[2025-03-22 04:31:36 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 04:32:11 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 0 loss:0.3527309000492096 norm:0.020527534186840057 max memory_allocated 22561.91259765625 
[2025-03-22 04:32:44 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 1 loss:0.3371700048446655 norm:0.010522427968680859 max memory_allocated 22561.91259765625 
[2025-03-22 04:33:16 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 2 loss:0.3184833824634552 norm:0.004329625982791185 max memory_allocated 22561.91259765625 
[2025-03-22 04:33:49 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 3 loss:0.30920565128326416 norm:0.0019512600265443325 max memory_allocated 22561.91259765625 
[2025-03-22 04:34:22 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 4 loss:0.30539417266845703 norm:0.0012183643411844969 max memory_allocated 22561.91259765625 
[2025-03-22 04:34:55 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 5 loss:0.3030368685722351 norm:0.0009425603784620762 max memory_allocated 22561.91259765625 
[2025-03-22 04:35:27 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 6 loss:0.30146896839141846 norm:0.0008495714864693582 max memory_allocated 22561.91259765625 
[2025-03-22 04:36:00 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 7 loss:0.30024710297584534 norm:0.0007985515403561294 max memory_allocated 22561.91259765625 
[2025-03-22 04:36:33 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 8 loss:0.2992035448551178 norm:0.0007575097260996699 max memory_allocated 22561.91259765625 
[2025-03-22 04:37:05 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 9 loss:0.2983300983905792 norm:0.0007198223611339927 max memory_allocated 22561.91259765625 
[2025-03-22 04:37:38 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 10 loss:0.29768529534339905 norm:0.0007003707578405738 max memory_allocated 22561.91259765625 
[2025-03-22 04:38:11 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 11 loss:0.2971866726875305 norm:0.0006808218313381076 max memory_allocated 22561.91259765625 
[2025-03-22 04:38:43 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 12 loss:0.2967722415924072 norm:0.0006852355436421931 max memory_allocated 22561.91259765625 
[2025-03-22 04:39:16 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 13 loss:0.2963985204696655 norm:0.0006681978120468557 max memory_allocated 22561.91259765625 
[2025-03-22 04:39:49 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 14 loss:0.29614442586898804 norm:0.0006777590606361628 max memory_allocated 22561.91259765625 
[2025-03-22 04:40:21 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 15 loss:0.29578080773353577 norm:0.0006752692279405892 max memory_allocated 22561.91259765625 
[2025-03-22 04:40:54 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 16 loss:0.29534366726875305 norm:0.0006472659297287464 max memory_allocated 22561.91259765625 
[2025-03-22 04:41:27 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 17 loss:0.2950918674468994 norm:0.0006345700821839273 max memory_allocated 22561.91259765625 
[2025-03-22 04:42:00 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 18 loss:0.29487890005111694 norm:0.0006123822531662881 max memory_allocated 22561.91259765625 
[2025-03-22 04:42:32 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 19 loss:0.29474353790283203 norm:0.0005970933707430959 max memory_allocated 22561.91259765625 
[2025-03-22 04:42:41 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 04:43:17 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 0 loss:0.4382827579975128 norm:0.023978281766176224 max memory_allocated 22562.08447265625 
[2025-03-22 04:43:50 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 1 loss:0.4193766415119171 norm:0.01137535646557808 max memory_allocated 22562.08447265625 
[2025-03-22 04:44:22 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 2 loss:0.39911743998527527 norm:0.004986099898815155 max memory_allocated 22562.08447265625 
[2025-03-22 04:44:55 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 3 loss:0.39002448320388794 norm:0.00268715457059443 max memory_allocated 22562.08447265625 
[2025-03-22 04:45:27 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 4 loss:0.3863769769668579 norm:0.002033223630860448 max memory_allocated 22562.08447265625 
[2025-03-22 04:46:00 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 5 loss:0.3835560381412506 norm:0.0009482720633968711 max memory_allocated 22562.08447265625 
[2025-03-22 04:46:33 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 6 loss:0.3819274604320526 norm:0.0007212464115582407 max memory_allocated 22562.08447265625 
[2025-03-22 04:47:05 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 7 loss:0.38066911697387695 norm:0.0007148720906116068 max memory_allocated 22562.08447265625 
[2025-03-22 04:47:38 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 8 loss:0.3796524405479431 norm:0.0007133574690669775 max memory_allocated 22562.08447265625 
[2025-03-22 04:48:11 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 9 loss:0.3788801431655884 norm:0.0007054625893943012 max memory_allocated 22562.08447265625 
[2025-03-22 04:48:44 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 10 loss:0.37830954790115356 norm:0.0006998283788561821 max memory_allocated 22562.08447265625 
[2025-03-22 04:49:16 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 11 loss:0.37780797481536865 norm:0.0006978220771998167 max memory_allocated 22562.08447265625 
[2025-03-22 04:49:49 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 12 loss:0.377430260181427 norm:0.0006894437246955931 max memory_allocated 22562.08447265625 
[2025-03-22 04:50:22 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 13 loss:0.3770763874053955 norm:0.0006751602631993592 max memory_allocated 22562.08447265625 
[2025-03-22 04:50:54 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 14 loss:0.3767874538898468 norm:0.0006719644879922271 max memory_allocated 22562.08447265625 
[2025-03-22 04:51:27 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 15 loss:0.37659138441085815 norm:0.0006721734534949064 max memory_allocated 22562.08447265625 
[2025-03-22 04:52:00 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 16 loss:0.37644749879837036 norm:0.0006762732518836856 max memory_allocated 22562.08447265625 
[2025-03-22 04:52:32 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 17 loss:0.3762870132923126 norm:0.0006748565938323736 max memory_allocated 22562.08447265625 
[2025-03-22 04:53:05 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 18 loss:0.37615180015563965 norm:0.0006732416222803295 max memory_allocated 22562.08447265625 
[2025-03-22 04:53:38 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 19 loss:0.37601643800735474 norm:0.000669347180519253 max memory_allocated 22562.08447265625 
[2025-03-22 04:53:47 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 04:54:23 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 0 loss:0.5327869653701782 norm:0.018336648121476173 max memory_allocated 22562.25634765625 
[2025-03-22 04:54:55 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 1 loss:0.5179296731948853 norm:0.010259734466671944 max memory_allocated 22562.25634765625 
[2025-03-22 04:55:28 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 2 loss:0.5004503726959229 norm:0.005600607022643089 max memory_allocated 22562.25634765625 
[2025-03-22 04:56:00 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 3 loss:0.4915817677974701 norm:0.0036335065960884094 max memory_allocated 22562.25634765625 
[2025-03-22 04:56:33 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 4 loss:0.48819923400878906 norm:0.0028754021041095257 max memory_allocated 22562.25634765625 
[2025-03-22 04:57:06 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 5 loss:0.48581141233444214 norm:0.0023285867646336555 max memory_allocated 22562.25634765625 
[2025-03-22 04:57:38 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 6 loss:0.4839000105857849 norm:0.0019532041624188423 max memory_allocated 22562.25634765625 
[2025-03-22 04:58:11 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 7 loss:0.48223501443862915 norm:0.0016144347609952092 max memory_allocated 22562.25634765625 
[2025-03-22 04:58:44 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 8 loss:0.4810209274291992 norm:0.0014422303065657616 max memory_allocated 22562.25634765625 
[2025-03-22 04:59:16 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 9 loss:0.47996747493743896 norm:0.0012851502979174256 max memory_allocated 22562.25634765625 
[2025-03-22 04:59:49 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 10 loss:0.4789660573005676 norm:0.001111118937842548 max memory_allocated 22562.25634765625 
[2025-03-22 05:00:22 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 11 loss:0.4779326021671295 norm:0.0007581254467368126 max memory_allocated 22562.25634765625 
[2025-03-22 05:00:55 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 12 loss:0.4774188995361328 norm:0.0007612337940372527 max memory_allocated 22562.25634765625 
[2025-03-22 05:01:27 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 13 loss:0.4770694673061371 norm:0.0007523975218646228 max memory_allocated 22562.25634765625 
[2025-03-22 05:02:00 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 14 loss:0.47673866152763367 norm:0.0007635299116373062 max memory_allocated 22562.25634765625 
[2025-03-22 05:02:33 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 15 loss:0.47645503282546997 norm:0.0007582686957903206 max memory_allocated 22562.25634765625 
[2025-03-22 05:03:05 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 16 loss:0.4762938618659973 norm:0.0007480556378141046 max memory_allocated 22562.25634765625 
[2025-03-22 05:03:38 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 17 loss:0.4761313796043396 norm:0.000744884368032217 max memory_allocated 22562.25634765625 
[2025-03-22 05:04:11 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 18 loss:0.47590649127960205 norm:0.0007288168999366462 max memory_allocated 22562.25634765625 
[2025-03-22 05:04:43 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 19 loss:0.4756461977958679 norm:0.0007278048433363438 max memory_allocated 22562.25634765625 
[2025-03-22 05:04:52 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 05:05:28 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 0 loss:0.6745463609695435 norm:0.021969802677631378 max memory_allocated 22562.42822265625 
[2025-03-22 05:06:01 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 1 loss:0.656857967376709 norm:0.01236614491790533 max memory_allocated 22562.42822265625 
[2025-03-22 05:06:33 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 2 loss:0.634788990020752 norm:0.006743664387613535 max memory_allocated 22562.42822265625 
[2025-03-22 05:07:06 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 3 loss:0.6222853064537048 norm:0.004021135624498129 max memory_allocated 22562.42822265625 
[2025-03-22 05:07:38 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 4 loss:0.6178807616233826 norm:0.0031234538182616234 max memory_allocated 22562.42822265625 
[2025-03-22 05:08:11 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 5 loss:0.614835262298584 norm:0.0026397884357720613 max memory_allocated 22562.42822265625 
[2025-03-22 05:08:44 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 6 loss:0.6126894950866699 norm:0.0022763852030038834 max memory_allocated 22562.42822265625 
[2025-03-22 05:09:16 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 7 loss:0.6109932661056519 norm:0.0020438150968402624 max memory_allocated 22562.42822265625 
[2025-03-22 05:09:49 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 8 loss:0.6092499494552612 norm:0.0011214795522391796 max memory_allocated 22562.42822265625 
[2025-03-22 05:10:22 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 9 loss:0.6083582043647766 norm:0.0011079792166128755 max memory_allocated 22562.42822265625 
[2025-03-22 05:10:55 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 10 loss:0.6077220439910889 norm:0.0010775955161079764 max memory_allocated 22562.42822265625 
[2025-03-22 05:11:27 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 11 loss:0.6070057153701782 norm:0.0010319408029317856 max memory_allocated 22562.42822265625 
[2025-03-22 05:12:00 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 12 loss:0.6065191030502319 norm:0.0010396981379017234 max memory_allocated 22562.42822265625 
[2025-03-22 05:12:33 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 13 loss:0.6062179803848267 norm:0.001029532984830439 max memory_allocated 22562.42822265625 
[2025-03-22 05:13:05 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 14 loss:0.606050968170166 norm:0.0010439668549224734 max memory_allocated 22562.42822265625 
[2025-03-22 05:13:38 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 15 loss:0.6056992411613464 norm:0.000998420873656869 max memory_allocated 22562.42822265625 
[2025-03-22 05:14:11 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 16 loss:0.6054361462593079 norm:0.0010085850954055786 max memory_allocated 22562.42822265625 
[2025-03-22 05:14:43 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 17 loss:0.6052250862121582 norm:0.00100375572219491 max memory_allocated 22562.42822265625 
[2025-03-22 05:15:16 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 18 loss:0.6050750017166138 norm:0.0010208955500274897 max memory_allocated 22562.42822265625 
[2025-03-22 05:15:49 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 19 loss:0.6048330068588257 norm:0.001016326597891748 max memory_allocated 22562.42822265625 
[2025-03-22 05:15:58 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 05:16:33 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 0 loss:0.813524067401886 norm:0.010051075369119644 max memory_allocated 22562.60009765625 
[2025-03-22 05:17:06 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 1 loss:0.8044701814651489 norm:0.006950361654162407 max memory_allocated 22562.60009765625 
[2025-03-22 05:17:39 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 2 loss:0.7862488031387329 norm:0.003447949420660734 max memory_allocated 22562.60009765625 
[2025-03-22 05:18:11 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 3 loss:0.7754242420196533 norm:0.002187772188335657 max memory_allocated 22562.60009765625 
[2025-03-22 05:18:44 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 4 loss:0.770205557346344 norm:0.0016964082606136799 max memory_allocated 22562.60009765625 
[2025-03-22 05:19:16 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 5 loss:0.7676662802696228 norm:0.0014739113394171 max memory_allocated 22562.60009765625 
[2025-03-22 05:19:49 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 6 loss:0.7658815383911133 norm:0.0014383657835423946 max memory_allocated 22562.60009765625 
[2025-03-22 05:20:22 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 7 loss:0.7645530104637146 norm:0.001424970105290413 max memory_allocated 22562.60009765625 
[2025-03-22 05:20:55 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 8 loss:0.7634130120277405 norm:0.001361367292702198 max memory_allocated 22562.60009765625 
[2025-03-22 05:21:27 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 9 loss:0.762607753276825 norm:0.00135136884637177 max memory_allocated 22562.60009765625 
[2025-03-22 05:22:00 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 10 loss:0.7623646259307861 norm:0.0014026355929672718 max memory_allocated 22562.60009765625 
[2025-03-22 05:22:33 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 11 loss:0.7619577646255493 norm:0.0012806220911443233 max memory_allocated 22562.60009765625 
[2025-03-22 05:23:05 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 12 loss:0.7606587409973145 norm:0.001255732960999012 max memory_allocated 22562.60009765625 
[2025-03-22 05:23:38 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 13 loss:0.7603998184204102 norm:0.0012789034517481923 max memory_allocated 22562.60009765625 
[2025-03-22 05:24:11 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 14 loss:0.7601841688156128 norm:0.0012294359039515257 max memory_allocated 22562.60009765625 
[2025-03-22 05:24:43 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 15 loss:0.7601017951965332 norm:0.001234942814335227 max memory_allocated 22562.60009765625 
[2025-03-22 05:25:16 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 16 loss:0.7593934535980225 norm:0.0012014145031571388 max memory_allocated 22562.60009765625 
[2025-03-22 05:25:49 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 17 loss:0.7588337063789368 norm:0.0012101909378543496 max memory_allocated 22562.60009765625 
[2025-03-22 05:26:21 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 18 loss:0.7596439719200134 norm:0.0012667356058955193 max memory_allocated 22562.60009765625 
[2025-03-22 05:26:54 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 19 loss:0.7586167454719543 norm:0.0012661610962823033 max memory_allocated 22562.60009765625 
[2025-03-22 05:27:03 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 05:27:39 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 0 loss:0.9628268480300903 norm:0.010263052769005299 max memory_allocated 22562.77197265625 
[2025-03-22 05:28:11 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 1 loss:0.9463953375816345 norm:0.006019094958901405 max memory_allocated 22562.77197265625 
[2025-03-22 05:28:44 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 2 loss:0.9269071221351624 norm:0.003185795620083809 max memory_allocated 22562.77197265625 
[2025-03-22 05:29:17 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 3 loss:0.9192603230476379 norm:0.0022203114349395037 max memory_allocated 22562.77197265625 
[2025-03-22 05:29:49 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 4 loss:0.9152629375457764 norm:0.001688657095655799 max memory_allocated 22562.77197265625 
[2025-03-22 05:30:22 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 5 loss:0.9123501777648926 norm:0.0014537995448336005 max memory_allocated 22562.77197265625 
[2025-03-22 05:30:55 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 6 loss:0.9100863933563232 norm:0.0013120905496180058 max memory_allocated 22562.77197265625 
[2025-03-22 05:31:27 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 7 loss:0.9084413647651672 norm:0.0011374825844541192 max memory_allocated 22562.77197265625 
[2025-03-22 05:32:00 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 8 loss:0.9072785377502441 norm:0.001119281048886478 max memory_allocated 22562.77197265625 
[2025-03-22 05:32:33 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 9 loss:0.9062595963478088 norm:0.001080110901966691 max memory_allocated 22562.77197265625 
[2025-03-22 05:33:05 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 10 loss:0.905572772026062 norm:0.0010730638168752193 max memory_allocated 22562.77197265625 
[2025-03-22 05:33:38 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 11 loss:0.9051068425178528 norm:0.0010612087789922953 max memory_allocated 22562.77197265625 
[2025-03-22 05:34:11 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 12 loss:0.904721736907959 norm:0.0010560952359810472 max memory_allocated 22562.77197265625 
[2025-03-22 05:34:43 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 13 loss:0.9043552279472351 norm:0.0010443574283272028 max memory_allocated 22562.77197265625 
[2025-03-22 05:35:16 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 14 loss:0.904029905796051 norm:0.001039596158079803 max memory_allocated 22562.77197265625 
[2025-03-22 05:35:49 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 15 loss:0.9038363099098206 norm:0.0010262907017022371 max memory_allocated 22562.77197265625 
[2025-03-22 05:36:22 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 16 loss:0.9036034345626831 norm:0.0010184769053012133 max memory_allocated 22562.77197265625 
[2025-03-22 05:36:54 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 17 loss:0.9034562706947327 norm:0.0010150580201297998 max memory_allocated 22562.77197265625 
[2025-03-22 05:37:27 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 18 loss:0.903331995010376 norm:0.001010149484500289 max memory_allocated 22562.77197265625 
[2025-03-22 05:38:00 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 19 loss:0.9031833410263062 norm:0.0010105744004249573 max memory_allocated 22562.77197265625 
[2025-03-22 05:38:09 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 05:38:45 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 0 loss:1.1685638427734375 norm:0.018740009516477585 max memory_allocated 22562.94384765625 
[2025-03-22 05:39:17 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 1 loss:1.1487226486206055 norm:0.012160256505012512 max memory_allocated 22562.94384765625 
[2025-03-22 05:39:50 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 2 loss:1.123806118965149 norm:0.006608904339373112 max memory_allocated 22562.94384765625 
[2025-03-22 05:40:23 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 3 loss:1.1114474534988403 norm:0.0038154011126607656 max memory_allocated 22562.94384765625 
[2025-03-22 05:40:55 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 4 loss:1.105618953704834 norm:0.002956307725980878 max memory_allocated 22562.94384765625 
[2025-03-22 05:41:28 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 5 loss:1.101493000984192 norm:0.002504866337403655 max memory_allocated 22562.94384765625 
[2025-03-22 05:42:01 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 6 loss:1.0979564189910889 norm:0.0018173326971009374 max memory_allocated 22562.94384765625 
[2025-03-22 05:42:33 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 7 loss:1.0954928398132324 norm:0.0014678676379844546 max memory_allocated 22562.94384765625 
[2025-03-22 05:43:06 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 8 loss:1.093747615814209 norm:0.0013174236519262195 max memory_allocated 22562.94384765625 
[2025-03-22 05:43:39 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 9 loss:1.0926090478897095 norm:0.0012849530903622508 max memory_allocated 22562.94384765625 
[2025-03-22 05:44:11 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 10 loss:1.091761827468872 norm:0.0012718242360278964 max memory_allocated 22562.94384765625 
[2025-03-22 05:44:44 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 11 loss:1.0911965370178223 norm:0.0012645730748772621 max memory_allocated 22562.94384765625 
[2025-03-22 05:45:17 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 12 loss:1.0907046794891357 norm:0.0012525527272373438 max memory_allocated 22562.94384765625 
[2025-03-22 05:45:49 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 13 loss:1.0902928113937378 norm:0.001251298119314015 max memory_allocated 22562.94384765625 
[2025-03-22 05:46:22 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 14 loss:1.089997410774231 norm:0.0012472664238885045 max memory_allocated 22562.94384765625 
[2025-03-22 05:46:55 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 15 loss:1.0897483825683594 norm:0.0012361634289845824 max memory_allocated 22562.94384765625 
[2025-03-22 05:47:28 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 16 loss:1.0895171165466309 norm:0.0012463395250961185 max memory_allocated 22562.94384765625 
[2025-03-22 05:48:00 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 17 loss:1.0892637968063354 norm:0.0012417577672749758 max memory_allocated 22562.94384765625 
[2025-03-22 05:48:33 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 18 loss:1.0890671014785767 norm:0.0012332942569628358 max memory_allocated 22562.94384765625 
[2025-03-22 05:49:06 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 19 loss:1.088891625404358 norm:0.0012284757103770971 max memory_allocated 22562.94384765625 
[2025-03-22 05:49:15 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 05:49:51 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 0 loss:1.3394173383712769 norm:0.02647215686738491 max memory_allocated 22563.11572265625 
[2025-03-22 05:50:23 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 1 loss:1.3227481842041016 norm:0.017921358346939087 max memory_allocated 22563.11572265625 
[2025-03-22 05:50:56 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 2 loss:1.2987250089645386 norm:0.011316533200442791 max memory_allocated 22563.11572265625 
[2025-03-22 05:51:29 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 3 loss:1.2876895666122437 norm:0.009095178917050362 max memory_allocated 22563.11572265625 
[2025-03-22 05:52:01 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 4 loss:1.280799150466919 norm:0.0066304439678788185 max memory_allocated 22563.11572265625 
[2025-03-22 05:52:34 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 5 loss:1.275892972946167 norm:0.005381570663303137 max memory_allocated 22563.11572265625 
[2025-03-22 05:53:07 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 6 loss:1.2721501588821411 norm:0.00445631705224514 max memory_allocated 22563.11572265625 
[2025-03-22 05:53:39 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 7 loss:1.2698447704315186 norm:0.003974880091845989 max memory_allocated 22563.11572265625 
[2025-03-22 05:54:12 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 8 loss:1.2679896354675293 norm:0.0035641666036099195 max memory_allocated 22563.11572265625 
[2025-03-22 05:54:45 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 9 loss:1.2666782140731812 norm:0.0032074982300400734 max memory_allocated 22563.11572265625 
[2025-03-22 05:55:17 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 10 loss:1.2656681537628174 norm:0.0029347892850637436 max memory_allocated 22563.11572265625 
[2025-03-22 05:55:50 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 11 loss:1.264803409576416 norm:0.002707267878577113 max memory_allocated 22563.11572265625 
[2025-03-22 05:56:23 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 12 loss:1.2639636993408203 norm:0.0024847581516951323 max memory_allocated 22563.11572265625 
[2025-03-22 05:56:55 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 13 loss:1.2632982730865479 norm:0.0022936903405934572 max memory_allocated 22563.11572265625 
[2025-03-22 05:57:28 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 14 loss:1.2626605033874512 norm:0.0021350267343223095 max memory_allocated 22563.11572265625 
[2025-03-22 05:58:01 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 15 loss:1.2618547677993774 norm:0.0018764464184641838 max memory_allocated 22563.11572265625 
[2025-03-22 05:58:33 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 16 loss:1.261283278465271 norm:0.0017573791556060314 max memory_allocated 22563.11572265625 
[2025-03-22 05:59:06 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 17 loss:1.2608908414840698 norm:0.0016794129041954875 max memory_allocated 22563.11572265625 
[2025-03-22 05:59:39 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 18 loss:1.2605584859848022 norm:0.0016036758897826076 max memory_allocated 22563.11572265625 
[2025-03-22 06:00:12 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 19 loss:1.2601690292358398 norm:0.0015874358359724283 max memory_allocated 22563.11572265625 
[2025-03-22 06:00:21 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 06:00:56 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 0 loss:1.5266791582107544 norm:0.009495219215750694 max memory_allocated 22563.28759765625 
[2025-03-22 06:01:29 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 1 loss:1.507789134979248 norm:0.005188486538827419 max memory_allocated 22563.28759765625 
[2025-03-22 06:02:02 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 2 loss:1.484279751777649 norm:0.002551223849877715 max memory_allocated 22563.28759765625 
[2025-03-22 06:02:34 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 3 loss:1.474498987197876 norm:0.0015970785170793533 max memory_allocated 22563.28759765625 
[2025-03-22 06:03:07 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 4 loss:1.4697006940841675 norm:0.0013558085775002837 max memory_allocated 22563.28759765625 
[2025-03-22 06:03:40 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 5 loss:1.4660491943359375 norm:0.0012543468037620187 max memory_allocated 22563.28759765625 
[2025-03-22 06:04:13 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 6 loss:1.4633502960205078 norm:0.0012037311680614948 max memory_allocated 22563.28759765625 
[2025-03-22 06:04:45 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 7 loss:1.4613381624221802 norm:0.0011782572837546468 max memory_allocated 22563.28759765625 
[2025-03-22 06:05:18 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 8 loss:1.4599720239639282 norm:0.0011531192576512694 max memory_allocated 22563.28759765625 
[2025-03-22 06:05:51 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 9 loss:1.4589064121246338 norm:0.0011351605644449592 max memory_allocated 22563.28759765625 
[2025-03-22 06:06:24 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 10 loss:1.4581103324890137 norm:0.0011215915437787771 max memory_allocated 22563.28759765625 
[2025-03-22 06:06:56 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 11 loss:1.4574072360992432 norm:0.0011066414881497622 max memory_allocated 22563.28759765625 
[2025-03-22 06:07:29 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 12 loss:1.4568580389022827 norm:0.0011009240988641977 max memory_allocated 22563.28759765625 
[2025-03-22 06:08:02 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 13 loss:1.456343412399292 norm:0.0010993904434144497 max memory_allocated 22563.28759765625 
[2025-03-22 06:08:34 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 14 loss:1.4558899402618408 norm:0.0010980985825881362 max memory_allocated 22563.28759765625 
[2025-03-22 06:09:07 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 15 loss:1.4555625915527344 norm:0.0010903760557994246 max memory_allocated 22563.28759765625 
[2025-03-22 06:09:40 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 16 loss:1.455160140991211 norm:0.001077877008356154 max memory_allocated 22563.28759765625 
[2025-03-22 06:10:12 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 17 loss:1.4548771381378174 norm:0.0010774716502055526 max memory_allocated 22563.28759765625 
[2025-03-22 06:10:45 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 18 loss:1.4547008275985718 norm:0.00107617920730263 max memory_allocated 22563.28759765625 
[2025-03-22 06:11:18 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 19 loss:1.4544696807861328 norm:0.001068726647645235 max memory_allocated 22563.28759765625 
[2025-03-22 06:11:27 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 06:12:03 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 0 loss:1.7524986267089844 norm:0.02529807761311531 max memory_allocated 22563.45947265625 
[2025-03-22 06:12:35 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 1 loss:1.724229335784912 norm:0.012895546853542328 max memory_allocated 22563.45947265625 
[2025-03-22 06:13:08 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 2 loss:1.696093201637268 norm:0.006109217181801796 max memory_allocated 22563.45947265625 
[2025-03-22 06:13:40 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 3 loss:1.6824960708618164 norm:0.0034133587032556534 max memory_allocated 22563.45947265625 
[2025-03-22 06:14:13 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 4 loss:1.6758253574371338 norm:0.0026382014621049166 max memory_allocated 22563.45947265625 
[2025-03-22 06:14:46 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 5 loss:1.6706289052963257 norm:0.0021263258531689644 max memory_allocated 22563.45947265625 
[2025-03-22 06:15:19 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 6 loss:1.6669371128082275 norm:0.0018035263055935502 max memory_allocated 22563.45947265625 
[2025-03-22 06:15:51 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 7 loss:1.6643434762954712 norm:0.0015947980573400855 max memory_allocated 22563.45947265625 
[2025-03-22 06:16:24 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 8 loss:1.66257643699646 norm:0.0014351124409586191 max memory_allocated 22563.45947265625 
[2025-03-22 06:16:57 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 9 loss:1.661287784576416 norm:0.0013371232198551297 max memory_allocated 22563.45947265625 
[2025-03-22 06:17:29 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 10 loss:1.660282850265503 norm:0.0012754512717947364 max memory_allocated 22563.45947265625 
[2025-03-22 06:18:02 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 11 loss:1.6595282554626465 norm:0.0012245180550962687 max memory_allocated 22563.45947265625 
[2025-03-22 06:18:35 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 12 loss:1.65890371799469 norm:0.00119340093806386 max memory_allocated 22563.45947265625 
[2025-03-22 06:19:07 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 13 loss:1.6583176851272583 norm:0.0011660157470032573 max memory_allocated 22563.45947265625 
[2025-03-22 06:19:40 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 14 loss:1.6578381061553955 norm:0.0011458649532869458 max memory_allocated 22563.45947265625 
[2025-03-22 06:20:13 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 15 loss:1.6574422121047974 norm:0.0011327038519084454 max memory_allocated 22563.45947265625 
[2025-03-22 06:20:45 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 16 loss:1.6570788621902466 norm:0.0011174449464306235 max memory_allocated 22563.45947265625 
[2025-03-22 06:21:18 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 17 loss:1.6567833423614502 norm:0.0011112785432487726 max memory_allocated 22563.45947265625 
[2025-03-22 06:21:51 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 18 loss:1.6565451622009277 norm:0.0011094457004219294 max memory_allocated 22563.45947265625 
[2025-03-22 06:22:24 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 19 loss:1.6563407182693481 norm:0.0010993400355800986 max memory_allocated 22563.45947265625 
[2025-03-22 06:22:33 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 06:23:08 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 0 loss:1.9652431011199951 norm:0.02146119438111782 max memory_allocated 22563.63134765625 
[2025-03-22 06:23:41 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 1 loss:1.932674527168274 norm:0.00948707852512598 max memory_allocated 22563.63134765625 
[2025-03-22 06:24:14 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 2 loss:1.9043171405792236 norm:0.005783627741038799 max memory_allocated 22563.63134765625 
[2025-03-22 06:24:46 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 3 loss:1.8888051509857178 norm:0.002554545411840081 max memory_allocated 22563.63134765625 
[2025-03-22 06:25:19 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 4 loss:1.8820769786834717 norm:0.0021001696586608887 max memory_allocated 22563.63134765625 
[2025-03-22 06:25:52 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 5 loss:1.8770866394042969 norm:0.001965356059372425 max memory_allocated 22563.63134765625 
[2025-03-22 06:26:24 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 6 loss:1.8736318349838257 norm:0.001845295075327158 max memory_allocated 22563.63134765625 
[2025-03-22 06:26:57 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 7 loss:1.8710112571716309 norm:0.001724605681374669 max memory_allocated 22563.63134765625 
[2025-03-22 06:27:30 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 8 loss:1.8693140745162964 norm:0.001665592542849481 max memory_allocated 22563.63134765625 
[2025-03-22 06:28:02 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 9 loss:1.8682119846343994 norm:0.0015676141483709216 max memory_allocated 22563.63134765625 
[2025-03-22 06:28:35 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 10 loss:1.8671215772628784 norm:0.00148784217890352 max memory_allocated 22563.63134765625 
[2025-03-22 06:29:08 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 11 loss:1.8663790225982666 norm:0.0014707609079778194 max memory_allocated 22563.63134765625 
[2025-03-22 06:29:41 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 12 loss:1.8657057285308838 norm:0.001452665077522397 max memory_allocated 22563.63134765625 
[2025-03-22 06:30:13 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 13 loss:1.8651773929595947 norm:0.001406112452968955 max memory_allocated 22563.63134765625 
[2025-03-22 06:30:46 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 14 loss:1.8647578954696655 norm:0.0013584542321041226 max memory_allocated 22563.63134765625 
[2025-03-22 06:31:19 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 15 loss:1.8644225597381592 norm:0.0013526256661862135 max memory_allocated 22563.63134765625 
[2025-03-22 06:31:51 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 16 loss:1.8640193939208984 norm:0.0013489084085449576 max memory_allocated 22563.63134765625 
[2025-03-22 06:32:24 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 17 loss:1.863675832748413 norm:0.0013081690995022655 max memory_allocated 22563.63134765625 
[2025-03-22 06:32:57 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 18 loss:1.8633418083190918 norm:0.0013040071353316307 max memory_allocated 22563.63134765625 
[2025-03-22 06:33:30 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 19 loss:1.863042950630188 norm:0.0012782481499016285 max memory_allocated 22563.63134765625 
[2025-03-22 06:33:39 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 06:33:42 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 06:34:15 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 0 loss:2.272165536880493 norm:0.04809371381998062 max memory_allocated 22563.91845703125 
[2025-03-22 06:34:47 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 1 loss:2.242933750152588 norm:0.042592670768499374 max memory_allocated 22563.91845703125 
[2025-03-22 06:35:20 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 2 loss:2.2103271484375 norm:0.03369192034006119 max memory_allocated 22563.91845703125 
[2025-03-22 06:35:53 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 3 loss:2.1913809776306152 norm:0.02750781551003456 max memory_allocated 22563.91845703125 
[2025-03-22 06:36:26 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 4 loss:2.181583881378174 norm:0.023823119699954987 max memory_allocated 22563.91845703125 
[2025-03-22 06:36:58 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 5 loss:2.173841953277588 norm:0.02123260498046875 max memory_allocated 22563.91845703125 
[2025-03-22 06:37:31 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 6 loss:2.1673426628112793 norm:0.018919171765446663 max memory_allocated 22563.91845703125 
[2025-03-22 06:38:04 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 7 loss:2.162336587905884 norm:0.01745433174073696 max memory_allocated 22563.91845703125 
[2025-03-22 06:38:37 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 8 loss:2.1582913398742676 norm:0.016661014407873154 max memory_allocated 22563.91845703125 
[2025-03-22 06:39:10 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 9 loss:2.1548099517822266 norm:0.016139937564730644 max memory_allocated 22563.91845703125 
[2025-03-22 06:39:42 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 10 loss:2.1518054008483887 norm:0.015633475035429 max memory_allocated 22563.91845703125 
[2025-03-22 06:40:15 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 11 loss:2.1495065689086914 norm:0.015144572593271732 max memory_allocated 22563.91845703125 
[2025-03-22 06:40:48 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 12 loss:2.147221326828003 norm:0.015251461416482925 max memory_allocated 22563.91845703125 
[2025-03-22 06:41:21 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 13 loss:2.1450958251953125 norm:0.014232409186661243 max memory_allocated 22563.91845703125 
[2025-03-22 06:41:54 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 14 loss:2.1428587436676025 norm:0.014045752584934235 max memory_allocated 22563.91845703125 
[2025-03-22 06:42:26 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 15 loss:2.141329526901245 norm:0.013583356514573097 max memory_allocated 22563.91845703125 
[2025-03-22 06:42:59 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 16 loss:2.139751672744751 norm:0.01384116243571043 max memory_allocated 22563.91845703125 
[2025-03-22 06:43:32 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 17 loss:2.13834547996521 norm:0.013199642300605774 max memory_allocated 22563.91845703125 
[2025-03-22 06:44:05 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 18 loss:2.1370580196380615 norm:0.013251097872853279 max memory_allocated 22563.91845703125 
[2025-03-22 06:44:38 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 19 loss:2.1361327171325684 norm:0.012999454513192177 max memory_allocated 22563.91845703125 
[2025-03-22 06:44:47 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 06:44:50 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 06:45:23 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 0 loss:2.6011903285980225 norm:0.048394203186035156 max memory_allocated 22564.09033203125 
[2025-03-22 06:45:55 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 1 loss:2.559910774230957 norm:0.04094924032688141 max memory_allocated 22564.09033203125 
[2025-03-22 06:46:28 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 2 loss:2.5144267082214355 norm:0.031075164675712585 max memory_allocated 22564.09033203125 
[2025-03-22 06:47:01 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 3 loss:2.4889965057373047 norm:0.02559119462966919 max memory_allocated 22564.09033203125 
[2025-03-22 06:47:34 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 4 loss:2.4770355224609375 norm:0.021650414913892746 max memory_allocated 22564.09033203125 
[2025-03-22 06:48:07 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 5 loss:2.4682159423828125 norm:0.018511593341827393 max memory_allocated 22564.09033203125 
[2025-03-22 06:48:39 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 6 loss:2.4622607231140137 norm:0.01609228551387787 max memory_allocated 22564.09033203125 
[2025-03-22 06:49:12 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 7 loss:2.4583628177642822 norm:0.01455605961382389 max memory_allocated 22564.09033203125 
[2025-03-22 06:49:45 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 8 loss:2.4554848670959473 norm:0.013691057451069355 max memory_allocated 22564.09033203125 
[2025-03-22 06:50:18 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 9 loss:2.453720808029175 norm:0.013899242505431175 max memory_allocated 22564.09033203125 
[2025-03-22 06:50:51 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 10 loss:2.4518532752990723 norm:0.013252925127744675 max memory_allocated 22564.09033203125 
[2025-03-22 06:51:23 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 11 loss:2.4503884315490723 norm:0.013550070114433765 max memory_allocated 22564.09033203125 
[2025-03-22 06:51:56 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 12 loss:2.44868803024292 norm:0.01247736718505621 max memory_allocated 22564.09033203125 
[2025-03-22 06:52:29 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 13 loss:2.4476592540740967 norm:0.011977454647421837 max memory_allocated 22564.09033203125 
[2025-03-22 06:53:02 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 14 loss:2.446619749069214 norm:0.011748744174838066 max memory_allocated 22564.09033203125 
[2025-03-22 06:53:35 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 15 loss:2.446223497390747 norm:0.01205231063067913 max memory_allocated 22564.09033203125 
[2025-03-22 06:54:08 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 16 loss:2.445078134536743 norm:0.011610778048634529 max memory_allocated 22564.09033203125 
[2025-03-22 06:54:40 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 17 loss:2.444687604904175 norm:0.011697567999362946 max memory_allocated 22564.09033203125 
[2025-03-22 06:55:13 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 18 loss:2.4437685012817383 norm:0.01118097547441721 max memory_allocated 22564.09033203125 
[2025-03-22 06:55:46 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 19 loss:2.443415641784668 norm:0.011293141171336174 max memory_allocated 22564.09033203125 
[2025-03-22 06:55:55 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 06:55:58 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 06:56:31 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 0 loss:3.4561057090759277 norm:0.09754844754934311 max memory_allocated 22564.26220703125 
[2025-03-22 06:57:04 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 1 loss:3.360118865966797 norm:0.06751563400030136 max memory_allocated 22564.26220703125 
[2025-03-22 06:57:36 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 2 loss:3.2192740440368652 norm:0.03841947019100189 max memory_allocated 22564.26220703125 
[2025-03-22 06:58:09 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 3 loss:3.1566615104675293 norm:0.030482647940516472 max memory_allocated 22564.26220703125 
[2025-03-22 06:58:42 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 4 loss:3.127094268798828 norm:0.02778739668428898 max memory_allocated 22564.26220703125 
[2025-03-22 06:59:15 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 5 loss:3.1143829822540283 norm:0.029669860377907753 max memory_allocated 22564.26220703125 
[2025-03-22 06:59:47 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 6 loss:3.0990397930145264 norm:0.02998863346874714 max memory_allocated 22564.26220703125 
[2025-03-22 07:00:20 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 7 loss:3.0860464572906494 norm:0.029232721775770187 max memory_allocated 22564.26220703125 
[2025-03-22 07:00:53 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 8 loss:3.075880289077759 norm:0.02908584475517273 max memory_allocated 22564.26220703125 
[2025-03-22 07:01:26 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 9 loss:3.0679850578308105 norm:0.029225928708910942 max memory_allocated 22564.26220703125 
[2025-03-22 07:01:59 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 10 loss:3.0705747604370117 norm:0.0319015271961689 max memory_allocated 22564.26220703125 
[2025-03-22 07:02:31 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 11 loss:3.0609686374664307 norm:0.03061879798769951 max memory_allocated 22564.26220703125 
[2025-03-22 07:03:04 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 12 loss:3.0587565898895264 norm:0.03279438242316246 max memory_allocated 22564.26220703125 
[2025-03-22 07:03:37 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 13 loss:3.058096408843994 norm:0.034363117069005966 max memory_allocated 22564.26220703125 
[2025-03-22 07:04:10 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 14 loss:3.0486278533935547 norm:0.03234231844544411 max memory_allocated 22564.26220703125 
[2025-03-22 07:04:43 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 15 loss:3.0473036766052246 norm:0.03193175792694092 max memory_allocated 22564.26220703125 
[2025-03-22 07:05:15 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 16 loss:3.0481882095336914 norm:0.033094003796577454 max memory_allocated 22564.26220703125 
[2025-03-22 07:05:48 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 17 loss:3.050370931625366 norm:0.03514035418629646 max memory_allocated 22564.26220703125 
[2025-03-22 07:06:21 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 18 loss:3.045480251312256 norm:0.03660212457180023 max memory_allocated 22564.26220703125 
[2025-03-22 07:06:54 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 19 loss:3.0439293384552 norm:0.0337776318192482 max memory_allocated 22564.26220703125 
[2025-03-22 07:07:03 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 07:07:06 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 07:07:39 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 0 loss:6.6762309074401855 norm:0.41247209906578064 max memory_allocated 22564.43408203125 
[2025-03-22 07:08:11 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 1 loss:6.230245590209961 norm:0.32363760471343994 max memory_allocated 22564.43408203125 
[2025-03-22 07:08:44 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 2 loss:5.906981945037842 norm:0.2809954583644867 max memory_allocated 22564.43408203125 
[2025-03-22 07:09:17 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 3 loss:5.717195987701416 norm:0.27985191345214844 max memory_allocated 22564.43408203125 
[2025-03-22 07:09:50 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 4 loss:5.618244171142578 norm:0.2562319040298462 max memory_allocated 22564.43408203125 
[2025-03-22 07:10:22 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 5 loss:5.526327133178711 norm:0.2480718195438385 max memory_allocated 22564.43408203125 
[2025-03-22 07:10:55 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 6 loss:5.47157096862793 norm:0.24375379085540771 max memory_allocated 22564.43408203125 
[2025-03-22 07:11:28 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 7 loss:5.4246344566345215 norm:0.2283364236354828 max memory_allocated 22564.43408203125 
[2025-03-22 07:12:01 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 8 loss:5.387130260467529 norm:0.2140439748764038 max memory_allocated 22564.43408203125 
[2025-03-22 07:12:34 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 9 loss:5.357245445251465 norm:0.20597347617149353 max memory_allocated 22564.43408203125 
[2025-03-22 07:13:06 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 10 loss:5.335445404052734 norm:0.19910986721515656 max memory_allocated 22564.43408203125 
[2025-03-22 07:13:39 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 11 loss:5.315101146697998 norm:0.19150584936141968 max memory_allocated 22564.43408203125 
[2025-03-22 07:14:12 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 12 loss:5.2998528480529785 norm:0.18517006933689117 max memory_allocated 22564.43408203125 
[2025-03-22 07:14:45 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 13 loss:5.286025524139404 norm:0.1835508495569229 max memory_allocated 22564.43408203125 
[2025-03-22 07:15:17 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 14 loss:5.271304130554199 norm:0.1785435527563095 max memory_allocated 22564.43408203125 
[2025-03-22 07:15:50 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 15 loss:5.2638959884643555 norm:0.1786661595106125 max memory_allocated 22564.43408203125 
[2025-03-22 07:16:23 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 16 loss:5.250065803527832 norm:0.17016299068927765 max memory_allocated 22564.43408203125 
[2025-03-22 07:16:56 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 17 loss:5.242665767669678 norm:0.16465327143669128 max memory_allocated 22564.43408203125 
[2025-03-22 07:17:29 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 18 loss:5.2364020347595215 norm:0.16439491510391235 max memory_allocated 22564.43408203125 
[2025-03-22 07:18:02 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 19 loss:5.230544567108154 norm:0.16167664527893066 max memory_allocated 22564.43408203125 
[2025-03-22 07:18:11 root] (main_calibration_a2.py 370): INFO 21313.390976667404
[2025-03-22 07:18:16 root] (main_calibration_a2.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-22 07:19:25 root] (main_calibration_a2.py 158): INFO wikitext2 : 8.684081077575684
[2025-03-22 07:19:26 root] (main_calibration_a2.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-22 07:21:14 root] (main_calibration_a2.py 158): INFO c4 : 12.723217010498047
