[2025-03-22 01:18:05 root] (main_calibration_a2.py 274): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-loss/Llama-2-7b-hf-w4a4-mse', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=1, loss_type='mse')
[2025-03-22 01:21:13 root] (main_calibration_a2.py 341): INFO === start quantization ===
[2025-03-22 01:21:13 root] (main_calibration_a2.py 347): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-22 01:21:13 root] (abq_llm_calibration_a2.py 62): INFO Starting ...
[2025-03-22 01:21:16 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 01:21:20 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 01:21:52 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 0 loss:9.669206338003278e-05 norm:7.6053554948885e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:22:25 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 1 loss:6.028800635249354e-05 norm:5.0532184104667976e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:22:58 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 2 loss:5.177126149646938e-05 norm:3.9881717384560034e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:23:31 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 3 loss:4.525292388279922e-05 norm:3.1071307603269815e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:24:04 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 4 loss:4.3977626773994416e-05 norm:2.9416194593068212e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:24:37 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 5 loss:4.1534003685228527e-05 norm:2.6385383534943685e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:25:09 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 6 loss:4.241072019794956e-05 norm:2.852159559552092e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:25:42 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 7 loss:4.1282637539552525e-05 norm:2.501706148905214e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:26:14 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 8 loss:4.0502298361388966e-05 norm:2.361236693104729e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:26:47 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 9 loss:3.922254472854547e-05 norm:2.1613528588204645e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:27:19 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 10 loss:3.942519470001571e-05 norm:2.0827752450713888e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:27:52 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 11 loss:3.9409111195709556e-05 norm:1.990475539059844e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:28:24 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 12 loss:3.9320075302384794e-05 norm:1.900906499940902e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:28:57 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 13 loss:3.890338848577812e-05 norm:1.7430678781238385e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:29:30 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 14 loss:3.85026287403889e-05 norm:1.637657987885177e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:30:02 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 15 loss:3.8651967770420015e-05 norm:1.640033042349387e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:30:35 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 16 loss:3.841777652269229e-05 norm:1.5160209841269534e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:31:07 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 17 loss:3.8405902159865946e-05 norm:1.4530161934089847e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:31:40 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 18 loss:3.840967110591009e-05 norm:1.3571607269113883e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:32:12 root] (abq_llm_calibration_a2.py 394): INFO layer 0 iter 19 loss:3.818902769125998e-05 norm:1.2865455573773943e-05 max memory_allocated 22562.10595703125 
[2025-03-22 01:32:21 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 01:32:24 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 01:32:57 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 0 loss:0.04350019618868828 norm:0.016543354839086533 max memory_allocated 22562.27783203125 
[2025-03-22 01:33:29 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 1 loss:0.03215941786766052 norm:0.012305540032684803 max memory_allocated 22562.27783203125 
[2025-03-22 01:34:02 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 2 loss:0.026358500123023987 norm:0.01054301206022501 max memory_allocated 22562.27783203125 
[2025-03-22 01:34:34 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 3 loss:0.023696376010775566 norm:0.012122414074838161 max memory_allocated 22562.27783203125 
[2025-03-22 01:35:07 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 4 loss:0.020976541563868523 norm:0.010116015560925007 max memory_allocated 22562.27783203125 
[2025-03-22 01:35:40 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 5 loss:0.017935067415237427 norm:0.008457417599856853 max memory_allocated 22562.27783203125 
[2025-03-22 01:36:12 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 6 loss:0.017188457772135735 norm:0.007934699766337872 max memory_allocated 22562.27783203125 
[2025-03-22 01:36:45 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 7 loss:0.01643254980444908 norm:0.008828895166516304 max memory_allocated 22562.27783203125 
[2025-03-22 01:37:17 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 8 loss:0.01576443575322628 norm:0.007181310094892979 max memory_allocated 22562.27783203125 
[2025-03-22 01:37:50 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 9 loss:0.015697114169597626 norm:0.007255063392221928 max memory_allocated 22562.27783203125 
[2025-03-22 01:38:22 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 10 loss:0.016538413241505623 norm:0.025490548461675644 max memory_allocated 22562.27783203125 
[2025-03-22 01:38:55 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 11 loss:0.014902832917869091 norm:0.006302982568740845 max memory_allocated 22562.27783203125 
[2025-03-22 01:39:28 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 12 loss:0.01485989335924387 norm:0.005731611978262663 max memory_allocated 22562.27783203125 
[2025-03-22 01:40:00 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 13 loss:0.015073616988956928 norm:0.005817723926156759 max memory_allocated 22562.27783203125 
[2025-03-22 01:40:33 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 14 loss:0.015227757394313812 norm:0.006058350205421448 max memory_allocated 22562.27783203125 
[2025-03-22 01:41:05 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 15 loss:0.015352923423051834 norm:0.006814752705395222 max memory_allocated 22562.27783203125 
[2025-03-22 01:41:38 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 16 loss:0.014877385459840298 norm:0.00561847910284996 max memory_allocated 22562.27783203125 
[2025-03-22 01:42:10 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 17 loss:0.01575343683362007 norm:0.00728979567065835 max memory_allocated 22562.27783203125 
[2025-03-22 01:42:43 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 18 loss:0.015544367954134941 norm:0.007831565104424953 max memory_allocated 22562.27783203125 
[2025-03-22 01:43:15 root] (abq_llm_calibration_a2.py 394): INFO layer 1 iter 19 loss:0.015358555130660534 norm:0.00839861761778593 max memory_allocated 22562.27783203125 
[2025-03-22 01:43:24 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 01:43:28 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 01:44:00 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 0 loss:0.007863222621381283 norm:0.0004714331589639187 max memory_allocated 22562.44970703125 
[2025-03-22 01:44:33 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 1 loss:0.007724378723651171 norm:0.0003394999075680971 max memory_allocated 22562.44970703125 
[2025-03-22 01:45:06 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 2 loss:0.00749907037243247 norm:0.0006497854483313859 max memory_allocated 22562.44970703125 
[2025-03-22 01:45:38 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 3 loss:0.007267113775014877 norm:0.0010777006391435862 max memory_allocated 22562.44970703125 
[2025-03-22 01:46:11 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 4 loss:0.0071961646899580956 norm:0.0011626016348600388 max memory_allocated 22562.44970703125 
[2025-03-22 01:46:44 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 5 loss:0.007161383982747793 norm:0.0011432018363848329 max memory_allocated 22562.44970703125 
[2025-03-22 01:47:17 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 6 loss:0.007140309549868107 norm:0.0011622277088463306 max memory_allocated 22562.44970703125 
[2025-03-22 01:47:49 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 7 loss:0.007123422808945179 norm:0.0011634022230282426 max memory_allocated 22562.44970703125 
[2025-03-22 01:48:22 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 8 loss:0.007114599924534559 norm:0.0011575958924368024 max memory_allocated 22562.44970703125 
[2025-03-22 01:48:55 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 9 loss:0.007099850103259087 norm:0.0011508218012750149 max memory_allocated 22562.44970703125 
[2025-03-22 01:49:28 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 10 loss:0.007103029638528824 norm:0.0011339329648762941 max memory_allocated 22562.44970703125 
[2025-03-22 01:50:00 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 11 loss:0.0070899780839681625 norm:0.0011289605172351003 max memory_allocated 22562.44970703125 
[2025-03-22 01:50:33 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 12 loss:0.007076645735651255 norm:0.0011287350207567215 max memory_allocated 22562.44970703125 
[2025-03-22 01:51:06 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 13 loss:0.007092015352100134 norm:0.0011239505838602781 max memory_allocated 22562.44970703125 
[2025-03-22 01:51:39 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 14 loss:0.00707064475864172 norm:0.0011327494867146015 max memory_allocated 22562.44970703125 
[2025-03-22 01:52:11 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 15 loss:0.007069609593600035 norm:0.001129426877014339 max memory_allocated 22562.44970703125 
[2025-03-22 01:52:44 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 16 loss:0.007072190288454294 norm:0.0011364989914000034 max memory_allocated 22562.44970703125 
[2025-03-22 01:53:17 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 17 loss:0.00706457020714879 norm:0.0011277655139565468 max memory_allocated 22562.44970703125 
[2025-03-22 01:53:50 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 18 loss:0.0070629180409014225 norm:0.0011255079880356789 max memory_allocated 22562.44970703125 
[2025-03-22 01:54:22 root] (abq_llm_calibration_a2.py 394): INFO layer 2 iter 19 loss:0.0070618316531181335 norm:0.0011247311485931277 max memory_allocated 22562.44970703125 
[2025-03-22 01:54:32 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 01:55:07 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 0 loss:0.008444861508905888 norm:0.00017106956511270255 max memory_allocated 22562.50634765625 
[2025-03-22 01:55:40 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 1 loss:0.008243517950177193 norm:6.647558620898053e-05 max memory_allocated 22562.50634765625 
[2025-03-22 01:56:13 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 2 loss:0.008162428624927998 norm:5.1824954425683245e-05 max memory_allocated 22562.50634765625 
[2025-03-22 01:56:45 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 3 loss:0.008111252449452877 norm:3.6833331250818446e-05 max memory_allocated 22562.50634765625 
[2025-03-22 01:57:18 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 4 loss:0.008078590035438538 norm:2.7082989618065767e-05 max memory_allocated 22562.50634765625 
[2025-03-22 01:57:50 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 5 loss:0.008048411458730698 norm:2.0579052943503484e-05 max memory_allocated 22562.50634765625 
[2025-03-22 01:58:23 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 6 loss:0.008026087656617165 norm:1.7364020095556043e-05 max memory_allocated 22562.50634765625 
[2025-03-22 01:58:56 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 7 loss:0.008019091561436653 norm:1.7427903003408574e-05 max memory_allocated 22562.50634765625 
[2025-03-22 01:59:28 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 8 loss:0.008015991188585758 norm:1.5972362234606408e-05 max memory_allocated 22562.50634765625 
[2025-03-22 02:00:01 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 9 loss:0.008006762713193893 norm:1.5016679753898643e-05 max memory_allocated 22562.50634765625 
[2025-03-22 02:00:34 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 10 loss:0.007996207103133202 norm:1.4633020327892154e-05 max memory_allocated 22562.50634765625 
[2025-03-22 02:01:06 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 11 loss:0.007993301376700401 norm:1.410460754414089e-05 max memory_allocated 22562.50634765625 
[2025-03-22 02:01:39 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 12 loss:0.007992099039256573 norm:1.3860631952411495e-05 max memory_allocated 22562.50634765625 
[2025-03-22 02:02:12 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 13 loss:0.007988547906279564 norm:1.3448250683723018e-05 max memory_allocated 22562.50634765625 
[2025-03-22 02:02:44 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 14 loss:0.007985681295394897 norm:1.3234900507086422e-05 max memory_allocated 22562.50634765625 
[2025-03-22 02:03:17 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 15 loss:0.007982461713254452 norm:1.3112924534652848e-05 max memory_allocated 22562.50634765625 
[2025-03-22 02:03:50 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 16 loss:0.007979371584951878 norm:1.3134976143192034e-05 max memory_allocated 22562.50634765625 
[2025-03-22 02:04:22 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 17 loss:0.007978308945894241 norm:1.2774576134688687e-05 max memory_allocated 22562.50634765625 
[2025-03-22 02:04:55 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 18 loss:0.00797294918447733 norm:1.25219357869355e-05 max memory_allocated 22562.50634765625 
[2025-03-22 02:05:28 root] (abq_llm_calibration_a2.py 394): INFO layer 3 iter 19 loss:0.007967421784996986 norm:1.2522387805802282e-05 max memory_allocated 22562.50634765625 
[2025-03-22 02:05:37 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 02:06:12 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 0 loss:0.011420155875384808 norm:0.0006370936171151698 max memory_allocated 22562.67822265625 
[2025-03-22 02:06:45 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 1 loss:0.010930223390460014 norm:0.00020751109695993364 max memory_allocated 22562.67822265625 
[2025-03-22 02:07:18 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 2 loss:0.010630880482494831 norm:8.250633254647255e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:07:50 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 3 loss:0.010486947372555733 norm:5.088936450192705e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:08:23 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 4 loss:0.010420390404760838 norm:4.0413135138805956e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:08:56 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 5 loss:0.010377106256783009 norm:3.5707820643438026e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:09:28 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 6 loss:0.01034646388143301 norm:3.1524708901997656e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:10:01 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 7 loss:0.010320659726858139 norm:2.9504300982807763e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:10:34 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 8 loss:0.01029608491808176 norm:2.909476461354643e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:11:06 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 9 loss:0.01027724426239729 norm:2.7488527848618105e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:11:39 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 10 loss:0.010252430103719234 norm:2.749916529865004e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:12:12 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 11 loss:0.01022564247250557 norm:2.7270296413917094e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:12:44 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 12 loss:0.010206609033048153 norm:2.62626326730242e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:13:17 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 13 loss:0.010191155597567558 norm:2.4792943804641254e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:13:50 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 14 loss:0.010180341079831123 norm:2.4134684281307273e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:14:22 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 15 loss:0.010170843452215195 norm:2.4046790713327937e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:14:55 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 16 loss:0.010164202190935612 norm:2.27784548769705e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:15:28 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 17 loss:0.01015313807874918 norm:2.1755708075943403e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:16:00 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 18 loss:0.010147680528461933 norm:2.2271648049354553e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:16:33 root] (abq_llm_calibration_a2.py 394): INFO layer 4 iter 19 loss:0.010141739621758461 norm:2.259832945128437e-05 max memory_allocated 22562.67822265625 
[2025-03-22 02:16:42 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 02:17:18 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 0 loss:0.014575877226889133 norm:0.0003576493763830513 max memory_allocated 22562.85009765625 
[2025-03-22 02:17:50 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 1 loss:0.014028573408722878 norm:0.00017624287283979356 max memory_allocated 22562.85009765625 
[2025-03-22 02:18:23 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 2 loss:0.013622612692415714 norm:9.039202996063977e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:18:56 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 3 loss:0.013432473875582218 norm:5.871963730896823e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:19:28 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 4 loss:0.013303942047059536 norm:4.736623304779641e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:20:01 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 5 loss:0.013215522281825542 norm:4.100475416635163e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:20:34 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 6 loss:0.0131587665528059 norm:3.9328704588115215e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:21:06 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 7 loss:0.013106385245919228 norm:3.550405745045282e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:21:39 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 8 loss:0.01305893249809742 norm:3.320847463328391e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:22:12 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 9 loss:0.013012697920203209 norm:3.167566319461912e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:22:44 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 10 loss:0.012982120737433434 norm:3.138979081995785e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:23:17 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 11 loss:0.012958690524101257 norm:3.059835216845386e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:23:50 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 12 loss:0.012950589880347252 norm:2.9917680876678787e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:24:22 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 13 loss:0.012947414070367813 norm:2.9431606890284456e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:24:55 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 14 loss:0.012937280349433422 norm:2.8778082196367905e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:25:28 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 15 loss:0.012923171743750572 norm:2.860781387425959e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:26:00 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 16 loss:0.01291155256330967 norm:2.871665492421016e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:26:33 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 17 loss:0.012905716896057129 norm:2.7221569325774908e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:27:06 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 18 loss:0.012909379787743092 norm:2.7262412913842127e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:27:39 root] (abq_llm_calibration_a2.py 394): INFO layer 5 iter 19 loss:0.012898369692265987 norm:2.6737909138319083e-05 max memory_allocated 22562.85009765625 
[2025-03-22 02:27:48 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 02:28:23 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 0 loss:0.020406512543559074 norm:0.0011298238532617688 max memory_allocated 22563.02197265625 
[2025-03-22 02:28:56 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 1 loss:0.01911875605583191 norm:0.0004663477302528918 max memory_allocated 22563.02197265625 
[2025-03-22 02:29:29 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 2 loss:0.01836187019944191 norm:0.0002661119506228715 max memory_allocated 22563.02197265625 
[2025-03-22 02:30:01 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 3 loss:0.01791805401444435 norm:0.00017650006338953972 max memory_allocated 22563.02197265625 
[2025-03-22 02:30:34 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 4 loss:0.017622997984290123 norm:0.00013029792171437293 max memory_allocated 22563.02197265625 
[2025-03-22 02:31:07 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 5 loss:0.017444495111703873 norm:0.0001113152684411034 max memory_allocated 22563.02197265625 
[2025-03-22 02:31:39 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 6 loss:0.01731693744659424 norm:9.778255480341613e-05 max memory_allocated 22563.02197265625 
[2025-03-22 02:32:12 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 7 loss:0.017231665551662445 norm:8.896217332221568e-05 max memory_allocated 22563.02197265625 
[2025-03-22 02:32:45 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 8 loss:0.01715882308781147 norm:8.160634752130136e-05 max memory_allocated 22563.02197265625 
[2025-03-22 02:33:17 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 9 loss:0.017099393531680107 norm:7.31080726836808e-05 max memory_allocated 22563.02197265625 
[2025-03-22 02:33:50 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 10 loss:0.01705211214721203 norm:6.413177470676601e-05 max memory_allocated 22563.02197265625 
[2025-03-22 02:34:23 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 11 loss:0.01702699437737465 norm:6.166097591631114e-05 max memory_allocated 22563.02197265625 
[2025-03-22 02:34:56 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 12 loss:0.017003778368234634 norm:5.7858698710333556e-05 max memory_allocated 22563.02197265625 
[2025-03-22 02:35:28 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 13 loss:0.016984038054943085 norm:5.578369746217504e-05 max memory_allocated 22563.02197265625 
[2025-03-22 02:36:01 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 14 loss:0.01696135476231575 norm:5.230884926277213e-05 max memory_allocated 22563.02197265625 
[2025-03-22 02:36:34 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 15 loss:0.01695793867111206 norm:5.076705929241143e-05 max memory_allocated 22563.02197265625 
[2025-03-22 02:37:06 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 16 loss:0.016951320692896843 norm:5.238509766058996e-05 max memory_allocated 22563.02197265625 
[2025-03-22 02:37:39 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 17 loss:0.016951613128185272 norm:5.054810753790662e-05 max memory_allocated 22563.02197265625 
[2025-03-22 02:38:12 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 18 loss:0.016938522458076477 norm:4.9236226914217696e-05 max memory_allocated 22563.02197265625 
[2025-03-22 02:38:45 root] (abq_llm_calibration_a2.py 394): INFO layer 6 iter 19 loss:0.016925157979130745 norm:4.807243749382906e-05 max memory_allocated 22563.02197265625 
[2025-03-22 02:38:54 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 02:39:29 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 0 loss:0.0272359699010849 norm:0.001915699103847146 max memory_allocated 22563.19384765625 
[2025-03-22 02:40:02 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 1 loss:0.024458302184939384 norm:0.0005590352811850607 max memory_allocated 22563.19384765625 
[2025-03-22 02:40:35 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 2 loss:0.023395594209432602 norm:0.00033039198024198413 max memory_allocated 22563.19384765625 
[2025-03-22 02:41:07 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 3 loss:0.02283148095011711 norm:0.00021648775145877153 max memory_allocated 22563.19384765625 
[2025-03-22 02:41:40 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 4 loss:0.02246115356683731 norm:0.000169958351762034 max memory_allocated 22563.19384765625 
[2025-03-22 02:42:13 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 5 loss:0.02216312289237976 norm:0.00013556673366110772 max memory_allocated 22563.19384765625 
[2025-03-22 02:42:46 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 6 loss:0.022001001983880997 norm:0.00012518453877419233 max memory_allocated 22563.19384765625 
[2025-03-22 02:43:18 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 7 loss:0.021909091621637344 norm:0.0001112692552851513 max memory_allocated 22563.19384765625 
[2025-03-22 02:43:51 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 8 loss:0.021843723952770233 norm:9.991596743930131e-05 max memory_allocated 22563.19384765625 
[2025-03-22 02:44:24 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 9 loss:0.021799322217702866 norm:9.321530524175614e-05 max memory_allocated 22563.19384765625 
[2025-03-22 02:44:56 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 10 loss:0.02176295779645443 norm:8.726715168450028e-05 max memory_allocated 22563.19384765625 
[2025-03-22 02:45:29 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 11 loss:0.021724902093410492 norm:8.598584099672735e-05 max memory_allocated 22563.19384765625 
[2025-03-22 02:46:02 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 12 loss:0.021679705008864403 norm:7.959168578963727e-05 max memory_allocated 22563.19384765625 
[2025-03-22 02:46:35 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 13 loss:0.021648360416293144 norm:7.826594082871452e-05 max memory_allocated 22563.19384765625 
[2025-03-22 02:47:07 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 14 loss:0.021613167598843575 norm:7.47158846934326e-05 max memory_allocated 22563.19384765625 
[2025-03-22 02:47:40 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 15 loss:0.021582817658782005 norm:7.249267218867317e-05 max memory_allocated 22563.19384765625 
[2025-03-22 02:48:13 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 16 loss:0.02156343311071396 norm:6.862283771624789e-05 max memory_allocated 22563.19384765625 
[2025-03-22 02:48:45 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 17 loss:0.02154243364930153 norm:6.636400212300941e-05 max memory_allocated 22563.19384765625 
[2025-03-22 02:49:18 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 18 loss:0.021526429802179337 norm:6.372639472829178e-05 max memory_allocated 22563.19384765625 
[2025-03-22 02:49:51 root] (abq_llm_calibration_a2.py 394): INFO layer 7 iter 19 loss:0.02152033895254135 norm:6.250462320167571e-05 max memory_allocated 22563.19384765625 
[2025-03-22 02:50:00 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 02:50:36 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 0 loss:0.03262971341609955 norm:0.0013765598414465785 max memory_allocated 22563.36572265625 
[2025-03-22 02:51:08 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 1 loss:0.030609644949436188 norm:0.0005426958668977022 max memory_allocated 22563.36572265625 
[2025-03-22 02:51:41 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 2 loss:0.029395606368780136 norm:0.0002832942409440875 max memory_allocated 22563.36572265625 
[2025-03-22 02:52:14 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 3 loss:0.028669189661741257 norm:0.00018501882732380182 max memory_allocated 22563.36572265625 
[2025-03-22 02:52:47 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 4 loss:0.028202617540955544 norm:0.00014350937271956354 max memory_allocated 22563.36572265625 
[2025-03-22 02:53:19 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 5 loss:0.02789061889052391 norm:0.00012155638250987977 max memory_allocated 22563.36572265625 
[2025-03-22 02:53:52 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 6 loss:0.027719786390662193 norm:0.00010714952077250928 max memory_allocated 22563.36572265625 
[2025-03-22 02:54:25 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 7 loss:0.027635253965854645 norm:9.738208609633148e-05 max memory_allocated 22563.36572265625 
[2025-03-22 02:54:57 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 8 loss:0.02756432443857193 norm:9.110564860748127e-05 max memory_allocated 22563.36572265625 
[2025-03-22 02:55:30 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 9 loss:0.027508843690156937 norm:8.640379382995889e-05 max memory_allocated 22563.36572265625 
[2025-03-22 02:56:03 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 10 loss:0.02746356464922428 norm:8.32231598906219e-05 max memory_allocated 22563.36572265625 
[2025-03-22 02:56:36 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 11 loss:0.027419324964284897 norm:8.061017433647066e-05 max memory_allocated 22563.36572265625 
[2025-03-22 02:57:08 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 12 loss:0.027378704398870468 norm:7.771948003210127e-05 max memory_allocated 22563.36572265625 
[2025-03-22 02:57:41 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 13 loss:0.027334092184901237 norm:7.68039099057205e-05 max memory_allocated 22563.36572265625 
[2025-03-22 02:58:14 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 14 loss:0.02730686217546463 norm:7.636799273313954e-05 max memory_allocated 22563.36572265625 
[2025-03-22 02:58:47 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 15 loss:0.027265198528766632 norm:7.62784038670361e-05 max memory_allocated 22563.36572265625 
[2025-03-22 02:59:19 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 16 loss:0.02723626233637333 norm:7.464361988240853e-05 max memory_allocated 22563.36572265625 
[2025-03-22 02:59:52 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 17 loss:0.027213845402002335 norm:7.373363769147545e-05 max memory_allocated 22563.36572265625 
[2025-03-22 03:00:25 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 18 loss:0.02719109132885933 norm:7.188404561020434e-05 max memory_allocated 22563.36572265625 
[2025-03-22 03:00:57 root] (abq_llm_calibration_a2.py 394): INFO layer 8 iter 19 loss:0.02717287838459015 norm:7.160823588492349e-05 max memory_allocated 22563.36572265625 
[2025-03-22 03:01:07 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 03:01:42 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 0 loss:0.04259825125336647 norm:0.0028284755535423756 max memory_allocated 22563.53759765625 
[2025-03-22 03:02:15 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 1 loss:0.03916719928383827 norm:0.0012263953685760498 max memory_allocated 22563.53759765625 
[2025-03-22 03:02:48 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 2 loss:0.0363549180328846 norm:0.00039070562343113124 max memory_allocated 22563.53759765625 
[2025-03-22 03:03:20 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 3 loss:0.03537628427147865 norm:0.00024281615333165973 max memory_allocated 22563.53759765625 
[2025-03-22 03:03:53 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 4 loss:0.03482765331864357 norm:0.0002054746582871303 max memory_allocated 22563.53759765625 
[2025-03-22 03:04:26 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 5 loss:0.03446093201637268 norm:0.00017825652321334928 max memory_allocated 22563.53759765625 
[2025-03-22 03:04:59 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 6 loss:0.03425419703125954 norm:0.00015955774870235473 max memory_allocated 22563.53759765625 
[2025-03-22 03:05:31 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 7 loss:0.03415365517139435 norm:0.00014967418974265456 max memory_allocated 22563.53759765625 
[2025-03-22 03:06:04 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 8 loss:0.03405541926622391 norm:0.00013707725156564265 max memory_allocated 22563.53759765625 
[2025-03-22 03:06:37 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 9 loss:0.033955156803131104 norm:0.0001293492823606357 max memory_allocated 22563.53759765625 
[2025-03-22 03:07:09 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 10 loss:0.03387831524014473 norm:0.00012257012713234872 max memory_allocated 22563.53759765625 
[2025-03-22 03:07:42 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 11 loss:0.03381174057722092 norm:0.00011266855290159583 max memory_allocated 22563.53759765625 
[2025-03-22 03:08:15 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 12 loss:0.033752162009477615 norm:0.00010691457282518968 max memory_allocated 22563.53759765625 
[2025-03-22 03:08:48 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 13 loss:0.033701036125421524 norm:9.969310485757887e-05 max memory_allocated 22563.53759765625 
[2025-03-22 03:09:20 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 14 loss:0.033647600561380386 norm:9.715442865854129e-05 max memory_allocated 22563.53759765625 
[2025-03-22 03:09:53 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 15 loss:0.03360297158360481 norm:9.40771569730714e-05 max memory_allocated 22563.53759765625 
[2025-03-22 03:10:26 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 16 loss:0.03355688601732254 norm:8.979074482340366e-05 max memory_allocated 22563.53759765625 
[2025-03-22 03:10:58 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 17 loss:0.03351420536637306 norm:8.77188504091464e-05 max memory_allocated 22563.53759765625 
[2025-03-22 03:11:31 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 18 loss:0.03347119688987732 norm:8.704456558916718e-05 max memory_allocated 22563.53759765625 
[2025-03-22 03:12:04 root] (abq_llm_calibration_a2.py 394): INFO layer 9 iter 19 loss:0.033439211547374725 norm:8.618028368800879e-05 max memory_allocated 22563.53759765625 
[2025-03-22 03:12:13 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 03:12:49 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 0 loss:0.05070370435714722 norm:0.003566398750990629 max memory_allocated 22563.70947265625 
[2025-03-22 03:13:21 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 1 loss:0.047192785888910294 norm:0.0015079730655997992 max memory_allocated 22563.70947265625 
[2025-03-22 03:13:54 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 2 loss:0.04435155168175697 norm:0.0006398664554581046 max memory_allocated 22563.70947265625 
[2025-03-22 03:14:27 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 3 loss:0.0428498312830925 norm:0.00027338656946085393 max memory_allocated 22563.70947265625 
[2025-03-22 03:15:00 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 4 loss:0.041963014751672745 norm:0.00017116645176429302 max memory_allocated 22563.70947265625 
[2025-03-22 03:15:32 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 5 loss:0.041484128683805466 norm:0.00015396474918816239 max memory_allocated 22563.70947265625 
[2025-03-22 03:16:05 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 6 loss:0.041242312639951706 norm:0.00014397871564142406 max memory_allocated 22563.70947265625 
[2025-03-22 03:16:38 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 7 loss:0.04107596352696419 norm:0.00013439831673167646 max memory_allocated 22563.70947265625 
[2025-03-22 03:17:10 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 8 loss:0.04093657061457634 norm:0.0001255210372619331 max memory_allocated 22563.70947265625 
[2025-03-22 03:17:43 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 9 loss:0.04082507640123367 norm:0.00012342627451289445 max memory_allocated 22563.70947265625 
[2025-03-22 03:18:16 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 10 loss:0.04073052853345871 norm:0.00011956398520851508 max memory_allocated 22563.70947265625 
[2025-03-22 03:18:49 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 11 loss:0.04064982756972313 norm:0.00011349238775437698 max memory_allocated 22563.70947265625 
[2025-03-22 03:19:21 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 12 loss:0.04057318717241287 norm:0.00011045414430554956 max memory_allocated 22563.70947265625 
[2025-03-22 03:19:54 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 13 loss:0.04051791504025459 norm:0.00010754010872915387 max memory_allocated 22563.70947265625 
[2025-03-22 03:20:27 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 14 loss:0.04045999050140381 norm:0.00010364848276367411 max memory_allocated 22563.70947265625 
[2025-03-22 03:21:00 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 15 loss:0.04040271416306496 norm:0.00010148356523131952 max memory_allocated 22563.70947265625 
[2025-03-22 03:21:32 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 16 loss:0.04034901410341263 norm:9.847967885434628e-05 max memory_allocated 22563.70947265625 
[2025-03-22 03:22:05 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 17 loss:0.040299877524375916 norm:9.564846550347283e-05 max memory_allocated 22563.70947265625 
[2025-03-22 03:22:38 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 18 loss:0.04025857523083687 norm:9.377077367389575e-05 max memory_allocated 22563.70947265625 
[2025-03-22 03:23:11 root] (abq_llm_calibration_a2.py 394): INFO layer 10 iter 19 loss:0.04021943360567093 norm:9.163433423964307e-05 max memory_allocated 22563.70947265625 
[2025-03-22 03:23:20 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 03:23:55 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 0 loss:0.056923698633909225 norm:0.0024455776438117027 max memory_allocated 22563.88134765625 
[2025-03-22 03:24:28 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 1 loss:0.052559901028871536 norm:0.0009438513079658151 max memory_allocated 22563.88134765625 
[2025-03-22 03:25:01 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 2 loss:0.05013839527964592 norm:0.0004887765389867127 max memory_allocated 22563.88134765625 
[2025-03-22 03:25:33 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 3 loss:0.04864340275526047 norm:0.0002799178473651409 max memory_allocated 22563.88134765625 
[2025-03-22 03:26:06 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 4 loss:0.04772832244634628 norm:0.0002007146249525249 max memory_allocated 22563.88134765625 
[2025-03-22 03:26:39 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 5 loss:0.047268688678741455 norm:0.00016622360271867365 max memory_allocated 22563.88134765625 
[2025-03-22 03:27:12 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 6 loss:0.04701735079288483 norm:0.00014902243856340647 max memory_allocated 22563.88134765625 
[2025-03-22 03:27:44 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 7 loss:0.04686274379491806 norm:0.00014002659008838236 max memory_allocated 22563.88134765625 
[2025-03-22 03:28:17 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 8 loss:0.04672372713685036 norm:0.00013292445510160178 max memory_allocated 22563.88134765625 
[2025-03-22 03:28:50 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 9 loss:0.04661056026816368 norm:0.00012404299923218787 max memory_allocated 22563.88134765625 
[2025-03-22 03:29:23 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 10 loss:0.04651207476854324 norm:0.00012225614045746624 max memory_allocated 22563.88134765625 
[2025-03-22 03:29:55 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 11 loss:0.04642005264759064 norm:0.00011408609134377912 max memory_allocated 22563.88134765625 
[2025-03-22 03:30:28 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 12 loss:0.04632936790585518 norm:0.00010919163469225168 max memory_allocated 22563.88134765625 
[2025-03-22 03:31:01 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 13 loss:0.046284593641757965 norm:0.00010800408199429512 max memory_allocated 22563.88134765625 
[2025-03-22 03:31:34 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 14 loss:0.04621215537190437 norm:0.00010450282570673153 max memory_allocated 22563.88134765625 
[2025-03-22 03:32:06 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 15 loss:0.04615211859345436 norm:0.00010193618800258264 max memory_allocated 22563.88134765625 
[2025-03-22 03:32:39 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 16 loss:0.04609736055135727 norm:0.00010079515050165355 max memory_allocated 22563.88134765625 
[2025-03-22 03:33:12 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 17 loss:0.04605628550052643 norm:9.812402277020738e-05 max memory_allocated 22563.88134765625 
[2025-03-22 03:33:45 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 18 loss:0.04601449891924858 norm:9.877492266241461e-05 max memory_allocated 22563.88134765625 
[2025-03-22 03:34:17 root] (abq_llm_calibration_a2.py 394): INFO layer 11 iter 19 loss:0.04597453027963638 norm:9.641966607887298e-05 max memory_allocated 22563.88134765625 
[2025-03-22 03:34:26 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 03:35:02 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 0 loss:0.0640462338924408 norm:0.001506804022938013 max memory_allocated 22564.05322265625 
[2025-03-22 03:35:35 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 1 loss:0.060617879033088684 norm:0.0006580515182577074 max memory_allocated 22564.05322265625 
[2025-03-22 03:36:08 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 2 loss:0.05799996107816696 norm:0.0003563771606422961 max memory_allocated 22564.05322265625 
[2025-03-22 03:36:40 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 3 loss:0.05640516057610512 norm:0.0002322420186828822 max memory_allocated 22564.05322265625 
[2025-03-22 03:37:13 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 4 loss:0.05545959621667862 norm:0.0001848751271609217 max memory_allocated 22564.05322265625 
[2025-03-22 03:37:46 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 5 loss:0.05502014607191086 norm:0.0001525725529063493 max memory_allocated 22564.05322265625 
[2025-03-22 03:38:18 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 6 loss:0.054755792021751404 norm:0.00014082706184126437 max memory_allocated 22564.05322265625 
[2025-03-22 03:38:51 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 7 loss:0.054567866027355194 norm:0.00013058453623671085 max memory_allocated 22564.05322265625 
[2025-03-22 03:39:24 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 8 loss:0.054423630237579346 norm:0.0001245383609784767 max memory_allocated 22564.05322265625 
[2025-03-22 03:39:57 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 9 loss:0.05429724603891373 norm:0.00011708862439263612 max memory_allocated 22564.05322265625 
[2025-03-22 03:40:29 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 10 loss:0.05418907105922699 norm:0.00011172032100148499 max memory_allocated 22564.05322265625 
[2025-03-22 03:41:02 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 11 loss:0.05407724529504776 norm:0.0001067175981006585 max memory_allocated 22564.05322265625 
[2025-03-22 03:41:35 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 12 loss:0.05399526655673981 norm:0.00010677940008463338 max memory_allocated 22564.05322265625 
[2025-03-22 03:42:08 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 13 loss:0.05393128842115402 norm:0.00010310237848898396 max memory_allocated 22564.05322265625 
[2025-03-22 03:42:40 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 14 loss:0.05386469140648842 norm:0.0001015058733173646 max memory_allocated 22564.05322265625 
[2025-03-22 03:43:13 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 15 loss:0.05381244793534279 norm:0.00010169750021304935 max memory_allocated 22564.05322265625 
[2025-03-22 03:43:46 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 16 loss:0.05376516655087471 norm:9.881275036605075e-05 max memory_allocated 22564.05322265625 
[2025-03-22 03:44:19 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 17 loss:0.05370549485087395 norm:9.543697524350137e-05 max memory_allocated 22564.05322265625 
[2025-03-22 03:44:51 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 18 loss:0.05364728346467018 norm:9.328492888016626e-05 max memory_allocated 22564.05322265625 
[2025-03-22 03:45:24 root] (abq_llm_calibration_a2.py 394): INFO layer 12 iter 19 loss:0.053615495562553406 norm:9.215343015966937e-05 max memory_allocated 22564.05322265625 
[2025-03-22 03:45:33 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 03:46:09 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 0 loss:0.08167214691638947 norm:0.0070610204711556435 max memory_allocated 22564.22509765625 
[2025-03-22 03:46:41 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 1 loss:0.0738832876086235 norm:0.00250993762165308 max memory_allocated 22564.22509765625 
[2025-03-22 03:47:14 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 2 loss:0.06935346871614456 norm:0.0012243769597262144 max memory_allocated 22564.22509765625 
[2025-03-22 03:47:47 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 3 loss:0.06710251420736313 norm:0.0007210937910713255 max memory_allocated 22564.22509765625 
[2025-03-22 03:48:20 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 4 loss:0.06588437408208847 norm:0.00038995780050754547 max memory_allocated 22564.22509765625 
[2025-03-22 03:48:52 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 5 loss:0.0653015673160553 norm:0.00030212392448447645 max memory_allocated 22564.22509765625 
[2025-03-22 03:49:25 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 6 loss:0.06494013965129852 norm:0.0002670055255293846 max memory_allocated 22564.22509765625 
[2025-03-22 03:49:58 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 7 loss:0.06467567384243011 norm:0.00023931983741931617 max memory_allocated 22564.22509765625 
[2025-03-22 03:50:31 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 8 loss:0.06444007158279419 norm:0.00021863450820092112 max memory_allocated 22564.22509765625 
[2025-03-22 03:51:03 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 9 loss:0.06427041441202164 norm:0.00020749248506035656 max memory_allocated 22564.22509765625 
[2025-03-22 03:51:36 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 10 loss:0.06411617994308472 norm:0.00019144984253216535 max memory_allocated 22564.22509765625 
[2025-03-22 03:52:09 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 11 loss:0.06397125124931335 norm:0.0001805007632356137 max memory_allocated 22564.22509765625 
[2025-03-22 03:52:42 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 12 loss:0.06386341154575348 norm:0.00017508777091279626 max memory_allocated 22564.22509765625 
[2025-03-22 03:53:14 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 13 loss:0.06374232470989227 norm:0.00016530469292774796 max memory_allocated 22564.22509765625 
[2025-03-22 03:53:47 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 14 loss:0.06363066285848618 norm:0.0001635148364584893 max memory_allocated 22564.22509765625 
[2025-03-22 03:54:20 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 15 loss:0.06353729963302612 norm:0.00015574631106574088 max memory_allocated 22564.22509765625 
[2025-03-22 03:54:53 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 16 loss:0.0634414330124855 norm:0.00014913697668816894 max memory_allocated 22564.22509765625 
[2025-03-22 03:55:25 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 17 loss:0.063348188996315 norm:0.00014578175614587963 max memory_allocated 22564.22509765625 
[2025-03-22 03:55:58 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 18 loss:0.06327088922262192 norm:0.00014221054152585566 max memory_allocated 22564.22509765625 
[2025-03-22 03:56:31 root] (abq_llm_calibration_a2.py 394): INFO layer 13 iter 19 loss:0.06319870799779892 norm:0.0001423290668753907 max memory_allocated 22564.22509765625 
[2025-03-22 03:56:40 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 03:57:16 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 0 loss:0.08725704252719879 norm:0.0026488450821489096 max memory_allocated 22564.39697265625 
[2025-03-22 03:57:48 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 1 loss:0.08237400650978088 norm:0.0011409461731091142 max memory_allocated 22564.39697265625 
[2025-03-22 03:58:21 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 2 loss:0.07877857983112335 norm:0.0005624608602374792 max memory_allocated 22564.39697265625 
[2025-03-22 03:58:54 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 3 loss:0.07649078220129013 norm:0.00029691215604543686 max memory_allocated 22564.39697265625 
[2025-03-22 03:59:27 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 4 loss:0.07560202479362488 norm:0.00023050450545269996 max memory_allocated 22564.39697265625 
[2025-03-22 03:59:59 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 5 loss:0.07516595721244812 norm:0.0002042751293629408 max memory_allocated 22564.39697265625 
[2025-03-22 04:00:32 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 6 loss:0.0748462975025177 norm:0.00018652361177373677 max memory_allocated 22564.39697265625 
[2025-03-22 04:01:05 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 7 loss:0.07458072900772095 norm:0.00017510489851702005 max memory_allocated 22564.39697265625 
[2025-03-22 04:01:37 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 8 loss:0.07437621057033539 norm:0.0001635761873330921 max memory_allocated 22564.39697265625 
[2025-03-22 04:02:10 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 9 loss:0.07419745624065399 norm:0.0001581257674843073 max memory_allocated 22564.39697265625 
[2025-03-22 04:02:43 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 10 loss:0.07406064867973328 norm:0.00015427180915139616 max memory_allocated 22564.39697265625 
[2025-03-22 04:03:15 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 11 loss:0.07392504066228867 norm:0.0001491088914917782 max memory_allocated 22564.39697265625 
[2025-03-22 04:03:48 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 12 loss:0.07380649447441101 norm:0.0001460893836338073 max memory_allocated 22564.39697265625 
[2025-03-22 04:04:21 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 13 loss:0.07369301468133926 norm:0.00014315490261651576 max memory_allocated 22564.39697265625 
[2025-03-22 04:04:54 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 14 loss:0.07357157766819 norm:0.00014011816529091448 max memory_allocated 22564.39697265625 
[2025-03-22 04:05:26 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 15 loss:0.07346765697002411 norm:0.0001377320586470887 max memory_allocated 22564.39697265625 
[2025-03-22 04:05:59 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 16 loss:0.07338957488536835 norm:0.0001361392787657678 max memory_allocated 22564.39697265625 
[2025-03-22 04:06:32 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 17 loss:0.07330518215894699 norm:0.00013623603445012122 max memory_allocated 22564.39697265625 
[2025-03-22 04:07:05 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 18 loss:0.07322032749652863 norm:0.00013640850374940783 max memory_allocated 22564.39697265625 
[2025-03-22 04:07:37 root] (abq_llm_calibration_a2.py 394): INFO layer 14 iter 19 loss:0.07315437495708466 norm:0.0001345760829281062 max memory_allocated 22564.39697265625 
[2025-03-22 04:07:46 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 04:08:22 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 0 loss:0.11285816133022308 norm:0.008094893768429756 max memory_allocated 22564.56884765625 
[2025-03-22 04:08:55 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 1 loss:0.1041809618473053 norm:0.0032158123794943094 max memory_allocated 22564.56884765625 
[2025-03-22 04:09:27 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 2 loss:0.09622019529342651 norm:0.0012633419828489423 max memory_allocated 22564.56884765625 
[2025-03-22 04:10:00 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 3 loss:0.09286771714687347 norm:0.0006346248555928469 max memory_allocated 22564.56884765625 
[2025-03-22 04:10:33 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 4 loss:0.09174191951751709 norm:0.00042146132909692824 max memory_allocated 22564.56884765625 
[2025-03-22 04:11:06 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 5 loss:0.0911395400762558 norm:0.00034405579208396375 max memory_allocated 22564.56884765625 
[2025-03-22 04:11:38 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 6 loss:0.09064200520515442 norm:0.0003190420975442976 max memory_allocated 22564.56884765625 
[2025-03-22 04:12:11 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 7 loss:0.09028199315071106 norm:0.0003029475046787411 max memory_allocated 22564.56884765625 
[2025-03-22 04:12:44 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 8 loss:0.0899716466665268 norm:0.0002789981372188777 max memory_allocated 22564.56884765625 
[2025-03-22 04:13:16 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 9 loss:0.08972398191690445 norm:0.00026398090994916856 max memory_allocated 22564.56884765625 
[2025-03-22 04:13:49 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 10 loss:0.08949930965900421 norm:0.000247079849941656 max memory_allocated 22564.56884765625 
[2025-03-22 04:14:22 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 11 loss:0.08927938342094421 norm:0.0002263487986056134 max memory_allocated 22564.56884765625 
[2025-03-22 04:14:55 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 12 loss:0.0891152173280716 norm:0.00022204181004781276 max memory_allocated 22564.56884765625 
[2025-03-22 04:15:27 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 13 loss:0.08893779665231705 norm:0.000215788371860981 max memory_allocated 22564.56884765625 
[2025-03-22 04:16:00 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 14 loss:0.088788241147995 norm:0.00020975249935872853 max memory_allocated 22564.56884765625 
[2025-03-22 04:16:33 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 15 loss:0.08867111802101135 norm:0.00020900447270832956 max memory_allocated 22564.56884765625 
[2025-03-22 04:17:05 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 16 loss:0.08855773508548737 norm:0.00019842437177430838 max memory_allocated 22564.56884765625 
[2025-03-22 04:17:38 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 17 loss:0.08847298473119736 norm:0.00019653757044579834 max memory_allocated 22564.56884765625 
[2025-03-22 04:18:11 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 18 loss:0.08837713301181793 norm:0.00019572905148379505 max memory_allocated 22564.56884765625 
[2025-03-22 04:18:43 root] (abq_llm_calibration_a2.py 394): INFO layer 15 iter 19 loss:0.08827436715364456 norm:0.000192939696717076 max memory_allocated 22564.56884765625 
[2025-03-22 04:18:53 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 04:19:28 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 0 loss:0.14539101719856262 norm:0.010704576037824154 max memory_allocated 22564.74072265625 
[2025-03-22 04:20:01 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 1 loss:0.1358354389667511 norm:0.005023268051445484 max memory_allocated 22564.74072265625 
[2025-03-22 04:20:34 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 2 loss:0.12595798075199127 norm:0.0022219561506062746 max memory_allocated 22564.74072265625 
[2025-03-22 04:21:06 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 3 loss:0.12091795355081558 norm:0.0010792387183755636 max memory_allocated 22564.74072265625 
[2025-03-22 04:21:39 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 4 loss:0.1192193403840065 norm:0.0007168425945565104 max memory_allocated 22564.74072265625 
[2025-03-22 04:22:12 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 5 loss:0.11822795867919922 norm:0.0005471727927215397 max memory_allocated 22564.74072265625 
[2025-03-22 04:22:44 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 6 loss:0.11761908233165741 norm:0.0004992739413864911 max memory_allocated 22564.74072265625 
[2025-03-22 04:23:17 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 7 loss:0.11711665242910385 norm:0.0004711490182671696 max memory_allocated 22564.74072265625 
[2025-03-22 04:23:50 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 8 loss:0.1166459321975708 norm:0.00043777990504167974 max memory_allocated 22564.74072265625 
[2025-03-22 04:24:22 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 9 loss:0.1162533164024353 norm:0.00039941983413882554 max memory_allocated 22564.74072265625 
[2025-03-22 04:24:55 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 10 loss:0.11589141190052032 norm:0.00037385214818641543 max memory_allocated 22564.74072265625 
[2025-03-22 04:25:28 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 11 loss:0.11557333171367645 norm:0.00034975860035046935 max memory_allocated 22564.74072265625 
[2025-03-22 04:26:01 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 12 loss:0.1153039038181305 norm:0.00033482038998045027 max memory_allocated 22564.74072265625 
[2025-03-22 04:26:33 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 13 loss:0.11508222669363022 norm:0.00032223056768998504 max memory_allocated 22564.74072265625 
[2025-03-22 04:27:06 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 14 loss:0.11488112807273865 norm:0.0003106469812337309 max memory_allocated 22564.74072265625 
[2025-03-22 04:27:39 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 15 loss:0.11467448621988297 norm:0.00029820145573467016 max memory_allocated 22564.74072265625 
[2025-03-22 04:28:11 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 16 loss:0.11449666321277618 norm:0.0002888701856136322 max memory_allocated 22564.74072265625 
[2025-03-22 04:28:44 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 17 loss:0.1143537163734436 norm:0.00028088362887501717 max memory_allocated 22564.74072265625 
[2025-03-22 04:29:17 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 18 loss:0.1142614558339119 norm:0.00027505712932907045 max memory_allocated 22564.74072265625 
[2025-03-22 04:29:50 root] (abq_llm_calibration_a2.py 394): INFO layer 16 iter 19 loss:0.11418068408966064 norm:0.0002708466781768948 max memory_allocated 22564.74072265625 
[2025-03-22 04:29:59 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 04:30:34 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 0 loss:0.1617102175951004 norm:0.009324146434664726 max memory_allocated 22564.91259765625 
[2025-03-22 04:31:07 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 1 loss:0.15298201143741608 norm:0.004309697076678276 max memory_allocated 22564.91259765625 
[2025-03-22 04:31:40 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 2 loss:0.14403748512268066 norm:0.0015910123474895954 max memory_allocated 22564.91259765625 
[2025-03-22 04:32:12 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 3 loss:0.14025534689426422 norm:0.0006580938352271914 max memory_allocated 22564.91259765625 
[2025-03-22 04:32:45 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 4 loss:0.13918839395046234 norm:0.0005550873465836048 max memory_allocated 22564.91259765625 
[2025-03-22 04:33:18 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 5 loss:0.13837258517742157 norm:0.000462704076198861 max memory_allocated 22564.91259765625 
[2025-03-22 04:33:51 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 6 loss:0.13784529268741608 norm:0.0004244549199938774 max memory_allocated 22564.91259765625 
[2025-03-22 04:34:23 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 7 loss:0.13748416304588318 norm:0.0004033176228404045 max memory_allocated 22564.91259765625 
[2025-03-22 04:34:56 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 8 loss:0.13712218403816223 norm:0.00038881314685568213 max memory_allocated 22564.91259765625 
[2025-03-22 04:35:29 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 9 loss:0.1368248462677002 norm:0.00038076008786447346 max memory_allocated 22564.91259765625 
[2025-03-22 04:36:01 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 10 loss:0.1365257352590561 norm:0.0003670113510452211 max memory_allocated 22564.91259765625 
[2025-03-22 04:36:34 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 11 loss:0.13625414669513702 norm:0.0003494732081890106 max memory_allocated 22564.91259765625 
[2025-03-22 04:37:07 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 12 loss:0.13599126040935516 norm:0.00033390874159522355 max memory_allocated 22564.91259765625 
[2025-03-22 04:37:39 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 13 loss:0.13575857877731323 norm:0.0003304942511022091 max memory_allocated 22564.91259765625 
[2025-03-22 04:38:12 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 14 loss:0.13556569814682007 norm:0.000323493528412655 max memory_allocated 22564.91259765625 
[2025-03-22 04:38:45 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 15 loss:0.13541489839553833 norm:0.00031925953226163983 max memory_allocated 22564.91259765625 
[2025-03-22 04:39:18 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 16 loss:0.1352403461933136 norm:0.00031132480944506824 max memory_allocated 22564.91259765625 
[2025-03-22 04:39:50 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 17 loss:0.13510185480117798 norm:0.00030821404652670026 max memory_allocated 22564.91259765625 
[2025-03-22 04:40:23 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 18 loss:0.13497653603553772 norm:0.0002948583569377661 max memory_allocated 22564.91259765625 
[2025-03-22 04:40:56 root] (abq_llm_calibration_a2.py 394): INFO layer 17 iter 19 loss:0.13486942648887634 norm:0.00029221572913229465 max memory_allocated 22564.91259765625 
[2025-03-22 04:41:05 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 04:41:41 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 0 loss:0.20303402841091156 norm:0.017414091154932976 max memory_allocated 22565.08447265625 
[2025-03-22 04:42:13 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 1 loss:0.19213594496250153 norm:0.007180228363722563 max memory_allocated 22565.08447265625 
[2025-03-22 04:42:46 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 2 loss:0.18150018155574799 norm:0.003022301709279418 max memory_allocated 22565.08447265625 
[2025-03-22 04:43:19 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 3 loss:0.1757057160139084 norm:0.0014237628784030676 max memory_allocated 22565.08447265625 
[2025-03-22 04:43:51 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 4 loss:0.17340774834156036 norm:0.0007527502602897584 max memory_allocated 22565.08447265625 
[2025-03-22 04:44:24 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 5 loss:0.17239849269390106 norm:0.0005214862176217139 max memory_allocated 22565.08447265625 
[2025-03-22 04:44:57 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 6 loss:0.17181703448295593 norm:0.0004992933245375752 max memory_allocated 22565.08447265625 
[2025-03-22 04:45:29 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 7 loss:0.17128553986549377 norm:0.00047845765948295593 max memory_allocated 22565.08447265625 
[2025-03-22 04:46:02 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 8 loss:0.1708931028842926 norm:0.00047859156620688736 max memory_allocated 22565.08447265625 
[2025-03-22 04:46:35 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 9 loss:0.17049220204353333 norm:0.0004540910595096648 max memory_allocated 22565.08447265625 
[2025-03-22 04:47:08 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 10 loss:0.1701430231332779 norm:0.00044131960021331906 max memory_allocated 22565.08447265625 
[2025-03-22 04:47:40 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 11 loss:0.1697908639907837 norm:0.0004176360962446779 max memory_allocated 22565.08447265625 
[2025-03-22 04:48:13 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 12 loss:0.1695251613855362 norm:0.0004114698385819793 max memory_allocated 22565.08447265625 
[2025-03-22 04:48:46 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 13 loss:0.16928717494010925 norm:0.00040324576548300683 max memory_allocated 22565.08447265625 
[2025-03-22 04:49:18 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 14 loss:0.16909566521644592 norm:0.0003990083932876587 max memory_allocated 22565.08447265625 
[2025-03-22 04:49:51 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 15 loss:0.16889822483062744 norm:0.00038317314465530217 max memory_allocated 22565.08447265625 
[2025-03-22 04:50:24 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 16 loss:0.16868242621421814 norm:0.00036375655326992273 max memory_allocated 22565.08447265625 
[2025-03-22 04:50:57 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 17 loss:0.16854161024093628 norm:0.00035917977220378816 max memory_allocated 22565.08447265625 
[2025-03-22 04:51:29 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 18 loss:0.16841432452201843 norm:0.00035939563531428576 max memory_allocated 22565.08447265625 
[2025-03-22 04:52:02 root] (abq_llm_calibration_a2.py 394): INFO layer 18 iter 19 loss:0.1683485060930252 norm:0.00036076721153222024 max memory_allocated 22565.08447265625 
[2025-03-22 04:52:11 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 04:52:47 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 0 loss:0.23466889560222626 norm:0.013989787548780441 max memory_allocated 22565.25634765625 
[2025-03-22 04:53:19 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 1 loss:0.22643183171749115 norm:0.00712562957778573 max memory_allocated 22565.25634765625 
[2025-03-22 04:53:52 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 2 loss:0.2164006233215332 norm:0.003037602175027132 max memory_allocated 22565.25634765625 
[2025-03-22 04:54:25 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 3 loss:0.2120249718427658 norm:0.001640831003896892 max memory_allocated 22565.25634765625 
[2025-03-22 04:54:57 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 4 loss:0.21013078093528748 norm:0.0006846939213573933 max memory_allocated 22565.25634765625 
[2025-03-22 04:55:30 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 5 loss:0.20932739973068237 norm:0.0004288087657187134 max memory_allocated 22565.25634765625 
[2025-03-22 04:56:03 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 6 loss:0.2087666392326355 norm:0.00041217595571652055 max memory_allocated 22565.25634765625 
[2025-03-22 04:56:36 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 7 loss:0.2082386612892151 norm:0.0003823275037575513 max memory_allocated 22565.25634765625 
[2025-03-22 04:57:08 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 8 loss:0.2078261822462082 norm:0.0003847020852845162 max memory_allocated 22565.25634765625 
[2025-03-22 04:57:41 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 9 loss:0.2073795199394226 norm:0.0003715819329954684 max memory_allocated 22565.25634765625 
[2025-03-22 04:58:14 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 10 loss:0.2070067822933197 norm:0.00036817093496210873 max memory_allocated 22565.25634765625 
[2025-03-22 04:58:46 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 11 loss:0.20666250586509705 norm:0.00035517659853212535 max memory_allocated 22565.25634765625 
[2025-03-22 04:59:19 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 12 loss:0.20638972520828247 norm:0.0003438119310885668 max memory_allocated 22565.25634765625 
[2025-03-22 04:59:52 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 13 loss:0.2060968428850174 norm:0.00033138974686153233 max memory_allocated 22565.25634765625 
[2025-03-22 05:00:24 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 14 loss:0.20588845014572144 norm:0.00033806514693424106 max memory_allocated 22565.25634765625 
[2025-03-22 05:00:57 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 15 loss:0.20572693645954132 norm:0.000333005387801677 max memory_allocated 22565.25634765625 
[2025-03-22 05:01:30 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 16 loss:0.20555460453033447 norm:0.0003294600755907595 max memory_allocated 22565.25634765625 
[2025-03-22 05:02:03 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 17 loss:0.20538730919361115 norm:0.00032454734900966287 max memory_allocated 22565.25634765625 
[2025-03-22 05:02:35 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 18 loss:0.2052808403968811 norm:0.0003186687536071986 max memory_allocated 22565.25634765625 
[2025-03-22 05:03:08 root] (abq_llm_calibration_a2.py 394): INFO layer 19 iter 19 loss:0.2051698863506317 norm:0.00032092322362586856 max memory_allocated 22565.25634765625 
[2025-03-22 05:03:17 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 05:03:53 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 0 loss:0.2938494086265564 norm:0.010285702534019947 max memory_allocated 22565.42822265625 
[2025-03-22 05:04:25 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 1 loss:0.284699946641922 norm:0.005436487961560488 max memory_allocated 22565.42822265625 
[2025-03-22 05:04:58 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 2 loss:0.2746921181678772 norm:0.002809356665238738 max memory_allocated 22565.42822265625 
[2025-03-22 05:05:31 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 3 loss:0.2692115604877472 norm:0.0015973573317751288 max memory_allocated 22565.42822265625 
[2025-03-22 05:06:03 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 4 loss:0.2673526108264923 norm:0.001314954599365592 max memory_allocated 22565.42822265625 
[2025-03-22 05:06:36 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 5 loss:0.26584523916244507 norm:0.0008324197260662913 max memory_allocated 22565.42822265625 
[2025-03-22 05:07:09 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 6 loss:0.26492390036582947 norm:0.0007213565404526889 max memory_allocated 22565.42822265625 
[2025-03-22 05:07:41 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 7 loss:0.26411259174346924 norm:0.0006627006223425269 max memory_allocated 22565.42822265625 
[2025-03-22 05:08:14 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 8 loss:0.2634802460670471 norm:0.0006275710766203701 max memory_allocated 22565.42822265625 
[2025-03-22 05:08:47 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 9 loss:0.26297807693481445 norm:0.0006008345517329872 max memory_allocated 22565.42822265625 
[2025-03-22 05:09:20 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 10 loss:0.2625075578689575 norm:0.0005801749648526311 max memory_allocated 22565.42822265625 
[2025-03-22 05:09:52 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 11 loss:0.2620656490325928 norm:0.0005512632778845727 max memory_allocated 22565.42822265625 
[2025-03-22 05:10:25 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 12 loss:0.2616634964942932 norm:0.0005343431839719415 max memory_allocated 22565.42822265625 
[2025-03-22 05:10:58 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 13 loss:0.2613392174243927 norm:0.0005188965005800128 max memory_allocated 22565.42822265625 
[2025-03-22 05:11:30 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 14 loss:0.2610743045806885 norm:0.0005123895825818181 max memory_allocated 22565.42822265625 
[2025-03-22 05:12:03 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 15 loss:0.2608785331249237 norm:0.0005075728986412287 max memory_allocated 22565.42822265625 
[2025-03-22 05:12:36 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 16 loss:0.2606644928455353 norm:0.0004957496421411633 max memory_allocated 22565.42822265625 
[2025-03-22 05:13:08 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 17 loss:0.2604963481426239 norm:0.0004906480899080634 max memory_allocated 22565.42822265625 
[2025-03-22 05:13:41 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 18 loss:0.260320782661438 norm:0.00048455336946062744 max memory_allocated 22565.42822265625 
[2025-03-22 05:14:14 root] (abq_llm_calibration_a2.py 394): INFO layer 20 iter 19 loss:0.26020747423171997 norm:0.00047801368054933846 max memory_allocated 22565.42822265625 
[2025-03-22 05:14:23 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 05:14:59 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 0 loss:0.33809107542037964 norm:0.009054761379957199 max memory_allocated 22565.60009765625 
[2025-03-22 05:15:31 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 1 loss:0.32948294281959534 norm:0.004002228379249573 max memory_allocated 22565.60009765625 
[2025-03-22 05:16:04 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 2 loss:0.3223467469215393 norm:0.002024657791480422 max memory_allocated 22565.60009765625 
[2025-03-22 05:16:37 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 3 loss:0.31825563311576843 norm:0.0007541246595792472 max memory_allocated 22565.60009765625 
[2025-03-22 05:17:09 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 4 loss:0.31683483719825745 norm:0.0005252232076600194 max memory_allocated 22565.60009765625 
[2025-03-22 05:17:42 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 5 loss:0.31597957015037537 norm:0.0004836613079532981 max memory_allocated 22565.60009765625 
[2025-03-22 05:18:15 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 6 loss:0.31527161598205566 norm:0.0004666228196583688 max memory_allocated 22565.60009765625 
[2025-03-22 05:18:47 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 7 loss:0.31464987993240356 norm:0.00046971553820185363 max memory_allocated 22565.60009765625 
[2025-03-22 05:19:20 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 8 loss:0.314098596572876 norm:0.00044761563185602427 max memory_allocated 22565.60009765625 
[2025-03-22 05:19:53 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 9 loss:0.31353890895843506 norm:0.00042404994019307196 max memory_allocated 22565.60009765625 
[2025-03-22 05:20:25 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 10 loss:0.3130953907966614 norm:0.0004178832459729165 max memory_allocated 22565.60009765625 
[2025-03-22 05:20:58 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 11 loss:0.312697172164917 norm:0.00040824589086696506 max memory_allocated 22565.60009765625 
[2025-03-22 05:21:31 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 12 loss:0.31236088275909424 norm:0.0004037432372570038 max memory_allocated 22565.60009765625 
[2025-03-22 05:22:03 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 13 loss:0.31212469935417175 norm:0.00041983462870121 max memory_allocated 22565.60009765625 
[2025-03-22 05:22:36 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 14 loss:0.31190600991249084 norm:0.0004107717250008136 max memory_allocated 22565.60009765625 
[2025-03-22 05:23:09 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 15 loss:0.3117281496524811 norm:0.00040974884177558124 max memory_allocated 22565.60009765625 
[2025-03-22 05:23:42 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 16 loss:0.3115721642971039 norm:0.00040897357393987477 max memory_allocated 22565.60009765625 
[2025-03-22 05:24:14 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 17 loss:0.3114701807498932 norm:0.00041161634726449847 max memory_allocated 22565.60009765625 
[2025-03-22 05:24:47 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 18 loss:0.3113793730735779 norm:0.0004078217316418886 max memory_allocated 22565.60009765625 
[2025-03-22 05:25:20 root] (abq_llm_calibration_a2.py 394): INFO layer 21 iter 19 loss:0.3112875819206238 norm:0.0003973727289121598 max memory_allocated 22565.60009765625 
[2025-03-22 05:25:29 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 05:26:05 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 0 loss:0.41653507947921753 norm:0.008162511512637138 max memory_allocated 22565.77197265625 
[2025-03-22 05:26:37 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 1 loss:0.40628066658973694 norm:0.0033531642984598875 max memory_allocated 22565.77197265625 
[2025-03-22 05:27:10 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 2 loss:0.39739686250686646 norm:0.0018060459988191724 max memory_allocated 22565.77197265625 
[2025-03-22 05:27:43 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 3 loss:0.3929119110107422 norm:0.0008885769639164209 max memory_allocated 22565.77197265625 
[2025-03-22 05:28:15 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 4 loss:0.39135515689849854 norm:0.000714691705070436 max memory_allocated 22565.77197265625 
[2025-03-22 05:28:48 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 5 loss:0.3903440535068512 norm:0.000686358253005892 max memory_allocated 22565.77197265625 
[2025-03-22 05:29:21 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 6 loss:0.38946476578712463 norm:0.0006418928969651461 max memory_allocated 22565.77197265625 
[2025-03-22 05:29:53 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 7 loss:0.38875681161880493 norm:0.0006201820215210319 max memory_allocated 22565.77197265625 
[2025-03-22 05:30:26 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 8 loss:0.3881167769432068 norm:0.0006070315139368176 max memory_allocated 22565.77197265625 
[2025-03-22 05:30:59 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 9 loss:0.387481153011322 norm:0.0005874037160538137 max memory_allocated 22565.77197265625 
[2025-03-22 05:31:31 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 10 loss:0.38702473044395447 norm:0.0005780116189271212 max memory_allocated 22565.77197265625 
[2025-03-22 05:32:04 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 11 loss:0.3866051733493805 norm:0.0005632648826576769 max memory_allocated 22565.77197265625 
[2025-03-22 05:32:37 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 12 loss:0.38629162311553955 norm:0.0005641412572003901 max memory_allocated 22565.77197265625 
[2025-03-22 05:33:09 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 13 loss:0.3859964907169342 norm:0.0005582212470471859 max memory_allocated 22565.77197265625 
[2025-03-22 05:33:42 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 14 loss:0.38584375381469727 norm:0.0005440935492515564 max memory_allocated 22565.77197265625 
[2025-03-22 05:34:15 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 15 loss:0.3854866027832031 norm:0.0005248397355899215 max memory_allocated 22565.77197265625 
[2025-03-22 05:34:48 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 16 loss:0.3855218291282654 norm:0.000539363594725728 max memory_allocated 22565.77197265625 
[2025-03-22 05:35:20 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 17 loss:0.38529282808303833 norm:0.0005104370065964758 max memory_allocated 22565.77197265625 
[2025-03-22 05:35:53 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 18 loss:0.38509857654571533 norm:0.0005193406250327826 max memory_allocated 22565.77197265625 
[2025-03-22 05:36:26 root] (abq_llm_calibration_a2.py 394): INFO layer 22 iter 19 loss:0.3848824203014374 norm:0.0005517859826795757 max memory_allocated 22565.77197265625 
[2025-03-22 05:36:35 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 05:37:11 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 0 loss:0.4773634970188141 norm:0.004672255367040634 max memory_allocated 22565.94384765625 
[2025-03-22 05:37:43 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 1 loss:0.46965155005455017 norm:0.0025173621252179146 max memory_allocated 22565.94384765625 
[2025-03-22 05:38:16 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 2 loss:0.4614619314670563 norm:0.0015218084445223212 max memory_allocated 22565.94384765625 
[2025-03-22 05:38:49 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 3 loss:0.4577297270298004 norm:0.0009527624351903796 max memory_allocated 22565.94384765625 
[2025-03-22 05:39:21 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 4 loss:0.4560924768447876 norm:0.0007088024285621941 max memory_allocated 22565.94384765625 
[2025-03-22 05:39:54 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 5 loss:0.45497605204582214 norm:0.0005617860588245094 max memory_allocated 22565.94384765625 
[2025-03-22 05:40:27 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 6 loss:0.45400798320770264 norm:0.0005071196937933564 max memory_allocated 22565.94384765625 
[2025-03-22 05:40:59 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 7 loss:0.453145831823349 norm:0.0004770718514919281 max memory_allocated 22565.94384765625 
[2025-03-22 05:41:32 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 8 loss:0.4523829221725464 norm:0.00045993237290531397 max memory_allocated 22565.94384765625 
[2025-03-22 05:42:05 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 9 loss:0.4517907202243805 norm:0.0004555313498713076 max memory_allocated 22565.94384765625 
[2025-03-22 05:42:37 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 10 loss:0.4513171315193176 norm:0.0004512988089118153 max memory_allocated 22565.94384765625 
[2025-03-22 05:43:10 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 11 loss:0.45096874237060547 norm:0.00044557556975632906 max memory_allocated 22565.94384765625 
[2025-03-22 05:43:43 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 12 loss:0.45068076252937317 norm:0.0004445390950422734 max memory_allocated 22565.94384765625 
[2025-03-22 05:44:15 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 13 loss:0.45045632123947144 norm:0.00043624889804050326 max memory_allocated 22565.94384765625 
[2025-03-22 05:44:48 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 14 loss:0.45025181770324707 norm:0.0004296441038604826 max memory_allocated 22565.94384765625 
[2025-03-22 05:45:21 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 15 loss:0.4501092731952667 norm:0.0004274205712135881 max memory_allocated 22565.94384765625 
[2025-03-22 05:45:54 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 16 loss:0.44995981454849243 norm:0.00042520253919065 max memory_allocated 22565.94384765625 
[2025-03-22 05:46:26 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 17 loss:0.44984135031700134 norm:0.00042503722943365574 max memory_allocated 22565.94384765625 
[2025-03-22 05:46:59 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 18 loss:0.4497305452823639 norm:0.0004204197903163731 max memory_allocated 22565.94384765625 
[2025-03-22 05:47:32 root] (abq_llm_calibration_a2.py 394): INFO layer 23 iter 19 loss:0.449601411819458 norm:0.00042076612589880824 max memory_allocated 22565.94384765625 
[2025-03-22 05:47:41 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 05:48:17 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 0 loss:0.5625317096710205 norm:0.00831147562712431 max memory_allocated 22566.11572265625 
[2025-03-22 05:48:49 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 1 loss:0.5523854494094849 norm:0.004008142277598381 max memory_allocated 22566.11572265625 
[2025-03-22 05:49:22 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 2 loss:0.5424116849899292 norm:0.0022332097869366407 max memory_allocated 22566.11572265625 
[2025-03-22 05:49:55 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 3 loss:0.5371587872505188 norm:0.001038297195918858 max memory_allocated 22566.11572265625 
[2025-03-22 05:50:27 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 4 loss:0.5348774194717407 norm:0.0006314878701232374 max memory_allocated 22566.11572265625 
[2025-03-22 05:51:00 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 5 loss:0.5335873365402222 norm:0.0005641274619847536 max memory_allocated 22566.11572265625 
[2025-03-22 05:51:33 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 6 loss:0.5324233770370483 norm:0.0005797394551336765 max memory_allocated 22566.11572265625 
[2025-03-22 05:52:06 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 7 loss:0.5314116477966309 norm:0.0005853977054357529 max memory_allocated 22566.11572265625 
[2025-03-22 05:52:38 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 8 loss:0.530514121055603 norm:0.0005648611695505679 max memory_allocated 22566.11572265625 
[2025-03-22 05:53:11 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 9 loss:0.5298165082931519 norm:0.0005540032871067524 max memory_allocated 22566.11572265625 
[2025-03-22 05:53:44 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 10 loss:0.5293121337890625 norm:0.0005673825507983565 max memory_allocated 22566.11572265625 
[2025-03-22 05:54:16 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 11 loss:0.5289279222488403 norm:0.0005433089099824429 max memory_allocated 22566.11572265625 
[2025-03-22 05:54:49 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 12 loss:0.5285623073577881 norm:0.0005480817053467035 max memory_allocated 22566.11572265625 
[2025-03-22 05:55:22 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 13 loss:0.5283469557762146 norm:0.0005558779230341315 max memory_allocated 22566.11572265625 
[2025-03-22 05:55:55 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 14 loss:0.5280649662017822 norm:0.0005500558181665838 max memory_allocated 22566.11572265625 
[2025-03-22 05:56:27 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 15 loss:0.5278863906860352 norm:0.00055301608517766 max memory_allocated 22566.11572265625 
[2025-03-22 05:57:00 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 16 loss:0.527776300907135 norm:0.0005492002237588167 max memory_allocated 22566.11572265625 
[2025-03-22 05:57:33 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 17 loss:0.5275165438652039 norm:0.0005424165283329785 max memory_allocated 22566.11572265625 
[2025-03-22 05:58:05 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 18 loss:0.5274356007575989 norm:0.0005516454111784697 max memory_allocated 22566.11572265625 
[2025-03-22 05:58:38 root] (abq_llm_calibration_a2.py 394): INFO layer 24 iter 19 loss:0.5272853374481201 norm:0.0005425268318504095 max memory_allocated 22566.11572265625 
[2025-03-22 05:58:47 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 05:59:23 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 0 loss:0.6502524018287659 norm:0.016010912135243416 max memory_allocated 22566.28759765625 
[2025-03-22 05:59:56 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 1 loss:0.6398845911026001 norm:0.009733716025948524 max memory_allocated 22566.28759765625 
[2025-03-22 06:00:28 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 2 loss:0.6285852193832397 norm:0.006253126077353954 max memory_allocated 22566.28759765625 
[2025-03-22 06:01:01 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 3 loss:0.6239319443702698 norm:0.004280789289623499 max memory_allocated 22566.28759765625 
[2025-03-22 06:01:34 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 4 loss:0.6201846599578857 norm:0.0027327341958880424 max memory_allocated 22566.28759765625 
[2025-03-22 06:02:06 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 5 loss:0.618126630783081 norm:0.002571529010310769 max memory_allocated 22566.28759765625 
[2025-03-22 06:02:39 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 6 loss:0.6165034770965576 norm:0.0023338990285992622 max memory_allocated 22566.28759765625 
[2025-03-22 06:03:12 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 7 loss:0.6150953769683838 norm:0.0021227009128779173 max memory_allocated 22566.28759765625 
[2025-03-22 06:03:44 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 8 loss:0.6137331128120422 norm:0.0017256831051781774 max memory_allocated 22566.28759765625 
[2025-03-22 06:04:17 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 9 loss:0.6128513813018799 norm:0.001751854200847447 max memory_allocated 22566.28759765625 
[2025-03-22 06:04:50 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 10 loss:0.6123260259628296 norm:0.0015201237984001637 max memory_allocated 22566.28759765625 
[2025-03-22 06:05:23 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 11 loss:0.611884355545044 norm:0.0015064331237226725 max memory_allocated 22566.28759765625 
[2025-03-22 06:05:55 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 12 loss:0.6108443140983582 norm:0.0011620139703154564 max memory_allocated 22566.28759765625 
[2025-03-22 06:06:28 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 13 loss:0.6110196113586426 norm:0.0013033875729888678 max memory_allocated 22566.28759765625 
[2025-03-22 06:07:01 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 14 loss:0.6102023124694824 norm:0.0010976684279739857 max memory_allocated 22566.28759765625 
[2025-03-22 06:07:33 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 15 loss:0.6101581454277039 norm:0.001090338220819831 max memory_allocated 22566.28759765625 
[2025-03-22 06:08:06 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 16 loss:0.6096556186676025 norm:0.0009334306232631207 max memory_allocated 22566.28759765625 
[2025-03-22 06:08:39 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 17 loss:0.6093165874481201 norm:0.0009722156682983041 max memory_allocated 22566.28759765625 
[2025-03-22 06:09:12 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 18 loss:0.6090512871742249 norm:0.0008889629389159381 max memory_allocated 22566.28759765625 
[2025-03-22 06:09:44 root] (abq_llm_calibration_a2.py 394): INFO layer 25 iter 19 loss:0.609028697013855 norm:0.000917589059099555 max memory_allocated 22566.28759765625 
[2025-03-22 06:09:53 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 06:10:29 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 0 loss:0.7546601295471191 norm:0.0068833669647574425 max memory_allocated 22566.45947265625 
[2025-03-22 06:11:02 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 1 loss:0.7422325611114502 norm:0.0035727359354496002 max memory_allocated 22566.45947265625 
[2025-03-22 06:11:34 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 2 loss:0.7282200455665588 norm:0.0016886177472770214 max memory_allocated 22566.45947265625 
[2025-03-22 06:12:07 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 3 loss:0.7222956418991089 norm:0.0010366513160988688 max memory_allocated 22566.45947265625 
[2025-03-22 06:12:40 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 4 loss:0.7197099924087524 norm:0.0008734798757359385 max memory_allocated 22566.45947265625 
[2025-03-22 06:13:13 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 5 loss:0.7178547382354736 norm:0.000815067789517343 max memory_allocated 22566.45947265625 
[2025-03-22 06:13:45 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 6 loss:0.7161461710929871 norm:0.0007871162961237133 max memory_allocated 22566.45947265625 
[2025-03-22 06:14:18 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 7 loss:0.7146804928779602 norm:0.0007283103768713772 max memory_allocated 22566.45947265625 
[2025-03-22 06:14:51 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 8 loss:0.713495671749115 norm:0.000700577103998512 max memory_allocated 22566.45947265625 
[2025-03-22 06:15:24 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 9 loss:0.7126007080078125 norm:0.0006757450755685568 max memory_allocated 22566.45947265625 
[2025-03-22 06:15:56 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 10 loss:0.7119605541229248 norm:0.000673720904160291 max memory_allocated 22566.45947265625 
[2025-03-22 06:16:29 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 11 loss:0.7115435600280762 norm:0.0006504051852971315 max memory_allocated 22566.45947265625 
[2025-03-22 06:17:02 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 12 loss:0.7111480832099915 norm:0.0006330417818389833 max memory_allocated 22566.45947265625 
[2025-03-22 06:17:34 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 13 loss:0.7107581496238708 norm:0.0006237420602701604 max memory_allocated 22566.45947265625 
[2025-03-22 06:18:07 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 14 loss:0.7104862332344055 norm:0.0006118446472100914 max memory_allocated 22566.45947265625 
[2025-03-22 06:18:40 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 15 loss:0.7102395296096802 norm:0.0006045492482371628 max memory_allocated 22566.45947265625 
[2025-03-22 06:19:13 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 16 loss:0.7099634408950806 norm:0.0005984356394037604 max memory_allocated 22566.45947265625 
[2025-03-22 06:19:45 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 17 loss:0.7097390294075012 norm:0.0005994262173771858 max memory_allocated 22566.45947265625 
[2025-03-22 06:20:18 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 18 loss:0.7095336318016052 norm:0.000596517464146018 max memory_allocated 22566.45947265625 
[2025-03-22 06:20:51 root] (abq_llm_calibration_a2.py 394): INFO layer 26 iter 19 loss:0.7094225883483887 norm:0.0005866515566594899 max memory_allocated 22566.45947265625 
[2025-03-22 06:21:00 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 06:21:36 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 0 loss:0.8645890355110168 norm:0.011338531970977783 max memory_allocated 22566.63134765625 
[2025-03-22 06:22:08 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 1 loss:0.8496187925338745 norm:0.0054445271380245686 max memory_allocated 22566.63134765625 
[2025-03-22 06:22:41 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 2 loss:0.8343378305435181 norm:0.0028383361641317606 max memory_allocated 22566.63134765625 
[2025-03-22 06:23:14 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 3 loss:0.8270672559738159 norm:0.0014491945039480925 max memory_allocated 22566.63134765625 
[2025-03-22 06:23:46 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 4 loss:0.8237295150756836 norm:0.0006636788602918386 max memory_allocated 22566.63134765625 
[2025-03-22 06:24:19 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 5 loss:0.821482241153717 norm:0.000595599296502769 max memory_allocated 22566.63134765625 
[2025-03-22 06:24:52 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 6 loss:0.8194094300270081 norm:0.0005645278142765164 max memory_allocated 22566.63134765625 
[2025-03-22 06:25:25 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 7 loss:0.8177360892295837 norm:0.0005555743700824678 max memory_allocated 22566.63134765625 
[2025-03-22 06:25:57 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 8 loss:0.8165200352668762 norm:0.0005489761242642999 max memory_allocated 22566.63134765625 
[2025-03-22 06:26:30 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 9 loss:0.8156394958496094 norm:0.0005530843045562506 max memory_allocated 22566.63134765625 
[2025-03-22 06:27:03 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 10 loss:0.814992368221283 norm:0.0005462050903588533 max memory_allocated 22566.63134765625 
[2025-03-22 06:27:35 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 11 loss:0.8144757151603699 norm:0.0005391790182329714 max memory_allocated 22566.63134765625 
[2025-03-22 06:28:08 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 12 loss:0.8141219615936279 norm:0.0005318443290889263 max memory_allocated 22566.63134765625 
[2025-03-22 06:28:41 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 13 loss:0.8137990832328796 norm:0.0005269868997856975 max memory_allocated 22566.63134765625 
[2025-03-22 06:29:14 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 14 loss:0.8135167360305786 norm:0.00051780667854473 max memory_allocated 22566.63134765625 
[2025-03-22 06:29:46 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 15 loss:0.8132518529891968 norm:0.0005251397378742695 max memory_allocated 22566.63134765625 
[2025-03-22 06:30:19 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 16 loss:0.81306391954422 norm:0.000524587812833488 max memory_allocated 22566.63134765625 
[2025-03-22 06:30:52 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 17 loss:0.812877357006073 norm:0.0005218064179643989 max memory_allocated 22566.63134765625 
[2025-03-22 06:31:25 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 18 loss:0.8126302361488342 norm:0.0005185930640436709 max memory_allocated 22566.63134765625 
[2025-03-22 06:31:57 root] (abq_llm_calibration_a2.py 394): INFO layer 27 iter 19 loss:0.8124374747276306 norm:0.0005171690718270838 max memory_allocated 22566.63134765625 
[2025-03-22 06:32:07 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 06:32:10 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 06:32:42 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 0 loss:1.0094833374023438 norm:0.02554825134575367 max memory_allocated 22566.91845703125 
[2025-03-22 06:33:15 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 1 loss:0.9922962784767151 norm:0.01980767585337162 max memory_allocated 22566.91845703125 
[2025-03-22 06:33:48 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 2 loss:0.9728851318359375 norm:0.014098726212978363 max memory_allocated 22566.91845703125 
[2025-03-22 06:34:21 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 3 loss:0.9632097482681274 norm:0.011362405493855476 max memory_allocated 22566.91845703125 
[2025-03-22 06:34:54 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 4 loss:0.9590546488761902 norm:0.009832232259213924 max memory_allocated 22566.91845703125 
[2025-03-22 06:35:27 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 5 loss:0.9556829929351807 norm:0.008405540138483047 max memory_allocated 22566.91845703125 
[2025-03-22 06:35:59 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 6 loss:0.952798068523407 norm:0.007317014969885349 max memory_allocated 22566.91845703125 
[2025-03-22 06:36:32 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 7 loss:0.9506078362464905 norm:0.006715714931488037 max memory_allocated 22566.91845703125 
[2025-03-22 06:37:05 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 8 loss:0.9489967823028564 norm:0.00622187927365303 max memory_allocated 22566.91845703125 
[2025-03-22 06:37:38 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 9 loss:0.9479084610939026 norm:0.005967163946479559 max memory_allocated 22566.91845703125 
[2025-03-22 06:38:11 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 10 loss:0.9468961358070374 norm:0.005780770909041166 max memory_allocated 22566.91845703125 
[2025-03-22 06:38:44 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 11 loss:0.9461655616760254 norm:0.005737369414418936 max memory_allocated 22566.91845703125 
[2025-03-22 06:39:16 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 12 loss:0.9454793334007263 norm:0.005647875834256411 max memory_allocated 22566.91845703125 
[2025-03-22 06:39:49 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 13 loss:0.9448414444923401 norm:0.005636975634843111 max memory_allocated 22566.91845703125 
[2025-03-22 06:40:22 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 14 loss:0.9443202018737793 norm:0.0054748691618442535 max memory_allocated 22566.91845703125 
[2025-03-22 06:40:55 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 15 loss:0.9439159035682678 norm:0.005623550619930029 max memory_allocated 22566.91845703125 
[2025-03-22 06:41:28 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 16 loss:0.9435723423957825 norm:0.005509976297616959 max memory_allocated 22566.91845703125 
[2025-03-22 06:42:01 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 17 loss:0.9432077407836914 norm:0.005493598524481058 max memory_allocated 22566.91845703125 
[2025-03-22 06:42:34 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 18 loss:0.9429841637611389 norm:0.005385608412325382 max memory_allocated 22566.91845703125 
[2025-03-22 06:43:07 root] (abq_llm_calibration_a2.py 394): INFO layer 28 iter 19 loss:0.9426249861717224 norm:0.005323851481080055 max memory_allocated 22566.91845703125 
[2025-03-22 06:43:16 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 06:43:19 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 06:43:51 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 0 loss:1.1752772331237793 norm:0.02549039199948311 max memory_allocated 22567.09033203125 
[2025-03-22 06:44:24 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 1 loss:1.1512696743011475 norm:0.01993739977478981 max memory_allocated 22567.09033203125 
[2025-03-22 06:44:57 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 2 loss:1.1284856796264648 norm:0.014761771075427532 max memory_allocated 22567.09033203125 
[2025-03-22 06:45:30 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 3 loss:1.115684986114502 norm:0.012071061879396439 max memory_allocated 22567.09033203125 
[2025-03-22 06:46:03 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 4 loss:1.109271764755249 norm:0.00981784425675869 max memory_allocated 22567.09033203125 
[2025-03-22 06:46:36 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 5 loss:1.1045888662338257 norm:0.008316335268318653 max memory_allocated 22567.09033203125 
[2025-03-22 06:47:08 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 6 loss:1.1012890338897705 norm:0.007683524861931801 max memory_allocated 22567.09033203125 
[2025-03-22 06:47:41 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 7 loss:1.098927617073059 norm:0.007377329748123884 max memory_allocated 22567.09033203125 
[2025-03-22 06:48:14 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 8 loss:1.0973026752471924 norm:0.007278532721102238 max memory_allocated 22567.09033203125 
[2025-03-22 06:48:47 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 9 loss:1.0959362983703613 norm:0.006969867739826441 max memory_allocated 22567.09033203125 
[2025-03-22 06:49:20 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 10 loss:1.0950175523757935 norm:0.006874741520732641 max memory_allocated 22567.09033203125 
[2025-03-22 06:49:53 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 11 loss:1.0941071510314941 norm:0.006834856234490871 max memory_allocated 22567.09033203125 
[2025-03-22 06:50:26 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 12 loss:1.0934938192367554 norm:0.006880098953843117 max memory_allocated 22567.09033203125 
[2025-03-22 06:50:58 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 13 loss:1.092894196510315 norm:0.007202291861176491 max memory_allocated 22567.09033203125 
[2025-03-22 06:51:31 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 14 loss:1.0922861099243164 norm:0.007095942739397287 max memory_allocated 22567.09033203125 
[2025-03-22 06:52:04 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 15 loss:1.091694951057434 norm:0.006846821401268244 max memory_allocated 22567.09033203125 
[2025-03-22 06:52:37 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 16 loss:1.0912567377090454 norm:0.006614052690565586 max memory_allocated 22567.09033203125 
[2025-03-22 06:53:10 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 17 loss:1.0906918048858643 norm:0.006286726798862219 max memory_allocated 22567.09033203125 
[2025-03-22 06:53:43 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 18 loss:1.0904442071914673 norm:0.006428160704672337 max memory_allocated 22567.09033203125 
[2025-03-22 06:54:16 root] (abq_llm_calibration_a2.py 394): INFO layer 29 iter 19 loss:1.090122103691101 norm:0.006091803312301636 max memory_allocated 22567.09033203125 
[2025-03-22 06:54:25 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 06:54:28 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 06:55:01 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 0 loss:2.8570497035980225 norm:0.8260383605957031 max memory_allocated 22567.26220703125 
[2025-03-22 06:55:33 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 1 loss:3.065972089767456 norm:1.4696918725967407 max memory_allocated 22567.26220703125 
[2025-03-22 06:56:06 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 2 loss:2.6001789569854736 norm:0.5641605257987976 max memory_allocated 22567.26220703125 
[2025-03-22 06:56:39 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 3 loss:2.03839373588562 norm:0.3171542286872864 max memory_allocated 22567.26220703125 
[2025-03-22 06:57:12 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 4 loss:1.784067988395691 norm:0.19561268389225006 max memory_allocated 22567.26220703125 
[2025-03-22 06:57:44 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 5 loss:1.7480908632278442 norm:0.1704147905111313 max memory_allocated 22567.26220703125 
[2025-03-22 06:58:17 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 6 loss:1.7203636169433594 norm:0.15295493602752686 max memory_allocated 22567.26220703125 
[2025-03-22 06:58:50 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 7 loss:1.7138471603393555 norm:0.1253262758255005 max memory_allocated 22567.26220703125 
[2025-03-22 06:59:23 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 8 loss:1.7100651264190674 norm:0.12066265940666199 max memory_allocated 22567.26220703125 
[2025-03-22 06:59:56 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 9 loss:1.6868035793304443 norm:0.10421784222126007 max memory_allocated 22567.26220703125 
[2025-03-22 07:00:28 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 10 loss:1.66250479221344 norm:0.0948537290096283 max memory_allocated 22567.26220703125 
[2025-03-22 07:01:01 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 11 loss:1.6605950593948364 norm:0.09433168917894363 max memory_allocated 22567.26220703125 
[2025-03-22 07:01:34 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 12 loss:1.653160572052002 norm:0.09230237454175949 max memory_allocated 22567.26220703125 
[2025-03-22 07:02:07 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 13 loss:1.6568766832351685 norm:0.09766741842031479 max memory_allocated 22567.26220703125 
[2025-03-22 07:02:40 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 14 loss:1.6504061222076416 norm:0.1025235503911972 max memory_allocated 22567.26220703125 
[2025-03-22 07:03:12 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 15 loss:1.6563458442687988 norm:0.10941590368747711 max memory_allocated 22567.26220703125 
[2025-03-22 07:03:45 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 16 loss:1.6530402898788452 norm:0.1119040846824646 max memory_allocated 22567.26220703125 
[2025-03-22 07:04:18 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 17 loss:1.6469707489013672 norm:0.10396802425384521 max memory_allocated 22567.26220703125 
[2025-03-22 07:04:51 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 18 loss:1.6579217910766602 norm:0.1113193929195404 max memory_allocated 22567.26220703125 
[2025-03-22 07:05:23 root] (abq_llm_calibration_a2.py 394): INFO layer 30 iter 19 loss:1.6403343677520752 norm:0.10355829447507858 max memory_allocated 22567.26220703125 
[2025-03-22 07:05:33 root] (abq_llm_calibration_a2.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 07:05:36 root] (abq_llm_calibration_a2.py 276): INFO use compensation vector
[2025-03-22 07:06:08 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 0 loss:4.067381381988525 norm:0.3030979335308075 max memory_allocated 22567.43408203125 
[2025-03-22 07:06:41 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 1 loss:3.6279659271240234 norm:0.2250937521457672 max memory_allocated 22567.43408203125 
[2025-03-22 07:07:14 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 2 loss:3.282569646835327 norm:0.15991348028182983 max memory_allocated 22567.43408203125 
[2025-03-22 07:07:47 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 3 loss:3.179927349090576 norm:0.1547766625881195 max memory_allocated 22567.43408203125 
[2025-03-22 07:08:19 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 4 loss:3.1342344284057617 norm:0.14989173412322998 max memory_allocated 22567.43408203125 
[2025-03-22 07:08:52 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 5 loss:3.0955162048339844 norm:0.13868118822574615 max memory_allocated 22567.43408203125 
[2025-03-22 07:09:25 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 6 loss:3.0676164627075195 norm:0.1302822381258011 max memory_allocated 22567.43408203125 
[2025-03-22 07:09:58 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 7 loss:3.0420989990234375 norm:0.12122143805027008 max memory_allocated 22567.43408203125 
[2025-03-22 07:10:30 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 8 loss:3.0163073539733887 norm:0.11545336991548538 max memory_allocated 22567.43408203125 
[2025-03-22 07:11:03 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 9 loss:2.9872560501098633 norm:0.1056135892868042 max memory_allocated 22567.43408203125 
[2025-03-22 07:11:36 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 10 loss:2.9678680896759033 norm:0.09957046806812286 max memory_allocated 22567.43408203125 
[2025-03-22 07:12:09 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 11 loss:2.953077793121338 norm:0.09848403185606003 max memory_allocated 22567.43408203125 
[2025-03-22 07:12:42 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 12 loss:2.939493179321289 norm:0.09478731453418732 max memory_allocated 22567.43408203125 
[2025-03-22 07:13:14 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 13 loss:2.9253532886505127 norm:0.09086380898952484 max memory_allocated 22567.43408203125 
[2025-03-22 07:13:47 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 14 loss:2.915959119796753 norm:0.08955980837345123 max memory_allocated 22567.43408203125 
[2025-03-22 07:14:20 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 15 loss:2.9087843894958496 norm:0.0882849246263504 max memory_allocated 22567.43408203125 
[2025-03-22 07:14:53 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 16 loss:2.904221534729004 norm:0.09475719183683395 max memory_allocated 22567.43408203125 
[2025-03-22 07:15:26 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 17 loss:2.894986629486084 norm:0.09123045206069946 max memory_allocated 22567.43408203125 
[2025-03-22 07:15:58 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 18 loss:2.884237051010132 norm:0.08582673966884613 max memory_allocated 22567.43408203125 
[2025-03-22 07:16:31 root] (abq_llm_calibration_a2.py 394): INFO layer 31 iter 19 loss:2.879589796066284 norm:0.07923111319541931 max memory_allocated 22567.43408203125 
[2025-03-22 07:16:41 root] (main_calibration_a2.py 370): INFO 21327.99998807907
[2025-03-22 07:16:46 root] (main_calibration_a2.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-22 07:17:56 root] (main_calibration_a2.py 158): INFO wikitext2 : 9.924477577209473
[2025-03-22 07:17:56 root] (main_calibration_a2.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-22 07:19:42 root] (main_calibration_a2.py 158): INFO c4 : 14.056642532348633
[2025-03-22 09:02:19 root] (main_calibration_a2.py 169): INFO {'wikitext2': 9.924477577209473, 'c4': 14.056642532348633, 'results': {'winogrande': {'acc': 0.5461720599842147, 'acc_stderr': 0.013992441563707075}, 'piqa': {'acc': 0.6746463547334058, 'acc_stderr': 0.010931036623525197, 'acc_norm': 0.6789989118607181, 'acc_norm_stderr': 0.01089264157470791}, 'boolq': {'acc': 0.6262996941896024, 'acc_stderr': 0.008461461177104}, 'hellaswag': {'acc': 0.43218482374029077, 'acc_stderr': 0.004943673388276271, 'acc_norm': 0.5664210316669986, 'acc_norm_stderr': 0.0049455580698525275}, 'arc_easy': {'acc': 0.5370370370370371, 'acc_stderr': 0.01023159724913105, 'acc_norm': 0.4372895622895623, 'acc_norm_stderr': 0.010178768429321585}, 'arc_challenge': {'acc': 0.2841296928327645, 'acc_stderr': 0.013179442447653886, 'acc_norm': 0.3148464163822526, 'acc_norm_stderr': 0.013572657703084948}}, 'versions': {'winogrande': 0, 'piqa': 0, 'boolq': 1, 'hellaswag': 0, 'arc_easy': 0, 'arc_challenge': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 09:02:19 root] (main_calibration_a2.py 172): INFO 28.41,53.70,62.63,43.22,67.46,54.62
