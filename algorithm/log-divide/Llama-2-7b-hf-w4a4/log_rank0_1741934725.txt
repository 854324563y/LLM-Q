[2025-03-14 06:45:25 root] (main_divide_blocks.py 274): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-divide/Llama-2-7b-hf-w4a4', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=False, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=10, let=False, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, error_threshold=0.2, similarity_threshold=0.999, sensitivity_threshold=0.01, max_block_size=3)
[2025-03-14 06:48:47 root] (main_divide_blocks.py 342): INFO === start quantization ===
[2025-03-14 06:48:48 root] (main_divide_blocks.py 348): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-14 06:48:48 root] (abq_llm_divide_blocks.py 61): INFO Starting ...
[2025-03-14 06:48:49 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 0 similarity and sensitivity ===
[2025-03-14 06:48:50 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 1 similarity and sensitivity ===
[2025-03-14 06:48:53 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9360119700431824
[2025-03-14 06:48:53 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 2 similarity and sensitivity ===
[2025-03-14 06:48:57 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9443559306008475
[2025-03-14 06:48:57 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 3 similarity and sensitivity ===
[2025-03-14 06:49:00 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.998046338558197
[2025-03-14 06:49:00 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 4 similarity and sensitivity ===
[2025-03-14 06:49:02 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9985570311546326
[2025-03-14 06:49:02 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 5 similarity and sensitivity ===
[2025-03-14 06:49:04 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9995824609483991
[2025-03-14 06:49:05 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 6 similarity and sensitivity ===
[2025-03-14 06:49:09 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.99713534116745
[2025-03-14 06:49:09 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 7 similarity and sensitivity ===
[2025-03-14 06:49:12 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999115807669503
[2025-03-14 06:49:12 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 8 similarity and sensitivity ===
[2025-03-14 06:49:14 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9995706677436829
[2025-03-14 06:49:14 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 9 similarity and sensitivity ===
[2025-03-14 06:49:18 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.99960515328816
[2025-03-14 06:49:18 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 10 similarity and sensitivity ===
[2025-03-14 06:49:22 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998485020228794
[2025-03-14 06:49:22 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 11 similarity and sensitivity ===
[2025-03-14 06:49:26 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9980622359684536
[2025-03-14 06:49:26 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 12 similarity and sensitivity ===
[2025-03-14 06:49:30 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9990899392536708
[2025-03-14 06:49:30 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 13 similarity and sensitivity ===
[2025-03-14 06:49:34 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9995642900466919
[2025-03-14 06:49:34 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 14 similarity and sensitivity ===
[2025-03-14 06:49:38 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998711517878941
[2025-03-14 06:49:39 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 15 similarity and sensitivity ===
[2025-03-14 06:49:42 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9995314053126744
[2025-03-14 06:49:42 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 16 similarity and sensitivity ===
[2025-03-14 06:49:46 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9994118979998997
[2025-03-14 06:49:46 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 17 similarity and sensitivity ===
[2025-03-14 06:49:50 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998564379555839
[2025-03-14 06:49:51 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 18 similarity and sensitivity ===
[2025-03-14 06:49:52 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9993684376989093
[2025-03-14 06:49:52 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 19 similarity and sensitivity ===
[2025-03-14 06:49:56 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998666729245868
[2025-03-14 06:49:56 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 20 similarity and sensitivity ===
[2025-03-14 06:49:59 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998541985239301
[2025-03-14 06:50:00 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 21 similarity and sensitivity ===
[2025-03-14 06:50:04 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996138555662972
[2025-03-14 06:50:04 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 22 similarity and sensitivity ===
[2025-03-14 06:50:08 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997891357966832
[2025-03-14 06:50:09 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 23 similarity and sensitivity ===
[2025-03-14 06:50:10 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9992668543543134
[2025-03-14 06:50:11 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 24 similarity and sensitivity ===
[2025-03-14 06:50:15 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9993422968047005
[2025-03-14 06:50:15 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 25 similarity and sensitivity ===
[2025-03-14 06:50:19 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9992739728518895
[2025-03-14 06:50:19 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 26 similarity and sensitivity ===
[2025-03-14 06:50:23 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997433253696987
[2025-03-14 06:50:23 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 27 similarity and sensitivity ===
[2025-03-14 06:50:26 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9990935495921544
[2025-03-14 06:50:26 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 28 similarity and sensitivity ===
[2025-03-14 06:50:28 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9994541917528424
[2025-03-14 06:50:28 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 29 similarity and sensitivity ===
[2025-03-14 06:50:32 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996153882571629
[2025-03-14 06:50:33 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 30 similarity and sensitivity ===
[2025-03-14 06:50:36 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9995548810277667
[2025-03-14 06:50:36 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 31 similarity and sensitivity ===
[2025-03-14 06:50:40 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9964709963117327
[2025-03-14 06:50:41 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 0 ===
[2025-03-14 06:50:51 root] (abq_llm_divide_blocks.py 278): INFO layer 0 loss_mean: 0.00021139730233699083
[2025-03-14 06:50:51 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 1 ===
[2025-03-14 06:51:02 root] (abq_llm_divide_blocks.py 278): INFO layer 1 loss_mean: 0.029242858290672302
[2025-03-14 06:51:02 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 2 ===
[2025-03-14 06:51:13 root] (abq_llm_divide_blocks.py 278): INFO layer 2 loss_mean: 0.0020485015120357275
[2025-03-14 06:51:13 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 3 ===
[2025-03-14 06:51:23 root] (abq_llm_divide_blocks.py 278): INFO layer 3 loss_mean: 0.002238768618553877
[2025-03-14 06:51:23 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 4 ===
[2025-03-14 06:51:34 root] (abq_llm_divide_blocks.py 278): INFO layer 4 loss_mean: 0.0015604542568325996
[2025-03-14 06:51:34 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 5 ===
[2025-03-14 06:51:45 root] (abq_llm_divide_blocks.py 278): INFO layer 5 loss_mean: 0.002369323978200555
[2025-03-14 06:51:45 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 6 ===
[2025-03-14 06:51:56 root] (abq_llm_divide_blocks.py 278): INFO layer 6 loss_mean: 0.007093082647770643
[2025-03-14 06:51:56 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 7 ===
[2025-03-14 06:52:06 root] (abq_llm_divide_blocks.py 278): INFO layer 7 loss_mean: 0.007695554755628109
[2025-03-14 06:52:06 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 8 ===
[2025-03-14 06:52:17 root] (abq_llm_divide_blocks.py 278): INFO layer 8 loss_mean: 0.00911157950758934
[2025-03-14 06:52:17 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 9 ===
[2025-03-14 06:52:28 root] (abq_llm_divide_blocks.py 278): INFO layer 9 loss_mean: 0.008946052752435207
[2025-03-14 06:52:28 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 10 ===
[2025-03-14 06:52:39 root] (abq_llm_divide_blocks.py 278): INFO layer 10 loss_mean: 0.00989164412021637
[2025-03-14 06:52:39 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 11 ===
[2025-03-14 06:52:49 root] (abq_llm_divide_blocks.py 278): INFO layer 11 loss_mean: 0.009533513337373734
[2025-03-14 06:52:49 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 12 ===
[2025-03-14 06:53:00 root] (abq_llm_divide_blocks.py 278): INFO layer 12 loss_mean: 0.00902614276856184
[2025-03-14 06:53:00 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 13 ===
[2025-03-14 06:53:11 root] (abq_llm_divide_blocks.py 278): INFO layer 13 loss_mean: 0.011870363727211952
[2025-03-14 06:53:11 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 14 ===
[2025-03-14 06:53:22 root] (abq_llm_divide_blocks.py 278): INFO layer 14 loss_mean: 0.012384077534079552
[2025-03-14 06:53:22 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 15 ===
[2025-03-14 06:53:33 root] (abq_llm_divide_blocks.py 278): INFO layer 15 loss_mean: 0.017059123143553734
[2025-03-14 06:53:33 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 16 ===
[2025-03-14 06:53:44 root] (abq_llm_divide_blocks.py 278): INFO layer 16 loss_mean: 0.019212007522583008
[2025-03-14 06:53:44 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 17 ===
[2025-03-14 06:53:54 root] (abq_llm_divide_blocks.py 278): INFO layer 17 loss_mean: 0.020454565063118935
[2025-03-14 06:53:54 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 18 ===
[2025-03-14 06:54:05 root] (abq_llm_divide_blocks.py 278): INFO layer 18 loss_mean: 0.02325049601495266
[2025-03-14 06:54:05 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 19 ===
[2025-03-14 06:54:17 root] (abq_llm_divide_blocks.py 278): INFO layer 19 loss_mean: 0.02166445553302765
[2025-03-14 06:54:17 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 20 ===
[2025-03-14 06:54:27 root] (abq_llm_divide_blocks.py 278): INFO layer 20 loss_mean: 0.022523287683725357
[2025-03-14 06:54:27 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 21 ===
[2025-03-14 06:54:38 root] (abq_llm_divide_blocks.py 278): INFO layer 21 loss_mean: 0.021931037306785583
[2025-03-14 06:54:38 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 22 ===
[2025-03-14 06:54:49 root] (abq_llm_divide_blocks.py 278): INFO layer 22 loss_mean: 0.0259387344121933
[2025-03-14 06:54:49 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 23 ===
[2025-03-14 06:55:00 root] (abq_llm_divide_blocks.py 278): INFO layer 23 loss_mean: 0.023562829941511154
[2025-03-14 06:55:00 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 24 ===
[2025-03-14 06:55:11 root] (abq_llm_divide_blocks.py 278): INFO layer 24 loss_mean: 0.02637310139834881
[2025-03-14 06:55:11 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 25 ===
[2025-03-14 06:55:22 root] (abq_llm_divide_blocks.py 278): INFO layer 25 loss_mean: 0.03390054404735565
[2025-03-14 06:55:22 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 26 ===
[2025-03-14 06:55:33 root] (abq_llm_divide_blocks.py 278): INFO layer 26 loss_mean: 0.039924174547195435
[2025-03-14 06:55:33 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 27 ===
[2025-03-14 06:55:44 root] (abq_llm_divide_blocks.py 278): INFO layer 27 loss_mean: 0.04188976436853409
[2025-03-14 06:55:44 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 28 ===
[2025-03-14 06:55:54 root] (abq_llm_divide_blocks.py 278): INFO layer 28 loss_mean: 0.058319635689258575
[2025-03-14 06:55:54 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 29 ===
[2025-03-14 06:56:05 root] (abq_llm_divide_blocks.py 278): INFO layer 29 loss_mean: 0.08634884655475616
[2025-03-14 06:56:05 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 30 ===
[2025-03-14 06:56:17 root] (abq_llm_divide_blocks.py 278): INFO layer 30 loss_mean: 0.2876339256763458
[2025-03-14 06:56:17 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 31 ===
[2025-03-14 06:56:28 root] (abq_llm_divide_blocks.py 278): INFO layer 31 loss_mean: 1.0892561674118042
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 0-1: size=1, error_sum=0.0002, min_similarity=1.0000, max_sensitivity_diff=0.0000
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 1-2: size=1, error_sum=0.0292, min_similarity=0.9360, max_sensitivity_diff=0.7228
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 2-3: size=1, error_sum=0.0020, min_similarity=0.9444, max_sensitivity_diff=0.2538
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 3-4: size=1, error_sum=0.0022, min_similarity=0.9980, max_sensitivity_diff=0.0800
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 4-5: size=1, error_sum=0.0016, min_similarity=0.9986, max_sensitivity_diff=0.0706
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 5-6: size=1, error_sum=0.0024, min_similarity=0.9996, max_sensitivity_diff=0.0824
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 6-7: size=1, error_sum=0.0071, min_similarity=0.9971, max_sensitivity_diff=0.1698
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 7-8: size=1, error_sum=0.0077, min_similarity=0.9999, max_sensitivity_diff=0.0119
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 8-11: size=3, error_sum=0.0279, min_similarity=0.9996, max_sensitivity_diff=0.0125
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 11-12: size=1, error_sum=0.0095, min_similarity=0.9981, max_sensitivity_diff=0.0653
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 12-13: size=1, error_sum=0.0090, min_similarity=0.9991, max_sensitivity_diff=0.0429
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 13-14: size=1, error_sum=0.0119, min_similarity=0.9996, max_sensitivity_diff=0.0156
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 14-15: size=1, error_sum=0.0124, min_similarity=0.9999, max_sensitivity_diff=0.0256
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 15-16: size=1, error_sum=0.0171, min_similarity=0.9995, max_sensitivity_diff=0.0429
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 16-19: size=3, error_sum=0.0629, min_similarity=0.9994, max_sensitivity_diff=0.0120
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 19-20: size=1, error_sum=0.0217, min_similarity=0.9999, max_sensitivity_diff=0.0021
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 20-21: size=1, error_sum=0.0225, min_similarity=0.9999, max_sensitivity_diff=0.0170
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 21-22: size=1, error_sum=0.0219, min_similarity=0.9996, max_sensitivity_diff=0.0173
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 22-23: size=1, error_sum=0.0259, min_similarity=0.9998, max_sensitivity_diff=0.0191
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 23-24: size=1, error_sum=0.0236, min_similarity=0.9993, max_sensitivity_diff=0.0338
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 24-27: size=3, error_sum=0.1002, min_similarity=0.9993, max_sensitivity_diff=0.0535
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 27-30: size=3, error_sum=0.1866, min_similarity=0.9995, max_sensitivity_diff=0.0119
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 30-31: size=1, error_sum=0.2876, min_similarity=0.9996, max_sensitivity_diff=0.0958
[2025-03-14 06:56:28 quantize.utils_divide] (utils_divide.py 110): INFO Block 31-32: size=1, error_sum=1.0893, min_similarity=0.9965, max_sensitivity_diff=0.0162
[2025-03-14 06:56:28 root] (abq_llm_divide_blocks.py 294): INFO blocks: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 27), (27, 30), (30, 31), (31, 32)]
[2025-03-14 06:56:28 root] (main_divide_blocks.py 371): INFO 460.75168085098267
