[2025-03-14 08:27:01 root] (main_divide_blocks.py 276): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-divide/llama-13b-hf-w4a4', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=False, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=10, let=False, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, error_threshold=0.2, similarity_threshold=0.999, sensitivity_threshold=0.1, max_block_size=3, reload=False)
[2025-03-14 08:32:02 root] (main_divide_blocks.py 357): INFO === start quantization ===
[2025-03-14 08:32:02 root] (main_divide_blocks.py 363): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-14 08:32:02 root] (abq_llm_divide_blocks.py 66): INFO Starting ...
[2025-03-14 08:32:04 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 0 similarity and sensitivity ===
[2025-03-14 08:32:04 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.320279788970947
[2025-03-14 08:32:04 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 1 similarity and sensitivity ===
[2025-03-14 08:32:06 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9615949051720756
[2025-03-14 08:32:06 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.6141661882400515
[2025-03-14 08:32:06 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 2 similarity and sensitivity ===
[2025-03-14 08:32:07 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.981979250907898
[2025-03-14 08:32:07 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 4.066954597405025
[2025-03-14 08:32:07 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 3 similarity and sensitivity ===
[2025-03-14 08:32:09 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9900599632944379
[2025-03-14 08:32:09 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7448118703705924
[2025-03-14 08:32:09 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 4 similarity and sensitivity ===
[2025-03-14 08:32:11 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.999608039855957
[2025-03-14 08:32:11 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.762608848299299
[2025-03-14 08:32:11 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 5 similarity and sensitivity ===
[2025-03-14 08:32:13 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9996611731392997
[2025-03-14 08:32:13 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.693883468423571
[2025-03-14 08:32:13 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 6 similarity and sensitivity ===
[2025-03-14 08:32:14 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998395698411124
[2025-03-14 08:32:15 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7523171663284303
[2025-03-14 08:32:15 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 7 similarity and sensitivity ===
[2025-03-14 08:32:16 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9992820194789341
[2025-03-14 08:32:16 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.731710420336042
[2025-03-14 08:32:16 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 8 similarity and sensitivity ===
[2025-03-14 08:32:18 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9989871723311288
[2025-03-14 08:32:18 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.6759087102753774
[2025-03-14 08:32:18 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 9 similarity and sensitivity ===
[2025-03-14 08:32:22 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.99907933814185
[2025-03-14 08:32:23 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.715382119587489
[2025-03-14 08:32:23 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 10 similarity and sensitivity ===
[2025-03-14 08:32:24 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9996614541326251
[2025-03-14 08:32:24 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.69973144020353
[2025-03-14 08:32:24 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 11 similarity and sensitivity ===
[2025-03-14 08:32:26 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999208705765861
[2025-03-14 08:32:26 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.701703068188259
[2025-03-14 08:32:26 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 12 similarity and sensitivity ===
[2025-03-14 08:32:28 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999057565416608
[2025-03-14 08:32:28 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.753949097224644
[2025-03-14 08:32:28 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 13 similarity and sensitivity ===
[2025-03-14 08:32:30 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9992532985551017
[2025-03-14 08:32:30 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.6923445037433082
[2025-03-14 08:32:30 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 14 similarity and sensitivity ===
[2025-03-14 08:32:32 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998063615390232
[2025-03-14 08:32:32 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7501482844352725
[2025-03-14 08:32:32 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 15 similarity and sensitivity ===
[2025-03-14 08:32:34 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998798029763358
[2025-03-14 08:32:34 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7240645374570573
[2025-03-14 08:32:34 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 16 similarity and sensitivity ===
[2025-03-14 08:32:36 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9997897403580802
[2025-03-14 08:32:36 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7159415824072703
[2025-03-14 08:32:36 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 17 similarity and sensitivity ===
[2025-03-14 08:32:38 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999437417302813
[2025-03-14 08:32:38 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7310768008232116
[2025-03-14 08:32:38 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 18 similarity and sensitivity ===
[2025-03-14 08:32:40 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9997287307466779
[2025-03-14 08:32:40 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7162366492407664
[2025-03-14 08:32:40 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 19 similarity and sensitivity ===
[2025-03-14 08:32:42 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998310378619603
[2025-03-14 08:32:42 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7300322192055835
[2025-03-14 08:32:42 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 20 similarity and sensitivity ===
[2025-03-14 08:32:43 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998751538140433
[2025-03-14 08:32:44 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7048156329563686
[2025-03-14 08:32:44 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 21 similarity and sensitivity ===
[2025-03-14 08:32:45 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9995440074375698
[2025-03-14 08:32:46 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.715273421151297
[2025-03-14 08:32:46 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 22 similarity and sensitivity ===
[2025-03-14 08:32:47 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999141778264727
[2025-03-14 08:32:47 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7128319569996426
[2025-03-14 08:32:47 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 23 similarity and sensitivity ===
[2025-03-14 08:32:49 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9996859942163739
[2025-03-14 08:32:49 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.738313409260342
[2025-03-14 08:32:49 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 24 similarity and sensitivity ===
[2025-03-14 08:32:51 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998900975499835
[2025-03-14 08:32:51 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.718493754523141
[2025-03-14 08:32:51 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 25 similarity and sensitivity ===
[2025-03-14 08:32:53 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998655489512852
[2025-03-14 08:32:53 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.733975638662066
[2025-03-14 08:32:53 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 26 similarity and sensitivity ===
[2025-03-14 08:32:55 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999262179647174
[2025-03-14 08:32:55 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.737500841276986
[2025-03-14 08:32:55 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 27 similarity and sensitivity ===
[2025-03-14 08:32:57 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999001026153564
[2025-03-14 08:32:57 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7577734027590073
[2025-03-14 08:32:57 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 28 similarity and sensitivity ===
[2025-03-14 08:32:59 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9997256653649467
[2025-03-14 08:32:59 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7355907474245345
[2025-03-14 08:32:59 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 29 similarity and sensitivity ===
[2025-03-14 08:33:01 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.999829820224217
[2025-03-14 08:33:01 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7765845741544455
[2025-03-14 08:33:01 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 30 similarity and sensitivity ===
[2025-03-14 08:33:03 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999498214040484
[2025-03-14 08:33:03 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7676370246069766
[2025-03-14 08:33:03 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 31 similarity and sensitivity ===
[2025-03-14 08:33:04 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998759201594761
[2025-03-14 08:33:05 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.783556478364127
[2025-03-14 08:33:05 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 32 similarity and sensitivity ===
[2025-03-14 08:33:06 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999594432967049
[2025-03-14 08:33:06 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.8043460028512137
[2025-03-14 08:33:06 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 33 similarity and sensitivity ===
[2025-03-14 08:33:08 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999618870871407
[2025-03-14 08:33:08 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.8258079392569404
[2025-03-14 08:33:08 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 34 similarity and sensitivity ===
[2025-03-14 08:33:10 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.999891860144479
[2025-03-14 08:33:10 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.8291522434779575
[2025-03-14 08:33:10 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 35 similarity and sensitivity ===
[2025-03-14 08:33:12 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9997418693133763
[2025-03-14 08:33:12 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.785220425469535
[2025-03-14 08:33:12 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 36 similarity and sensitivity ===
[2025-03-14 08:33:14 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9996395707130432
[2025-03-14 08:33:14 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.805184800284249
[2025-03-14 08:33:14 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 37 similarity and sensitivity ===
[2025-03-14 08:33:16 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998987827982221
[2025-03-14 08:33:16 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.836404524530683
[2025-03-14 08:33:16 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 38 similarity and sensitivity ===
[2025-03-14 08:33:17 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9997256909097944
[2025-03-14 08:33:18 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.853971035139902
[2025-03-14 08:33:18 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 39 similarity and sensitivity ===
[2025-03-14 08:33:19 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9977353811264038
[2025-03-14 08:33:22 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7991774899618966
[2025-03-14 08:33:22 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 0 ===
[2025-03-14 08:33:36 root] (abq_llm_divide_blocks.py 283): INFO layer 0 loss_mean: 0.002226109616458416
[2025-03-14 08:33:36 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 1 ===
[2025-03-14 08:33:50 root] (abq_llm_divide_blocks.py 283): INFO layer 1 loss_mean: 0.004792365245521069
[2025-03-14 08:33:50 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 2 ===
[2025-03-14 08:34:04 root] (abq_llm_divide_blocks.py 283): INFO layer 2 loss_mean: 1.3331352472305298
[2025-03-14 08:34:04 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 3 ===
[2025-03-14 08:34:18 root] (abq_llm_divide_blocks.py 283): INFO layer 3 loss_mean: 0.004918871447443962
[2025-03-14 08:34:18 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 4 ===
[2025-03-14 08:34:35 root] (abq_llm_divide_blocks.py 283): INFO layer 4 loss_mean: 0.011435621418058872
[2025-03-14 08:34:37 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 5 ===
[2025-03-14 08:34:51 root] (abq_llm_divide_blocks.py 283): INFO layer 5 loss_mean: 0.017215542495250702
[2025-03-14 08:34:51 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 6 ===
[2025-03-14 08:35:05 root] (abq_llm_divide_blocks.py 283): INFO layer 6 loss_mean: 4.561468124389648
[2025-03-14 08:35:05 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 7 ===
[2025-03-14 08:35:18 root] (abq_llm_divide_blocks.py 283): INFO layer 7 loss_mean: 0.012619167566299438
[2025-03-14 08:35:18 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 8 ===
[2025-03-14 08:35:33 root] (abq_llm_divide_blocks.py 283): INFO layer 8 loss_mean: 0.014345774427056313
[2025-03-14 08:35:33 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 9 ===
[2025-03-14 08:35:47 root] (abq_llm_divide_blocks.py 283): INFO layer 9 loss_mean: 0.015526057220995426
[2025-03-14 08:35:47 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 10 ===
[2025-03-14 08:36:01 root] (abq_llm_divide_blocks.py 283): INFO layer 10 loss_mean: 0.01991034299135208
[2025-03-14 08:36:01 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 11 ===
[2025-03-14 08:36:15 root] (abq_llm_divide_blocks.py 283): INFO layer 11 loss_mean: 0.022336026653647423
[2025-03-14 08:36:15 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 12 ===
[2025-03-14 08:36:28 root] (abq_llm_divide_blocks.py 283): INFO layer 12 loss_mean: 0.025731509551405907
[2025-03-14 08:36:28 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 13 ===
[2025-03-14 08:36:42 root] (abq_llm_divide_blocks.py 283): INFO layer 13 loss_mean: 0.030220109969377518
[2025-03-14 08:36:42 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 14 ===
[2025-03-14 08:36:56 root] (abq_llm_divide_blocks.py 283): INFO layer 14 loss_mean: 0.03975162282586098
[2025-03-14 08:36:56 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 15 ===
[2025-03-14 08:37:10 root] (abq_llm_divide_blocks.py 283): INFO layer 15 loss_mean: 0.0356915146112442
[2025-03-14 08:37:10 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 16 ===
[2025-03-14 08:37:24 root] (abq_llm_divide_blocks.py 283): INFO layer 16 loss_mean: 0.04594811052083969
[2025-03-14 08:37:24 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 17 ===
[2025-03-14 08:37:37 root] (abq_llm_divide_blocks.py 283): INFO layer 17 loss_mean: 0.05367680639028549
[2025-03-14 08:37:37 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 18 ===
[2025-03-14 08:37:51 root] (abq_llm_divide_blocks.py 283): INFO layer 18 loss_mean: 0.06273846328258514
[2025-03-14 08:37:51 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 19 ===
[2025-03-14 08:38:05 root] (abq_llm_divide_blocks.py 283): INFO layer 19 loss_mean: 0.06803489476442337
[2025-03-14 08:38:05 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 20 ===
[2025-03-14 08:38:19 root] (abq_llm_divide_blocks.py 283): INFO layer 20 loss_mean: 0.0823562741279602
[2025-03-14 08:38:19 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 21 ===
[2025-03-14 08:38:33 root] (abq_llm_divide_blocks.py 283): INFO layer 21 loss_mean: 0.08792723715305328
[2025-03-14 08:38:33 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 22 ===
[2025-03-14 08:38:47 root] (abq_llm_divide_blocks.py 283): INFO layer 22 loss_mean: 0.09362228214740753
[2025-03-14 08:38:47 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 23 ===
[2025-03-14 08:39:01 root] (abq_llm_divide_blocks.py 283): INFO layer 23 loss_mean: 0.08363264799118042
[2025-03-14 08:39:01 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 24 ===
[2025-03-14 08:39:15 root] (abq_llm_divide_blocks.py 283): INFO layer 24 loss_mean: 0.08683845400810242
[2025-03-14 08:39:15 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 25 ===
[2025-03-14 08:39:29 root] (abq_llm_divide_blocks.py 283): INFO layer 25 loss_mean: 0.09167320281267166
[2025-03-14 08:39:29 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 26 ===
[2025-03-14 08:39:43 root] (abq_llm_divide_blocks.py 283): INFO layer 26 loss_mean: 0.09354615956544876
[2025-03-14 08:39:43 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 27 ===
[2025-03-14 08:39:57 root] (abq_llm_divide_blocks.py 283): INFO layer 27 loss_mean: 0.09397178888320923
[2025-03-14 08:39:57 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 28 ===
[2025-03-14 08:40:11 root] (abq_llm_divide_blocks.py 283): INFO layer 28 loss_mean: 0.12101263552904129
[2025-03-14 08:40:11 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 29 ===
[2025-03-14 08:40:24 root] (abq_llm_divide_blocks.py 283): INFO layer 29 loss_mean: 0.09979680180549622
[2025-03-14 08:40:24 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 30 ===
[2025-03-14 08:40:38 root] (abq_llm_divide_blocks.py 283): INFO layer 30 loss_mean: 0.10218530893325806
[2025-03-14 08:40:38 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 31 ===
[2025-03-14 08:40:52 root] (abq_llm_divide_blocks.py 283): INFO layer 31 loss_mean: 0.11705990880727768
[2025-03-14 08:40:52 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 32 ===
[2025-03-14 08:41:06 root] (abq_llm_divide_blocks.py 283): INFO layer 32 loss_mean: 0.1194460466504097
[2025-03-14 08:41:06 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 33 ===
[2025-03-14 08:41:20 root] (abq_llm_divide_blocks.py 283): INFO layer 33 loss_mean: 0.14123192429542542
[2025-03-14 08:41:22 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 34 ===
[2025-03-14 08:41:36 root] (abq_llm_divide_blocks.py 283): INFO layer 34 loss_mean: 0.19288724660873413
[2025-03-14 08:41:36 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 35 ===
[2025-03-14 08:41:50 root] (abq_llm_divide_blocks.py 283): INFO layer 35 loss_mean: 0.21538779139518738
[2025-03-14 08:41:50 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 36 ===
[2025-03-14 08:42:04 root] (abq_llm_divide_blocks.py 283): INFO layer 36 loss_mean: 0.285514771938324
[2025-03-14 08:42:04 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 37 ===
[2025-03-14 08:42:17 root] (abq_llm_divide_blocks.py 283): INFO layer 37 loss_mean: 0.45537593960762024
[2025-03-14 08:42:17 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 38 ===
[2025-03-14 08:42:33 root] (abq_llm_divide_blocks.py 283): INFO layer 38 loss_mean: 1.735478162765503
[2025-03-14 08:42:33 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 39 ===
[2025-03-14 08:42:47 root] (abq_llm_divide_blocks.py 283): INFO layer 39 loss_mean: 9.131796836853027
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 0-1: size=1, error_sum=0.0022, min_similarity=1.0000, max_sensitivity_diff=0.0000
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 1-2: size=1, error_sum=0.0048, min_similarity=0.9616, max_sensitivity_diff=1.2939
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 2-3: size=1, error_sum=1.3331, min_similarity=0.9820, max_sensitivity_diff=0.4528
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 3-6: size=3, error_sum=0.0336, min_similarity=0.9996, max_sensitivity_diff=0.0687
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 6-7: size=1, error_sum=4.5615, min_similarity=0.9998, max_sensitivity_diff=0.0584
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 7-10: size=3, error_sum=0.0425, min_similarity=0.9990, max_sensitivity_diff=0.0558
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 10-13: size=3, error_sum=0.0680, min_similarity=0.9999, max_sensitivity_diff=0.0522
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 13-16: size=3, error_sum=0.1057, min_similarity=0.9998, max_sensitivity_diff=0.0578
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 16-19: size=3, error_sum=0.1624, min_similarity=0.9997, max_sensitivity_diff=0.0151
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 19-21: size=2, error_sum=0.1504, min_similarity=0.9999, max_sensitivity_diff=0.0252
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 21-23: size=2, error_sum=0.1815, min_similarity=0.9999, max_sensitivity_diff=0.0024
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 23-25: size=2, error_sum=0.1705, min_similarity=0.9999, max_sensitivity_diff=0.0198
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 25-27: size=2, error_sum=0.1852, min_similarity=0.9999, max_sensitivity_diff=0.0035
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 27-28: size=1, error_sum=0.0940, min_similarity=0.9999, max_sensitivity_diff=0.0203
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 28-29: size=1, error_sum=0.1210, min_similarity=0.9997, max_sensitivity_diff=0.0222
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 29-30: size=1, error_sum=0.0998, min_similarity=0.9998, max_sensitivity_diff=0.0410
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 30-31: size=1, error_sum=0.1022, min_similarity=0.9999, max_sensitivity_diff=0.0089
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 31-32: size=1, error_sum=0.1171, min_similarity=0.9999, max_sensitivity_diff=0.0159
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 32-33: size=1, error_sum=0.1194, min_similarity=1.0000, max_sensitivity_diff=0.0208
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 33-34: size=1, error_sum=0.1412, min_similarity=1.0000, max_sensitivity_diff=0.0215
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 34-35: size=1, error_sum=0.1929, min_similarity=0.9999, max_sensitivity_diff=0.0033
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 35-36: size=1, error_sum=0.2154, min_similarity=0.9997, max_sensitivity_diff=0.0439
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 36-37: size=1, error_sum=0.2855, min_similarity=0.9996, max_sensitivity_diff=0.0200
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 37-38: size=1, error_sum=0.4554, min_similarity=0.9999, max_sensitivity_diff=0.0312
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 38-39: size=1, error_sum=1.7355, min_similarity=0.9997, max_sensitivity_diff=0.0176
[2025-03-14 08:42:47 quantize.utils_divide] (utils_divide.py 110): INFO Block 39-40: size=1, error_sum=9.1318, min_similarity=0.9977, max_sensitivity_diff=0.0548
[2025-03-14 08:42:47 root] (abq_llm_divide_blocks.py 299): INFO blocks: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 7), (7, 10), (10, 13), (13, 16), (16, 19), (19, 21), (21, 23), (23, 25), (25, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32), (32, 33), (33, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40)]
[2025-03-14 08:42:47 root] (main_divide_blocks.py 386): INFO 645.62717461586
