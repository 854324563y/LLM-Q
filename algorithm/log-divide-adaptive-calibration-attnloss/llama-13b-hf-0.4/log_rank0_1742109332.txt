[2025-03-16 07:15:32 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/llama-13b-hf-0.4', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/llama-13b-hf/quant_map_llama-13b-hf_0.4.pkl', blocks_pkl='./log-divide/llama-13b-hf-w4a4/llama-13b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-16 07:17:04 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-16 07:17:04 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-16 07:17:05 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-16 07:17:05 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/llama-13b-hf/quant_map_llama-13b-hf_0.4.pkl
[2025-03-16 07:17:05 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/llama-13b-hf-w4a4/llama-13b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 7), (7, 10), (10, 13), (13, 16), (16, 19), (19, 22), (22, 25), (25, 28), (28, 30), (30, 32), (32, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40)]
[2025-03-16 07:17:05 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4, 5], [6], [7, 8, 9], [10, 11, 12], [13, 14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26, 27], [28, 29], [30, 31], [32, 33], [34], [35], [36], [37], [38], [39]]
[2025-03-16 07:17:07 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-16 07:17:07 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-16 07:17:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.01228626724332571 norm:0.012274431996047497 max memory_allocated 44355.7939453125 
[2025-03-16 07:18:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.006841652560979128 norm:0.0064192526042461395 max memory_allocated 44355.7939453125 
[2025-03-16 07:19:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.004316836595535278 norm:0.004162510856986046 max memory_allocated 44355.7939453125 
[2025-03-16 07:20:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0035236093681305647 norm:0.0030657516326755285 max memory_allocated 44355.7939453125 
[2025-03-16 07:20:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.0032379785552620888 norm:0.0025502261705696583 max memory_allocated 44355.7939453125 
[2025-03-16 07:21:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0030680429190397263 norm:0.0021557470317929983 max memory_allocated 44355.7939453125 
[2025-03-16 07:22:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.0029477751813828945 norm:0.0018402517307549715 max memory_allocated 44355.7939453125 
[2025-03-16 07:22:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0028111692517995834 norm:0.001569821615703404 max memory_allocated 44355.7939453125 
[2025-03-16 07:23:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0027361249085515738 norm:0.0014569833874702454 max memory_allocated 44355.7939453125 
[2025-03-16 07:24:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.0026721390895545483 norm:0.0013115069596096873 max memory_allocated 44355.7939453125 
[2025-03-16 07:25:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.00264039752073586 norm:0.0011995892273262143 max memory_allocated 44355.7939453125 
[2025-03-16 07:25:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.002596708480268717 norm:0.0010808455990627408 max memory_allocated 44355.7939453125 
[2025-03-16 07:26:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.002599494531750679 norm:0.0009940129239112139 max memory_allocated 44355.7939453125 
[2025-03-16 07:27:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.0025390111841261387 norm:0.0008669111994095147 max memory_allocated 44355.7939453125 
[2025-03-16 07:27:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0025455616414546967 norm:0.0008150936919264495 max memory_allocated 44355.7939453125 
[2025-03-16 07:28:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.002524977782741189 norm:0.0007868128595873713 max memory_allocated 44355.7939453125 
[2025-03-16 07:29:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.0025062558706849813 norm:0.0007579269586130977 max memory_allocated 44355.7939453125 
[2025-03-16 07:30:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0024940413422882557 norm:0.000730000261683017 max memory_allocated 44355.7939453125 
[2025-03-16 07:30:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.002502705901861191 norm:0.0007105389377102256 max memory_allocated 44355.7939453125 
[2025-03-16 07:31:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0024953430984169245 norm:0.0006625511450693011 max memory_allocated 44355.7939453125 
[2025-03-16 07:32:29 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-16 07:32:29 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-16 07:33:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.022139625623822212 norm:0.01589668169617653 max memory_allocated 44355.7939453125 
[2025-03-16 07:33:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.012355457060039043 norm:0.009958520531654358 max memory_allocated 44355.7939453125 
[2025-03-16 07:34:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.008448625914752483 norm:0.006660887971520424 max memory_allocated 44355.7939453125 
[2025-03-16 07:35:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.007371725980192423 norm:0.004657278768718243 max memory_allocated 44355.7939453125 
[2025-03-16 07:36:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.006936215795576572 norm:0.0038929434958845377 max memory_allocated 44355.7939453125 
[2025-03-16 07:36:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.0066392309963703156 norm:0.003513803705573082 max memory_allocated 44355.7939453125 
[2025-03-16 07:37:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.00641984585672617 norm:0.0031911986880004406 max memory_allocated 44355.7939453125 
[2025-03-16 07:38:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.006244294345378876 norm:0.00293587613850832 max memory_allocated 44355.7939453125 
[2025-03-16 07:38:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.0060921162366867065 norm:0.002696189796552062 max memory_allocated 44355.7939453125 
[2025-03-16 07:39:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.005972902290523052 norm:0.0024550470989197493 max memory_allocated 44355.7939453125 
[2025-03-16 07:40:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.005840667523443699 norm:0.0022187060676515102 max memory_allocated 44355.7939453125 
[2025-03-16 07:41:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.0057340641506016254 norm:0.002026097383350134 max memory_allocated 44355.7939453125 
[2025-03-16 07:41:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.0056426445953547955 norm:0.0018473027739673853 max memory_allocated 44355.7939453125 
[2025-03-16 07:42:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.00555648235604167 norm:0.001682480564340949 max memory_allocated 44355.7939453125 
[2025-03-16 07:43:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.005496652331203222 norm:0.001539930934086442 max memory_allocated 44355.7939453125 
[2025-03-16 07:44:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.005465368274599314 norm:0.0013971845619380474 max memory_allocated 44355.7939453125 
[2025-03-16 07:44:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.005402429960668087 norm:0.0012443345040082932 max memory_allocated 44355.7939453125 
[2025-03-16 07:45:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.005370797589421272 norm:0.0011143068550154567 max memory_allocated 44355.7939453125 
[2025-03-16 07:46:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.005346595775336027 norm:0.001024623867124319 max memory_allocated 44355.7939453125 
[2025-03-16 07:46:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.005326984450221062 norm:0.0010169313754886389 max memory_allocated 44355.7939453125 
[2025-03-16 07:47:51 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
[2025-03-16 07:47:52 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-16 07:48:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.01935388706624508 norm:0.009130174294114113 max memory_allocated 44355.7939453125 
[2025-03-16 07:49:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.01490760874003172 norm:0.006159904878586531 max memory_allocated 44355.7939453125 
[2025-03-16 07:50:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.01204361766576767 norm:0.005101717542856932 max memory_allocated 44355.7939453125 
[2025-03-16 07:50:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.011183010414242744 norm:0.004620993044227362 max memory_allocated 44355.7939453125 
[2025-03-16 07:51:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.010871196165680885 norm:0.004225876182317734 max memory_allocated 44355.7939453125 
[2025-03-16 07:52:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.010453596711158752 norm:0.0038654361851513386 max memory_allocated 44355.7939453125 
[2025-03-16 07:52:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.010040748864412308 norm:0.0036497958935797215 max memory_allocated 44355.7939453125 
[2025-03-16 07:53:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.009947407990694046 norm:0.003557051531970501 max memory_allocated 44355.7939453125 
[2025-03-16 07:54:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.01004802156239748 norm:0.003581894561648369 max memory_allocated 44355.7939453125 
[2025-03-16 07:55:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.009774720296263695 norm:0.0033408422023057938 max memory_allocated 44355.7939453125 
[2025-03-16 07:55:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.009657250717282295 norm:0.003228852292522788 max memory_allocated 44355.7939453125 
[2025-03-16 07:56:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.009701193310320377 norm:0.003297568531706929 max memory_allocated 44355.7939453125 
[2025-03-16 07:57:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.009903039783239365 norm:0.003322928911074996 max memory_allocated 44355.7939453125 
[2025-03-16 07:57:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.009639717638492584 norm:0.00306806736625731 max memory_allocated 44355.7939453125 
[2025-03-16 07:58:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.009742443449795246 norm:0.003253028029575944 max memory_allocated 44355.7939453125 
[2025-03-16 07:59:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.009708055295050144 norm:0.003110777586698532 max memory_allocated 44355.7939453125 
[2025-03-16 08:00:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.009826632216572762 norm:0.0031610080040991306 max memory_allocated 44355.7939453125 
[2025-03-16 08:00:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.00977671891450882 norm:0.0029799023177474737 max memory_allocated 44355.7939453125 
[2025-03-16 08:01:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.009664876386523247 norm:0.002713452558964491 max memory_allocated 44355.7939453125 
[2025-03-16 08:02:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.00958206970244646 norm:0.002617433201521635 max memory_allocated 44355.7939453125 
[2025-03-16 08:03:14 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4, 5] ===
[2025-03-16 08:05:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 0 loss:0.06852513551712036 norm:0.006819234229624271 max memory_allocated 62751.0654296875 
[2025-03-16 08:07:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 1 loss:0.04590895026922226 norm:0.0016651726327836514 max memory_allocated 62751.0654296875 
[2025-03-16 08:09:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 2 loss:0.03470228612422943 norm:0.0007706057513132691 max memory_allocated 62751.0654296875 
[2025-03-16 08:11:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 3 loss:0.02977096103131771 norm:0.00047043836093507707 max memory_allocated 62751.0654296875 
[2025-03-16 08:14:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 4 loss:0.027293123304843903 norm:0.0003621047653723508 max memory_allocated 62751.0654296875 
[2025-03-16 08:16:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 5 loss:0.025712840259075165 norm:0.00034034106647595763 max memory_allocated 62751.0654296875 
[2025-03-16 08:18:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 6 loss:0.024556264281272888 norm:0.0003103995695710182 max memory_allocated 62751.0654296875 
[2025-03-16 08:20:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 7 loss:0.023741204291582108 norm:0.00025836919667199254 max memory_allocated 62751.0654296875 
[2025-03-16 08:22:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 8 loss:0.023200878873467445 norm:0.0002646230277605355 max memory_allocated 62751.0654296875 
[2025-03-16 08:24:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 9 loss:0.02280360832810402 norm:0.000261651846813038 max memory_allocated 62751.0654296875 
[2025-03-16 08:26:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 10 loss:0.022525783628225327 norm:0.00024208494869526476 max memory_allocated 62751.0654296875 
[2025-03-16 08:28:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 11 loss:0.02236415259540081 norm:0.000276245700661093 max memory_allocated 62751.0654296875 
[2025-03-16 08:31:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 12 loss:0.022249706089496613 norm:0.00024902087170630693 max memory_allocated 62751.0654296875 
[2025-03-16 08:33:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 13 loss:0.022134508937597275 norm:0.00025127953267656267 max memory_allocated 62751.0654296875 
[2025-03-16 08:35:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 14 loss:0.022077342495322227 norm:0.0002599497092887759 max memory_allocated 62751.0654296875 
[2025-03-16 08:37:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 15 loss:0.021953336894512177 norm:0.0002131548390025273 max memory_allocated 62751.0654296875 
[2025-03-16 08:39:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 16 loss:0.021870872005820274 norm:0.00021540079615078866 max memory_allocated 62751.0654296875 
[2025-03-16 08:41:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 17 loss:0.021800929680466652 norm:0.00021940015722066164 max memory_allocated 62751.0654296875 
[2025-03-16 08:43:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 18 loss:0.021795613691210747 norm:0.00022937596077099442 max memory_allocated 62751.0654296875 
[2025-03-16 08:45:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 19 loss:0.02183939330279827 norm:0.0002346097317058593 max memory_allocated 62751.0654296875 
[2025-03-16 08:48:55 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [6] ===
[2025-03-16 08:49:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 0 loss:0.041799094527959824 norm:0.0009304628474637866 max memory_allocated 62751.0654296875 
[2025-03-16 08:50:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 1 loss:0.03164851292967796 norm:0.0003647023404482752 max memory_allocated 62751.0654296875 
[2025-03-16 08:51:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 2 loss:0.02579198032617569 norm:0.0002519695262890309 max memory_allocated 62751.0654296875 
[2025-03-16 08:51:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 3 loss:0.023768534883856773 norm:0.00022277601237874478 max memory_allocated 62751.0654296875 
[2025-03-16 08:52:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 4 loss:0.022818008437752724 norm:0.00024454991216771305 max memory_allocated 62751.0654296875 
[2025-03-16 08:53:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 5 loss:0.022249186411499977 norm:0.00025381584418937564 max memory_allocated 62751.0654296875 
[2025-03-16 08:53:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 6 loss:0.02194938436150551 norm:0.0002774662571027875 max memory_allocated 62751.0654296875 
[2025-03-16 08:54:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 7 loss:0.02155148983001709 norm:0.00018786177679430693 max memory_allocated 62751.0654296875 
[2025-03-16 08:55:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 8 loss:0.02143092267215252 norm:0.0002105514140566811 max memory_allocated 62751.0654296875 
[2025-03-16 08:56:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 9 loss:0.02132573164999485 norm:0.000206893848371692 max memory_allocated 62751.0654296875 
[2025-03-16 08:56:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 10 loss:0.021306416019797325 norm:0.00020620904979296029 max memory_allocated 62751.0654296875 
[2025-03-16 08:57:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 11 loss:0.02120564505457878 norm:0.00019121693912893534 max memory_allocated 62751.0654296875 
[2025-03-16 08:58:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 12 loss:0.02118876948952675 norm:0.00014136251411400735 max memory_allocated 62751.0654296875 
[2025-03-16 08:58:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 13 loss:0.02144893817603588 norm:0.00032070453744381666 max memory_allocated 62751.0654296875 
[2025-03-16 08:59:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 14 loss:0.02116103284060955 norm:0.0001188533497042954 max memory_allocated 62751.0654296875 
[2025-03-16 09:00:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 15 loss:0.021325502544641495 norm:0.0001796371361706406 max memory_allocated 62751.0654296875 
[2025-03-16 09:01:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 16 loss:0.021137073636054993 norm:0.00013981698430143297 max memory_allocated 62751.0654296875 
[2025-03-16 09:01:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 17 loss:0.02121291123330593 norm:0.0003079323214478791 max memory_allocated 62751.0654296875 
[2025-03-16 09:02:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 18 loss:0.021228652447462082 norm:0.00014676286082249135 max memory_allocated 62751.0654296875 
[2025-03-16 09:03:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 19 loss:0.021376729011535645 norm:0.000494598934892565 max memory_allocated 62751.0654296875 
[2025-03-16 09:04:13 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [7, 8, 9] ===
[2025-03-16 09:06:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 0 loss:0.07767418771982193 norm:0.0008826456032693386 max memory_allocated 62751.0654296875 
[2025-03-16 09:08:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 1 loss:0.05963843688368797 norm:0.00044166509178467095 max memory_allocated 62751.0654296875 
[2025-03-16 09:10:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 2 loss:0.04800451546907425 norm:0.00032116638612933457 max memory_allocated 62751.0654296875 
[2025-03-16 09:12:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 3 loss:0.042836688458919525 norm:0.0002760694769676775 max memory_allocated 62751.0654296875 
[2025-03-16 09:15:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 4 loss:0.04011417552828789 norm:0.00025323411682620645 max memory_allocated 62751.0654296875 
[2025-03-16 09:17:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 5 loss:0.03826894983649254 norm:0.0002390829467913136 max memory_allocated 62751.0654296875 
[2025-03-16 09:19:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 6 loss:0.03701658546924591 norm:0.0002355045871809125 max memory_allocated 62751.0654296875 
[2025-03-16 09:21:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 7 loss:0.0361967459321022 norm:0.00022851876565255225 max memory_allocated 62751.0654296875 
[2025-03-16 09:23:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 8 loss:0.035661481320858 norm:0.0002113435766659677 max memory_allocated 62751.0654296875 
[2025-03-16 09:25:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 9 loss:0.03529722988605499 norm:0.00021127346553839743 max memory_allocated 62751.0654296875 
[2025-03-16 09:27:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 10 loss:0.03502363711595535 norm:0.00020906834106426686 max memory_allocated 62751.0654296875 
[2025-03-16 09:29:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 11 loss:0.034843236207962036 norm:0.00021166398073546588 max memory_allocated 62751.0654296875 
[2025-03-16 09:32:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 12 loss:0.03471324220299721 norm:0.00020903813128825277 max memory_allocated 62751.0654296875 
[2025-03-16 09:34:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 13 loss:0.03460325673222542 norm:0.00020728442177642137 max memory_allocated 62751.0654296875 
[2025-03-16 09:36:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 14 loss:0.03455469012260437 norm:0.00022185839770827442 max memory_allocated 62751.0654296875 
[2025-03-16 09:38:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 15 loss:0.03447503596544266 norm:0.00021202355856075883 max memory_allocated 62751.0654296875 
[2025-03-16 09:40:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 16 loss:0.034418463706970215 norm:0.00021696535986848176 max memory_allocated 62751.0654296875 
[2025-03-16 09:42:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 17 loss:0.03434944897890091 norm:0.00020703219342976809 max memory_allocated 62751.0654296875 
[2025-03-16 09:44:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 18 loss:0.03433121740818024 norm:0.00021115582785569131 max memory_allocated 62751.0654296875 
[2025-03-16 09:46:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 19 loss:0.03431728854775429 norm:0.00021556831779889762 max memory_allocated 62751.0654296875 
[2025-03-16 09:49:59 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [10, 11, 12] ===
[2025-03-16 09:52:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 0 loss:0.08488365262746811 norm:0.0007152329199016094 max memory_allocated 62751.0654296875 
[2025-03-16 09:54:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 1 loss:0.06779328733682632 norm:0.0004175937210675329 max memory_allocated 62751.0654296875 
[2025-03-16 09:56:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 2 loss:0.05536305904388428 norm:0.0002924177679233253 max memory_allocated 62751.0654296875 
[2025-03-16 09:58:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 3 loss:0.0500367134809494 norm:0.00023916229838505387 max memory_allocated 62751.0654296875 
[2025-03-16 10:00:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 4 loss:0.04744219407439232 norm:0.0002089694025926292 max memory_allocated 62751.0654296875 
[2025-03-16 10:02:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 5 loss:0.04573066905140877 norm:0.00019429724488873035 max memory_allocated 62751.0654296875 
[2025-03-16 10:05:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 6 loss:0.04459032788872719 norm:0.00018450334027875215 max memory_allocated 62751.0654296875 
[2025-03-16 10:07:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 7 loss:0.04385627806186676 norm:0.00017797641339711845 max memory_allocated 62751.0654296875 
[2025-03-16 10:09:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 8 loss:0.043396227061748505 norm:0.00017273850971832871 max memory_allocated 62751.0654296875 
[2025-03-16 10:11:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 9 loss:0.04310142248868942 norm:0.00017065569409169257 max memory_allocated 62751.0654296875 
[2025-03-16 10:13:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 10 loss:0.04286736249923706 norm:0.00016710047202650458 max memory_allocated 62751.0654296875 
[2025-03-16 10:15:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 11 loss:0.04269528016448021 norm:0.00016520830104127526 max memory_allocated 62751.0654296875 
[2025-03-16 10:17:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 12 loss:0.042603231966495514 norm:0.00016569349099881947 max memory_allocated 62751.0654296875 
[2025-03-16 10:19:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 13 loss:0.04250092804431915 norm:0.00016174100164789706 max memory_allocated 62751.0654296875 
[2025-03-16 10:22:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 14 loss:0.042431820183992386 norm:0.0001605306169949472 max memory_allocated 62751.0654296875 
[2025-03-16 10:24:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 15 loss:0.042372964322566986 norm:0.00016063849034253508 max memory_allocated 62751.0654296875 
[2025-03-16 10:26:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 16 loss:0.04231330007314682 norm:0.0001589711318956688 max memory_allocated 62751.0654296875 
[2025-03-16 10:28:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 17 loss:0.04225236549973488 norm:0.00015757013170514256 max memory_allocated 62751.0654296875 
[2025-03-16 10:30:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 18 loss:0.04222472012042999 norm:0.0001596182119101286 max memory_allocated 62751.0654296875 
[2025-03-16 10:32:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 19 loss:0.04220674932003021 norm:0.00016009621322155 max memory_allocated 62751.0654296875 
[2025-03-16 10:35:40 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [13, 14, 15] ===
[2025-03-16 10:37:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 0 loss:0.09657740592956543 norm:0.000648789806291461 max memory_allocated 62751.0654296875 
[2025-03-16 10:40:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 1 loss:0.07946772873401642 norm:0.00038096573553048074 max memory_allocated 62751.0654296875 
[2025-03-16 10:42:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 2 loss:0.0658254325389862 norm:0.00028848910005763173 max memory_allocated 62751.0654296875 
[2025-03-16 10:44:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 3 loss:0.060273006558418274 norm:0.00024501688312739134 max memory_allocated 62751.0654296875 
[2025-03-16 10:46:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 4 loss:0.057147711515426636 norm:0.0002249028766527772 max memory_allocated 62751.0654296875 
[2025-03-16 10:48:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 5 loss:0.05509836599230766 norm:0.00021168698731344193 max memory_allocated 62751.0654296875 
[2025-03-16 10:50:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 6 loss:0.053873658180236816 norm:0.00020083633717149496 max memory_allocated 62751.0654296875 
[2025-03-16 10:52:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 7 loss:0.05310259386897087 norm:0.00019092057482339442 max memory_allocated 62751.0654296875 
[2025-03-16 10:54:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 8 loss:0.052578166127204895 norm:0.00018381699919700623 max memory_allocated 62751.0654296875 
[2025-03-16 10:57:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 9 loss:0.05221252888441086 norm:0.00018399456166662276 max memory_allocated 62751.0654296875 
[2025-03-16 10:59:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 10 loss:0.05198090523481369 norm:0.00018065969925373793 max memory_allocated 62751.0654296875 
[2025-03-16 11:01:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 11 loss:0.05172580108046532 norm:0.00017274533456657082 max memory_allocated 62751.0654296875 
[2025-03-16 11:03:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 12 loss:0.05151938647031784 norm:0.00016957407933659852 max memory_allocated 62751.0654296875 
[2025-03-16 11:05:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 13 loss:0.0513826422393322 norm:0.00016393601254094392 max memory_allocated 62751.0654296875 
[2025-03-16 11:07:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 14 loss:0.05130587890744209 norm:0.00016519674682058394 max memory_allocated 62751.0654296875 
[2025-03-16 11:09:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 15 loss:0.0512232780456543 norm:0.00016484013758599758 max memory_allocated 62751.0654296875 
[2025-03-16 11:12:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 16 loss:0.05114932730793953 norm:0.00016323682211805135 max memory_allocated 62751.0654296875 
[2025-03-16 11:14:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 17 loss:0.051107123494148254 norm:0.00015969318337738514 max memory_allocated 62751.0654296875 
[2025-03-16 11:16:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 18 loss:0.051043566316366196 norm:0.00015899758727755398 max memory_allocated 62751.0654296875 
[2025-03-16 11:18:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 19 loss:0.05098443478345871 norm:0.00016145998961292207 max memory_allocated 62751.0654296875 
[2025-03-16 11:21:20 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [16, 17, 18] ===
[2025-03-16 11:23:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 0 loss:0.11129418760538101 norm:0.0005644820630550385 max memory_allocated 62751.0654296875 
[2025-03-16 11:25:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 1 loss:0.09350970387458801 norm:0.00033542909659445286 max memory_allocated 62751.0654296875 
[2025-03-16 11:27:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 2 loss:0.07845596969127655 norm:0.0002500198897905648 max memory_allocated 62751.0654296875 
[2025-03-16 11:30:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 3 loss:0.07322943210601807 norm:0.00022442617046181113 max memory_allocated 62751.0654296875 
[2025-03-16 11:32:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 4 loss:0.06990306079387665 norm:0.0002088467444991693 max memory_allocated 62751.0654296875 
[2025-03-16 11:34:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 5 loss:0.06786318123340607 norm:0.0001967108401004225 max memory_allocated 62751.0654296875 
[2025-03-16 11:36:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 6 loss:0.06676514446735382 norm:0.00018704890680965036 max memory_allocated 62751.0654296875 
[2025-03-16 11:38:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 7 loss:0.06615012884140015 norm:0.00017999866395257413 max memory_allocated 62751.0654296875 
[2025-03-16 11:40:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 8 loss:0.06573455035686493 norm:0.00017675360140856355 max memory_allocated 62751.0654296875 
[2025-03-16 11:42:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 9 loss:0.06543440371751785 norm:0.0001703242742223665 max memory_allocated 62751.0654296875 
[2025-03-16 11:44:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 10 loss:0.06515005975961685 norm:0.00016236407100223005 max memory_allocated 62751.0654296875 
[2025-03-16 11:47:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 11 loss:0.06492652744054794 norm:0.00015891683869995177 max memory_allocated 62751.0654296875 
[2025-03-16 11:49:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 12 loss:0.06477626413106918 norm:0.00015645919484086335 max memory_allocated 62751.0654296875 
[2025-03-16 11:51:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 13 loss:0.06462790071964264 norm:0.00015378111856989563 max memory_allocated 62751.0654296875 
[2025-03-16 11:53:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 14 loss:0.06448493897914886 norm:0.0001520243677077815 max memory_allocated 62751.0654296875 
[2025-03-16 11:55:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 15 loss:0.06438111513853073 norm:0.00015025046013761312 max memory_allocated 62751.0654296875 
[2025-03-16 11:57:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 16 loss:0.06428992003202438 norm:0.00014915468636900187 max memory_allocated 62751.0654296875 
[2025-03-16 11:59:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 17 loss:0.06417883187532425 norm:0.00014586707402486354 max memory_allocated 62751.0654296875 
[2025-03-16 12:01:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 18 loss:0.06407823413610458 norm:0.00014530145563185215 max memory_allocated 62751.0654296875 
[2025-03-16 12:04:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 19 loss:0.06401965022087097 norm:0.00014461323735304177 max memory_allocated 62751.0654296875 
[2025-03-16 12:07:04 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [19, 20, 21] ===
[2025-03-16 12:09:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 0 loss:0.1513335257768631 norm:0.0006997283780947328 max memory_allocated 62751.3154296875 
[2025-03-16 12:11:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 1 loss:0.1271437555551529 norm:0.0004205862060189247 max memory_allocated 62751.3154296875 
[2025-03-16 12:13:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 2 loss:0.106022909283638 norm:0.0002852406178135425 max memory_allocated 62751.3154296875 
[2025-03-16 12:15:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 3 loss:0.09928517788648605 norm:0.0002581986482255161 max memory_allocated 62751.3154296875 
[2025-03-16 12:17:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 4 loss:0.09527549147605896 norm:0.00024136045249179006 max memory_allocated 62751.3154296875 
[2025-03-16 12:20:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 5 loss:0.09346790611743927 norm:0.00022927389363758266 max memory_allocated 62751.3154296875 
[2025-03-16 12:22:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 6 loss:0.09256088733673096 norm:0.00022086234821472317 max memory_allocated 62751.3154296875 
[2025-03-16 12:24:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 7 loss:0.09192728251218796 norm:0.0002090609777951613 max memory_allocated 62751.3154296875 
[2025-03-16 12:26:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 8 loss:0.0914309024810791 norm:0.00020410868455655873 max memory_allocated 62751.3154296875 
[2025-03-16 12:28:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 9 loss:0.09105594456195831 norm:0.0001966628769878298 max memory_allocated 62751.3154296875 
[2025-03-16 12:30:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 10 loss:0.09070749580860138 norm:0.0001930450089275837 max memory_allocated 62751.3154296875 
[2025-03-16 12:32:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 11 loss:0.09038384258747101 norm:0.0001905984536278993 max memory_allocated 62751.3154296875 
[2025-03-16 12:34:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 12 loss:0.09008719027042389 norm:0.0001844721264205873 max memory_allocated 62751.3154296875 
[2025-03-16 12:37:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 13 loss:0.08986414223909378 norm:0.00018214347073808312 max memory_allocated 62751.3154296875 
[2025-03-16 12:39:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 14 loss:0.08967670798301697 norm:0.00018015227396972477 max memory_allocated 62751.3154296875 
[2025-03-16 12:41:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 15 loss:0.08948925882577896 norm:0.0001790858368622139 max memory_allocated 62751.3154296875 
[2025-03-16 12:43:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 16 loss:0.08932710438966751 norm:0.00017508894961792976 max memory_allocated 62751.3154296875 
[2025-03-16 12:45:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 17 loss:0.08918222039937973 norm:0.00017127393221016973 max memory_allocated 62751.3154296875 
[2025-03-16 12:47:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 18 loss:0.08906954526901245 norm:0.00016786425840109587 max memory_allocated 62751.3154296875 
[2025-03-16 12:49:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 19 loss:0.08896160125732422 norm:0.0001658349356148392 max memory_allocated 62751.3154296875 
[2025-03-16 12:52:44 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [22, 23, 24] ===
[2025-03-16 12:55:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 0 loss:0.21265211701393127 norm:0.0008319379412569106 max memory_allocated 62751.3154296875 
[2025-03-16 12:57:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 1 loss:0.18077650666236877 norm:0.0005149696953594685 max memory_allocated 62751.3154296875 
[2025-03-16 12:59:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 2 loss:0.15196938812732697 norm:0.0003880050790030509 max memory_allocated 62751.3154296875 
[2025-03-16 13:01:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 3 loss:0.14315009117126465 norm:0.00037402487942017615 max memory_allocated 62751.3154296875 
[2025-03-16 13:03:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 4 loss:0.13845129311084747 norm:0.0003618146583903581 max memory_allocated 62751.3154296875 
[2025-03-16 13:05:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 5 loss:0.13659945130348206 norm:0.00034694935311563313 max memory_allocated 62751.3154296875 
[2025-03-16 13:07:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 6 loss:0.13552799820899963 norm:0.0003349807229824364 max memory_allocated 62751.3154296875 
[2025-03-16 13:09:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 7 loss:0.13463008403778076 norm:0.0003251163871027529 max memory_allocated 62751.3154296875 
[2025-03-16 13:12:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 8 loss:0.13395139575004578 norm:0.00032002973603084683 max memory_allocated 62751.3154296875 
[2025-03-16 13:14:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 9 loss:0.13337969779968262 norm:0.0003211211587768048 max memory_allocated 62751.3154296875 
[2025-03-16 13:16:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 10 loss:0.13283315300941467 norm:0.00031052916892804205 max memory_allocated 62751.3154296875 
[2025-03-16 13:18:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 11 loss:0.13236047327518463 norm:0.000298305822070688 max memory_allocated 62751.3154296875 
[2025-03-16 13:20:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 12 loss:0.13193638622760773 norm:0.00029599107801914215 max memory_allocated 62751.3154296875 
[2025-03-16 13:22:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 13 loss:0.13154511153697968 norm:0.00028968247352167964 max memory_allocated 62751.3154296875 
[2025-03-16 13:24:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 14 loss:0.13124892115592957 norm:0.00028539347113110125 max memory_allocated 62751.3154296875 
[2025-03-16 13:26:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 15 loss:0.13096167147159576 norm:0.0002819885849021375 max memory_allocated 62751.3154296875 
[2025-03-16 13:29:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 16 loss:0.13073331117630005 norm:0.0002791481383610517 max memory_allocated 62751.3154296875 
[2025-03-16 13:31:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 17 loss:0.13048331439495087 norm:0.00027212477289140224 max memory_allocated 62751.3154296875 
[2025-03-16 13:33:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 18 loss:0.1302667260169983 norm:0.00026940018869936466 max memory_allocated 62751.3154296875 
[2025-03-16 13:35:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 19 loss:0.13009804487228394 norm:0.0002674776769708842 max memory_allocated 62751.3154296875 
[2025-03-16 13:38:23 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [25, 26, 27] ===
[2025-03-16 13:40:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 0 loss:0.28553903102874756 norm:0.0008679782040417194 max memory_allocated 62751.7841796875 
[2025-03-16 13:42:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 1 loss:0.2450951188802719 norm:0.0005326814716681838 max memory_allocated 62751.7841796875 
[2025-03-16 13:44:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 2 loss:0.2074688971042633 norm:0.00037907404475845397 max memory_allocated 62751.7841796875 
[2025-03-16 13:47:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 3 loss:0.1965194046497345 norm:0.000353072042344138 max memory_allocated 62751.7841796875 
[2025-03-16 13:49:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 4 loss:0.1917191445827484 norm:0.00033712125150486827 max memory_allocated 62751.7841796875 
[2025-03-16 13:51:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 5 loss:0.18995627760887146 norm:0.0003294568741694093 max memory_allocated 62751.7841796875 
[2025-03-16 13:53:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 6 loss:0.18884170055389404 norm:0.00032288202783092856 max memory_allocated 62751.7841796875 
[2025-03-16 13:55:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 7 loss:0.1878715306520462 norm:0.00031490903347730637 max memory_allocated 62751.7841796875 
[2025-03-16 13:57:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 8 loss:0.18706625699996948 norm:0.0003041755117010325 max memory_allocated 62751.7841796875 
[2025-03-16 13:59:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 9 loss:0.18637099862098694 norm:0.0002954191295430064 max memory_allocated 62751.7841796875 
[2025-03-16 14:01:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 10 loss:0.185727059841156 norm:0.0002895819488912821 max memory_allocated 62751.7841796875 
[2025-03-16 14:04:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 11 loss:0.1851387768983841 norm:0.00028530729468911886 max memory_allocated 62751.7841796875 
[2025-03-16 14:06:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 12 loss:0.18467317521572113 norm:0.00027961365412920713 max memory_allocated 62751.7841796875 
[2025-03-16 14:08:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 13 loss:0.18420052528381348 norm:0.00027570861857384443 max memory_allocated 62751.7841796875 
[2025-03-16 14:10:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 14 loss:0.1838228404521942 norm:0.0002730104315560311 max memory_allocated 62751.7841796875 
[2025-03-16 14:12:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 15 loss:0.18346188962459564 norm:0.00027289451099932194 max memory_allocated 62751.7841796875 
[2025-03-16 14:14:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 16 loss:0.18314175307750702 norm:0.00026823379448615015 max memory_allocated 62751.7841796875 
[2025-03-16 14:16:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 17 loss:0.18285201489925385 norm:0.000266267015831545 max memory_allocated 62751.7841796875 
[2025-03-16 14:18:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 18 loss:0.18256033957004547 norm:0.00026484529371373355 max memory_allocated 62751.7841796875 
[2025-03-16 14:21:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 19 loss:0.1822969764471054 norm:0.0002591955999378115 max memory_allocated 62751.7841796875 
[2025-03-16 14:24:04 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [28, 29] ===
[2025-03-16 14:25:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 0 loss:0.2917550504207611 norm:0.0008190660737454891 max memory_allocated 62751.7841796875 
[2025-03-16 14:27:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 1 loss:0.2599996030330658 norm:0.00045311273424886167 max memory_allocated 62751.7841796875 
[2025-03-16 14:28:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 2 loss:0.23028215765953064 norm:0.0003273416659794748 max memory_allocated 62751.7841796875 
[2025-03-16 14:29:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 3 loss:0.22215886414051056 norm:0.00030686272657476366 max memory_allocated 62751.7841796875 
[2025-03-16 14:31:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 4 loss:0.21950945258140564 norm:0.0002796312910504639 max memory_allocated 62751.7841796875 
[2025-03-16 14:32:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 5 loss:0.218428373336792 norm:0.0002774590393528342 max memory_allocated 62751.7841796875 
[2025-03-16 14:34:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 6 loss:0.2176855206489563 norm:0.0002801063237711787 max memory_allocated 62751.7841796875 
[2025-03-16 14:35:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 7 loss:0.2171023190021515 norm:0.0002598755818326026 max memory_allocated 62751.7841796875 
[2025-03-16 14:36:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 8 loss:0.21650448441505432 norm:0.00024372180632781237 max memory_allocated 62751.7841796875 
[2025-03-16 14:38:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 9 loss:0.21599188446998596 norm:0.00023868594144005328 max memory_allocated 62751.7841796875 
[2025-03-16 14:39:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 10 loss:0.2156044840812683 norm:0.00023471337044611573 max memory_allocated 62751.7841796875 
[2025-03-16 14:41:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 11 loss:0.21519052982330322 norm:0.00024238284095190465 max memory_allocated 62751.7841796875 
[2025-03-16 14:42:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 12 loss:0.21483838558197021 norm:0.0002551395446062088 max memory_allocated 62751.7841796875 
[2025-03-16 14:44:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 13 loss:0.21448735892772675 norm:0.00023536404478363693 max memory_allocated 62751.7841796875 
[2025-03-16 14:45:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 14 loss:0.2141917496919632 norm:0.00022915046429261565 max memory_allocated 62751.7841796875 
[2025-03-16 14:46:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 15 loss:0.21392065286636353 norm:0.00022872514091432095 max memory_allocated 62751.7841796875 
[2025-03-16 14:48:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 16 loss:0.21368172764778137 norm:0.0002286797680426389 max memory_allocated 62751.7841796875 
[2025-03-16 14:49:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 17 loss:0.21348698437213898 norm:0.00023098720703274012 max memory_allocated 62751.7841796875 
[2025-03-16 14:51:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 18 loss:0.21328571438789368 norm:0.00023580608831252903 max memory_allocated 62751.7841796875 
[2025-03-16 14:52:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 19 loss:0.2130851149559021 norm:0.00023341845371760428 max memory_allocated 62751.7841796875 
[2025-03-16 14:54:34 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [30, 31] ===
[2025-03-16 14:56:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 0 loss:0.34235793352127075 norm:0.0006328177405521274 max memory_allocated 62751.7841796875 
[2025-03-16 14:57:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 1 loss:0.3094322979450226 norm:0.000421343429479748 max memory_allocated 62751.7841796875 
[2025-03-16 14:58:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 2 loss:0.27752619981765747 norm:0.0002828384458553046 max memory_allocated 62751.7841796875 
[2025-03-16 15:00:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 3 loss:0.2686609923839569 norm:0.00026111974148079753 max memory_allocated 62751.7841796875 
[2025-03-16 15:01:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 4 loss:0.2661779522895813 norm:0.00025578789063729346 max memory_allocated 62751.7841796875 
[2025-03-16 15:03:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 5 loss:0.265033483505249 norm:0.0002465681463945657 max memory_allocated 62751.7841796875 
[2025-03-16 15:04:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 6 loss:0.2642802298069 norm:0.0002440213575027883 max memory_allocated 62751.7841796875 
[2025-03-16 15:06:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 7 loss:0.2635515630245209 norm:0.00023908258299343288 max memory_allocated 62751.7841796875 
[2025-03-16 15:07:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 8 loss:0.26291677355766296 norm:0.0002372652234043926 max memory_allocated 62751.7841796875 
[2025-03-16 15:08:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 9 loss:0.26236504316329956 norm:0.00023465372214559466 max memory_allocated 62751.7841796875 
[2025-03-16 15:10:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 10 loss:0.2618864178657532 norm:0.00022756292310077697 max memory_allocated 62751.7841796875 
[2025-03-16 15:11:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 11 loss:0.2614333927631378 norm:0.0002272852580063045 max memory_allocated 62751.7841796875 
[2025-03-16 15:13:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 12 loss:0.261059045791626 norm:0.0002291792188771069 max memory_allocated 62751.7841796875 
[2025-03-16 15:14:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 13 loss:0.26069319248199463 norm:0.00022917719616089016 max memory_allocated 62751.7841796875 
[2025-03-16 15:16:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 14 loss:0.26040175557136536 norm:0.00022200591047294438 max memory_allocated 62751.7841796875 
[2025-03-16 15:17:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 15 loss:0.2601023316383362 norm:0.00021849488257430494 max memory_allocated 62751.7841796875 
[2025-03-16 15:18:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 16 loss:0.2598229646682739 norm:0.0002255730942124501 max memory_allocated 62751.7841796875 
[2025-03-16 15:20:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 17 loss:0.25955262780189514 norm:0.00023112358758226037 max memory_allocated 62751.7841796875 
[2025-03-16 15:21:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 18 loss:0.2593643367290497 norm:0.00024589267559349537 max memory_allocated 62751.7841796875 
[2025-03-16 15:23:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 19 loss:0.25918418169021606 norm:0.0002568371710367501 max memory_allocated 62751.7841796875 
[2025-03-16 15:25:03 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [32, 33] ===
[2025-03-16 15:26:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 0 loss:0.4002455174922943 norm:0.0008541590650565922 max memory_allocated 62751.7841796875 
[2025-03-16 15:28:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 1 loss:0.3644205927848816 norm:0.0006865072064101696 max memory_allocated 62751.7841796875 
[2025-03-16 15:29:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 2 loss:0.3299606442451477 norm:0.0004279148415662348 max memory_allocated 62751.7841796875 
[2025-03-16 15:30:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 3 loss:0.3208032250404358 norm:0.0003125793591607362 max memory_allocated 62751.7841796875 
[2025-03-16 15:32:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 4 loss:0.31842777132987976 norm:0.00028649240266531706 max memory_allocated 62751.7841796875 
[2025-03-16 15:33:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 5 loss:0.3172188699245453 norm:0.0002766057732515037 max memory_allocated 62751.7841796875 
[2025-03-16 15:35:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 6 loss:0.31628865003585815 norm:0.000264021975453943 max memory_allocated 62751.7841796875 
[2025-03-16 15:36:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 7 loss:0.31553736329078674 norm:0.0002608434879221022 max memory_allocated 62751.7841796875 
[2025-03-16 15:37:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 8 loss:0.31490224599838257 norm:0.0002610786759760231 max memory_allocated 62751.7841796875 
[2025-03-16 15:39:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 9 loss:0.31436821818351746 norm:0.00026329903630539775 max memory_allocated 62751.7841796875 
[2025-03-16 15:40:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 10 loss:0.3138445317745209 norm:0.0002588556963019073 max memory_allocated 62751.7841796875 
[2025-03-16 15:42:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 11 loss:0.31340524554252625 norm:0.0002584043249953538 max memory_allocated 62751.7841796875 
[2025-03-16 15:43:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 12 loss:0.31296706199645996 norm:0.000251912948442623 max memory_allocated 62751.7841796875 
[2025-03-16 15:45:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 13 loss:0.3125641644001007 norm:0.0002474344219081104 max memory_allocated 62751.7841796875 
[2025-03-16 15:46:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 14 loss:0.31219467520713806 norm:0.000247057992964983 max memory_allocated 62751.7841796875 
[2025-03-16 15:47:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 15 loss:0.3118871748447418 norm:0.00024310000299010426 max memory_allocated 62751.7841796875 
[2025-03-16 15:49:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 16 loss:0.31163227558135986 norm:0.00024216229212470353 max memory_allocated 62751.7841796875 
[2025-03-16 15:50:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 17 loss:0.31136876344680786 norm:0.00024635886074975133 max memory_allocated 62751.7841796875 
[2025-03-16 15:52:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 18 loss:0.31112560629844666 norm:0.00024365280114579946 max memory_allocated 62751.7841796875 
[2025-03-16 15:53:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 19 loss:0.3109171986579895 norm:0.00024330485030077398 max memory_allocated 62751.7841796875 
[2025-03-16 15:55:33 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [34] ===
[2025-03-16 15:56:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 0 loss:0.3849852681159973 norm:0.0007304649916477501 max memory_allocated 62751.7841796875 
[2025-03-16 15:57:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 1 loss:0.3638782501220703 norm:0.0004315657133702189 max memory_allocated 62751.7841796875 
[2025-03-16 15:57:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 2 loss:0.3438038229942322 norm:0.00023228046484291553 max memory_allocated 62751.7841796875 
[2025-03-16 15:58:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 3 loss:0.33912837505340576 norm:0.0002076751843560487 max memory_allocated 62751.7841796875 
[2025-03-16 15:59:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 4 loss:0.3381204903125763 norm:0.0001993951154872775 max memory_allocated 62751.7841796875 
[2025-03-16 15:59:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 5 loss:0.3375239372253418 norm:0.00019970335415564477 max memory_allocated 62751.7841796875 
[2025-03-16 16:00:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 6 loss:0.33709168434143066 norm:0.00018999390886165202 max memory_allocated 62751.7841796875 
[2025-03-16 16:01:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 7 loss:0.33668869733810425 norm:0.00018813378119375557 max memory_allocated 62751.7841796875 
[2025-03-16 16:02:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 8 loss:0.3363373279571533 norm:0.0001835968141676858 max memory_allocated 62751.7841796875 
[2025-03-16 16:02:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 9 loss:0.3360295295715332 norm:0.0001799125602701679 max memory_allocated 62751.7841796875 
[2025-03-16 16:03:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 10 loss:0.33575519919395447 norm:0.00018853769870474935 max memory_allocated 62751.7841796875 
[2025-03-16 16:04:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 11 loss:0.33552616834640503 norm:0.00018254316819366068 max memory_allocated 62751.7841796875 
[2025-03-16 16:04:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 12 loss:0.335345983505249 norm:0.000175861336174421 max memory_allocated 62751.7841796875 
[2025-03-16 16:05:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 13 loss:0.3351515233516693 norm:0.00017213223327416927 max memory_allocated 62751.7841796875 
[2025-03-16 16:06:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 14 loss:0.3349606394767761 norm:0.00017045468848664314 max memory_allocated 62751.7841796875 
[2025-03-16 16:07:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 15 loss:0.33482950925827026 norm:0.00016802808386273682 max memory_allocated 62751.7841796875 
[2025-03-16 16:07:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 16 loss:0.334648996591568 norm:0.00016586013953201473 max memory_allocated 62751.7841796875 
[2025-03-16 16:08:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 17 loss:0.33453914523124695 norm:0.00016945452080108225 max memory_allocated 62751.7841796875 
[2025-03-16 16:09:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 18 loss:0.33442994952201843 norm:0.0001670453930273652 max memory_allocated 62751.7841796875 
[2025-03-16 16:09:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 19 loss:0.3342950940132141 norm:0.00016578516806475818 max memory_allocated 62751.7841796875 
[2025-03-16 16:10:51 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 16 with layers [35] ===
[2025-03-16 16:11:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 0 loss:0.421949177980423 norm:0.0008020103559829295 max memory_allocated 62751.7841796875 
[2025-03-16 16:12:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 1 loss:0.3994651734828949 norm:0.0004921077052131295 max memory_allocated 62751.7841796875 
[2025-03-16 16:13:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 2 loss:0.3784983158111572 norm:0.0002830229641404003 max memory_allocated 62751.7841796875 
[2025-03-16 16:13:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 3 loss:0.37387099862098694 norm:0.0002555392275098711 max memory_allocated 62751.7841796875 
[2025-03-16 16:14:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 4 loss:0.3727911710739136 norm:0.0002483401040080935 max memory_allocated 62751.7841796875 
[2025-03-16 16:15:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 5 loss:0.3720756769180298 norm:0.0002373082679696381 max memory_allocated 62751.7841796875 
[2025-03-16 16:15:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 6 loss:0.3715473711490631 norm:0.00022925002849660814 max memory_allocated 62751.7841796875 
[2025-03-16 16:16:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 7 loss:0.37107664346694946 norm:0.00022324203746393323 max memory_allocated 62751.7841796875 
[2025-03-16 16:17:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 8 loss:0.3707101047039032 norm:0.00022563365928363055 max memory_allocated 62751.7841796875 
[2025-03-16 16:18:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 9 loss:0.3703359067440033 norm:0.0002170367370126769 max memory_allocated 62751.7841796875 
[2025-03-16 16:18:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 10 loss:0.36998844146728516 norm:0.00020629653590731323 max memory_allocated 62751.7841796875 
[2025-03-16 16:19:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 11 loss:0.3696725070476532 norm:0.000205293414182961 max memory_allocated 62751.7841796875 
[2025-03-16 16:20:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 12 loss:0.3694024384021759 norm:0.00020370743004605174 max memory_allocated 62751.7841796875 
[2025-03-16 16:20:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 13 loss:0.36917147040367126 norm:0.00020219814905431122 max memory_allocated 62751.7841796875 
[2025-03-16 16:21:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 14 loss:0.36896824836730957 norm:0.00020342062634881586 max memory_allocated 62751.7841796875 
[2025-03-16 16:22:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 15 loss:0.36878740787506104 norm:0.00019390812667552382 max memory_allocated 62751.7841796875 
[2025-03-16 16:23:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 16 loss:0.36860135197639465 norm:0.00018817553063854575 max memory_allocated 62751.7841796875 
[2025-03-16 16:23:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 17 loss:0.36845022439956665 norm:0.00018987187650054693 max memory_allocated 62751.7841796875 
[2025-03-16 16:24:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 18 loss:0.3682868778705597 norm:0.0001923952077049762 max memory_allocated 62751.7841796875 
[2025-03-16 16:25:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 19 loss:0.36813661456108093 norm:0.00019026613153982908 max memory_allocated 62751.7841796875 
[2025-03-16 16:26:09 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 17 with layers [36] ===
[2025-03-16 16:26:10 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-16 16:26:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 0 loss:0.4678785800933838 norm:0.013869783841073513 max memory_allocated 62751.7841796875 
[2025-03-16 16:27:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 1 loss:0.44167524576187134 norm:0.010556323453783989 max memory_allocated 62751.7841796875 
[2025-03-16 16:28:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 2 loss:0.4175977110862732 norm:0.007788673508912325 max memory_allocated 62751.7841796875 
[2025-03-16 16:29:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 3 loss:0.41265901923179626 norm:0.006512871943414211 max memory_allocated 62751.7841796875 
[2025-03-16 16:29:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 4 loss:0.41121339797973633 norm:0.00534098083153367 max memory_allocated 62751.7841796875 
[2025-03-16 16:30:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 5 loss:0.4101930260658264 norm:0.004405046813189983 max memory_allocated 62751.7841796875 
[2025-03-16 16:31:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 6 loss:0.409505695104599 norm:0.003976583946496248 max memory_allocated 62751.7841796875 
[2025-03-16 16:31:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 7 loss:0.4089798033237457 norm:0.003838147036731243 max memory_allocated 62751.7841796875 
[2025-03-16 16:32:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 8 loss:0.40853428840637207 norm:0.003619767725467682 max memory_allocated 62751.7841796875 
[2025-03-16 16:33:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 9 loss:0.40817755460739136 norm:0.003547005821019411 max memory_allocated 62751.7841796875 
[2025-03-16 16:34:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 10 loss:0.40779030323028564 norm:0.0035145985893905163 max memory_allocated 62751.7841796875 
[2025-03-16 16:34:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 11 loss:0.4074627757072449 norm:0.003368380479514599 max memory_allocated 62751.7841796875 
[2025-03-16 16:35:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 12 loss:0.4070644676685333 norm:0.003199224127456546 max memory_allocated 62751.7841796875 
[2025-03-16 16:36:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 13 loss:0.4067789912223816 norm:0.0030855699442327023 max memory_allocated 62751.7841796875 
[2025-03-16 16:36:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 14 loss:0.40650007128715515 norm:0.002906010253354907 max memory_allocated 62751.7841796875 
[2025-03-16 16:37:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 15 loss:0.40624427795410156 norm:0.002895865822210908 max memory_allocated 62751.7841796875 
[2025-03-16 16:38:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 16 loss:0.40601474046707153 norm:0.002664471510797739 max memory_allocated 62751.7841796875 
[2025-03-16 16:39:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 17 loss:0.40592193603515625 norm:0.0028121676295995712 max memory_allocated 62751.7841796875 
[2025-03-16 16:39:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 18 loss:0.4058709740638733 norm:0.002846800023689866 max memory_allocated 62751.7841796875 
[2025-03-16 16:40:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 19 loss:0.4057522416114807 norm:0.0029964677523821592 max memory_allocated 62751.7841796875 
[2025-03-16 16:41:31 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 18 with layers [37] ===
[2025-03-16 16:41:32 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-16 16:42:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 0 loss:0.5394139289855957 norm:0.020465116947889328 max memory_allocated 62751.7841796875 
[2025-03-16 16:43:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 1 loss:0.5028716325759888 norm:0.014120375737547874 max memory_allocated 62751.7841796875 
[2025-03-16 16:43:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 2 loss:0.4715259373188019 norm:0.009910326451063156 max memory_allocated 62751.7841796875 
[2025-03-16 16:44:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 3 loss:0.4650430679321289 norm:0.008217241615056992 max memory_allocated 62751.7841796875 
[2025-03-16 16:45:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 4 loss:0.4631502628326416 norm:0.006932355463504791 max memory_allocated 62751.7841796875 
[2025-03-16 16:45:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 5 loss:0.4617759585380554 norm:0.005856345873326063 max memory_allocated 62751.7841796875 
[2025-03-16 16:46:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 6 loss:0.4608322083950043 norm:0.005024597980082035 max memory_allocated 62751.7841796875 
[2025-03-16 16:47:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 7 loss:0.4601181745529175 norm:0.004627895541489124 max memory_allocated 62751.7841796875 
[2025-03-16 16:48:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 8 loss:0.4596107602119446 norm:0.004738808609545231 max memory_allocated 62751.7841796875 
[2025-03-16 16:48:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 9 loss:0.4591068923473358 norm:0.004635277204215527 max memory_allocated 62751.7841796875 
[2025-03-16 16:49:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 10 loss:0.4586363732814789 norm:0.004500979091972113 max memory_allocated 62751.7841796875 
[2025-03-16 16:50:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 11 loss:0.45827341079711914 norm:0.0042280349880456924 max memory_allocated 62751.7841796875 
[2025-03-16 16:50:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 12 loss:0.4578453004360199 norm:0.004156921524554491 max memory_allocated 62751.7841796875 
[2025-03-16 16:51:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 13 loss:0.45742183923721313 norm:0.003830916481092572 max memory_allocated 62751.7841796875 
[2025-03-16 16:52:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 14 loss:0.4570891261100769 norm:0.003719605738297105 max memory_allocated 62751.7841796875 
[2025-03-16 16:53:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 15 loss:0.45680204033851624 norm:0.003517726669088006 max memory_allocated 62751.7841796875 
[2025-03-16 16:53:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 16 loss:0.4567091464996338 norm:0.003687493735924363 max memory_allocated 62751.7841796875 
[2025-03-16 16:54:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 17 loss:0.45643556118011475 norm:0.00358861917629838 max memory_allocated 62751.7841796875 
[2025-03-16 16:55:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 18 loss:0.4562632739543915 norm:0.0034579418133944273 max memory_allocated 62751.7841796875 
[2025-03-16 16:55:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 19 loss:0.45608073472976685 norm:0.0034591485746204853 max memory_allocated 62751.7841796875 
[2025-03-16 16:56:53 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 19 with layers [38] ===
[2025-03-16 16:56:53 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-16 16:57:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 0 loss:0.6498072743415833 norm:0.03300183266401291 max memory_allocated 62751.7841796875 
[2025-03-16 16:58:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 1 loss:0.6030210256576538 norm:0.022405507043004036 max memory_allocated 62751.7841796875 
[2025-03-16 16:59:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 2 loss:0.5680110454559326 norm:0.015496763400733471 max memory_allocated 62751.7841796875 
[2025-03-16 16:59:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 3 loss:0.5591899156570435 norm:0.013148537836968899 max memory_allocated 62751.7841796875 
[2025-03-16 17:00:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 4 loss:0.5564202070236206 norm:0.011692226864397526 max memory_allocated 62751.7841796875 
[2025-03-16 17:01:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 5 loss:0.5543342232704163 norm:0.010114713571965694 max memory_allocated 62751.7841796875 
[2025-03-16 17:01:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 6 loss:0.5523703098297119 norm:0.008836058899760246 max memory_allocated 62751.7841796875 
[2025-03-16 17:02:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 7 loss:0.5510196685791016 norm:0.007675035856664181 max memory_allocated 62751.7841796875 
[2025-03-16 17:03:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 8 loss:0.5500074625015259 norm:0.007066372781991959 max memory_allocated 62751.7841796875 
[2025-03-16 17:04:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 9 loss:0.5494115352630615 norm:0.006998894270509481 max memory_allocated 62751.7841796875 
[2025-03-16 17:04:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 10 loss:0.5488971471786499 norm:0.007105954457074404 max memory_allocated 62751.7841796875 
[2025-03-16 17:05:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 11 loss:0.5483783483505249 norm:0.006660448852926493 max memory_allocated 62751.7841796875 
[2025-03-16 17:06:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 12 loss:0.5478454828262329 norm:0.006377001293003559 max memory_allocated 62751.7841796875 
[2025-03-16 17:06:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 13 loss:0.5471940040588379 norm:0.005944185424596071 max memory_allocated 62751.7841796875 
[2025-03-16 17:07:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 14 loss:0.5466938018798828 norm:0.005716362502425909 max memory_allocated 62751.7841796875 
[2025-03-16 17:08:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 15 loss:0.5462414026260376 norm:0.005132791120558977 max memory_allocated 62751.7841796875 
[2025-03-16 17:09:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 16 loss:0.5459193587303162 norm:0.004750075750052929 max memory_allocated 62751.7841796875 
[2025-03-16 17:09:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 17 loss:0.5459408760070801 norm:0.005408032331615686 max memory_allocated 62751.7841796875 
[2025-03-16 17:10:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 18 loss:0.5459500551223755 norm:0.005881726276129484 max memory_allocated 62751.7841796875 
[2025-03-16 17:11:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 19 loss:0.5458093881607056 norm:0.0055923620238900185 max memory_allocated 62751.7841796875 
[2025-03-16 17:12:15 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 20 with layers [39] ===
[2025-03-16 17:12:15 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-16 17:13:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 0 loss:1.039482831954956 norm:0.09702524542808533 max memory_allocated 62751.7841796875 
[2025-03-16 17:13:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 1 loss:0.9455295205116272 norm:0.06574632972478867 max memory_allocated 62751.7841796875 
[2025-03-16 17:14:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 2 loss:0.8701110482215881 norm:0.03685571253299713 max memory_allocated 62751.7841796875 
[2025-03-16 17:15:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 3 loss:0.8521875143051147 norm:0.03424770385026932 max memory_allocated 62751.7841796875 
[2025-03-16 17:15:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 4 loss:0.8446464538574219 norm:0.03071148321032524 max memory_allocated 62751.7841796875 
[2025-03-16 17:16:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 5 loss:0.8388233184814453 norm:0.028670860454440117 max memory_allocated 62751.7841796875 
[2025-03-16 17:17:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 6 loss:0.8348421454429626 norm:0.026865165680646896 max memory_allocated 62751.7841796875 
[2025-03-16 17:18:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 7 loss:0.8317094445228577 norm:0.02648783288896084 max memory_allocated 62751.7841796875 
[2025-03-16 17:18:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 8 loss:0.8290292024612427 norm:0.024996556341648102 max memory_allocated 62751.7841796875 
[2025-03-16 17:19:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 9 loss:0.8275535702705383 norm:0.02417653426527977 max memory_allocated 62751.7841796875 
[2025-03-16 17:20:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 10 loss:0.8267691135406494 norm:0.022239811718463898 max memory_allocated 62751.7841796875 
[2025-03-16 17:20:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 11 loss:0.8244920969009399 norm:0.022984446957707405 max memory_allocated 62751.7841796875 
[2025-03-16 17:21:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 12 loss:0.8237394094467163 norm:0.022269707173109055 max memory_allocated 62751.7841796875 
[2025-03-16 17:22:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 13 loss:0.8227578997612 norm:0.022437572479248047 max memory_allocated 62751.7841796875 
[2025-03-16 17:23:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 14 loss:0.8218836784362793 norm:0.022086648270487785 max memory_allocated 62751.7841796875 
[2025-03-16 17:23:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 15 loss:0.820320725440979 norm:0.021066419780254364 max memory_allocated 62751.7841796875 
[2025-03-16 17:24:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 16 loss:0.8192450404167175 norm:0.020220492035150528 max memory_allocated 62751.7841796875 
[2025-03-16 17:25:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 17 loss:0.8184475302696228 norm:0.019883964210748672 max memory_allocated 62751.7841796875 
[2025-03-16 17:25:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 18 loss:0.8177412152290344 norm:0.019317859783768654 max memory_allocated 62751.7841796875 
[2025-03-16 17:26:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 19 loss:0.8170686960220337 norm:0.01887631230056286 max memory_allocated 62751.7841796875 
[2025-03-16 17:27:37 root] (main_calib_config3_attn.py 379): INFO 36633.16463518143
[2025-03-16 17:27:49 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
