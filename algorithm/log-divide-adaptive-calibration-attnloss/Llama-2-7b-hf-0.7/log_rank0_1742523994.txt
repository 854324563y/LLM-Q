[2025-03-21 02:26:34 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/Llama-2-7b-hf-0.7', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.7.pkl', blocks_pkl='./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-21 02:27:49 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-21 02:27:49 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-21 02:27:50 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-21 02:27:50 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.7.pkl
[2025-03-21 02:27:50 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 9), (9, 11), (11, 14), (14, 17), (17, 20), (20, 23), (23, 26), (26, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-21 02:27:50 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12, 13], [14, 15, 16], [17, 18, 19], [20, 21, 22], [23, 24, 25], [26, 27, 28], [29], [30], [31]]
[2025-03-21 02:27:52 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-21 02:27:52 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 02:28:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.002762023126706481 norm:0.00862133875489235 max memory_allocated 34633.880859375 
[2025-03-21 02:28:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.0014251968823373318 norm:0.005211437586694956 max memory_allocated 34633.880859375 
[2025-03-21 02:29:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.0012158052995800972 norm:0.004111037123948336 max memory_allocated 34633.880859375 
[2025-03-21 02:29:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0010655914666131139 norm:0.003168245777487755 max memory_allocated 34633.880859375 
[2025-03-21 02:30:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.001017234637401998 norm:0.002839579712599516 max memory_allocated 34633.880859375 
[2025-03-21 02:30:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0009604785009287298 norm:0.0023744262289255857 max memory_allocated 34633.880859375 
[2025-03-21 02:31:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.0009499102015979588 norm:0.002202455885708332 max memory_allocated 34633.880859375 
[2025-03-21 02:31:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0009393994114361703 norm:0.0019733228255063295 max memory_allocated 34633.880859375 
[2025-03-21 02:32:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0009216570178978145 norm:0.0017936569638550282 max memory_allocated 34633.880859375 
[2025-03-21 02:32:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.0009035616531036794 norm:0.0016353994142264128 max memory_allocated 34633.880859375 
[2025-03-21 02:32:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.0008996875258162618 norm:0.0015074826078489423 max memory_allocated 34633.880859375 
[2025-03-21 02:33:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0008837603381834924 norm:0.001346480450592935 max memory_allocated 34633.880859375 
[2025-03-21 02:33:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.0008799562929198146 norm:0.0012737333308905363 max memory_allocated 34633.880859375 
[2025-03-21 02:34:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.000856531725730747 norm:0.0011674445122480392 max memory_allocated 34633.880859375 
[2025-03-21 02:34:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0008340278291143477 norm:0.0010070153512060642 max memory_allocated 34633.880859375 
[2025-03-21 02:35:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.0008248801459558308 norm:0.0009323639096692204 max memory_allocated 34633.880859375 
[2025-03-21 02:35:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.0008199741714634001 norm:0.0008426188142038882 max memory_allocated 34633.880859375 
[2025-03-21 02:36:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0008405601256527007 norm:0.0007882423815317452 max memory_allocated 34633.880859375 
[2025-03-21 02:36:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.0008198311552405357 norm:0.0007250878261402249 max memory_allocated 34633.880859375 
[2025-03-21 02:36:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0008135517709888518 norm:0.0006574781727977097 max memory_allocated 34633.880859375 
[2025-03-21 02:37:41 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-21 02:37:41 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-21 02:37:41 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 02:38:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.024476654827594757 norm:0.02285216748714447 max memory_allocated 35100.7724609375 
[2025-03-21 02:38:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.013510181568562984 norm:0.012719551101326942 max memory_allocated 35100.7724609375 
[2025-03-21 02:39:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.009010379202663898 norm:0.008860551752150059 max memory_allocated 35100.7724609375 
[2025-03-21 02:39:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.008466183207929134 norm:0.011407675221562386 max memory_allocated 35100.7724609375 
[2025-03-21 02:40:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.007955262437462807 norm:0.008705846965312958 max memory_allocated 35100.7724609375 
[2025-03-21 02:40:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.007518646772950888 norm:0.007335225120186806 max memory_allocated 35100.7724609375 
[2025-03-21 02:40:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.007342655211687088 norm:0.00644112890586257 max memory_allocated 35100.7724609375 
[2025-03-21 02:41:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.007364500779658556 norm:0.006004399619996548 max memory_allocated 35100.7724609375 
[2025-03-21 02:41:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.00765644945204258 norm:0.005907200742512941 max memory_allocated 35100.7724609375 
[2025-03-21 02:42:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.008180016651749611 norm:0.006333693861961365 max memory_allocated 35100.7724609375 
[2025-03-21 02:42:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.008254296146333218 norm:0.006684667896479368 max memory_allocated 35100.7724609375 
[2025-03-21 02:43:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.007523925043642521 norm:0.006978326011449099 max memory_allocated 35100.7724609375 
[2025-03-21 02:43:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.007260838523507118 norm:0.007665522396564484 max memory_allocated 35100.7724609375 
[2025-03-21 02:44:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.006926564034074545 norm:0.007170607801526785 max memory_allocated 35100.7724609375 
[2025-03-21 02:44:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.006800762843340635 norm:0.0068222288973629475 max memory_allocated 35100.7724609375 
[2025-03-21 02:44:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.00678342767059803 norm:0.006549539044499397 max memory_allocated 35100.7724609375 
[2025-03-21 02:45:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.006941499654203653 norm:0.006618296727538109 max memory_allocated 35100.7724609375 
[2025-03-21 02:45:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.006819217465817928 norm:0.007175147533416748 max memory_allocated 35100.7724609375 
[2025-03-21 02:46:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.007370447739958763 norm:0.007351044099777937 max memory_allocated 35100.7724609375 
[2025-03-21 02:46:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.007315666880458593 norm:0.00726061686873436 max memory_allocated 35100.7724609375 
[2025-03-21 02:47:31 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-21 02:47:31 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
[2025-03-21 02:47:31 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 02:48:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.016169637441635132 norm:0.005040123127400875 max memory_allocated 35101.8349609375 
[2025-03-21 02:48:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.011375770904123783 norm:0.003692398313432932 max memory_allocated 35101.8349609375 
[2025-03-21 02:48:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.009398003108799458 norm:0.002888005692511797 max memory_allocated 35101.8349609375 
[2025-03-21 02:49:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.008832976222038269 norm:0.00243429746478796 max memory_allocated 35101.8349609375 
[2025-03-21 02:49:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.008408356457948685 norm:0.002121968427672982 max memory_allocated 35101.8349609375 
[2025-03-21 02:50:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.008071131072938442 norm:0.0018802033737301826 max memory_allocated 35101.8349609375 
[2025-03-21 02:50:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.007898055948317051 norm:0.0016452930867671967 max memory_allocated 35101.8349609375 
[2025-03-21 02:51:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.007805167697370052 norm:0.0013959453208371997 max memory_allocated 35101.8349609375 
[2025-03-21 02:51:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.007767775095999241 norm:0.0012127740774303675 max memory_allocated 35101.8349609375 
[2025-03-21 02:52:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.007733985316008329 norm:0.0010280584683641791 max memory_allocated 35101.8349609375 
[2025-03-21 02:52:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.007715161424130201 norm:0.000849193602334708 max memory_allocated 35101.8349609375 
[2025-03-21 02:53:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.007679816801100969 norm:0.0006949953967705369 max memory_allocated 35101.8349609375 
[2025-03-21 02:53:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.007673793472349644 norm:0.0006390690105035901 max memory_allocated 35101.8349609375 
[2025-03-21 02:53:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.007716562133282423 norm:0.00070722377859056 max memory_allocated 35101.8349609375 
[2025-03-21 02:54:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.007801232393831015 norm:0.0007377587025985122 max memory_allocated 35101.8349609375 
[2025-03-21 02:54:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.007866456173360348 norm:0.0008176382980309427 max memory_allocated 35101.8349609375 
[2025-03-21 02:55:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.007689598947763443 norm:0.0005969136254861951 max memory_allocated 35101.8349609375 
[2025-03-21 02:55:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.007600435055792332 norm:0.0003905173216480762 max memory_allocated 35101.8349609375 
[2025-03-21 02:56:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.0076003954745829105 norm:0.0004227531608194113 max memory_allocated 35101.8349609375 
[2025-03-21 02:56:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.007698222994804382 norm:0.0005436470964923501 max memory_allocated 35101.8349609375 
[2025-03-21 02:57:19 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2]
[2025-03-21 02:57:19 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4, 5] ===
[2025-03-21 02:58:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 0 loss:0.02722024917602539 norm:0.00046081957407295704 max memory_allocated 47477.6044921875 
[2025-03-21 03:00:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 1 loss:0.02214702032506466 norm:0.00024414368090219796 max memory_allocated 47477.6044921875 
[2025-03-21 03:01:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 2 loss:0.01916673593223095 norm:0.00020453623437788337 max memory_allocated 47477.6044921875 
[2025-03-21 03:02:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 3 loss:0.017174066975712776 norm:0.0001806735381251201 max memory_allocated 47477.6044921875 
[2025-03-21 03:04:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 4 loss:0.015843205153942108 norm:0.00021648366237059236 max memory_allocated 47477.6044921875 
[2025-03-21 03:05:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 5 loss:0.015039392746984959 norm:0.00015876471297815442 max memory_allocated 47477.6044921875 
[2025-03-21 03:06:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 6 loss:0.014566684141755104 norm:0.00023960771795827895 max memory_allocated 47477.6044921875 
[2025-03-21 03:08:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 7 loss:0.014331693761050701 norm:0.00022589707805309445 max memory_allocated 47477.6044921875 
[2025-03-21 03:09:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 8 loss:0.014203614555299282 norm:0.00026527390582486987 max memory_allocated 47477.6044921875 
[2025-03-21 03:10:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 9 loss:0.014134889468550682 norm:0.0002456044021528214 max memory_allocated 47477.6044921875 
[2025-03-21 03:12:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 10 loss:0.014109285548329353 norm:0.00025944001390598714 max memory_allocated 47477.6044921875 
[2025-03-21 03:13:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 11 loss:0.014067891985177994 norm:0.0002563426678534597 max memory_allocated 47477.6044921875 
[2025-03-21 03:14:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 12 loss:0.014050350524485111 norm:0.0002408301515970379 max memory_allocated 47477.6044921875 
[2025-03-21 03:16:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 13 loss:0.014064760878682137 norm:0.0002535683452151716 max memory_allocated 47477.6044921875 
[2025-03-21 03:17:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 14 loss:0.014038152992725372 norm:0.0002384801919106394 max memory_allocated 47477.6044921875 
[2025-03-21 03:18:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 15 loss:0.0140336062759161 norm:0.0002601342275738716 max memory_allocated 47477.6044921875 
[2025-03-21 03:20:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 16 loss:0.014004825614392757 norm:0.00023864471586421132 max memory_allocated 47477.6044921875 
[2025-03-21 03:21:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 17 loss:0.013993312604725361 norm:0.0002496622037142515 max memory_allocated 47477.6044921875 
[2025-03-21 03:22:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 18 loss:0.013972639106214046 norm:0.0002562437148299068 max memory_allocated 47477.6044921875 
[2025-03-21 03:24:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 19 loss:0.013973673805594444 norm:0.0002723176730796695 max memory_allocated 47477.6044921875 
[2025-03-21 03:26:19 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [3, 4, 5]
[2025-03-21 03:26:19 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [6, 7, 8] ===
[2025-03-21 03:27:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 0 loss:0.04405746981501579 norm:0.0006021736771799624 max memory_allocated 47477.7919921875 
[2025-03-21 03:29:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 1 loss:0.034161899238824844 norm:0.0003220522776246071 max memory_allocated 47477.7919921875 
[2025-03-21 03:30:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 2 loss:0.027568360790610313 norm:0.0002612221287563443 max memory_allocated 47477.7919921875 
[2025-03-21 03:31:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 3 loss:0.02476293221116066 norm:0.00022685559815727174 max memory_allocated 47477.7919921875 
[2025-03-21 03:33:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 4 loss:0.023117827251553535 norm:0.00020178761042188853 max memory_allocated 47477.7919921875 
[2025-03-21 03:34:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 5 loss:0.022016163915395737 norm:0.00019428464293014258 max memory_allocated 47477.7919921875 
[2025-03-21 03:35:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 6 loss:0.0213700532913208 norm:0.0001939767535077408 max memory_allocated 47477.7919921875 
[2025-03-21 03:37:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 7 loss:0.020988937467336655 norm:0.00019819126464426517 max memory_allocated 47477.7919921875 
[2025-03-21 03:38:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 8 loss:0.020706912502646446 norm:0.00018332613399252295 max memory_allocated 47477.7919921875 
[2025-03-21 03:39:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 9 loss:0.02049325406551361 norm:0.0001732402015477419 max memory_allocated 47477.7919921875 
[2025-03-21 03:41:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 10 loss:0.020348666235804558 norm:0.00017039683007169515 max memory_allocated 47477.7919921875 
[2025-03-21 03:42:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 11 loss:0.020271725952625275 norm:0.0001728014467516914 max memory_allocated 47477.7919921875 
[2025-03-21 03:43:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 12 loss:0.02018274925649166 norm:0.000173709137015976 max memory_allocated 47477.7919921875 
[2025-03-21 03:45:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 13 loss:0.02011903189122677 norm:0.00017567585746292025 max memory_allocated 47477.7919921875 
[2025-03-21 03:46:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 14 loss:0.02003987692296505 norm:0.00016542848607059568 max memory_allocated 47477.7919921875 
[2025-03-21 03:47:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 15 loss:0.019995957612991333 norm:0.00017134765221271664 max memory_allocated 47477.7919921875 
[2025-03-21 03:49:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 16 loss:0.01995287649333477 norm:0.00015684567915741354 max memory_allocated 47477.7919921875 
[2025-03-21 03:50:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 17 loss:0.019929204136133194 norm:0.0001691074576228857 max memory_allocated 47477.7919921875 
[2025-03-21 03:51:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 18 loss:0.019882842898368835 norm:0.00016784701438155025 max memory_allocated 47477.7919921875 
[2025-03-21 03:53:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 19 loss:0.019861813634634018 norm:0.00016873388085514307 max memory_allocated 47477.7919921875 
[2025-03-21 03:55:42 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [6, 7, 8]
[2025-03-21 03:55:42 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [9, 10] ===
[2025-03-21 03:56:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 0 loss:0.03407832980155945 norm:0.0004409788816701621 max memory_allocated 47477.7919921875 
[2025-03-21 03:57:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 1 loss:0.028149418532848358 norm:0.0003063632466364652 max memory_allocated 47477.7919921875 
[2025-03-21 03:58:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 2 loss:0.024385621771216393 norm:0.00021441996796056628 max memory_allocated 47477.7919921875 
[2025-03-21 03:59:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 3 loss:0.0227681752294302 norm:0.0001915657485369593 max memory_allocated 47477.7919921875 
[2025-03-21 04:00:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 4 loss:0.02164408564567566 norm:0.00016902164497878402 max memory_allocated 47477.7919921875 
[2025-03-21 04:01:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 5 loss:0.02091420255601406 norm:0.00016212086484301835 max memory_allocated 47477.7919921875 
[2025-03-21 04:02:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 6 loss:0.02046056278049946 norm:0.00014380415086634457 max memory_allocated 47477.7919921875 
[2025-03-21 04:02:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 7 loss:0.020201193168759346 norm:0.00013985908299218863 max memory_allocated 47477.7919921875 
[2025-03-21 04:03:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 8 loss:0.02003244124352932 norm:0.00013694180233869702 max memory_allocated 47477.7919921875 
[2025-03-21 04:04:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 9 loss:0.019931070506572723 norm:0.00013543743989430368 max memory_allocated 47477.7919921875 
[2025-03-21 04:05:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 10 loss:0.01983102783560753 norm:0.00013298136764205992 max memory_allocated 47477.7919921875 
[2025-03-21 04:06:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 11 loss:0.019751887768507004 norm:0.0001345808123005554 max memory_allocated 47477.7919921875 
[2025-03-21 04:07:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 12 loss:0.01974993757903576 norm:0.00015562640328425914 max memory_allocated 47477.7919921875 
[2025-03-21 04:08:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 13 loss:0.0196563471108675 norm:0.0001385397044941783 max memory_allocated 47477.7919921875 
[2025-03-21 04:09:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 14 loss:0.019599389284849167 norm:0.00014275778084993362 max memory_allocated 47477.7919921875 
[2025-03-21 04:10:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 15 loss:0.01959093287587166 norm:0.00014569979975931346 max memory_allocated 47477.7919921875 
[2025-03-21 04:11:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 16 loss:0.01955939270555973 norm:0.0001448080874979496 max memory_allocated 47477.7919921875 
[2025-03-21 04:11:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 17 loss:0.01954001933336258 norm:0.00013886066153645515 max memory_allocated 47477.7919921875 
[2025-03-21 04:12:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 18 loss:0.019521454349160194 norm:0.00014246070350054651 max memory_allocated 47477.7919921875 
[2025-03-21 04:13:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 19 loss:0.01953674852848053 norm:0.00014015103806741536 max memory_allocated 47477.7919921875 
[2025-03-21 04:15:18 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [9, 10]
[2025-03-21 04:15:18 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [11, 12, 13] ===
[2025-03-21 04:16:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 0 loss:0.04221983253955841 norm:0.0005661183386109769 max memory_allocated 47478.1044921875 
[2025-03-21 04:18:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 1 loss:0.0342034250497818 norm:0.0003146517265122384 max memory_allocated 47478.1044921875 
[2025-03-21 04:19:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 2 loss:0.028737220913171768 norm:0.0002354164607822895 max memory_allocated 47478.1044921875 
[2025-03-21 04:20:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 3 loss:0.026566240936517715 norm:0.00020349730039015412 max memory_allocated 47478.1044921875 
[2025-03-21 04:22:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 4 loss:0.02523825317621231 norm:0.00018312482279725373 max memory_allocated 47478.1044921875 
[2025-03-21 04:23:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 5 loss:0.02428155019879341 norm:0.00016737476107664406 max memory_allocated 47478.1044921875 
[2025-03-21 04:24:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 6 loss:0.023652244359254837 norm:0.00016017879534047097 max memory_allocated 47478.1044921875 
[2025-03-21 04:26:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 7 loss:0.02323232591152191 norm:0.00015396217349916697 max memory_allocated 47478.1044921875 
[2025-03-21 04:27:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 8 loss:0.02291754074394703 norm:0.0001431724667781964 max memory_allocated 47478.1044921875 
[2025-03-21 04:28:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 9 loss:0.02269204705953598 norm:0.00013871405099052936 max memory_allocated 47478.1044921875 
[2025-03-21 04:30:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 10 loss:0.02253681980073452 norm:0.0001358592853648588 max memory_allocated 47478.1044921875 
[2025-03-21 04:31:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 11 loss:0.022431887686252594 norm:0.00013646674051415175 max memory_allocated 47478.1044921875 
[2025-03-21 04:32:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 12 loss:0.022310080006718636 norm:0.0001316778943873942 max memory_allocated 47478.1044921875 
[2025-03-21 04:34:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 13 loss:0.02221575565636158 norm:0.00012458428682293743 max memory_allocated 47478.1044921875 
[2025-03-21 04:35:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 14 loss:0.022147392854094505 norm:0.00012289799633435905 max memory_allocated 47478.1044921875 
[2025-03-21 04:36:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 15 loss:0.02207805961370468 norm:0.00012148427776992321 max memory_allocated 47478.1044921875 
[2025-03-21 04:38:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 16 loss:0.022016186267137527 norm:0.00011599322169786319 max memory_allocated 47478.1044921875 
[2025-03-21 04:39:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 17 loss:0.021981922909617424 norm:0.00011789701966335997 max memory_allocated 47478.1044921875 
[2025-03-21 04:40:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 18 loss:0.02192925661802292 norm:0.0001134237099904567 max memory_allocated 47478.1044921875 
[2025-03-21 04:42:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 19 loss:0.021886631846427917 norm:0.0001109727454604581 max memory_allocated 47478.1044921875 
[2025-03-21 04:45:00 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [11, 12, 13]
[2025-03-21 04:45:00 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [14, 15, 16] ===
[2025-03-21 04:46:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 0 loss:0.04295175150036812 norm:0.0005351824802346528 max memory_allocated 47478.2919921875 
[2025-03-21 04:47:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 1 loss:0.03521785885095596 norm:0.000314686622004956 max memory_allocated 47478.2919921875 
[2025-03-21 04:49:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 2 loss:0.029524412006139755 norm:0.00023105628497432917 max memory_allocated 47478.2919921875 
[2025-03-21 04:50:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 3 loss:0.027360228821635246 norm:0.00018855073722079396 max memory_allocated 47478.2919921875 
[2025-03-21 04:51:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 4 loss:0.02602463588118553 norm:0.00016883175703696907 max memory_allocated 47478.2919921875 
[2025-03-21 04:53:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 5 loss:0.025048481300473213 norm:0.00015653674199711531 max memory_allocated 47478.2919921875 
[2025-03-21 04:54:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 6 loss:0.02437516301870346 norm:0.00014817231567576528 max memory_allocated 47478.2919921875 
[2025-03-21 04:55:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 7 loss:0.023939741775393486 norm:0.00013706387835554779 max memory_allocated 47478.2919921875 
[2025-03-21 04:57:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 8 loss:0.02363194152712822 norm:0.00013120572839397937 max memory_allocated 47478.2919921875 
[2025-03-21 04:58:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 9 loss:0.02338632568717003 norm:0.0001250244677066803 max memory_allocated 47478.2919921875 
[2025-03-21 04:59:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 10 loss:0.023202385753393173 norm:0.00012022048758808523 max memory_allocated 47478.2919921875 
[2025-03-21 05:01:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 11 loss:0.023069938644766808 norm:0.00011826942500192672 max memory_allocated 47478.2919921875 
[2025-03-21 05:02:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 12 loss:0.02294353023171425 norm:0.00011572935909498483 max memory_allocated 47478.2919921875 
[2025-03-21 05:03:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 13 loss:0.022833095863461494 norm:0.00010977286729030311 max memory_allocated 47478.2919921875 
[2025-03-21 05:05:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 14 loss:0.022743213921785355 norm:0.00010654106881702319 max memory_allocated 47478.2919921875 
[2025-03-21 05:06:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 15 loss:0.022670097649097443 norm:0.00010608982847770676 max memory_allocated 47478.2919921875 
[2025-03-21 05:07:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 16 loss:0.022608447819948196 norm:0.00010546082921791822 max memory_allocated 47478.2919921875 
[2025-03-21 05:09:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 17 loss:0.022543814033269882 norm:9.759025851963088e-05 max memory_allocated 47478.2919921875 
[2025-03-21 05:10:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 18 loss:0.022496499121189117 norm:9.294827759731561e-05 max memory_allocated 47478.2919921875 
[2025-03-21 05:11:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 19 loss:0.022471431642770767 norm:9.601716737961397e-05 max memory_allocated 47478.2919921875 
[2025-03-21 05:15:00 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [14, 15, 16]
[2025-03-21 05:15:00 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [17, 18, 19] ===
[2025-03-21 05:16:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 0 loss:0.037727076560258865 norm:0.00039874642970971763 max memory_allocated 47478.4794921875 
[2025-03-21 05:17:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 1 loss:0.03418663889169693 norm:0.00024824088905006647 max memory_allocated 47478.4794921875 
[2025-03-21 05:19:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 2 loss:0.03200569748878479 norm:0.00019384654297027737 max memory_allocated 47478.4794921875 
[2025-03-21 05:20:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 3 loss:0.030006058514118195 norm:0.00016578583745285869 max memory_allocated 47478.4794921875 
[2025-03-21 05:21:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 4 loss:0.028338221833109856 norm:0.00015042873565107584 max memory_allocated 47478.4794921875 
[2025-03-21 05:23:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 5 loss:0.027290264144539833 norm:0.00014022224058862776 max memory_allocated 47478.4794921875 
[2025-03-21 05:24:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 6 loss:0.02679973468184471 norm:0.00013930388377048075 max memory_allocated 47478.4794921875 
[2025-03-21 05:25:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 7 loss:0.026474662125110626 norm:0.00012921997404191643 max memory_allocated 47478.4794921875 
[2025-03-21 05:27:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 8 loss:0.02628253772854805 norm:0.00012048048665747046 max memory_allocated 47478.4794921875 
[2025-03-21 05:28:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 9 loss:0.026158729568123817 norm:0.00012063022586517036 max memory_allocated 47478.4794921875 
[2025-03-21 05:29:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 10 loss:0.02604670077562332 norm:0.00011070447362726554 max memory_allocated 47478.4794921875 
[2025-03-21 05:31:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 11 loss:0.02597985602915287 norm:0.00011133705993415788 max memory_allocated 47478.4794921875 
[2025-03-21 05:32:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 12 loss:0.025913970544934273 norm:0.00010747981286840513 max memory_allocated 47478.4794921875 
[2025-03-21 05:33:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 13 loss:0.025838568806648254 norm:0.00010805768397403881 max memory_allocated 47478.4794921875 
[2025-03-21 05:35:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 14 loss:0.02581108547747135 norm:0.00010340243170503527 max memory_allocated 47478.4794921875 
[2025-03-21 05:36:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 15 loss:0.025772325694561005 norm:0.00010254784137941897 max memory_allocated 47478.4794921875 
[2025-03-21 05:37:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 16 loss:0.025732528418302536 norm:0.00010561498493188992 max memory_allocated 47478.4794921875 
[2025-03-21 05:39:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 17 loss:0.025693379342556 norm:0.00010568395373411477 max memory_allocated 47478.4794921875 
[2025-03-21 05:40:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 18 loss:0.025660082697868347 norm:0.00010143191320821643 max memory_allocated 47478.4794921875 
[2025-03-21 05:41:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 19 loss:0.025626081973314285 norm:9.368624887429178e-05 max memory_allocated 47478.4794921875 
[2025-03-21 05:44:51 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [17, 18, 19]
[2025-03-21 05:44:51 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [20, 21, 22] ===
[2025-03-21 05:46:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 0 loss:0.05006062239408493 norm:0.0007505767862312496 max memory_allocated 47478.6669921875 
[2025-03-21 05:47:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 1 loss:0.04580290615558624 norm:0.00037829577922821045 max memory_allocated 47478.6669921875 
[2025-03-21 05:49:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 2 loss:0.04311905428767204 norm:0.00026242618332616985 max memory_allocated 47478.6669921875 
[2025-03-21 05:50:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 3 loss:0.040594421327114105 norm:0.0002142425801139325 max memory_allocated 47478.6669921875 
[2025-03-21 05:51:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 4 loss:0.038357023149728775 norm:0.00019439973402768373 max memory_allocated 47478.6669921875 
[2025-03-21 05:53:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 5 loss:0.03712944686412811 norm:0.00017925990687217563 max memory_allocated 47478.6669921875 
[2025-03-21 05:54:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 6 loss:0.03669488802552223 norm:0.00017369442502968013 max memory_allocated 47478.6669921875 
[2025-03-21 05:55:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 7 loss:0.03647775948047638 norm:0.00016147075803019106 max memory_allocated 47478.6669921875 
[2025-03-21 05:57:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 8 loss:0.03632742166519165 norm:0.0001573930203448981 max memory_allocated 47478.6669921875 
[2025-03-21 05:58:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 9 loss:0.036221694201231 norm:0.00015591476403642446 max memory_allocated 47478.6669921875 
[2025-03-21 05:59:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 10 loss:0.03608623519539833 norm:0.00014664304035250098 max memory_allocated 47478.6669921875 
[2025-03-21 06:01:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 11 loss:0.0359906367957592 norm:0.00014671770622953773 max memory_allocated 47478.6669921875 
[2025-03-21 06:02:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 12 loss:0.03590085357427597 norm:0.00014343939255923033 max memory_allocated 47478.6669921875 
[2025-03-21 06:03:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 13 loss:0.035828668624162674 norm:0.00013085472164675593 max memory_allocated 47478.6669921875 
[2025-03-21 06:05:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 14 loss:0.03575838357210159 norm:0.00013135529297869653 max memory_allocated 47478.6669921875 
[2025-03-21 06:06:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 15 loss:0.03570004552602768 norm:0.0001234688825206831 max memory_allocated 47478.6669921875 
[2025-03-21 06:07:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 16 loss:0.03564538061618805 norm:0.00012028760102111846 max memory_allocated 47478.6669921875 
[2025-03-21 06:09:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 17 loss:0.035593822598457336 norm:0.0001184152570203878 max memory_allocated 47478.6669921875 
[2025-03-21 06:10:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 18 loss:0.0355701670050621 norm:0.0001241821446456015 max memory_allocated 47478.6669921875 
[2025-03-21 06:11:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 19 loss:0.03554319590330124 norm:0.00012458641140256077 max memory_allocated 47478.6669921875 
[2025-03-21 06:14:41 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [20, 21, 22]
[2025-03-21 06:14:41 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [23, 24, 25] ===
[2025-03-21 06:16:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 0 loss:0.07107541710138321 norm:0.0014220655430108309 max memory_allocated 47478.8544921875 
[2025-03-21 06:17:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 1 loss:0.06510608643293381 norm:0.0004476391477510333 max memory_allocated 47478.8544921875 
[2025-03-21 06:18:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 2 loss:0.06140675023198128 norm:0.00034458067966625094 max memory_allocated 47478.8544921875 
[2025-03-21 06:20:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 3 loss:0.05759790912270546 norm:0.00026935807545669377 max memory_allocated 47478.8544921875 
[2025-03-21 06:21:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 4 loss:0.05488431826233864 norm:0.00025462015764787793 max memory_allocated 47478.8544921875 
[2025-03-21 06:22:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 5 loss:0.05400232970714569 norm:0.0002315325109520927 max memory_allocated 47478.8544921875 
[2025-03-21 06:24:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 6 loss:0.053703270852565765 norm:0.00022422320034820586 max memory_allocated 47478.8544921875 
[2025-03-21 06:25:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 7 loss:0.053517285734415054 norm:0.00023411870643030852 max memory_allocated 47478.8544921875 
[2025-03-21 06:26:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 8 loss:0.05331476032733917 norm:0.00021142713376320899 max memory_allocated 47478.8544921875 
[2025-03-21 06:28:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 9 loss:0.05313107371330261 norm:0.00019634679483715445 max memory_allocated 47478.8544921875 
[2025-03-21 06:29:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 10 loss:0.05302448570728302 norm:0.00019220389367546886 max memory_allocated 47478.8544921875 
[2025-03-21 06:30:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 11 loss:0.052896589040756226 norm:0.00019566022092476487 max memory_allocated 47478.8544921875 
[2025-03-21 06:32:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 12 loss:0.05279286205768585 norm:0.0001959046785486862 max memory_allocated 47478.8544921875 
[2025-03-21 06:33:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 13 loss:0.052707523107528687 norm:0.00018792696937453002 max memory_allocated 47478.8544921875 
[2025-03-21 06:34:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 14 loss:0.05266821011900902 norm:0.00019047156092710793 max memory_allocated 47478.8544921875 
[2025-03-21 06:36:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 15 loss:0.05258043855428696 norm:0.00017502451373729855 max memory_allocated 47478.8544921875 
[2025-03-21 06:37:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 16 loss:0.052527960389852524 norm:0.0001693903759587556 max memory_allocated 47478.8544921875 
[2025-03-21 06:38:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 17 loss:0.052471987903118134 norm:0.00017157374531961977 max memory_allocated 47478.8544921875 
[2025-03-21 06:40:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 18 loss:0.052397970110177994 norm:0.00017165919416584074 max memory_allocated 47478.8544921875 
[2025-03-21 06:41:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 19 loss:0.05234665051102638 norm:0.00016682226851116866 max memory_allocated 47478.8544921875 
[2025-03-21 06:44:42 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [23, 24, 25]
[2025-03-21 06:44:42 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [26, 27, 28] ===
[2025-03-21 06:44:43 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 06:46:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 0 loss:0.10303939133882523 norm:0.00979735516011715 max memory_allocated 47479.0419921875 
[2025-03-21 06:47:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 1 loss:0.09519000351428986 norm:0.0067692287266254425 max memory_allocated 47479.0419921875 
[2025-03-21 06:48:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 2 loss:0.08935370296239853 norm:0.004796188324689865 max memory_allocated 47479.0419921875 
[2025-03-21 06:50:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 3 loss:0.08393024653196335 norm:0.0036720542702823877 max memory_allocated 47479.0419921875 
[2025-03-21 06:51:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 4 loss:0.08151640743017197 norm:0.003059301059693098 max memory_allocated 47479.0419921875 
[2025-03-21 06:52:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 5 loss:0.08078702539205551 norm:0.0026163668371737003 max memory_allocated 47479.0419921875 
[2025-03-21 06:54:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 6 loss:0.08038614690303802 norm:0.002196941990405321 max memory_allocated 47479.0419921875 
[2025-03-21 06:55:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 7 loss:0.08004950731992722 norm:0.001870138687081635 max memory_allocated 47479.0419921875 
[2025-03-21 06:56:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 8 loss:0.07983469218015671 norm:0.0018636236200109124 max memory_allocated 47479.0419921875 
[2025-03-21 06:58:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 9 loss:0.07968828827142715 norm:0.0019030797993764281 max memory_allocated 47479.0419921875 
[2025-03-21 06:59:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 10 loss:0.0797024667263031 norm:0.0016364873154088855 max memory_allocated 47479.0419921875 
[2025-03-21 07:00:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 11 loss:0.0795055627822876 norm:0.001947652781382203 max memory_allocated 47479.0419921875 
[2025-03-21 07:02:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 12 loss:0.07934755086898804 norm:0.001679140841588378 max memory_allocated 47479.0419921875 
[2025-03-21 07:03:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 13 loss:0.07916589081287384 norm:0.0016415254212915897 max memory_allocated 47479.0419921875 
[2025-03-21 07:05:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 14 loss:0.07906073331832886 norm:0.0015804072609171271 max memory_allocated 47479.0419921875 
[2025-03-21 07:06:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 15 loss:0.07895360887050629 norm:0.0014971174532547593 max memory_allocated 47479.0419921875 
[2025-03-21 07:07:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 16 loss:0.07883081585168839 norm:0.0014429568545892835 max memory_allocated 47479.0419921875 
[2025-03-21 07:09:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 17 loss:0.07876025885343552 norm:0.0013891226844862103 max memory_allocated 47479.0419921875 
[2025-03-21 07:10:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 18 loss:0.07872185856103897 norm:0.0013555304612964392 max memory_allocated 47479.0419921875 
[2025-03-21 07:11:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 19 loss:0.07871273905038834 norm:0.0013028592802584171 max memory_allocated 47479.0419921875 
[2025-03-21 07:14:36 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [26, 27, 28]
[2025-03-21 07:14:36 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [29] ===
[2025-03-21 07:14:37 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 07:15:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 0 loss:0.09642669558525085 norm:0.008636209182441235 max memory_allocated 47479.0419921875 
[2025-03-21 07:15:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 1 loss:0.09144680202007294 norm:0.006765203084796667 max memory_allocated 47479.0419921875 
[2025-03-21 07:16:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 2 loss:0.08789999037981033 norm:0.004301972687244415 max memory_allocated 47479.0419921875 
[2025-03-21 07:16:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 3 loss:0.0857083722949028 norm:0.0033916179090738297 max memory_allocated 47479.0419921875 
[2025-03-21 07:16:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 4 loss:0.0851118266582489 norm:0.002886455738916993 max memory_allocated 47479.0419921875 
[2025-03-21 07:17:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 5 loss:0.08486801385879517 norm:0.002476670080795884 max memory_allocated 47479.0419921875 
[2025-03-21 07:17:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 6 loss:0.08467938005924225 norm:0.002126810373738408 max memory_allocated 47479.0419921875 
[2025-03-21 07:18:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 7 loss:0.08457200974225998 norm:0.0018294237088412046 max memory_allocated 47479.0419921875 
[2025-03-21 07:18:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 8 loss:0.08449467271566391 norm:0.0016427403315901756 max memory_allocated 47479.0419921875 
[2025-03-21 07:19:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 9 loss:0.08445204794406891 norm:0.0016581987729296088 max memory_allocated 47479.0419921875 
[2025-03-21 07:19:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 10 loss:0.08444377034902573 norm:0.0016891340492293239 max memory_allocated 47479.0419921875 
[2025-03-21 07:20:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 11 loss:0.08436818420886993 norm:0.0015633038710802794 max memory_allocated 47479.0419921875 
[2025-03-21 07:20:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 12 loss:0.0842849388718605 norm:0.0013891472481191158 max memory_allocated 47479.0419921875 
[2025-03-21 07:21:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 13 loss:0.08429528027772903 norm:0.0015175493899732828 max memory_allocated 47479.0419921875 
[2025-03-21 07:21:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 14 loss:0.08435243368148804 norm:0.001465535257011652 max memory_allocated 47479.0419921875 
[2025-03-21 07:21:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 15 loss:0.08429639041423798 norm:0.0015276572667062283 max memory_allocated 47479.0419921875 
[2025-03-21 07:22:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 16 loss:0.08435389399528503 norm:0.0011957479873672128 max memory_allocated 47479.0419921875 
[2025-03-21 07:22:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 17 loss:0.0842667743563652 norm:0.0014096435625106096 max memory_allocated 47479.0419921875 
[2025-03-21 07:23:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 18 loss:0.08426393568515778 norm:0.0012683043023571372 max memory_allocated 47479.0419921875 
[2025-03-21 07:23:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 19 loss:0.08418060839176178 norm:0.0012796401279047132 max memory_allocated 47479.0419921875 
[2025-03-21 07:24:41 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [29]
[2025-03-21 07:24:41 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [30] ===
[2025-03-21 07:24:42 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 07:25:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 0 loss:0.1346142441034317 norm:0.013968929648399353 max memory_allocated 47479.0419921875 
[2025-03-21 07:25:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 1 loss:0.11937884986400604 norm:0.010640107095241547 max memory_allocated 47479.0419921875 
[2025-03-21 07:26:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 2 loss:0.11204780638217926 norm:0.00753151997923851 max memory_allocated 47479.0419921875 
[2025-03-21 07:26:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 3 loss:0.10835624486207962 norm:0.006049243733286858 max memory_allocated 47479.0419921875 
[2025-03-21 07:27:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 4 loss:0.10731236636638641 norm:0.005205090623348951 max memory_allocated 47479.0419921875 
[2025-03-21 07:27:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 5 loss:0.10677541047334671 norm:0.004607249982655048 max memory_allocated 47479.0419921875 
[2025-03-21 07:27:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 6 loss:0.1064206212759018 norm:0.004179878160357475 max memory_allocated 47479.0419921875 
[2025-03-21 07:28:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 7 loss:0.10619136691093445 norm:0.0036833477206528187 max memory_allocated 47479.0419921875 
[2025-03-21 07:28:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 8 loss:0.10594768822193146 norm:0.0032544760033488274 max memory_allocated 47479.0419921875 
[2025-03-21 07:29:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 9 loss:0.10580665618181229 norm:0.00299187283962965 max memory_allocated 47479.0419921875 
[2025-03-21 07:29:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 10 loss:0.10567713528871536 norm:0.0027411039918661118 max memory_allocated 47479.0419921875 
[2025-03-21 07:30:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 11 loss:0.10572266578674316 norm:0.0027536312118172646 max memory_allocated 47479.0419921875 
[2025-03-21 07:30:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 12 loss:0.10551653057336807 norm:0.0026520523242652416 max memory_allocated 47479.0419921875 
[2025-03-21 07:31:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 13 loss:0.10555238276720047 norm:0.0025056088343262672 max memory_allocated 47479.0419921875 
[2025-03-21 07:31:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 14 loss:0.1053750216960907 norm:0.002358384430408478 max memory_allocated 47479.0419921875 
[2025-03-21 07:32:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 15 loss:0.10538577288389206 norm:0.0024589626118540764 max memory_allocated 47479.0419921875 
[2025-03-21 07:32:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 16 loss:0.1052931696176529 norm:0.0021765718702226877 max memory_allocated 47479.0419921875 
[2025-03-21 07:32:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 17 loss:0.10532446205615997 norm:0.0023159359116107225 max memory_allocated 47479.0419921875 
[2025-03-21 07:33:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 18 loss:0.10539748519659042 norm:0.0024205450899899006 max memory_allocated 47479.0419921875 
[2025-03-21 07:33:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 19 loss:0.10552667081356049 norm:0.002370618050917983 max memory_allocated 47479.0419921875 
[2025-03-21 07:34:44 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [30]
[2025-03-21 07:34:44 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [31] ===
[2025-03-21 07:34:44 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 07:35:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 0 loss:0.20496904850006104 norm:0.018588438630104065 max memory_allocated 47479.0419921875 
[2025-03-21 07:35:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 1 loss:0.19129802286624908 norm:0.014133862219750881 max memory_allocated 47479.0419921875 
[2025-03-21 07:36:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 2 loss:0.18242043256759644 norm:0.010453026741743088 max memory_allocated 47479.0419921875 
[2025-03-21 07:36:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 3 loss:0.17828404903411865 norm:0.008526048623025417 max memory_allocated 47479.0419921875 
[2025-03-21 07:37:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 4 loss:0.17642703652381897 norm:0.0072117154486477375 max memory_allocated 47479.0419921875 
[2025-03-21 07:37:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 5 loss:0.17551015317440033 norm:0.0064240870997309685 max memory_allocated 47479.0419921875 
[2025-03-21 07:37:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 6 loss:0.17505721747875214 norm:0.005799717269837856 max memory_allocated 47479.0419921875 
[2025-03-21 07:38:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 7 loss:0.17459803819656372 norm:0.005382824223488569 max memory_allocated 47479.0419921875 
[2025-03-21 07:38:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 8 loss:0.17464669048786163 norm:0.005323810037225485 max memory_allocated 47479.0419921875 
[2025-03-21 07:39:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 9 loss:0.17428843677043915 norm:0.005060616880655289 max memory_allocated 47479.0419921875 
[2025-03-21 07:39:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 10 loss:0.1741863489151001 norm:0.004939249716699123 max memory_allocated 47479.0419921875 
[2025-03-21 07:40:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 11 loss:0.1741447150707245 norm:0.0050139594823122025 max memory_allocated 47479.0419921875 
[2025-03-21 07:40:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 12 loss:0.17412348091602325 norm:0.004838964436203241 max memory_allocated 47479.0419921875 
[2025-03-21 07:41:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 13 loss:0.17390431463718414 norm:0.004593449179083109 max memory_allocated 47479.0419921875 
[2025-03-21 07:41:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 14 loss:0.17379005253314972 norm:0.004728860687464476 max memory_allocated 47479.0419921875 
[2025-03-21 07:42:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 15 loss:0.1736685335636139 norm:0.0049264379777014256 max memory_allocated 47479.0419921875 
[2025-03-21 07:42:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 16 loss:0.17343051731586456 norm:0.004566877614706755 max memory_allocated 47479.0419921875 
[2025-03-21 07:42:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 17 loss:0.17343991994857788 norm:0.004775321576744318 max memory_allocated 47479.0419921875 
[2025-03-21 07:43:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 18 loss:0.17345577478408813 norm:0.004787767305970192 max memory_allocated 47479.0419921875 
[2025-03-21 07:43:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 19 loss:0.17334453761577606 norm:0.004689435474574566 max memory_allocated 47479.0419921875 
[2025-03-21 07:44:52 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [31]
[2025-03-21 07:44:52 root] (main_calib_config3_attn.py 379): INFO 19022.938031196594
[2025-03-21 07:45:08 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-21 07:45:54 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.566652297973633
[2025-03-21 07:45:54 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-21 07:47:05 root] (main_calib_config3_attn.py 161): INFO c4 : 7.087307453155518
[2025-03-21 08:50:14 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.566652297973633, 'c4': 7.087307453155518, 'results': {'piqa': {'acc': 0.7823721436343852, 'acc_stderr': 0.009627407474840885, 'acc_norm': 0.7731229597388466, 'acc_norm_stderr': 0.009771584259215167}, 'arc_challenge': {'acc': 0.40017064846416384, 'acc_stderr': 0.014317197787809181, 'acc_norm': 0.40273037542662116, 'acc_norm_stderr': 0.01433223630679014}, 'boolq': {'acc': 0.7070336391437309, 'acc_stderr': 0.0079601537548866}, 'arc_easy': {'acc': 0.7091750841750841, 'acc_stderr': 0.009318815921176653, 'acc_norm': 0.5412457912457912, 'acc_norm_stderr': 0.010224815730255816}, 'winogrande': {'acc': 0.6669297553275454, 'acc_stderr': 0.013246194028070658}, 'hellaswag': {'acc': 0.5604461262696674, 'acc_stderr': 0.004953184534223994, 'acc_norm': 0.719577773351922, 'acc_norm_stderr': 0.0044828747322373476}}, 'versions': {'piqa': 0, 'arc_challenge': 0, 'boolq': 1, 'arc_easy': 0, 'winogrande': 0, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-21 08:50:14 root] (main_calib_config3_attn.py 175): INFO 40.02,70.92,70.70,56.04,78.24,66.69
