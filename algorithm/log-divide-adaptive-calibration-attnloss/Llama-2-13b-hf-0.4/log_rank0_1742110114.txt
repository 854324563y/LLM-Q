[2025-03-16 07:28:34 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/Llama-2-13b-hf-0.4', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/Llama-2-13b-hf/quant_map_Llama-2-13b-hf_0.4.pkl', blocks_pkl='./log-divide/Llama-2-13b-hf-w4a4/Llama-2-13b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-16 07:28:36 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-16 07:28:36 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-16 07:28:36 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-16 07:28:36 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/Llama-2-13b-hf/quant_map_Llama-2-13b-hf_0.4.pkl
[2025-03-16 07:28:36 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/Llama-2-13b-hf-w4a4/Llama-2-13b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 9), (9, 12), (12, 15), (15, 18), (18, 21), (21, 24), (24, 27), (27, 30), (30, 33), (33, 36), (36, 37), (37, 38), (38, 39), (39, 40)]
[2025-03-16 07:28:36 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17], [18, 19, 20], [21, 22, 23], [24, 25, 26], [27, 28, 29], [30, 31, 32], [33, 34, 35], [36], [37], [38], [39]]
[2025-03-16 07:28:45 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-16 07:28:45 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-16 07:29:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.010323721915483475 norm:0.014946574345231056 max memory_allocated 44358.7939453125 
[2025-03-16 07:30:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.005699658766388893 norm:0.008258647285401821 max memory_allocated 44358.7939453125 
[2025-03-16 07:30:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.003884692210704088 norm:0.005237219389528036 max memory_allocated 44358.7939453125 
[2025-03-16 07:31:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0032581498380750418 norm:0.004073811694979668 max memory_allocated 44358.7939453125 
[2025-03-16 07:32:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.0030715144239366055 norm:0.0035019423812627792 max memory_allocated 44358.7939453125 
[2025-03-16 07:33:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0028920730110257864 norm:0.002865179907530546 max memory_allocated 44358.7939453125 
[2025-03-16 07:33:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.002874556463211775 norm:0.002493254840373993 max memory_allocated 44358.7939453125 
[2025-03-16 07:34:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.002789960475638509 norm:0.0020435629412531853 max memory_allocated 44358.7939453125 
[2025-03-16 07:35:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.00268835318274796 norm:0.0018256905023008585 max memory_allocated 44358.7939453125 
[2025-03-16 07:36:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.0025748922489583492 norm:0.0016920724883675575 max memory_allocated 44358.7939453125 
[2025-03-16 07:36:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.0026289094239473343 norm:0.0015630859415978193 max memory_allocated 44358.7939453125 
[2025-03-16 07:37:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0025102468207478523 norm:0.0015044466126710176 max memory_allocated 44358.7939453125 
[2025-03-16 07:38:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.002550604287534952 norm:0.0013970572035759687 max memory_allocated 44358.7939453125 
[2025-03-16 07:38:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.0025005959905683994 norm:0.0012865593889728189 max memory_allocated 44358.7939453125 
[2025-03-16 07:39:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0025623603723943233 norm:0.0013490616111084819 max memory_allocated 44358.7939453125 
[2025-03-16 07:40:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.002429389161989093 norm:0.0011444888077676296 max memory_allocated 44358.7939453125 
[2025-03-16 07:41:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.002466819016262889 norm:0.0011540588457137346 max memory_allocated 44358.7939453125 
[2025-03-16 07:41:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.00246571097522974 norm:0.0011418343055993319 max memory_allocated 44358.7939453125 
[2025-03-16 07:42:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.002463587326928973 norm:0.0011645155027508736 max memory_allocated 44358.7939453125 
[2025-03-16 07:43:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0024297861382365227 norm:0.000997025053948164 max memory_allocated 44358.7939453125 
[2025-03-16 07:44:13 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-16 07:44:15 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-16 07:45:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.023174772039055824 norm:0.014597122557461262 max memory_allocated 44358.7939453125 
[2025-03-16 07:45:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.01426510140299797 norm:0.008723451755940914 max memory_allocated 44358.7939453125 
[2025-03-16 07:46:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.010302052833139896 norm:0.005979773588478565 max memory_allocated 44358.7939453125 
[2025-03-16 07:47:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.009157965891063213 norm:0.00479549914598465 max memory_allocated 44358.7939453125 
[2025-03-16 07:47:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.008694926276803017 norm:0.0040064649656414986 max memory_allocated 44358.7939453125 
[2025-03-16 07:48:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.008436834439635277 norm:0.0034612405579537153 max memory_allocated 44358.7939453125 
[2025-03-16 07:49:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.008240675553679466 norm:0.0030728676356375217 max memory_allocated 44358.7939453125 
[2025-03-16 07:50:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.00813225843012333 norm:0.0026813477743417025 max memory_allocated 44358.7939453125 
[2025-03-16 07:50:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.00799476820975542 norm:0.002302328823134303 max memory_allocated 44358.7939453125 
[2025-03-16 07:51:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.007842025719583035 norm:0.001994355348870158 max memory_allocated 44358.7939453125 
[2025-03-16 07:52:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.007722079288214445 norm:0.0017214968102052808 max memory_allocated 44358.7939453125 
[2025-03-16 07:52:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.007632075343281031 norm:0.001562691992148757 max memory_allocated 44358.7939453125 
[2025-03-16 07:53:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.0076034655794501305 norm:0.001489682705141604 max memory_allocated 44358.7939453125 
[2025-03-16 07:54:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.007583592087030411 norm:0.0015626555541530252 max memory_allocated 44358.7939453125 
[2025-03-16 07:55:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.007636805064976215 norm:0.0014097276143729687 max memory_allocated 44358.7939453125 
[2025-03-16 07:55:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.007521842606365681 norm:0.0013956823386251926 max memory_allocated 44358.7939453125 
[2025-03-16 07:56:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.007569389883428812 norm:0.0012344273272901773 max memory_allocated 44358.7939453125 
[2025-03-16 07:57:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.007467172108590603 norm:0.0012915923725813627 max memory_allocated 44358.7939453125 
[2025-03-16 07:58:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.00754505442455411 norm:0.0011837572092190385 max memory_allocated 44358.7939453125 
[2025-03-16 07:58:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.007440377492457628 norm:0.0012108136434108019 max memory_allocated 44358.7939453125 
[2025-03-16 07:59:45 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
[2025-03-16 07:59:49 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-16 08:00:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.029852252453565598 norm:0.014195245690643787 max memory_allocated 44358.7939453125 
[2025-03-16 08:01:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.02051466889679432 norm:0.009242987260222435 max memory_allocated 44358.7939453125 
[2025-03-16 08:02:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.01570083387196064 norm:0.006097255274653435 max memory_allocated 44358.7939453125 
[2025-03-16 08:02:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.014247966930270195 norm:0.004842390771955252 max memory_allocated 44358.7939453125 
[2025-03-16 08:03:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.013643987476825714 norm:0.003970078658312559 max memory_allocated 44358.7939453125 
[2025-03-16 08:04:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.013236429542303085 norm:0.00343128084205091 max memory_allocated 44358.7939453125 
[2025-03-16 08:04:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.012895829044282436 norm:0.0029997588135302067 max memory_allocated 44358.7939453125 
[2025-03-16 08:05:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.012623484246432781 norm:0.002647638786584139 max memory_allocated 44358.7939453125 
[2025-03-16 08:06:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.01243017241358757 norm:0.002315588528290391 max memory_allocated 44358.7939453125 
[2025-03-16 08:07:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.012293829582631588 norm:0.0019637099467217922 max memory_allocated 44358.7939453125 
[2025-03-16 08:07:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.012182499282062054 norm:0.001689218683168292 max memory_allocated 44358.7939453125 
[2025-03-16 08:08:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.012194829061627388 norm:0.0016289004124701023 max memory_allocated 44358.7939453125 
[2025-03-16 08:09:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.012157576158642769 norm:0.0016141985543072224 max memory_allocated 44358.7939453125 
[2025-03-16 08:09:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.012149511836469173 norm:0.0015849788906052709 max memory_allocated 44358.7939453125 
[2025-03-16 08:10:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.012143515050411224 norm:0.0015529196243733168 max memory_allocated 44358.7939453125 
[2025-03-16 08:11:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.012163687497377396 norm:0.0015619844198226929 max memory_allocated 44358.7939453125 
[2025-03-16 08:12:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.01216251403093338 norm:0.0013411547988653183 max memory_allocated 44358.7939453125 
[2025-03-16 08:12:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.012109652161598206 norm:0.0013820065651088953 max memory_allocated 44358.7939453125 
[2025-03-16 08:13:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.012057051062583923 norm:0.0012276350753381848 max memory_allocated 44358.7939453125 
[2025-03-16 08:14:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.012067765928804874 norm:0.0012922545429319143 max memory_allocated 44358.7939453125 
[2025-03-16 08:15:20 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4, 5] ===
[2025-03-16 08:17:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 0 loss:0.08137516677379608 norm:0.004112974274903536 max memory_allocated 62760.0654296875 
[2025-03-16 08:19:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 1 loss:0.05505110323429108 norm:0.021297985687851906 max memory_allocated 62760.0654296875 
[2025-03-16 08:22:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 2 loss:0.04120941087603569 norm:0.00628896476700902 max memory_allocated 62760.0654296875 
[2025-03-16 08:24:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 3 loss:0.03596086800098419 norm:0.001580437645316124 max memory_allocated 62760.0654296875 
[2025-03-16 08:26:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 4 loss:0.03371936455368996 norm:0.00167197291739285 max memory_allocated 62760.0654296875 
[2025-03-16 08:28:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 5 loss:0.03230646997690201 norm:0.002002925146371126 max memory_allocated 62760.0654296875 
[2025-03-16 08:30:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 6 loss:0.03130955249071121 norm:0.002358009573072195 max memory_allocated 62760.0654296875 
[2025-03-16 08:32:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 7 loss:0.03018665686249733 norm:0.0011644965270534158 max memory_allocated 62760.0654296875 
[2025-03-16 08:34:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 8 loss:0.030063191428780556 norm:0.0020158516708761454 max memory_allocated 62760.0654296875 
[2025-03-16 08:37:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 9 loss:0.028980981558561325 norm:0.0010356539860367775 max memory_allocated 62760.0654296875 
[2025-03-16 08:39:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 10 loss:0.028297215700149536 norm:0.0010010928381234407 max memory_allocated 62760.0654296875 
[2025-03-16 08:41:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 11 loss:0.028025425970554352 norm:0.0009809761540964246 max memory_allocated 62760.0654296875 
[2025-03-16 08:43:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 12 loss:0.027884384617209435 norm:0.0008470249595120549 max memory_allocated 62760.0654296875 
[2025-03-16 08:45:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 13 loss:0.02852129563689232 norm:0.0010943915694952011 max memory_allocated 62760.0654296875 
[2025-03-16 08:47:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 14 loss:0.027844984084367752 norm:0.0010348857613280416 max memory_allocated 62760.0654296875 
[2025-03-16 08:49:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 15 loss:0.027918828651309013 norm:0.0009689056314527988 max memory_allocated 62760.0654296875 
[2025-03-16 08:52:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 16 loss:0.028153039515018463 norm:0.0011014931369572878 max memory_allocated 62760.0654296875 
[2025-03-16 08:54:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 17 loss:0.02791959047317505 norm:0.0008493795758113265 max memory_allocated 62760.0654296875 
[2025-03-16 08:56:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 18 loss:0.027986660599708557 norm:0.000960670760832727 max memory_allocated 62760.0654296875 
[2025-03-16 08:58:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 19 loss:0.027876798063516617 norm:0.0009214110905304551 max memory_allocated 62760.0654296875 
[2025-03-16 09:01:33 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [6, 7, 8] ===
[2025-03-16 09:03:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 0 loss:0.08382347971200943 norm:0.0016570196021348238 max memory_allocated 62760.0654296875 
[2025-03-16 09:05:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 1 loss:0.05951090529561043 norm:0.0006629070267081261 max memory_allocated 62760.0654296875 
[2025-03-16 09:08:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 2 loss:0.04677898436784744 norm:0.00043121108319610357 max memory_allocated 62760.0654296875 
[2025-03-16 09:10:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 3 loss:0.04110366106033325 norm:0.0003468839277047664 max memory_allocated 62760.0654296875 
[2025-03-16 09:12:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 4 loss:0.03818041458725929 norm:0.000311297073494643 max memory_allocated 62760.0654296875 
[2025-03-16 09:14:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 5 loss:0.036250896751880646 norm:0.0002818330249283463 max memory_allocated 62760.0654296875 
[2025-03-16 09:16:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 6 loss:0.03488792106509209 norm:0.00027725458494387567 max memory_allocated 62760.0654296875 
[2025-03-16 09:18:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 7 loss:0.033926691859960556 norm:0.00026049138978123665 max memory_allocated 62760.0654296875 
[2025-03-16 09:20:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 8 loss:0.03327295184135437 norm:0.00025566789554432034 max memory_allocated 62760.0654296875 
[2025-03-16 09:23:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 9 loss:0.03277302533388138 norm:0.00025120278587564826 max memory_allocated 62760.0654296875 
[2025-03-16 09:25:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 10 loss:0.03245930373668671 norm:0.0002477300586178899 max memory_allocated 62760.0654296875 
[2025-03-16 09:27:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 11 loss:0.03220818191766739 norm:0.00024861309793777764 max memory_allocated 62760.0654296875 
[2025-03-16 09:29:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 12 loss:0.032006677240133286 norm:0.00025802815798670053 max memory_allocated 62760.0654296875 
[2025-03-16 09:31:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 13 loss:0.03183867409825325 norm:0.0002541017020121217 max memory_allocated 62760.0654296875 
[2025-03-16 09:33:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 14 loss:0.03171771019697189 norm:0.00025491893757134676 max memory_allocated 62760.0654296875 
[2025-03-16 09:36:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 15 loss:0.03161177411675453 norm:0.00024343703989870846 max memory_allocated 62760.0654296875 
[2025-03-16 09:38:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 16 loss:0.031495314091444016 norm:0.0002465146535541862 max memory_allocated 62760.0654296875 
[2025-03-16 09:40:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 17 loss:0.03142261877655983 norm:0.00023917786893434823 max memory_allocated 62760.0654296875 
[2025-03-16 09:42:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 18 loss:0.031363219022750854 norm:0.0002378751232754439 max memory_allocated 62760.0654296875 
[2025-03-16 09:44:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 19 loss:0.03132147341966629 norm:0.00023630402574781328 max memory_allocated 62760.0654296875 
[2025-03-16 09:47:35 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [9, 10, 11] ===
[2025-03-16 09:49:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 0 loss:0.11832799017429352 norm:0.0035034073516726494 max memory_allocated 62760.5341796875 
[2025-03-16 09:52:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 1 loss:0.07989618182182312 norm:0.001399251283146441 max memory_allocated 62760.5341796875 
[2025-03-16 09:54:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 2 loss:0.06130297854542732 norm:0.0007489406270906329 max memory_allocated 62760.5341796875 
[2025-03-16 09:56:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 3 loss:0.052872832864522934 norm:0.0005189223447814584 max memory_allocated 62760.5341796875 
[2025-03-16 09:58:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 4 loss:0.048847272992134094 norm:0.0004293799283914268 max memory_allocated 62760.5341796875 
[2025-03-16 10:00:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 5 loss:0.04644392058253288 norm:0.00038287509232759476 max memory_allocated 62760.5341796875 
[2025-03-16 10:02:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 6 loss:0.044747572392225266 norm:0.00035498448414728045 max memory_allocated 62760.5341796875 
[2025-03-16 10:04:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 7 loss:0.043525829911231995 norm:0.00034886691719293594 max memory_allocated 62760.5341796875 
[2025-03-16 10:07:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 8 loss:0.042629994451999664 norm:0.0003241214726585895 max memory_allocated 62760.5341796875 
[2025-03-16 10:09:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 9 loss:0.041965603828430176 norm:0.0003049198421649635 max memory_allocated 62760.5341796875 
[2025-03-16 10:11:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 10 loss:0.041503939777612686 norm:0.0002945205196738243 max memory_allocated 62760.5341796875 
[2025-03-16 10:13:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 11 loss:0.04111913591623306 norm:0.0002974159433506429 max memory_allocated 62760.5341796875 
[2025-03-16 10:15:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 12 loss:0.04082070663571358 norm:0.0002867175790015608 max memory_allocated 62760.5341796875 
[2025-03-16 10:17:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 13 loss:0.04064612463116646 norm:0.00027853078790940344 max memory_allocated 62760.5341796875 
[2025-03-16 10:19:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 14 loss:0.04043808579444885 norm:0.00027735810726881027 max memory_allocated 62760.5341796875 
[2025-03-16 10:22:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 15 loss:0.040290717035532 norm:0.0002645116182975471 max memory_allocated 62760.5341796875 
[2025-03-16 10:24:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 16 loss:0.04018578678369522 norm:0.00026693238760344684 max memory_allocated 62760.5341796875 
[2025-03-16 10:26:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 17 loss:0.04005143791437149 norm:0.00026919448282569647 max memory_allocated 62760.5341796875 
[2025-03-16 10:28:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 18 loss:0.039924293756484985 norm:0.00026042424724437296 max memory_allocated 62760.5341796875 
[2025-03-16 10:30:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 19 loss:0.03987152874469757 norm:0.0002677694137673825 max memory_allocated 62760.5341796875 
[2025-03-16 10:33:35 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [12, 13, 14] ===
[2025-03-16 10:35:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 0 loss:0.10433775186538696 norm:0.0017754783621057868 max memory_allocated 62760.5341796875 
[2025-03-16 10:38:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 1 loss:0.07809969782829285 norm:0.000845983624458313 max memory_allocated 62760.5341796875 
[2025-03-16 10:40:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 2 loss:0.06283873319625854 norm:0.000523335940670222 max memory_allocated 62760.5341796875 
[2025-03-16 10:42:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 3 loss:0.0555260069668293 norm:0.0003793902578763664 max memory_allocated 62760.5341796875 
[2025-03-16 10:44:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 4 loss:0.05191972851753235 norm:0.0003125601797364652 max memory_allocated 62760.5341796875 
[2025-03-16 10:46:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 5 loss:0.04973328858613968 norm:0.0002811956510413438 max memory_allocated 62760.5341796875 
[2025-03-16 10:48:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 6 loss:0.04825209081172943 norm:0.00026146273012273014 max memory_allocated 62760.5341796875 
[2025-03-16 10:50:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 7 loss:0.04723310470581055 norm:0.00024781597312539816 max memory_allocated 62760.5341796875 
[2025-03-16 10:53:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 8 loss:0.04646945744752884 norm:0.00023593346122652292 max memory_allocated 62760.5341796875 
[2025-03-16 10:55:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 9 loss:0.04599028825759888 norm:0.0002289803815074265 max memory_allocated 62760.5341796875 
[2025-03-16 10:57:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 10 loss:0.04562173783779144 norm:0.00022301213175524026 max memory_allocated 62760.5341796875 
[2025-03-16 10:59:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 11 loss:0.04530762508511543 norm:0.00021657210891135037 max memory_allocated 62760.5341796875 
[2025-03-16 11:01:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 12 loss:0.04508108273148537 norm:0.00021174649009481072 max memory_allocated 62760.5341796875 
[2025-03-16 11:03:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 13 loss:0.04486006870865822 norm:0.0002029094466706738 max memory_allocated 62760.5341796875 
[2025-03-16 11:05:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 14 loss:0.04468616470694542 norm:0.0002005907881539315 max memory_allocated 62760.5341796875 
[2025-03-16 11:08:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 15 loss:0.04454479366540909 norm:0.00019506693934090436 max memory_allocated 62760.5341796875 
[2025-03-16 11:10:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 16 loss:0.04445094242691994 norm:0.00018621489289216697 max memory_allocated 62760.5341796875 
[2025-03-16 11:12:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 17 loss:0.04434191808104515 norm:0.00018871427164413035 max memory_allocated 62760.5341796875 
[2025-03-16 11:14:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 18 loss:0.044292204082012177 norm:0.00018764850392472 max memory_allocated 62760.5341796875 
[2025-03-16 11:16:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 19 loss:0.04422619938850403 norm:0.0001875588350230828 max memory_allocated 62760.5341796875 
[2025-03-16 11:19:37 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [15, 16, 17] ===
[2025-03-16 11:21:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 0 loss:0.0938825011253357 norm:0.0013293788069859147 max memory_allocated 62760.5341796875 
[2025-03-16 11:24:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 1 loss:0.07485038787126541 norm:0.000606901419814676 max memory_allocated 62760.5341796875 
[2025-03-16 11:26:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 2 loss:0.06167130172252655 norm:0.00038377358578145504 max memory_allocated 62760.5341796875 
[2025-03-16 11:28:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 3 loss:0.055749427527189255 norm:0.0002875790523830801 max memory_allocated 62760.5341796875 
[2025-03-16 11:30:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 4 loss:0.0527983084321022 norm:0.00024936802219599485 max memory_allocated 62760.5341796875 
[2025-03-16 11:32:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 5 loss:0.05084114521741867 norm:0.0002267491800012067 max memory_allocated 62760.5341796875 
[2025-03-16 11:34:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 6 loss:0.04950125515460968 norm:0.00021220477356109768 max memory_allocated 62760.5341796875 
[2025-03-16 11:36:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 7 loss:0.04852961376309395 norm:0.00020357573521323502 max memory_allocated 62760.5341796875 
[2025-03-16 11:39:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 8 loss:0.04788362979888916 norm:0.00019639349193312228 max memory_allocated 62760.5341796875 
[2025-03-16 11:41:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 9 loss:0.047407396137714386 norm:0.00019100021745543927 max memory_allocated 62760.5341796875 
[2025-03-16 11:43:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 10 loss:0.047043249011039734 norm:0.00018507050117477775 max memory_allocated 62760.5341796875 
[2025-03-16 11:45:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 11 loss:0.04676354303956032 norm:0.00018035306129604578 max memory_allocated 62760.5341796875 
[2025-03-16 11:47:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 12 loss:0.04654155299067497 norm:0.0001787262735888362 max memory_allocated 62760.5341796875 
[2025-03-16 11:49:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 13 loss:0.046328987926244736 norm:0.00017317532910965383 max memory_allocated 62760.5341796875 
[2025-03-16 11:51:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 14 loss:0.046172622591257095 norm:0.00016790298104751855 max memory_allocated 62760.5341796875 
[2025-03-16 11:54:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 15 loss:0.0460580512881279 norm:0.0001652570499572903 max memory_allocated 62760.5341796875 
[2025-03-16 11:56:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 16 loss:0.04595460370182991 norm:0.00016054655134212226 max memory_allocated 62760.5341796875 
[2025-03-16 11:58:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 17 loss:0.04585892707109451 norm:0.0001605212310096249 max memory_allocated 62760.5341796875 
[2025-03-16 12:00:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 18 loss:0.045777514576911926 norm:0.00015747912402730435 max memory_allocated 62760.5341796875 
[2025-03-16 12:02:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 19 loss:0.045707203447818756 norm:0.00015524473565164953 max memory_allocated 62760.5341796875 
[2025-03-16 12:05:38 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [18, 19, 20] ===
[2025-03-16 12:07:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 0 loss:0.08902721107006073 norm:0.0008713505230844021 max memory_allocated 62761.2373046875 
[2025-03-16 12:10:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 1 loss:0.07408834993839264 norm:0.0004068975686095655 max memory_allocated 62761.2373046875 
[2025-03-16 12:12:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 2 loss:0.06258399784564972 norm:0.00027760874945670366 max memory_allocated 62761.2373046875 
[2025-03-16 12:14:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 3 loss:0.05844256281852722 norm:0.00022560660727322102 max memory_allocated 62761.2373046875 
[2025-03-16 12:16:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 4 loss:0.05600111559033394 norm:0.00019775453256443143 max memory_allocated 62761.2373046875 
[2025-03-16 12:18:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 5 loss:0.054215434938669205 norm:0.00018309996812604368 max memory_allocated 62761.2373046875 
[2025-03-16 12:20:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 6 loss:0.05291258916258812 norm:0.0001719603023957461 max memory_allocated 62761.2373046875 
[2025-03-16 12:22:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 7 loss:0.05210043117403984 norm:0.0001638812682358548 max memory_allocated 62761.2373046875 
[2025-03-16 12:25:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 8 loss:0.05155256763100624 norm:0.00015603868814650923 max memory_allocated 62761.2373046875 
[2025-03-16 12:27:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 9 loss:0.05117763951420784 norm:0.0001504182437201962 max memory_allocated 62761.2373046875 
[2025-03-16 12:29:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 10 loss:0.05090733617544174 norm:0.00014357642794493586 max memory_allocated 62762.2373046875 
[2025-03-16 12:31:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 11 loss:0.050702109932899475 norm:0.000140103860758245 max memory_allocated 62762.2373046875 
[2025-03-16 12:33:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 12 loss:0.050529301166534424 norm:0.00013643497368320823 max memory_allocated 62762.2373046875 
[2025-03-16 12:35:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 13 loss:0.05041218176484108 norm:0.00013365058111958206 max memory_allocated 62762.2373046875 
[2025-03-16 12:37:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 14 loss:0.050273824483156204 norm:0.00013217516243457794 max memory_allocated 62762.2373046875 
[2025-03-16 12:40:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 15 loss:0.05015259608626366 norm:0.0001301317533943802 max memory_allocated 62762.2373046875 
[2025-03-16 12:42:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 16 loss:0.05007846653461456 norm:0.0001268764608539641 max memory_allocated 62762.2373046875 
[2025-03-16 12:44:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 17 loss:0.0500066913664341 norm:0.00012625542876776308 max memory_allocated 62762.2373046875 
[2025-03-16 12:46:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 18 loss:0.049913693219423294 norm:0.00012373521167319268 max memory_allocated 62762.2373046875 
[2025-03-16 12:48:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 19 loss:0.04983018338680267 norm:0.00012070990487700328 max memory_allocated 62762.2373046875 
[2025-03-16 12:51:40 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [21, 22, 23] ===
[2025-03-16 12:53:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 0 loss:0.10843033343553543 norm:0.0008773640729486942 max memory_allocated 62762.2373046875 
[2025-03-16 12:56:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 1 loss:0.09144233912229538 norm:0.00045711052371189 max memory_allocated 62762.2373046875 
[2025-03-16 12:58:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 2 loss:0.0770845115184784 norm:0.0003027745697181672 max memory_allocated 62762.2373046875 
[2025-03-16 13:00:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 3 loss:0.07261974364519119 norm:0.0002507423341739923 max memory_allocated 62762.2373046875 
[2025-03-16 13:02:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 4 loss:0.06992600858211517 norm:0.0002285970258526504 max memory_allocated 62762.2373046875 
[2025-03-16 13:04:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 5 loss:0.06779511272907257 norm:0.00022104839445091784 max memory_allocated 62762.2373046875 
[2025-03-16 13:06:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 6 loss:0.06642496585845947 norm:0.00021227561228442937 max memory_allocated 62762.2373046875 
[2025-03-16 13:08:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 7 loss:0.06558003276586533 norm:0.00021026376634836197 max memory_allocated 62762.2373046875 
[2025-03-16 13:11:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 8 loss:0.06503926217556 norm:0.00020307546947151423 max memory_allocated 62762.2373046875 
[2025-03-16 13:13:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 9 loss:0.06468462944030762 norm:0.00020119157852604985 max memory_allocated 62762.2373046875 
[2025-03-16 13:15:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 10 loss:0.06435465812683105 norm:0.00020089259487576783 max memory_allocated 62762.2373046875 
[2025-03-16 13:17:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 11 loss:0.06411217153072357 norm:0.0002096985699608922 max memory_allocated 62762.2373046875 
[2025-03-16 13:19:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 12 loss:0.0638638511300087 norm:0.00018107898358721286 max memory_allocated 62762.2373046875 
[2025-03-16 13:21:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 13 loss:0.0636645182967186 norm:0.00016131317534018308 max memory_allocated 62762.2373046875 
[2025-03-16 13:23:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 14 loss:0.06348283588886261 norm:0.00017055936041288078 max memory_allocated 62762.2373046875 
[2025-03-16 13:26:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 15 loss:0.0633208379149437 norm:0.00016461929772049189 max memory_allocated 62762.2373046875 
[2025-03-16 13:28:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 16 loss:0.06317633390426636 norm:0.00015186145901679993 max memory_allocated 62762.2373046875 
[2025-03-16 13:30:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 17 loss:0.06305062025785446 norm:0.00015544469351880252 max memory_allocated 62762.2373046875 
[2025-03-16 13:32:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 18 loss:0.0629274994134903 norm:0.00015082914615049958 max memory_allocated 62762.2373046875 
[2025-03-16 13:34:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 19 loss:0.06282114237546921 norm:0.00014550266496371478 max memory_allocated 62762.2373046875 
[2025-03-16 13:37:40 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [24, 25, 26] ===
[2025-03-16 13:39:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 0 loss:0.1294272541999817 norm:0.0008205755730159581 max memory_allocated 62762.2373046875 
[2025-03-16 13:42:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 1 loss:0.11189468950033188 norm:0.0004087577108293772 max memory_allocated 62762.2373046875 
[2025-03-16 13:44:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 2 loss:0.0956626608967781 norm:0.00027476472314447165 max memory_allocated 62762.2373046875 
[2025-03-16 13:46:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 3 loss:0.091397225856781 norm:0.00024238781770691276 max memory_allocated 62762.2373046875 
[2025-03-16 13:48:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 4 loss:0.0884070098400116 norm:0.00021778029622510076 max memory_allocated 62762.2373046875 
[2025-03-16 13:50:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 5 loss:0.08604966849088669 norm:0.00020786536333616823 max memory_allocated 62762.2373046875 
[2025-03-16 13:52:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 6 loss:0.0847475454211235 norm:0.0001987981959246099 max memory_allocated 62762.2373046875 
[2025-03-16 13:54:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 7 loss:0.08402270078659058 norm:0.0001927644043462351 max memory_allocated 62762.2373046875 
[2025-03-16 13:57:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 8 loss:0.08357107639312744 norm:0.00018084226758219302 max memory_allocated 62762.2373046875 
[2025-03-16 13:59:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 9 loss:0.08318215608596802 norm:0.00017607564222998917 max memory_allocated 62762.2373046875 
[2025-03-16 14:01:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 10 loss:0.08286529779434204 norm:0.00017450589803047478 max memory_allocated 62762.2373046875 
[2025-03-16 14:03:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 11 loss:0.08256158232688904 norm:0.00016708532348275185 max memory_allocated 62762.2373046875 
[2025-03-16 14:05:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 12 loss:0.082326740026474 norm:0.00016412726836279035 max memory_allocated 62762.2373046875 
[2025-03-16 14:07:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 13 loss:0.08213487267494202 norm:0.00016409462841693312 max memory_allocated 62762.2373046875 
[2025-03-16 14:09:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 14 loss:0.08192352950572968 norm:0.0001607906015124172 max memory_allocated 62762.2373046875 
[2025-03-16 14:12:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 15 loss:0.08172767609357834 norm:0.00015531785902567208 max memory_allocated 62762.2373046875 
[2025-03-16 14:14:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 16 loss:0.08157649636268616 norm:0.00015692543820478022 max memory_allocated 62762.2373046875 
[2025-03-16 14:16:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 17 loss:0.08142058551311493 norm:0.00015334888303186744 max memory_allocated 62762.2373046875 
[2025-03-16 14:18:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 18 loss:0.0812925398349762 norm:0.00014952305355109274 max memory_allocated 62762.2373046875 
[2025-03-16 14:20:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 19 loss:0.0811895877122879 norm:0.0001549113803775981 max memory_allocated 62762.2373046875 
[2025-03-16 14:23:35 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [27, 28, 29] ===
[2025-03-16 14:25:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 0 loss:0.15983819961547852 norm:0.0005705727962777019 max memory_allocated 62762.2373046875 
[2025-03-16 14:28:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 1 loss:0.14078140258789062 norm:0.0003777424863073975 max memory_allocated 62762.2373046875 
[2025-03-16 14:30:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 2 loss:0.12214697897434235 norm:0.0002788317506201565 max memory_allocated 62762.2373046875 
[2025-03-16 14:32:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 3 loss:0.11722481995820999 norm:0.0002447213919367641 max memory_allocated 62762.2373046875 
[2025-03-16 14:34:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 4 loss:0.11369621753692627 norm:0.00022220908431336284 max memory_allocated 62762.2373046875 
[2025-03-16 14:36:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 5 loss:0.11132882535457611 norm:0.00021511684462893754 max memory_allocated 62762.2373046875 
[2025-03-16 14:38:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 6 loss:0.11023504287004471 norm:0.00021002537687309086 max memory_allocated 62762.2373046875 
[2025-03-16 14:40:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 7 loss:0.10958559811115265 norm:0.00019491888815537095 max memory_allocated 62762.2373046875 
[2025-03-16 14:43:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 8 loss:0.10909494012594223 norm:0.0001875908492365852 max memory_allocated 62762.2373046875 
[2025-03-16 14:45:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 9 loss:0.10866546630859375 norm:0.00018875126261264086 max memory_allocated 62762.2373046875 
[2025-03-16 14:47:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 10 loss:0.10832566767930984 norm:0.0001884974626591429 max memory_allocated 62762.2373046875 
[2025-03-16 14:49:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 11 loss:0.10799868404865265 norm:0.00018424839072395116 max memory_allocated 62762.2373046875 
[2025-03-16 14:51:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 12 loss:0.10773113369941711 norm:0.0001883811637526378 max memory_allocated 62762.2373046875 
[2025-03-16 14:53:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 13 loss:0.10747608542442322 norm:0.00018802465638145804 max memory_allocated 62762.2373046875 
[2025-03-16 14:55:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 14 loss:0.107252337038517 norm:0.0001830827386584133 max memory_allocated 62762.2373046875 
[2025-03-16 14:58:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 15 loss:0.10703948140144348 norm:0.000186096818652004 max memory_allocated 62762.2373046875 
[2025-03-16 15:00:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 16 loss:0.10682431608438492 norm:0.00016639367095194757 max memory_allocated 62762.2373046875 
[2025-03-16 15:02:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 17 loss:0.10664011538028717 norm:0.00017339864280074835 max memory_allocated 62762.2373046875 
[2025-03-16 15:04:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 18 loss:0.10649710148572922 norm:0.00014972250210121274 max memory_allocated 62762.2373046875 
[2025-03-16 15:06:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 19 loss:0.10638782382011414 norm:0.00015035559772513807 max memory_allocated 62762.2373046875 
[2025-03-16 15:09:34 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [30, 31, 32] ===
[2025-03-16 15:11:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 0 loss:0.20934613049030304 norm:0.0007622141856700182 max memory_allocated 62765.1748046875 
[2025-03-16 15:14:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 1 loss:0.1841055154800415 norm:0.00043250632006675005 max memory_allocated 62765.1748046875 
[2025-03-16 15:16:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 2 loss:0.15957006812095642 norm:0.0002628735965117812 max memory_allocated 62765.1748046875 
[2025-03-16 15:18:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 3 loss:0.15277782082557678 norm:0.00022311676002573222 max memory_allocated 62765.1748046875 
[2025-03-16 15:20:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 4 loss:0.148530513048172 norm:0.0002079698460875079 max memory_allocated 62765.1748046875 
[2025-03-16 15:22:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 5 loss:0.14651983976364136 norm:0.0001932802697410807 max memory_allocated 62765.1748046875 
[2025-03-16 15:24:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 6 loss:0.14565637707710266 norm:0.00018597945745568722 max memory_allocated 62765.1748046875 
[2025-03-16 15:26:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 7 loss:0.14499975740909576 norm:0.00017748291429597884 max memory_allocated 62765.1748046875 
[2025-03-16 15:29:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 8 loss:0.14446493983268738 norm:0.0001704929891275242 max memory_allocated 62765.1748046875 
[2025-03-16 15:31:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 9 loss:0.14401592314243317 norm:0.00016722574946470559 max memory_allocated 62765.1748046875 
[2025-03-16 15:33:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 10 loss:0.14359495043754578 norm:0.00016187030996661633 max memory_allocated 62765.1748046875 
[2025-03-16 15:35:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 11 loss:0.14323902130126953 norm:0.00015659394557587802 max memory_allocated 62765.1748046875 
[2025-03-16 15:37:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 12 loss:0.14291955530643463 norm:0.00015104351041372865 max memory_allocated 62765.1748046875 
[2025-03-16 15:39:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 13 loss:0.14263181388378143 norm:0.00014800243661738932 max memory_allocated 62765.1748046875 
[2025-03-16 15:41:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 14 loss:0.14238578081130981 norm:0.00014473489136435091 max memory_allocated 62765.1748046875 
[2025-03-16 15:44:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 15 loss:0.14215704798698425 norm:0.0001424159127054736 max memory_allocated 62765.1748046875 
[2025-03-16 15:46:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 16 loss:0.14193512499332428 norm:0.00014239642769098282 max memory_allocated 62765.1748046875 
[2025-03-16 15:48:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 17 loss:0.14174365997314453 norm:0.00014234553964342922 max memory_allocated 62765.1748046875 
[2025-03-16 15:50:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 18 loss:0.14156819880008698 norm:0.00014289814862422645 max memory_allocated 62765.1748046875 
[2025-03-16 15:52:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 19 loss:0.14140526950359344 norm:0.0001414288126397878 max memory_allocated 62765.1748046875 
[2025-03-16 15:55:34 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [33, 34, 35] ===
[2025-03-16 15:57:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 0 loss:0.2700704336166382 norm:0.0006930884555913508 max memory_allocated 62765.1748046875 
[2025-03-16 16:00:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 1 loss:0.23909562826156616 norm:0.0004266324685886502 max memory_allocated 62765.1748046875 
[2025-03-16 16:02:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 2 loss:0.21067725121974945 norm:0.00028765614842996 max memory_allocated 62765.1748046875 
[2025-03-16 16:04:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 3 loss:0.20220127701759338 norm:0.00024949180078692734 max memory_allocated 62765.1748046875 
[2025-03-16 16:06:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 4 loss:0.1976248025894165 norm:0.00023076657089404762 max memory_allocated 62765.1748046875 
[2025-03-16 16:08:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 5 loss:0.19577327370643616 norm:0.00021603534696623683 max memory_allocated 62765.1748046875 
[2025-03-16 16:10:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 6 loss:0.19478166103363037 norm:0.00020661752205342054 max memory_allocated 62765.1748046875 
[2025-03-16 16:12:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 7 loss:0.19395282864570618 norm:0.00019863058696500957 max memory_allocated 62765.1748046875 
[2025-03-16 16:15:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 8 loss:0.1932336837053299 norm:0.00019139779033139348 max memory_allocated 62765.1748046875 
[2025-03-16 16:17:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 9 loss:0.19261951744556427 norm:0.0001860859483713284 max memory_allocated 62765.1748046875 
[2025-03-16 16:19:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 10 loss:0.19205695390701294 norm:0.00018157099839299917 max memory_allocated 62765.1748046875 
[2025-03-16 16:21:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 11 loss:0.1915523111820221 norm:0.00017529759497847408 max memory_allocated 62765.1748046875 
[2025-03-16 16:23:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 12 loss:0.19110418856143951 norm:0.00017249774828087538 max memory_allocated 62765.1748046875 
[2025-03-16 16:25:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 13 loss:0.19071336090564728 norm:0.00016674220387358218 max memory_allocated 62765.1748046875 
[2025-03-16 16:27:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 14 loss:0.19035956263542175 norm:0.00016610941383987665 max memory_allocated 62765.1748046875 
[2025-03-16 16:30:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 15 loss:0.19001442193984985 norm:0.00016255959053523839 max memory_allocated 62765.1748046875 
[2025-03-16 16:32:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 16 loss:0.18973016738891602 norm:0.00016192527255043387 max memory_allocated 62765.1748046875 
[2025-03-16 16:34:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 17 loss:0.1894768327474594 norm:0.00015980619355104864 max memory_allocated 62765.1748046875 
[2025-03-16 16:36:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 18 loss:0.18926101922988892 norm:0.00015603900828864425 max memory_allocated 62765.1748046875 
[2025-03-16 16:38:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 19 loss:0.1890481859445572 norm:0.0001562842953717336 max memory_allocated 62765.1748046875 
[2025-03-16 16:41:34 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [36] ===
[2025-03-16 16:41:34 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-16 16:42:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 0 loss:0.23293252289295197 norm:0.009021265432238579 max memory_allocated 62765.1748046875 
[2025-03-16 16:43:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 1 loss:0.21895143389701843 norm:0.006761455908417702 max memory_allocated 62765.1748046875 
[2025-03-16 16:43:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 2 loss:0.20746150612831116 norm:0.004800348076969385 max memory_allocated 62765.1748046875 
[2025-03-16 16:44:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 3 loss:0.20380057394504547 norm:0.003911237698048353 max memory_allocated 62765.1748046875 
[2025-03-16 16:45:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 4 loss:0.2022765427827835 norm:0.003216202836483717 max memory_allocated 62765.1748046875 
[2025-03-16 16:45:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 5 loss:0.20161980390548706 norm:0.0026462639216333628 max memory_allocated 62765.1748046875 
[2025-03-16 16:46:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 6 loss:0.2011280059814453 norm:0.002257442567497492 max memory_allocated 62765.1748046875 
[2025-03-16 16:47:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 7 loss:0.2007790505886078 norm:0.002137805102393031 max memory_allocated 62765.1748046875 
[2025-03-16 16:48:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 8 loss:0.20050163567066193 norm:0.002126969862729311 max memory_allocated 62765.1748046875 
[2025-03-16 16:48:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 9 loss:0.20021827518939972 norm:0.0019588603172451258 max memory_allocated 62765.1748046875 
[2025-03-16 16:49:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 10 loss:0.19998830556869507 norm:0.0019143236568197608 max memory_allocated 62765.1748046875 
[2025-03-16 16:50:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 11 loss:0.19973506033420563 norm:0.0017372058937326074 max memory_allocated 62765.1748046875 
[2025-03-16 16:51:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 12 loss:0.19952324032783508 norm:0.001637644600123167 max memory_allocated 62765.1748046875 
[2025-03-16 16:51:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 13 loss:0.1994239240884781 norm:0.001692894846200943 max memory_allocated 62765.1748046875 
[2025-03-16 16:52:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 14 loss:0.1992669254541397 norm:0.0017281874315813184 max memory_allocated 62765.1748046875 
[2025-03-16 16:53:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 15 loss:0.19910180568695068 norm:0.0016120397485792637 max memory_allocated 62765.1748046875 
[2025-03-16 16:53:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 16 loss:0.19895172119140625 norm:0.0015631517162546515 max memory_allocated 62765.1748046875 
[2025-03-16 16:54:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 17 loss:0.1988433599472046 norm:0.0015041010919958353 max memory_allocated 62765.1748046875 
[2025-03-16 16:55:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 18 loss:0.1987420916557312 norm:0.0014890236780047417 max memory_allocated 62765.1748046875 
[2025-03-16 16:56:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 19 loss:0.19866511225700378 norm:0.0015078387223184109 max memory_allocated 62765.1748046875 
[2025-03-16 16:57:04 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [37] ===
[2025-03-16 16:57:04 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-16 16:57:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 0 loss:0.2580084800720215 norm:0.009170254692435265 max memory_allocated 62765.1748046875 
[2025-03-16 16:58:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 1 loss:0.24335555732250214 norm:0.007361755706369877 max memory_allocated 62765.1748046875 
[2025-03-16 16:59:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 2 loss:0.2307528406381607 norm:0.005122989881783724 max memory_allocated 62765.1748046875 
[2025-03-16 17:00:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 3 loss:0.226744145154953 norm:0.004202383104711771 max memory_allocated 62765.1748046875 
[2025-03-16 17:00:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 4 loss:0.2252262681722641 norm:0.003507409244775772 max memory_allocated 62765.1748046875 
[2025-03-16 17:01:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 5 loss:0.22456853091716766 norm:0.002939923433586955 max memory_allocated 62765.1748046875 
[2025-03-16 17:02:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 6 loss:0.2240903675556183 norm:0.0024930189829319715 max memory_allocated 62765.1748046875 
[2025-03-16 17:02:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 7 loss:0.22369442880153656 norm:0.002269313670694828 max memory_allocated 62765.1748046875 
[2025-03-16 17:03:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 8 loss:0.22342132031917572 norm:0.0022652053739875555 max memory_allocated 62765.1748046875 
[2025-03-16 17:04:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 9 loss:0.22317038476467133 norm:0.0021983396727591753 max memory_allocated 62765.1748046875 
[2025-03-16 17:05:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 10 loss:0.22293201088905334 norm:0.002133514964953065 max memory_allocated 62765.1748046875 
[2025-03-16 17:05:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 11 loss:0.22277431190013885 norm:0.002150642918422818 max memory_allocated 62765.1748046875 
[2025-03-16 17:06:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 12 loss:0.22260701656341553 norm:0.0020290121901780367 max memory_allocated 62765.1748046875 
[2025-03-16 17:07:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 13 loss:0.22246283292770386 norm:0.0019742469303309917 max memory_allocated 62765.1748046875 
[2025-03-16 17:07:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 14 loss:0.22236062586307526 norm:0.0019488777033984661 max memory_allocated 62765.1748046875 
[2025-03-16 17:08:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 15 loss:0.22220803797245026 norm:0.001879481365904212 max memory_allocated 62765.1748046875 
[2025-03-16 17:09:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 16 loss:0.22205530107021332 norm:0.0017891956958919764 max memory_allocated 62765.1748046875 
[2025-03-16 17:10:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 17 loss:0.22199051082134247 norm:0.001874554785899818 max memory_allocated 62765.1748046875 
[2025-03-16 17:10:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 18 loss:0.22189131379127502 norm:0.0018189294496551156 max memory_allocated 62765.1748046875 
[2025-03-16 17:11:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 19 loss:0.22173316776752472 norm:0.001664987183175981 max memory_allocated 62765.1748046875 
[2025-03-16 17:12:33 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 16 with layers [38] ===
[2025-03-16 17:12:33 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-16 17:13:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 0 loss:0.3023860454559326 norm:0.015070604160428047 max memory_allocated 62765.1748046875 
[2025-03-16 17:14:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 1 loss:0.28296637535095215 norm:0.010513965040445328 max memory_allocated 62765.1748046875 
[2025-03-16 17:14:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 2 loss:0.2680397033691406 norm:0.007159772794693708 max memory_allocated 62765.1748046875 
[2025-03-16 17:15:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 3 loss:0.2635680139064789 norm:0.00602363795042038 max memory_allocated 62765.1748046875 
[2025-03-16 17:16:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 4 loss:0.26182955503463745 norm:0.005194059107452631 max memory_allocated 62765.1748046875 
[2025-03-16 17:16:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 5 loss:0.2608935236930847 norm:0.004529749974608421 max memory_allocated 62765.1748046875 
[2025-03-16 17:17:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 6 loss:0.26030251383781433 norm:0.004107234999537468 max memory_allocated 62765.1748046875 
[2025-03-16 17:18:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 7 loss:0.25982382893562317 norm:0.0037493526469916105 max memory_allocated 62765.1748046875 
[2025-03-16 17:19:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 8 loss:0.2594878673553467 norm:0.003582569770514965 max memory_allocated 62765.1748046875 
[2025-03-16 17:19:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 9 loss:0.259313702583313 norm:0.0037313629873096943 max memory_allocated 62765.1748046875 
[2025-03-16 17:20:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 10 loss:0.2591150999069214 norm:0.0036220315378159285 max memory_allocated 62765.1748046875 
[2025-03-16 17:21:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 11 loss:0.2588058114051819 norm:0.003319940296933055 max memory_allocated 62765.1748046875 
[2025-03-16 17:22:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 12 loss:0.25872039794921875 norm:0.0034312757197767496 max memory_allocated 62765.1748046875 
[2025-03-16 17:22:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 13 loss:0.25880366563796997 norm:0.0035841597709804773 max memory_allocated 62765.1748046875 
[2025-03-16 17:23:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 14 loss:0.25874194502830505 norm:0.0032863866072148085 max memory_allocated 62765.1748046875 
[2025-03-16 17:24:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 15 loss:0.2583300471305847 norm:0.003142950590699911 max memory_allocated 62765.1748046875 
[2025-03-16 17:24:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 16 loss:0.25816330313682556 norm:0.0030365826096385717 max memory_allocated 62765.1748046875 
[2025-03-16 17:25:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 17 loss:0.25800544023513794 norm:0.0027715207543224096 max memory_allocated 62765.1748046875 
[2025-03-16 17:26:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 18 loss:0.2579122483730316 norm:0.002845336217433214 max memory_allocated 62765.1748046875 
[2025-03-16 17:27:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 19 loss:0.25783106684684753 norm:0.002743263728916645 max memory_allocated 62765.1748046875 
[2025-03-16 17:28:03 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 17 with layers [39] ===
[2025-03-16 17:28:03 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-16 17:28:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 0 loss:0.4337228536605835 norm:0.04293124005198479 max memory_allocated 62765.1748046875 
[2025-03-16 17:29:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 1 loss:0.3877596855163574 norm:0.027377646416425705 max memory_allocated 62765.1748046875 
[2025-03-16 17:30:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 2 loss:0.35579949617385864 norm:0.018140112981200218 max memory_allocated 62765.1748046875 
[2025-03-16 17:30:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 3 loss:0.3468247354030609 norm:0.014641344547271729 max memory_allocated 62765.1748046875 
[2025-03-16 17:31:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 4 loss:0.3431919813156128 norm:0.012848800048232079 max memory_allocated 62765.1748046875 
[2025-03-16 17:32:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 5 loss:0.34059667587280273 norm:0.011763432994484901 max memory_allocated 62765.1748046875 
[2025-03-16 17:33:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 6 loss:0.33864885568618774 norm:0.010484648868441582 max memory_allocated 62765.1748046875 
[2025-03-16 17:33:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 7 loss:0.33725401759147644 norm:0.00939885713160038 max memory_allocated 62765.1748046875 
[2025-03-16 17:34:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 8 loss:0.3359939455986023 norm:0.008861425332725048 max memory_allocated 62765.1748046875 
[2025-03-16 17:35:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 9 loss:0.33510488271713257 norm:0.008151126094162464 max memory_allocated 62765.1748046875 
[2025-03-16 17:36:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 10 loss:0.3343203365802765 norm:0.00792510062456131 max memory_allocated 62765.1748046875 
[2025-03-16 17:36:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 11 loss:0.33353671431541443 norm:0.007329073268920183 max memory_allocated 62765.1748046875 
[2025-03-16 17:37:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 12 loss:0.3327910304069519 norm:0.006695338990539312 max memory_allocated 62765.1748046875 
[2025-03-16 17:38:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 13 loss:0.3325710892677307 norm:0.00690507423132658 max memory_allocated 62765.1748046875 
[2025-03-16 17:38:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 14 loss:0.33226603269577026 norm:0.007223221473395824 max memory_allocated 62765.1748046875 
[2025-03-16 17:39:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 15 loss:0.33179226517677307 norm:0.006679660640656948 max memory_allocated 62765.1748046875 
[2025-03-16 17:40:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 16 loss:0.3310709297657013 norm:0.00612089317291975 max memory_allocated 62765.1748046875 
[2025-03-16 17:41:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 17 loss:0.33101680874824524 norm:0.00648144306614995 max memory_allocated 62765.1748046875 
[2025-03-16 17:41:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 18 loss:0.3308221101760864 norm:0.006167788058519363 max memory_allocated 62765.1748046875 
[2025-03-16 17:42:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 19 loss:0.3305920362472534 norm:0.006256584078073502 max memory_allocated 62765.1748046875 
[2025-03-16 17:43:34 root] (main_calib_config3_attn.py 379): INFO 36898.94287276268
[2025-03-16 17:43:44 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
