[2025-03-22 14:11:05 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/Llama-2-7b-hf-0.9', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.9.pkl', blocks_pkl='./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-22 14:11:27 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-22 14:11:27 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-22 14:11:27 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-22 14:11:27 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.9.pkl
[2025-03-22 14:11:27 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 9), (9, 11), (11, 14), (14, 17), (17, 20), (20, 23), (23, 26), (26, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-22 14:11:27 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12, 13], [14, 15, 16], [17, 18, 19], [20, 21, 22], [23, 24, 25], [26, 27, 28], [29], [30], [31]]
[2025-03-22 14:11:30 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-22 14:11:30 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:12:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.0013693541986867785 norm:0.0054475534707307816 max memory_allocated 34633.880859375 
[2025-03-22 14:12:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.00045549171045422554 norm:0.0022856032010167837 max memory_allocated 34633.880859375 
[2025-03-22 14:12:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.00037842465098947287 norm:0.001798146404325962 max memory_allocated 34633.880859375 
[2025-03-22 14:13:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.00034070340916514397 norm:0.0015376147348433733 max memory_allocated 34633.880859375 
[2025-03-22 14:13:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.00032643062877468765 norm:0.0013054257724434137 max memory_allocated 34633.880859375 
[2025-03-22 14:14:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.00031425993074662983 norm:0.0012626971583813429 max memory_allocated 34633.880859375 
[2025-03-22 14:14:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.00030531175434589386 norm:0.0011590749491006136 max memory_allocated 34633.880859375 
[2025-03-22 14:15:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0003133205755148083 norm:0.0011604322353377938 max memory_allocated 34633.880859375 
[2025-03-22 14:15:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.00029271331732161343 norm:0.0009998719906434417 max memory_allocated 34633.880859375 
[2025-03-22 14:16:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.0002920510887634009 norm:0.0010236114030703902 max memory_allocated 34633.880859375 
[2025-03-22 14:16:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.00027463515289127827 norm:0.0009033213718794286 max memory_allocated 34633.880859375 
[2025-03-22 14:17:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.00026416798937134445 norm:0.0008236975409090519 max memory_allocated 34633.880859375 
[2025-03-22 14:17:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.0002759012277238071 norm:0.0008519291295669973 max memory_allocated 34633.880859375 
[2025-03-22 14:18:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.00026873021852225065 norm:0.0007681826245971024 max memory_allocated 34633.880859375 
[2025-03-22 14:18:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0002742254873737693 norm:0.000805204501375556 max memory_allocated 34633.880859375 
[2025-03-22 14:19:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.000263813795754686 norm:0.0007455257000401616 max memory_allocated 34633.880859375 
[2025-03-22 14:19:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.0002649348753038794 norm:0.0007228761678561568 max memory_allocated 34633.880859375 
[2025-03-22 14:20:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0002755091991275549 norm:0.0007937729242257774 max memory_allocated 34633.880859375 
[2025-03-22 14:20:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.0002692094712983817 norm:0.0006627864204347134 max memory_allocated 34633.880859375 
[2025-03-22 14:21:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0002627784269861877 norm:0.00062232231721282 max memory_allocated 34633.880859375 
[2025-03-22 14:21:47 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-22 14:21:47 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-22 14:21:47 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:22:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.019915394484996796 norm:0.013019458390772343 max memory_allocated 35100.7724609375 
[2025-03-22 14:22:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.012332604266703129 norm:0.011926155537366867 max memory_allocated 35100.7724609375 
[2025-03-22 14:23:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.008409648202359676 norm:0.010812175460159779 max memory_allocated 35100.7724609375 
[2025-03-22 14:23:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.007286603096872568 norm:0.006757710594683886 max memory_allocated 35100.7724609375 
[2025-03-22 14:24:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.007879423908889294 norm:0.006366402842104435 max memory_allocated 35100.7724609375 
[2025-03-22 14:24:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.0074296267703175545 norm:0.007461955305188894 max memory_allocated 35100.7724609375 
[2025-03-22 14:25:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.007141724694520235 norm:0.00781127717345953 max memory_allocated 35100.7724609375 
[2025-03-22 14:25:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.007349605206400156 norm:0.007570873945951462 max memory_allocated 35100.7724609375 
[2025-03-22 14:26:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.008316513150930405 norm:0.009800044819712639 max memory_allocated 35100.7724609375 
[2025-03-22 14:26:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.007417032029479742 norm:0.008376730605959892 max memory_allocated 35100.7724609375 
[2025-03-22 14:27:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.007494161371141672 norm:0.00789159256964922 max memory_allocated 35100.7724609375 
[2025-03-22 14:27:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.00733224768191576 norm:0.007722684647887945 max memory_allocated 35100.7724609375 
[2025-03-22 14:28:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.006842713803052902 norm:0.006928704679012299 max memory_allocated 35100.7724609375 
[2025-03-22 14:28:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.006900967098772526 norm:0.0067886789329349995 max memory_allocated 35100.7724609375 
[2025-03-22 14:29:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.006806519813835621 norm:0.006694458425045013 max memory_allocated 35100.7724609375 
[2025-03-22 14:29:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.006960852071642876 norm:0.006863062735646963 max memory_allocated 35100.7724609375 
[2025-03-22 14:30:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.006922745145857334 norm:0.006427207496017218 max memory_allocated 35100.7724609375 
[2025-03-22 14:30:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.006971376948058605 norm:0.006336662452667952 max memory_allocated 35100.7724609375 
[2025-03-22 14:30:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.007231574971228838 norm:0.006453207693994045 max memory_allocated 35100.7724609375 
[2025-03-22 14:31:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.007214650511741638 norm:0.006063560023903847 max memory_allocated 35100.7724609375 
[2025-03-22 14:32:04 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-22 14:32:04 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
[2025-03-22 14:32:04 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:32:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.0072628287598490715 norm:0.0032562981359660625 max memory_allocated 35100.8349609375 
[2025-03-22 14:33:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.006878525950014591 norm:0.00321390968747437 max memory_allocated 35100.8349609375 
[2025-03-22 14:33:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.006686438340693712 norm:0.0029129963368177414 max memory_allocated 35100.8349609375 
[2025-03-22 14:34:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.006442907731980085 norm:0.0024691561702638865 max memory_allocated 35100.8349609375 
[2025-03-22 14:34:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.006137995049357414 norm:0.0022267817985266447 max memory_allocated 35100.8349609375 
[2025-03-22 14:34:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.005857527256011963 norm:0.00200772937387228 max memory_allocated 35100.8349609375 
[2025-03-22 14:35:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.005713560618460178 norm:0.0017820147331804037 max memory_allocated 35100.8349609375 
[2025-03-22 14:35:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.005664835684001446 norm:0.001556216855533421 max memory_allocated 35100.8349609375 
[2025-03-22 14:36:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.005639038980007172 norm:0.0013998978538438678 max memory_allocated 35100.8349609375 
[2025-03-22 14:36:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.005606662482023239 norm:0.0011953237699344754 max memory_allocated 35100.8349609375 
[2025-03-22 14:37:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.0055867750197649 norm:0.0010774292750284076 max memory_allocated 35100.8349609375 
[2025-03-22 14:37:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.005558268632739782 norm:0.0009670165600255132 max memory_allocated 35100.8349609375 
[2025-03-22 14:38:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.00554546108469367 norm:0.0008777302573435009 max memory_allocated 35100.8349609375 
[2025-03-22 14:38:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.005536527372896671 norm:0.0008157306001521647 max memory_allocated 35100.8349609375 
[2025-03-22 14:39:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.005541757680475712 norm:0.0008168834610842168 max memory_allocated 35100.8349609375 
[2025-03-22 14:39:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.005558242555707693 norm:0.0009459044085815549 max memory_allocated 35100.8349609375 
[2025-03-22 14:40:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.005561156664043665 norm:0.0009496820857748389 max memory_allocated 35100.8349609375 
[2025-03-22 14:40:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.005549554713070393 norm:0.0008382024825550616 max memory_allocated 35100.8349609375 
[2025-03-22 14:41:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.005535464268177748 norm:0.00073364278068766 max memory_allocated 35100.8349609375 
[2025-03-22 14:41:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.005537926219403744 norm:0.0007641763659194112 max memory_allocated 35100.8349609375 
[2025-03-22 14:42:22 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2]
[2025-03-22 14:42:22 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4, 5] ===
[2025-03-22 14:43:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 0 loss:0.012524799443781376 norm:0.00024041777942329645 max memory_allocated 47477.6044921875 
[2025-03-22 14:45:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 1 loss:0.011064564809203148 norm:0.00017982414283324033 max memory_allocated 47477.6044921875 
[2025-03-22 14:46:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 2 loss:0.01039129588752985 norm:0.00015907257329672575 max memory_allocated 47477.6044921875 
[2025-03-22 14:48:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 3 loss:0.009747503325343132 norm:0.00012360684922896326 max memory_allocated 47477.6044921875 
[2025-03-22 14:49:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 4 loss:0.00910048559308052 norm:0.00013587158173322678 max memory_allocated 47477.6044921875 
[2025-03-22 14:51:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 5 loss:0.008613512851297855 norm:0.00016982934903353453 max memory_allocated 47477.6044921875 
[2025-03-22 14:52:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 6 loss:0.00828403327614069 norm:0.00019480679475236684 max memory_allocated 47477.6044921875 
[2025-03-22 14:53:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 7 loss:0.008070103824138641 norm:0.00022427886142395437 max memory_allocated 47477.6044921875 
[2025-03-22 14:55:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 8 loss:0.007982593961060047 norm:0.00020158237020950764 max memory_allocated 47477.6044921875 
[2025-03-22 14:56:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 9 loss:0.00792756024748087 norm:0.0002045435830950737 max memory_allocated 47477.6044921875 
[2025-03-22 14:58:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 10 loss:0.007909591309726238 norm:0.00020312800188548863 max memory_allocated 47477.6044921875 
[2025-03-22 14:59:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 11 loss:0.007907996885478497 norm:0.00021966146596241742 max memory_allocated 47477.6044921875 
[2025-03-22 15:01:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 12 loss:0.007902211509644985 norm:0.0002141558943549171 max memory_allocated 47477.6044921875 
[2025-03-22 15:02:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 13 loss:0.007880506105720997 norm:0.0002212074032286182 max memory_allocated 47477.6044921875 
[2025-03-22 15:03:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 14 loss:0.007854398339986801 norm:0.00019533501472324133 max memory_allocated 47477.6044921875 
[2025-03-22 15:05:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 15 loss:0.00785201694816351 norm:0.00020589407358784229 max memory_allocated 47477.6044921875 
[2025-03-22 15:06:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 16 loss:0.007836317643523216 norm:0.00021371481125243008 max memory_allocated 47477.6044921875 
[2025-03-22 15:08:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 17 loss:0.007820798084139824 norm:0.00020260260498616844 max memory_allocated 47477.6044921875 
[2025-03-22 15:09:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 18 loss:0.007812829688191414 norm:0.00019888086535502225 max memory_allocated 47477.6044921875 
[2025-03-22 15:11:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 19 loss:0.0078005557879805565 norm:0.0002191364619648084 max memory_allocated 47477.6044921875 
[2025-03-22 15:13:04 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [3, 4, 5]
[2025-03-22 15:13:04 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [6, 7, 8] ===
[2025-03-22 15:14:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 0 loss:0.01717234030365944 norm:0.0004353466792963445 max memory_allocated 47478.7919921875 
[2025-03-22 15:16:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 1 loss:0.014630615711212158 norm:0.00026420297217555344 max memory_allocated 47478.7919921875 
[2025-03-22 15:17:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 2 loss:0.013516864739358425 norm:0.00022086850367486477 max memory_allocated 47478.7919921875 
[2025-03-22 15:18:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 3 loss:0.012523774988949299 norm:0.0001893393200589344 max memory_allocated 47478.7919921875 
[2025-03-22 15:20:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 4 loss:0.011682542972266674 norm:0.0001668781042098999 max memory_allocated 47478.7919921875 
[2025-03-22 15:21:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 5 loss:0.011027518659830093 norm:0.0001603331184014678 max memory_allocated 47478.7919921875 
[2025-03-22 15:23:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 6 loss:0.010604843497276306 norm:0.00016139535000547767 max memory_allocated 47478.7919921875 
[2025-03-22 15:24:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 7 loss:0.010342849418520927 norm:0.00015409072511829436 max memory_allocated 47478.7919921875 
[2025-03-22 15:26:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 8 loss:0.010210408829152584 norm:0.00016969401622191072 max memory_allocated 47478.7919921875 
[2025-03-22 15:27:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 9 loss:0.010101265273988247 norm:0.00014980300329625607 max memory_allocated 47478.7919921875 
[2025-03-22 15:28:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 10 loss:0.00998248253017664 norm:0.0001327160425717011 max memory_allocated 47478.7919921875 
[2025-03-22 15:30:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 11 loss:0.009953591041266918 norm:0.00014214249677024782 max memory_allocated 47478.7919921875 
[2025-03-22 15:31:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 12 loss:0.009882276877760887 norm:0.0001352430263068527 max memory_allocated 47478.7919921875 
[2025-03-22 15:33:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 13 loss:0.009845227934420109 norm:0.000128060084534809 max memory_allocated 47478.7919921875 
[2025-03-22 15:34:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 14 loss:0.009786112233996391 norm:0.00012290195445530117 max memory_allocated 47478.7919921875 
[2025-03-22 15:36:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 15 loss:0.009754855185747147 norm:0.0001241877762367949 max memory_allocated 47478.7919921875 
[2025-03-22 15:37:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 16 loss:0.009721394628286362 norm:0.00012120167957618833 max memory_allocated 47478.7919921875 
[2025-03-22 15:38:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 17 loss:0.009710167534649372 norm:0.0001147747752838768 max memory_allocated 47478.7919921875 
[2025-03-22 15:40:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 18 loss:0.009707016870379448 norm:0.00012495525879785419 max memory_allocated 47478.7919921875 
[2025-03-22 15:41:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 19 loss:0.009687275625765324 norm:0.00013105085236020386 max memory_allocated 47478.7919921875 
[2025-03-22 15:43:45 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [6, 7, 8]
[2025-03-22 15:43:45 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [9, 10] ===
[2025-03-22 15:44:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 0 loss:0.0163946021348238 norm:0.0003824048035312444 max memory_allocated 47478.7919921875 
[2025-03-22 15:45:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 1 loss:0.013854147866368294 norm:0.00024399433459620923 max memory_allocated 47478.7919921875 
[2025-03-22 15:46:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 2 loss:0.012740176171064377 norm:0.00019361477461643517 max memory_allocated 47478.7919921875 
[2025-03-22 15:47:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 3 loss:0.012018509209156036 norm:0.00017287781520280987 max memory_allocated 47478.7919921875 
[2025-03-22 15:48:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 4 loss:0.011399809271097183 norm:0.00014955017832107842 max memory_allocated 47478.7919921875 
[2025-03-22 15:49:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 5 loss:0.01091204397380352 norm:0.00014175196702126414 max memory_allocated 47478.7919921875 
[2025-03-22 15:50:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 6 loss:0.010550161823630333 norm:0.00012529268860816956 max memory_allocated 47478.7919921875 
[2025-03-22 15:51:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 7 loss:0.010334761813282967 norm:0.00011121884745080024 max memory_allocated 47478.7919921875 
[2025-03-22 15:52:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 8 loss:0.010244359262287617 norm:0.00011590473150135949 max memory_allocated 47478.7919921875 
[2025-03-22 15:53:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 9 loss:0.010146470740437508 norm:0.00011375884059816599 max memory_allocated 47478.7919921875 
[2025-03-22 15:54:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 10 loss:0.010044608265161514 norm:0.0001086624397430569 max memory_allocated 47478.7919921875 
[2025-03-22 15:55:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 11 loss:0.009973369538784027 norm:0.00010462479258421808 max memory_allocated 47478.7919921875 
[2025-03-22 15:56:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 12 loss:0.009947149083018303 norm:0.00010694263619370759 max memory_allocated 47478.7919921875 
[2025-03-22 15:57:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 13 loss:0.009899672120809555 norm:0.00011359759810147807 max memory_allocated 47478.7919921875 
[2025-03-22 15:58:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 14 loss:0.00989232026040554 norm:0.00011484146671136841 max memory_allocated 47478.7919921875 
[2025-03-22 15:59:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 15 loss:0.009857924655079842 norm:0.00010953299351967871 max memory_allocated 47478.7919921875 
[2025-03-22 16:00:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 16 loss:0.009841486811637878 norm:0.00010877086606342345 max memory_allocated 47478.7919921875 
[2025-03-22 16:01:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 17 loss:0.009891592897474766 norm:0.00012716653873212636 max memory_allocated 47478.7919921875 
[2025-03-22 16:01:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 18 loss:0.009840247221291065 norm:0.00010467386891832575 max memory_allocated 47478.7919921875 
[2025-03-22 16:02:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 19 loss:0.00981889944523573 norm:0.00010643616406014189 max memory_allocated 47478.7919921875 
[2025-03-22 16:04:11 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [9, 10]
[2025-03-22 16:04:11 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [11, 12, 13] ===
[2025-03-22 16:05:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 0 loss:0.01874895580112934 norm:0.00045689145918004215 max memory_allocated 47478.7919921875 
[2025-03-22 16:07:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 1 loss:0.01608176901936531 norm:0.0002708722895476967 max memory_allocated 47478.7919921875 
[2025-03-22 16:08:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 2 loss:0.014808852225542068 norm:0.00021376792574301362 max memory_allocated 47478.7919921875 
[2025-03-22 16:10:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 3 loss:0.013919668272137642 norm:0.00017697745352052152 max memory_allocated 47478.7919921875 
[2025-03-22 16:11:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 4 loss:0.013206209987401962 norm:0.00016292488726321608 max memory_allocated 47478.7919921875 
[2025-03-22 16:12:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 5 loss:0.012632181867957115 norm:0.0001517368946224451 max memory_allocated 47478.7919921875 
[2025-03-22 16:14:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 6 loss:0.012194344773888588 norm:0.00014115650265011936 max memory_allocated 47478.7919921875 
[2025-03-22 16:15:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 7 loss:0.011867407709360123 norm:0.00012395357771310955 max memory_allocated 47478.7919921875 
[2025-03-22 16:17:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 8 loss:0.01165079977363348 norm:0.00011660397285595536 max memory_allocated 47478.7919921875 
[2025-03-22 16:18:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 9 loss:0.011499183252453804 norm:0.00011063228157581761 max memory_allocated 47478.7919921875 
[2025-03-22 16:20:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 10 loss:0.011394461616873741 norm:0.00010998317884514108 max memory_allocated 47478.7919921875 
[2025-03-22 16:21:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 11 loss:0.011288410983979702 norm:0.00010096226469613612 max memory_allocated 47478.7919921875 
[2025-03-22 16:22:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 12 loss:0.011194674298167229 norm:9.687853162176907e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:24:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 13 loss:0.011140137910842896 norm:9.189703996526077e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:25:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 14 loss:0.011093982495367527 norm:8.99329170351848e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:27:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 15 loss:0.011043661274015903 norm:8.619803702458739e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:28:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 16 loss:0.0110134556889534 norm:7.86047603469342e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:30:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 17 loss:0.010986223816871643 norm:7.804124470567331e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:31:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 18 loss:0.010961547493934631 norm:8.084075670922175e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:32:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 19 loss:0.010927604511380196 norm:8.221129246521741e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:34:52 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [11, 12, 13]
[2025-03-22 16:34:52 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [14, 15, 16] ===
[2025-03-22 16:36:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 0 loss:0.01893753558397293 norm:0.00048654654528945684 max memory_allocated 47481.2919921875 
[2025-03-22 16:37:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 1 loss:0.016201842576265335 norm:0.00027974590193480253 max memory_allocated 47481.2919921875 
[2025-03-22 16:39:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 2 loss:0.01495218463242054 norm:0.00021101429592818022 max memory_allocated 47481.2919921875 
[2025-03-22 16:40:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 3 loss:0.01405726931989193 norm:0.0001667737669777125 max memory_allocated 47481.2919921875 
[2025-03-22 16:42:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 4 loss:0.013358933851122856 norm:0.00014879439549986273 max memory_allocated 47481.2919921875 
[2025-03-22 16:43:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 5 loss:0.01278666965663433 norm:0.00013784288603346795 max memory_allocated 47481.2919921875 
[2025-03-22 16:45:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 6 loss:0.012355036102235317 norm:0.00012778621749021113 max memory_allocated 47481.2919921875 
[2025-03-22 16:46:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 7 loss:0.012046409770846367 norm:0.00011798848572652787 max memory_allocated 47481.2919921875 
[2025-03-22 16:47:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 8 loss:0.011824461631476879 norm:0.00011222312605241314 max memory_allocated 47481.2919921875 
[2025-03-22 16:49:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 9 loss:0.01166678685694933 norm:0.00010577905777608976 max memory_allocated 47481.2919921875 
[2025-03-22 16:50:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 10 loss:0.011555267497897148 norm:0.00010690935596358031 max memory_allocated 47481.2919921875 
[2025-03-22 16:52:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 11 loss:0.011467703618109226 norm:9.989229147322476e-05 max memory_allocated 47481.2919921875 
[2025-03-22 16:53:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 12 loss:0.011391252279281616 norm:9.281020174967125e-05 max memory_allocated 47481.2919921875 
[2025-03-22 16:55:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 13 loss:0.011312599293887615 norm:9.030531509779394e-05 max memory_allocated 47481.2919921875 
[2025-03-22 16:56:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 14 loss:0.011239280924201012 norm:8.922864799387753e-05 max memory_allocated 47481.2919921875 
[2025-03-22 16:57:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 15 loss:0.011178209446370602 norm:8.500213880324736e-05 max memory_allocated 47481.2919921875 
[2025-03-22 16:59:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 16 loss:0.01113564521074295 norm:8.369893475901335e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:00:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 17 loss:0.01110133994370699 norm:7.75544613134116e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:02:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 18 loss:0.011053254827857018 norm:7.241249841172248e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:03:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 19 loss:0.011019540019333363 norm:7.354001718340442e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:05:34 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [14, 15, 16]
[2025-03-22 17:05:34 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [17, 18, 19] ===
[2025-03-22 17:07:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 0 loss:0.01924799010157585 norm:0.00032777435262687504 max memory_allocated 47481.2919921875 
[2025-03-22 17:08:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 1 loss:0.01762920618057251 norm:0.00020625547040253878 max memory_allocated 47481.2919921875 
[2025-03-22 17:09:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 2 loss:0.016632288694381714 norm:0.0001561778481118381 max memory_allocated 47481.2919921875 
[2025-03-22 17:11:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 3 loss:0.015822209417819977 norm:0.00012999963655602187 max memory_allocated 47481.2919921875 
[2025-03-22 17:12:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 4 loss:0.015027923509478569 norm:0.00011855142656713724 max memory_allocated 47481.2919921875 
[2025-03-22 17:14:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 5 loss:0.014407775364816189 norm:0.00010829664824996144 max memory_allocated 47481.2919921875 
[2025-03-22 17:15:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 6 loss:0.014051180332899094 norm:0.00010755989933386445 max memory_allocated 47481.2919921875 
[2025-03-22 17:17:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 7 loss:0.013847872614860535 norm:9.841527935350314e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:18:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 8 loss:0.013722063042223454 norm:9.153212158707902e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:20:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 9 loss:0.013636941090226173 norm:8.255205466412008e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:21:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 10 loss:0.013573327101767063 norm:8.045011782087386e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:22:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 11 loss:0.013505644164979458 norm:7.875721348682418e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:24:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 12 loss:0.01344453264027834 norm:7.619908865308389e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:25:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 13 loss:0.01338263414800167 norm:7.246094901347533e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:27:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 14 loss:0.013359323143959045 norm:6.882788147777319e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:28:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 15 loss:0.01331036165356636 norm:6.648932321695611e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:30:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 16 loss:0.013268601149320602 norm:6.106661021476611e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:31:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 17 loss:0.013240554369986057 norm:6.016196130076423e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:32:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 18 loss:0.013220012187957764 norm:5.844057159265503e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:34:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 19 loss:0.013198787346482277 norm:5.4984517191769555e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:36:16 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [17, 18, 19]
[2025-03-22 17:36:16 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [20, 21, 22] ===
[2025-03-22 17:37:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 0 loss:0.02636326476931572 norm:0.00046780024422332644 max memory_allocated 47481.2919921875 
[2025-03-22 17:39:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 1 loss:0.024219054728746414 norm:0.0002928103203885257 max memory_allocated 47481.2919921875 
[2025-03-22 17:40:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 2 loss:0.022904671728610992 norm:0.00021497927082236856 max memory_allocated 47481.2919921875 
[2025-03-22 17:42:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 3 loss:0.02167079597711563 norm:0.00017861738160718232 max memory_allocated 47481.2919921875 
[2025-03-22 17:43:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 4 loss:0.020464986562728882 norm:0.00015993219858501107 max memory_allocated 47481.2919921875 
[2025-03-22 17:44:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 5 loss:0.019816940650343895 norm:0.00014470727182924747 max memory_allocated 47481.2919921875 
[2025-03-22 17:46:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 6 loss:0.01959674246609211 norm:0.00013971693988423795 max memory_allocated 47481.2919921875 
[2025-03-22 17:47:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 7 loss:0.019464926794171333 norm:0.00013603719708044082 max memory_allocated 47481.2919921875 
[2025-03-22 17:49:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 8 loss:0.019328879192471504 norm:0.00012787952437065542 max memory_allocated 47481.2919921875 
[2025-03-22 17:50:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 9 loss:0.019222836941480637 norm:0.00012103917106287554 max memory_allocated 47481.2919921875 
[2025-03-22 17:52:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 10 loss:0.01912682130932808 norm:0.00011604239989537746 max memory_allocated 47481.2919921875 
[2025-03-22 17:53:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 11 loss:0.019041527062654495 norm:0.00010730654321378097 max memory_allocated 47481.2919921875 
[2025-03-22 17:55:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 12 loss:0.018963424488902092 norm:0.00010396874858997762 max memory_allocated 47481.2919921875 
[2025-03-22 17:56:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 13 loss:0.01889687031507492 norm:0.00010229033068753779 max memory_allocated 47481.2919921875 
[2025-03-22 17:57:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 14 loss:0.01882704347372055 norm:9.296290227212012e-05 max memory_allocated 47481.2919921875 
[2025-03-22 17:59:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 15 loss:0.018776340410113335 norm:9.396513632964343e-05 max memory_allocated 47481.2919921875 
[2025-03-22 18:00:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 16 loss:0.01872427761554718 norm:9.411418432136998e-05 max memory_allocated 47481.2919921875 
[2025-03-22 18:02:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 17 loss:0.018671156838536263 norm:9.065970516530797e-05 max memory_allocated 47481.2919921875 
[2025-03-22 18:03:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 18 loss:0.018639691174030304 norm:8.291128324344754e-05 max memory_allocated 47481.2919921875 
[2025-03-22 18:05:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 19 loss:0.018607033416628838 norm:7.677130633965135e-05 max memory_allocated 47481.2919921875 
[2025-03-22 18:06:59 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [20, 21, 22]
[2025-03-22 18:06:59 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [23, 24, 25] ===
[2025-03-22 18:08:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 0 loss:0.036807335913181305 norm:0.0004834172432310879 max memory_allocated 47481.2919921875 
[2025-03-22 18:09:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 1 loss:0.03436636924743652 norm:0.00033391089527867734 max memory_allocated 47481.2919921875 
[2025-03-22 18:11:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 2 loss:0.03263557329773903 norm:0.00027843815041705966 max memory_allocated 47481.2919921875 
[2025-03-22 18:12:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 3 loss:0.030699830502271652 norm:0.0002408979053143412 max memory_allocated 47481.2919921875 
[2025-03-22 18:14:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 4 loss:0.02930528298020363 norm:0.00022372783860191703 max memory_allocated 47481.2919921875 
[2025-03-22 18:15:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 5 loss:0.028875650838017464 norm:0.00020509470778051764 max memory_allocated 47481.2919921875 
[2025-03-22 18:17:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 6 loss:0.028678255155682564 norm:0.00019953293667640537 max memory_allocated 47481.2919921875 
[2025-03-22 18:18:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 7 loss:0.028515538200736046 norm:0.0002024189889198169 max memory_allocated 47481.2919921875 
[2025-03-22 18:19:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 8 loss:0.028343921527266502 norm:0.00017705306527204812 max memory_allocated 47481.2919921875 
[2025-03-22 18:21:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 9 loss:0.028234325349330902 norm:0.00016407520161010325 max memory_allocated 47481.2919921875 
[2025-03-22 18:22:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 10 loss:0.028154706582427025 norm:0.00015951675595715642 max memory_allocated 47481.2919921875 
[2025-03-22 18:24:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 11 loss:0.028036102652549744 norm:0.00014738804020453244 max memory_allocated 47481.2919921875 
[2025-03-22 18:25:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 12 loss:0.02795528806746006 norm:0.00015749095473438501 max memory_allocated 47481.2919921875 
[2025-03-22 18:27:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 13 loss:0.027859507128596306 norm:0.00015307440480683 max memory_allocated 47481.2919921875 
[2025-03-22 18:28:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 14 loss:0.02777097001671791 norm:0.00013488468539435416 max memory_allocated 47481.2919921875 
[2025-03-22 18:30:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 15 loss:0.027712348848581314 norm:0.00013991685409564525 max memory_allocated 47481.2919921875 
[2025-03-22 18:31:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 16 loss:0.027644310146570206 norm:0.0001310671359533444 max memory_allocated 47481.2919921875 
[2025-03-22 18:32:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 17 loss:0.02757907658815384 norm:0.00012370219337753952 max memory_allocated 47481.2919921875 
[2025-03-22 18:34:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 18 loss:0.027524646371603012 norm:0.00012089573283446953 max memory_allocated 47481.2919921875 
[2025-03-22 18:35:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 19 loss:0.02747981809079647 norm:0.00011447127326391637 max memory_allocated 47481.2919921875 
[2025-03-22 18:37:39 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [23, 24, 25]
[2025-03-22 18:37:39 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [26, 27, 28] ===
[2025-03-22 18:37:40 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 18:39:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 0 loss:0.05349146947264671 norm:0.0028671585023403168 max memory_allocated 47481.2919921875 
[2025-03-22 18:40:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 1 loss:0.0501001738011837 norm:0.0021155846770852804 max memory_allocated 47481.2919921875 
[2025-03-22 18:42:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 2 loss:0.047403935343027115 norm:0.0018084219191223383 max memory_allocated 47481.2919921875 
[2025-03-22 18:43:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 3 loss:0.04460984095931053 norm:0.0015570413088425994 max memory_allocated 47481.2919921875 
[2025-03-22 18:44:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 4 loss:0.04339775815606117 norm:0.0012936899438500404 max memory_allocated 47481.2919921875 
[2025-03-22 18:46:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 5 loss:0.043006062507629395 norm:0.0010787488427013159 max memory_allocated 47481.2919921875 
[2025-03-22 18:47:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 6 loss:0.0427870899438858 norm:0.0009200499043799937 max memory_allocated 47481.2919921875 
[2025-03-22 18:49:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 7 loss:0.042618926614522934 norm:0.0008678586455062032 max memory_allocated 47481.2919921875 
[2025-03-22 18:50:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 8 loss:0.042462173849344254 norm:0.000824266520794481 max memory_allocated 47481.2919921875 
[2025-03-22 18:52:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 9 loss:0.04234159737825394 norm:0.0008688154048286378 max memory_allocated 47481.2919921875 
[2025-03-22 18:53:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 10 loss:0.04221545532345772 norm:0.0007301822188310325 max memory_allocated 47481.2919921875 
[2025-03-22 18:54:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 11 loss:0.04207601025700569 norm:0.0007117516361176968 max memory_allocated 47481.2919921875 
[2025-03-22 18:56:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 12 loss:0.041957829147577286 norm:0.0006875192048028111 max memory_allocated 47481.2919921875 
[2025-03-22 18:57:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 13 loss:0.041873298585414886 norm:0.0006673079333268106 max memory_allocated 47481.2919921875 
[2025-03-22 18:59:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 14 loss:0.04180348291993141 norm:0.0006361130508594215 max memory_allocated 47481.2919921875 
[2025-03-22 19:00:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 15 loss:0.0417272225022316 norm:0.0005979753914289176 max memory_allocated 47481.2919921875 
[2025-03-22 19:02:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 16 loss:0.041671376675367355 norm:0.0005977905821055174 max memory_allocated 47481.2919921875 
[2025-03-22 19:03:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 17 loss:0.04162723198533058 norm:0.000614530174061656 max memory_allocated 47481.2919921875 
[2025-03-22 19:05:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 18 loss:0.0416269451379776 norm:0.000569546187762171 max memory_allocated 47481.2919921875 
[2025-03-22 19:06:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 19 loss:0.0415557436645031 norm:0.0005468828603625298 max memory_allocated 47481.2919921875 
[2025-03-22 19:08:23 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [26, 27, 28]
[2025-03-22 19:08:23 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [29] ===
[2025-03-22 19:08:24 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:08:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 0 loss:0.05052899569272995 norm:0.0039467173628509045 max memory_allocated 47481.2919921875 
[2025-03-22 19:09:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 1 loss:0.04811583086848259 norm:0.003107779659330845 max memory_allocated 47481.2919921875 
[2025-03-22 19:09:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 2 loss:0.0462726429104805 norm:0.002249877667054534 max memory_allocated 47481.2919921875 
[2025-03-22 19:10:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 3 loss:0.045183420181274414 norm:0.0017991733038797975 max memory_allocated 47481.2919921875 
[2025-03-22 19:10:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 4 loss:0.044911231845617294 norm:0.0015923917526379228 max memory_allocated 47481.2919921875 
[2025-03-22 19:11:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 5 loss:0.044778890907764435 norm:0.0013618402881547809 max memory_allocated 47481.2919921875 
[2025-03-22 19:11:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 6 loss:0.04466064274311066 norm:0.0011763793881982565 max memory_allocated 47481.2919921875 
[2025-03-22 19:12:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 7 loss:0.04455876350402832 norm:0.0010011231061071157 max memory_allocated 47481.2919921875 
[2025-03-22 19:12:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 8 loss:0.0444788783788681 norm:0.0009350532200187445 max memory_allocated 47481.2919921875 
[2025-03-22 19:13:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 9 loss:0.044456884264945984 norm:0.0008922253036871552 max memory_allocated 47481.2919921875 
[2025-03-22 19:13:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 10 loss:0.044394601136446 norm:0.0008636873681098223 max memory_allocated 47481.2919921875 
[2025-03-22 19:14:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 11 loss:0.04437930881977081 norm:0.0008508090395480394 max memory_allocated 47481.2919921875 
[2025-03-22 19:14:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 12 loss:0.04430669546127319 norm:0.0007882261415943503 max memory_allocated 47481.2919921875 
[2025-03-22 19:15:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 13 loss:0.04429090768098831 norm:0.0007366196950897574 max memory_allocated 47481.2919921875 
[2025-03-22 19:15:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 14 loss:0.044270798563957214 norm:0.0007817504229024053 max memory_allocated 47481.2919921875 
[2025-03-22 19:16:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 15 loss:0.044247716665267944 norm:0.0007012913702055812 max memory_allocated 47481.2919921875 
[2025-03-22 19:16:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 16 loss:0.044202495366334915 norm:0.0007537385099567473 max memory_allocated 47481.2919921875 
[2025-03-22 19:17:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 17 loss:0.044225797057151794 norm:0.0006566396332345903 max memory_allocated 47481.2919921875 
[2025-03-22 19:17:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 18 loss:0.044172581285238266 norm:0.0007109039579518139 max memory_allocated 47481.2919921875 
[2025-03-22 19:18:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 19 loss:0.04416771978139877 norm:0.0006265618721954525 max memory_allocated 47481.2919921875 
[2025-03-22 19:18:40 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [29]
[2025-03-22 19:18:40 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [30] ===
[2025-03-22 19:18:40 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:19:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 0 loss:0.07948044687509537 norm:0.010281323455274105 max memory_allocated 47481.2919921875 
[2025-03-22 19:19:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 1 loss:0.06795546412467957 norm:0.006319444160908461 max memory_allocated 47481.2919921875 
[2025-03-22 19:20:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 2 loss:0.06180265545845032 norm:0.004375653341412544 max memory_allocated 47481.2919921875 
[2025-03-22 19:20:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 3 loss:0.05943121761083603 norm:0.0034885015338659286 max memory_allocated 47481.2919921875 
[2025-03-22 19:21:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 4 loss:0.05895001441240311 norm:0.003103605704382062 max memory_allocated 47481.2919921875 
[2025-03-22 19:21:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 5 loss:0.05867792293429375 norm:0.002744100522249937 max memory_allocated 47481.2919921875 
[2025-03-22 19:22:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 6 loss:0.05852971598505974 norm:0.0025572816375643015 max memory_allocated 47481.2919921875 
[2025-03-22 19:22:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 7 loss:0.058412838727235794 norm:0.0023627951741218567 max memory_allocated 47481.2919921875 
[2025-03-22 19:23:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 8 loss:0.05827437713742256 norm:0.0021221560891717672 max memory_allocated 47481.2919921875 
[2025-03-22 19:23:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 9 loss:0.05812333524227142 norm:0.0019374500261619687 max memory_allocated 47481.2919921875 
[2025-03-22 19:24:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 10 loss:0.05805893614888191 norm:0.0018132501281797886 max memory_allocated 47481.2919921875 
[2025-03-22 19:24:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 11 loss:0.05803937092423439 norm:0.001698804902844131 max memory_allocated 47481.2919921875 
[2025-03-22 19:24:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 12 loss:0.058005668222904205 norm:0.0016888307873159647 max memory_allocated 47481.2919921875 
[2025-03-22 19:25:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 13 loss:0.05793190747499466 norm:0.0015005753375589848 max memory_allocated 47481.2919921875 
[2025-03-22 19:25:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 14 loss:0.05784404277801514 norm:0.0014003075193613768 max memory_allocated 47481.2919921875 
[2025-03-22 19:26:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 15 loss:0.05778044834733009 norm:0.0013566457200795412 max memory_allocated 47481.2919921875 
[2025-03-22 19:26:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 16 loss:0.05777581408619881 norm:0.0014446907443925738 max memory_allocated 47481.2919921875 
[2025-03-22 19:27:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 17 loss:0.057819925248622894 norm:0.0014626054326072335 max memory_allocated 47481.2919921875 
[2025-03-22 19:27:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 18 loss:0.057779233902692795 norm:0.0014769791159778833 max memory_allocated 47481.2919921875 
[2025-03-22 19:28:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 19 loss:0.05776732414960861 norm:0.0014303600182756782 max memory_allocated 47481.2919921875 
[2025-03-22 19:28:58 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [30]
[2025-03-22 19:28:58 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [31] ===
[2025-03-22 19:28:58 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:29:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 0 loss:0.10196244716644287 norm:0.0087535809725523 max memory_allocated 47481.2919921875 
[2025-03-22 19:29:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 1 loss:0.09696220606565475 norm:0.007127497810870409 max memory_allocated 47481.2919921875 
[2025-03-22 19:30:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 2 loss:0.0934363454580307 norm:0.005310743115842342 max memory_allocated 47481.2919921875 
[2025-03-22 19:30:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 3 loss:0.09178580343723297 norm:0.004374900832772255 max memory_allocated 47481.2919921875 
[2025-03-22 19:31:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 4 loss:0.09114857017993927 norm:0.0038980008102953434 max memory_allocated 47481.2919921875 
[2025-03-22 19:31:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 5 loss:0.09088622778654099 norm:0.0034642245154827833 max memory_allocated 47481.2919921875 
[2025-03-22 19:32:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 6 loss:0.09066909551620483 norm:0.0031477492302656174 max memory_allocated 47481.2919921875 
[2025-03-22 19:32:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 7 loss:0.09051083028316498 norm:0.002867745701223612 max memory_allocated 47481.2919921875 
[2025-03-22 19:33:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 8 loss:0.09037800878286362 norm:0.0026991115882992744 max memory_allocated 47481.2919921875 
[2025-03-22 19:33:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 9 loss:0.09030609577894211 norm:0.00273509812541306 max memory_allocated 47481.2919921875 
[2025-03-22 19:34:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 10 loss:0.0901796966791153 norm:0.0025263538118451834 max memory_allocated 47481.2919921875 
[2025-03-22 19:34:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 11 loss:0.09018998593091965 norm:0.002767454367130995 max memory_allocated 47481.2919921875 
[2025-03-22 19:35:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 12 loss:0.09011594951152802 norm:0.0026353327557444572 max memory_allocated 47481.2919921875 
[2025-03-22 19:35:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 13 loss:0.09008285403251648 norm:0.0026090105529874563 max memory_allocated 47481.2919921875 
[2025-03-22 19:36:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 14 loss:0.0900658518075943 norm:0.0027482835575938225 max memory_allocated 47481.2919921875 
[2025-03-22 19:36:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 15 loss:0.09005209058523178 norm:0.002562709152698517 max memory_allocated 47481.2919921875 
[2025-03-22 19:37:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 16 loss:0.08995698392391205 norm:0.0024339179508388042 max memory_allocated 47481.2919921875 
[2025-03-22 19:37:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 17 loss:0.08989278972148895 norm:0.0024778414517641068 max memory_allocated 47481.2919921875 
[2025-03-22 19:38:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 18 loss:0.08982671797275543 norm:0.00231930916197598 max memory_allocated 47481.2919921875 
[2025-03-22 19:38:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 19 loss:0.08982570469379425 norm:0.002471035346388817 max memory_allocated 47481.2919921875 
[2025-03-22 19:39:16 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [31]
[2025-03-22 19:39:16 root] (main_calib_config3_attn.py 379): INFO 19669.647856473923
[2025-03-22 19:39:22 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-22 19:40:12 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.517486572265625
[2025-03-22 19:40:12 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-22 19:41:31 root] (main_calib_config3_attn.py 161): INFO c4 : 7.031857013702393
[2025-03-22 20:42:25 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.517486572265625, 'c4': 7.031857013702393, 'results': {'piqa': {'acc': 0.7818280739934712, 'acc_stderr': 0.009636081958374381, 'acc_norm': 0.7698585418933623, 'acc_norm_stderr': 0.009820832826839801}, 'arc_easy': {'acc': 0.6986531986531986, 'acc_stderr': 0.009415259879351622, 'acc_norm': 0.5269360269360269, 'acc_norm_stderr': 0.010244884740620084}, 'hellaswag': {'acc': 0.5654252141007767, 'acc_stderr': 0.004946879874422681, 'acc_norm': 0.7259510057757419, 'acc_norm_stderr': 0.004451222241494048}, 'boolq': {'acc': 0.689908256880734, 'acc_stderr': 0.008089716685417732}, 'winogrande': {'acc': 0.6708760852407262, 'acc_stderr': 0.013206387089091472}, 'arc_challenge': {'acc': 0.4035836177474403, 'acc_stderr': 0.014337158914268445, 'acc_norm': 0.3984641638225256, 'acc_norm_stderr': 0.014306946052735567}}, 'versions': {'piqa': 0, 'arc_easy': 0, 'hellaswag': 0, 'boolq': 1, 'winogrande': 0, 'arc_challenge': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 20:42:25 root] (main_calib_config3_attn.py 175): INFO 40.36,69.87,68.99,56.54,78.18,67.09
