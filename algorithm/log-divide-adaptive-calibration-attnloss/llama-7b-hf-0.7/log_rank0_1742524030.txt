[2025-03-21 02:27:10 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/llama-7b-hf-0.7', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.7.pkl', blocks_pkl='./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-21 02:28:57 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-21 02:28:57 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-21 02:28:57 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-21 02:28:57 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.7.pkl
[2025-03-21 02:28:57 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 5), (5, 8), (8, 11), (11, 14), (14, 16), (16, 19), (19, 22), (22, 25), (25, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-21 02:28:57 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2, 3, 4], [5, 6, 7], [8, 9, 10], [11, 12, 13], [14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26], [27], [28], [29], [30], [31]]
[2025-03-21 02:28:59 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-21 02:28:59 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 02:29:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.0015039292629808187 norm:0.004173396620899439 max memory_allocated 34630.880859375 
[2025-03-21 02:29:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.0010539920767769217 norm:0.0034411668311804533 max memory_allocated 34630.880859375 
[2025-03-21 02:30:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.0009752794867381454 norm:0.0027457557152956724 max memory_allocated 34630.880859375 
[2025-03-21 02:30:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0009138669120147824 norm:0.002266368130221963 max memory_allocated 34630.880859375 
[2025-03-21 02:31:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.000831884506624192 norm:0.0018389764009043574 max memory_allocated 34630.880859375 
[2025-03-21 02:31:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0007909375126473606 norm:0.0014875817578285933 max memory_allocated 34630.880859375 
[2025-03-21 02:32:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.0007522910600528121 norm:0.001261349767446518 max memory_allocated 34630.880859375 
[2025-03-21 02:32:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0007347083301283419 norm:0.001066758530214429 max memory_allocated 34630.880859375 
[2025-03-21 02:33:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0007114814943633974 norm:0.0009247283451259136 max memory_allocated 34630.880859375 
[2025-03-21 02:33:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.0006905011832714081 norm:0.0008135936222970486 max memory_allocated 34630.880859375 
[2025-03-21 02:34:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.000685409118887037 norm:0.0007142670801840723 max memory_allocated 34630.880859375 
[2025-03-21 02:34:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0006857078988105059 norm:0.000640287296846509 max memory_allocated 34630.880859375 
[2025-03-21 02:34:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.000697033538017422 norm:0.0005534597439691424 max memory_allocated 34630.880859375 
[2025-03-21 02:35:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.000663626124151051 norm:0.00046510001993738115 max memory_allocated 34630.880859375 
[2025-03-21 02:35:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.000659625104162842 norm:0.00043632485903799534 max memory_allocated 34630.880859375 
[2025-03-21 02:36:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.0006570514524355531 norm:0.0004173520428594202 max memory_allocated 34630.880859375 
[2025-03-21 02:36:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.0006539371097460389 norm:0.0003918739384971559 max memory_allocated 34630.880859375 
[2025-03-21 02:37:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0006497016875073314 norm:0.0003546859079506248 max memory_allocated 34630.880859375 
[2025-03-21 02:37:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.0006530337268486619 norm:0.00035835261223837733 max memory_allocated 34630.880859375 
[2025-03-21 02:38:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0006538446177728474 norm:0.0003577436145860702 max memory_allocated 34630.880859375 
[2025-03-21 02:38:48 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-21 02:38:49 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-21 02:38:49 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 02:39:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.009848383255302906 norm:0.01633373089134693 max memory_allocated 35097.7724609375 
[2025-03-21 02:39:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.005168893374502659 norm:0.012119253166019917 max memory_allocated 35097.7724609375 
[2025-03-21 02:40:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.0039441874250769615 norm:0.007287562824785709 max memory_allocated 35097.7724609375 
[2025-03-21 02:40:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.003430919023230672 norm:0.004758351948112249 max memory_allocated 35097.7724609375 
[2025-03-21 02:41:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.0031775059178471565 norm:0.00412073964253068 max memory_allocated 35097.7724609375 
[2025-03-21 02:41:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.002994127571582794 norm:0.003611193038523197 max memory_allocated 35097.7724609375 
[2025-03-21 02:42:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.0028977449983358383 norm:0.0032606329768896103 max memory_allocated 35097.7724609375 
[2025-03-21 02:42:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.002818306442350149 norm:0.0029436906334012747 max memory_allocated 35097.7724609375 
[2025-03-21 02:42:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.0027360557578504086 norm:0.0027375686913728714 max memory_allocated 35097.7724609375 
[2025-03-21 02:43:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.0026295220013707876 norm:0.002492088358849287 max memory_allocated 35097.7724609375 
[2025-03-21 02:43:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.0025376183912158012 norm:0.002234157407656312 max memory_allocated 35097.7724609375 
[2025-03-21 02:44:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.0024962376337498426 norm:0.0020981826819479465 max memory_allocated 35097.7724609375 
[2025-03-21 02:44:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.0024533788673579693 norm:0.001909932354465127 max memory_allocated 35097.7724609375 
[2025-03-21 02:45:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.0024371088948100805 norm:0.001742446213029325 max memory_allocated 35097.7724609375 
[2025-03-21 02:45:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.002413002075627446 norm:0.0015905442414805293 max memory_allocated 35097.7724609375 
[2025-03-21 02:46:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.0023875096812844276 norm:0.0014235543785616755 max memory_allocated 35097.7724609375 
[2025-03-21 02:46:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.0023539045359939337 norm:0.001266934210434556 max memory_allocated 35097.7724609375 
[2025-03-21 02:47:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.002344029024243355 norm:0.001144589390605688 max memory_allocated 35097.7724609375 
[2025-03-21 02:47:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.0023412122391164303 norm:0.0010008648969233036 max memory_allocated 35097.7724609375 
[2025-03-21 02:47:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.002327683148905635 norm:0.0008635856211185455 max memory_allocated 35097.7724609375 
[2025-03-21 02:48:39 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-21 02:48:39 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2, 3, 4] ===
[2025-03-21 02:48:39 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 02:50:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 0 loss:0.029461553320288658 norm:0.013300348073244095 max memory_allocated 47468.5419921875 
[2025-03-21 02:51:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 1 loss:0.02062947116792202 norm:0.008767366409301758 max memory_allocated 47468.5419921875 
[2025-03-21 02:52:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 2 loss:0.015661591663956642 norm:0.005987605080008507 max memory_allocated 47468.5419921875 
[2025-03-21 02:54:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 3 loss:0.014274196699261665 norm:0.005205098539590836 max memory_allocated 47468.5419921875 
[2025-03-21 02:55:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 4 loss:0.013312599621713161 norm:0.003807148663327098 max memory_allocated 47468.5419921875 
[2025-03-21 02:56:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 5 loss:0.012449285015463829 norm:0.0034063048660755157 max memory_allocated 47468.5419921875 
[2025-03-21 02:58:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 6 loss:0.011916358955204487 norm:0.0031383575405925512 max memory_allocated 47468.5419921875 
[2025-03-21 02:59:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 7 loss:0.011678684502840042 norm:0.0026765877846628428 max memory_allocated 47468.5419921875 
[2025-03-21 03:00:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 8 loss:0.01154503133147955 norm:0.002711138455197215 max memory_allocated 47468.5419921875 
[2025-03-21 03:02:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 9 loss:0.01142568327486515 norm:0.0023041381500661373 max memory_allocated 47468.5419921875 
[2025-03-21 03:03:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 10 loss:0.011233426630496979 norm:0.00215521571226418 max memory_allocated 47468.5419921875 
[2025-03-21 03:04:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 11 loss:0.011145446449518204 norm:0.0019483582582324743 max memory_allocated 47468.5419921875 
[2025-03-21 03:06:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 12 loss:0.011105930432677269 norm:0.0017388915875926614 max memory_allocated 47468.5419921875 
[2025-03-21 03:07:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 13 loss:0.01105538010597229 norm:0.0015942538157105446 max memory_allocated 47468.5419921875 
[2025-03-21 03:08:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 14 loss:0.01125174667686224 norm:0.001765209250152111 max memory_allocated 47468.5419921875 
[2025-03-21 03:10:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 15 loss:0.43589258193969727 norm:0.5220937728881836 max memory_allocated 47468.5419921875 
[2025-03-21 03:11:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 16 loss:0.16742488741874695 norm:0.21862071752548218 max memory_allocated 47468.5419921875 
[2025-03-21 03:12:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 17 loss:0.0981219932436943 norm:0.14033889770507812 max memory_allocated 47468.5419921875 
[2025-03-21 03:14:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 18 loss:0.040066856890916824 norm:0.03746882081031799 max memory_allocated 47468.5419921875 
[2025-03-21 03:15:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 19 loss:0.024645963683724403 norm:0.007431729696691036 max memory_allocated 47468.5419921875 
[2025-03-21 03:17:56 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2, 3, 4]
[2025-03-21 03:17:56 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [5, 6, 7] ===
[2025-03-21 03:19:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 0 loss:0.03976287692785263 norm:0.0006114651332609355 max memory_allocated 47468.7294921875 
[2025-03-21 03:20:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 1 loss:0.03170717880129814 norm:0.00032308773370459676 max memory_allocated 47468.7294921875 
[2025-03-21 03:22:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 2 loss:0.026455245912075043 norm:0.00022810243535786867 max memory_allocated 47468.7294921875 
[2025-03-21 03:23:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 3 loss:0.02414732240140438 norm:0.0001833320566220209 max memory_allocated 47468.7294921875 
[2025-03-21 03:24:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 4 loss:0.022724052891135216 norm:0.0001738415885483846 max memory_allocated 47468.7294921875 
[2025-03-21 03:26:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 5 loss:0.021726323291659355 norm:0.0001664831070229411 max memory_allocated 47468.7294921875 
[2025-03-21 03:27:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 6 loss:0.021182529628276825 norm:0.00016828955267556012 max memory_allocated 47468.7294921875 
[2025-03-21 03:28:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 7 loss:0.020864225924015045 norm:0.00016797329590190202 max memory_allocated 47468.7294921875 
[2025-03-21 03:30:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 8 loss:0.020698172971606255 norm:0.00016491553105879575 max memory_allocated 47468.7294921875 
[2025-03-21 03:31:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 9 loss:0.02057706005871296 norm:0.00016831074026413262 max memory_allocated 47468.7294921875 
[2025-03-21 03:32:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 10 loss:0.02052687294781208 norm:0.00017667727661319077 max memory_allocated 47468.7294921875 
[2025-03-21 03:34:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 11 loss:0.020466618239879608 norm:0.00017615774413570762 max memory_allocated 47468.7294921875 
[2025-03-21 03:35:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 12 loss:0.02039838768541813 norm:0.00016904011135920882 max memory_allocated 47468.7294921875 
[2025-03-21 03:36:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 13 loss:0.020364196971058846 norm:0.00017223411123268306 max memory_allocated 47468.7294921875 
[2025-03-21 03:38:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 14 loss:0.0203156266361475 norm:0.00017344440857414156 max memory_allocated 47468.7294921875 
[2025-03-21 03:39:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 15 loss:0.020282259210944176 norm:0.00016948675329331309 max memory_allocated 47468.7294921875 
[2025-03-21 03:40:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 16 loss:0.020284082740545273 norm:0.0001746827329043299 max memory_allocated 47468.7294921875 
[2025-03-21 03:42:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 17 loss:0.02024293877184391 norm:0.00017466851568315178 max memory_allocated 47468.7294921875 
[2025-03-21 03:43:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 18 loss:0.020236268639564514 norm:0.00017470924649387598 max memory_allocated 47468.7294921875 
[2025-03-21 03:44:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 19 loss:0.0201946422457695 norm:0.00016229721950367093 max memory_allocated 47468.7294921875 
[2025-03-21 03:47:37 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [5, 6, 7]
[2025-03-21 03:47:37 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [8, 9, 10] ===
[2025-03-21 03:49:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 0 loss:0.04611115902662277 norm:0.0006653519230894744 max memory_allocated 47468.9169921875 
[2025-03-21 03:50:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 1 loss:0.03661951795220375 norm:0.0003687398857437074 max memory_allocated 47468.9169921875 
[2025-03-21 03:51:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 2 loss:0.03062816895544529 norm:0.0002537492255214602 max memory_allocated 47468.9169921875 
[2025-03-21 03:53:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 3 loss:0.028215812519192696 norm:0.00020987758762203157 max memory_allocated 47468.9169921875 
[2025-03-21 03:54:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 4 loss:0.026702359318733215 norm:0.00019025441724807024 max memory_allocated 47468.9169921875 
[2025-03-21 03:55:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 5 loss:0.025656357407569885 norm:0.00017511767509859055 max memory_allocated 47468.9169921875 
[2025-03-21 03:57:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 6 loss:0.025006365031003952 norm:0.0001689574564807117 max memory_allocated 47468.9169921875 
[2025-03-21 03:58:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 7 loss:0.02463686466217041 norm:0.00016295473324134946 max memory_allocated 47468.9169921875 
[2025-03-21 03:59:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 8 loss:0.02436649799346924 norm:0.00015620146587025374 max memory_allocated 47468.9169921875 
[2025-03-21 04:01:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 9 loss:0.024129552766680717 norm:0.00014740164624527097 max memory_allocated 47468.9169921875 
[2025-03-21 04:02:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 10 loss:0.02408045530319214 norm:0.00017562491120770574 max memory_allocated 47468.9169921875 
[2025-03-21 04:03:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 11 loss:0.02387978881597519 norm:0.00014189694775268435 max memory_allocated 47468.9169921875 
[2025-03-21 04:05:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 12 loss:0.02377200871706009 norm:0.00013812800170853734 max memory_allocated 47468.9169921875 
[2025-03-21 04:06:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 13 loss:0.023726578801870346 norm:0.00013609326560981572 max memory_allocated 47468.9169921875 
[2025-03-21 04:07:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 14 loss:0.02366195246577263 norm:0.00013209924509283155 max memory_allocated 47468.9169921875 
[2025-03-21 04:09:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 15 loss:0.023603176698088646 norm:0.00013278258848004043 max memory_allocated 47468.9169921875 
[2025-03-21 04:10:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 16 loss:0.023569203913211823 norm:0.00012937356950715184 max memory_allocated 47468.9169921875 
[2025-03-21 04:11:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 17 loss:0.02355234883725643 norm:0.00012921768939122558 max memory_allocated 47468.9169921875 
[2025-03-21 04:13:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 18 loss:0.023516522720456123 norm:0.00012980637256987393 max memory_allocated 47468.9169921875 
[2025-03-21 04:14:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 19 loss:0.023482000455260277 norm:0.00012594752479344606 max memory_allocated 47468.9169921875 
[2025-03-21 04:17:05 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [8, 9, 10]
[2025-03-21 04:17:05 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [11, 12, 13] ===
[2025-03-21 04:18:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 0 loss:0.04621407762169838 norm:0.00047351920511573553 max memory_allocated 47470.1044921875 
[2025-03-21 04:19:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 1 loss:0.038496341556310654 norm:0.0002883255365304649 max memory_allocated 47470.1044921875 
[2025-03-21 04:21:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 2 loss:0.03259441256523132 norm:0.0002076775417663157 max memory_allocated 47470.1044921875 
[2025-03-21 04:22:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 3 loss:0.030426548793911934 norm:0.00017901200044434518 max memory_allocated 47470.1044921875 
[2025-03-21 04:23:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 4 loss:0.029030904173851013 norm:0.00016507087275385857 max memory_allocated 47470.1044921875 
[2025-03-21 04:25:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 5 loss:0.02796279825270176 norm:0.0001453645818401128 max memory_allocated 47470.1044921875 
[2025-03-21 04:26:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 6 loss:0.027331838384270668 norm:0.00013787669013254344 max memory_allocated 47470.1044921875 
[2025-03-21 04:27:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 7 loss:0.026935769245028496 norm:0.0001279388670809567 max memory_allocated 47470.1044921875 
[2025-03-21 04:29:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 8 loss:0.02665068581700325 norm:0.00012089614028809592 max memory_allocated 47470.1044921875 
[2025-03-21 04:30:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 9 loss:0.02656838670372963 norm:0.00013813494297210127 max memory_allocated 47470.1044921875 
[2025-03-21 04:32:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 10 loss:0.026370234787464142 norm:0.00011698862363118678 max memory_allocated 47470.1044921875 
[2025-03-21 04:33:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 11 loss:0.026241933926939964 norm:0.00011059202370233834 max memory_allocated 47470.1044921875 
[2025-03-21 04:34:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 12 loss:0.026157451793551445 norm:0.00010542311065364629 max memory_allocated 47470.1044921875 
[2025-03-21 04:36:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 13 loss:0.026091916486620903 norm:0.00010320926230633631 max memory_allocated 47470.1044921875 
[2025-03-21 04:37:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 14 loss:0.02605106681585312 norm:0.00010208503226749599 max memory_allocated 47470.1044921875 
[2025-03-21 04:38:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 15 loss:0.026004772633314133 norm:9.882557787932456e-05 max memory_allocated 47470.1044921875 
[2025-03-21 04:40:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 16 loss:0.025960726663470268 norm:9.943368058884516e-05 max memory_allocated 47470.1044921875 
[2025-03-21 04:41:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 17 loss:0.025945056229829788 norm:9.992269042413682e-05 max memory_allocated 47470.1044921875 
[2025-03-21 04:42:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 18 loss:0.025912567973136902 norm:9.755814244272187e-05 max memory_allocated 47470.1044921875 
[2025-03-21 04:44:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 19 loss:0.025856025516986847 norm:9.63009224506095e-05 max memory_allocated 47470.1044921875 
[2025-03-21 04:46:57 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [11, 12, 13]
[2025-03-21 04:46:57 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [14, 15] ===
[2025-03-21 04:47:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 0 loss:0.0427788570523262 norm:0.00044731522211804986 max memory_allocated 47470.1044921875 
[2025-03-21 04:48:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 1 loss:0.036843929439783096 norm:0.00026087992591783404 max memory_allocated 47470.1044921875 
[2025-03-21 04:49:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 2 loss:0.03259529545903206 norm:0.00018076783453579992 max memory_allocated 47470.1044921875 
[2025-03-21 04:50:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 3 loss:0.030876973643898964 norm:0.00015757758228573948 max memory_allocated 47470.1044921875 
[2025-03-21 04:51:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 4 loss:0.029594149440526962 norm:0.00013880181359127164 max memory_allocated 47470.1044921875 
[2025-03-21 04:52:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 5 loss:0.028857916593551636 norm:0.00012727093417197466 max memory_allocated 47470.1044921875 
[2025-03-21 04:53:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 6 loss:0.028470680117607117 norm:0.00012110244279028848 max memory_allocated 47470.1044921875 
[2025-03-21 04:54:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 7 loss:0.028197629377245903 norm:0.0001114740880439058 max memory_allocated 47470.1044921875 
[2025-03-21 04:55:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 8 loss:0.028024500235915184 norm:0.00010517974442336708 max memory_allocated 47470.1044921875 
[2025-03-21 04:56:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 9 loss:0.027900882065296173 norm:0.00010433320130687207 max memory_allocated 47470.1044921875 
[2025-03-21 04:56:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 10 loss:0.02777235396206379 norm:9.155306179309264e-05 max memory_allocated 47470.1044921875 
[2025-03-21 04:57:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 11 loss:0.027685226872563362 norm:9.143945499090478e-05 max memory_allocated 47470.1044921875 
[2025-03-21 04:58:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 12 loss:0.02760343998670578 norm:8.135217649396509e-05 max memory_allocated 47470.1044921875 
[2025-03-21 04:59:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 13 loss:0.02754439041018486 norm:8.807911945041269e-05 max memory_allocated 47470.1044921875 
[2025-03-21 05:00:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 14 loss:0.027521252632141113 norm:8.552722283639014e-05 max memory_allocated 47470.1044921875 
[2025-03-21 05:01:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 15 loss:0.027455922216176987 norm:8.735037408769131e-05 max memory_allocated 47470.1044921875 
[2025-03-21 05:02:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 16 loss:0.027420923113822937 norm:8.489693573210388e-05 max memory_allocated 47470.1044921875 
[2025-03-21 05:03:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 17 loss:0.027369050309062004 norm:8.079335384536535e-05 max memory_allocated 47470.1044921875 
[2025-03-21 05:04:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 18 loss:0.027334945276379585 norm:8.352698932867497e-05 max memory_allocated 47470.1044921875 
[2025-03-21 05:05:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 19 loss:0.02733640745282173 norm:8.953078940976411e-05 max memory_allocated 47470.1044921875 
[2025-03-21 05:06:57 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [14, 15]
[2025-03-21 05:06:57 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [16, 17, 18] ===
[2025-03-21 05:08:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 0 loss:0.055833183228969574 norm:0.0004179068491794169 max memory_allocated 47470.4169921875 
[2025-03-21 05:09:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 1 loss:0.051000311970710754 norm:0.00028238375671207905 max memory_allocated 47470.4169921875 
[2025-03-21 05:11:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 2 loss:0.04715951159596443 norm:0.00022223884298000485 max memory_allocated 47470.4169921875 
[2025-03-21 05:12:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 3 loss:0.04384166747331619 norm:0.00019709838670678437 max memory_allocated 47470.4169921875 
[2025-03-21 05:13:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 4 loss:0.04182412102818489 norm:0.00019056307792197913 max memory_allocated 47470.4169921875 
[2025-03-21 05:15:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 5 loss:0.04095782712101936 norm:0.00017663135076873004 max memory_allocated 47470.4169921875 
[2025-03-21 05:16:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 6 loss:0.04058244079351425 norm:0.00017098399985115975 max memory_allocated 47470.4169921875 
[2025-03-21 05:17:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 7 loss:0.040331315249204636 norm:0.00016328427591361105 max memory_allocated 47470.4169921875 
[2025-03-21 05:19:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 8 loss:0.0401688814163208 norm:0.0001600531250005588 max memory_allocated 47470.4169921875 
[2025-03-21 05:20:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 9 loss:0.04005113244056702 norm:0.00015879565034992993 max memory_allocated 47470.4169921875 
[2025-03-21 05:21:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 10 loss:0.03994143009185791 norm:0.00015595299191772938 max memory_allocated 47470.4169921875 
[2025-03-21 05:23:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 11 loss:0.03988701477646828 norm:0.00015061629528645426 max memory_allocated 47470.4169921875 
[2025-03-21 05:24:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 12 loss:0.039808183908462524 norm:0.0001459734921809286 max memory_allocated 47470.4169921875 
[2025-03-21 05:25:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 13 loss:0.03974062576889992 norm:0.00014586493489332497 max memory_allocated 47470.4169921875 
[2025-03-21 05:27:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 14 loss:0.03967224434018135 norm:0.00014443199324887246 max memory_allocated 47470.4169921875 
[2025-03-21 05:28:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 15 loss:0.03962288796901703 norm:0.00014476565411314368 max memory_allocated 47470.4169921875 
[2025-03-21 05:29:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 16 loss:0.03959356248378754 norm:0.00014550152991432697 max memory_allocated 47470.4169921875 
[2025-03-21 05:31:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 17 loss:0.03957943618297577 norm:0.00014499702956527472 max memory_allocated 47470.4169921875 
[2025-03-21 05:32:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 18 loss:0.03954080492258072 norm:0.0001443954824935645 max memory_allocated 47470.4169921875 
[2025-03-21 05:34:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 19 loss:0.03950834274291992 norm:0.00014535605441778898 max memory_allocated 47470.4169921875 
[2025-03-21 05:37:01 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [16, 17, 18]
[2025-03-21 05:37:02 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [19, 20, 21] ===
[2025-03-21 05:38:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 0 loss:0.08442826569080353 norm:0.0009640445932745934 max memory_allocated 47470.4169921875 
[2025-03-21 05:39:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 1 loss:0.07870334386825562 norm:0.0005187886999920011 max memory_allocated 47470.4169921875 
[2025-03-21 05:41:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 2 loss:0.07317793369293213 norm:0.0003765284491237253 max memory_allocated 47470.4169921875 
[2025-03-21 05:42:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 3 loss:0.06803072988986969 norm:0.00031513659632764757 max memory_allocated 47470.4169921875 
[2025-03-21 05:43:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 4 loss:0.06586775183677673 norm:0.0002842056273948401 max memory_allocated 47470.4169921875 
[2025-03-21 05:45:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 5 loss:0.06522873044013977 norm:0.00024609942920506 max memory_allocated 47470.4169921875 
[2025-03-21 05:46:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 6 loss:0.06484591960906982 norm:0.00023068379960022867 max memory_allocated 47470.4169921875 
[2025-03-21 05:47:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 7 loss:0.06461475044488907 norm:0.00023042479006107897 max memory_allocated 47470.4169921875 
[2025-03-21 05:49:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 8 loss:0.06445419043302536 norm:0.00021099840523675084 max memory_allocated 47470.4169921875 
[2025-03-21 05:50:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 9 loss:0.06426387280225754 norm:0.0001963374379556626 max memory_allocated 47470.4169921875 
[2025-03-21 05:51:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 10 loss:0.06414993107318878 norm:0.0001948091376107186 max memory_allocated 47470.4169921875 
[2025-03-21 05:53:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 11 loss:0.06402295082807541 norm:0.00018735424964688718 max memory_allocated 47470.4169921875 
[2025-03-21 05:54:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 12 loss:0.0639372318983078 norm:0.00018320992239750922 max memory_allocated 47470.4169921875 
[2025-03-21 05:56:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 13 loss:0.06387568265199661 norm:0.00018630502745509148 max memory_allocated 47470.4169921875 
[2025-03-21 05:57:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 14 loss:0.06381704658269882 norm:0.00018047523917630315 max memory_allocated 47470.4169921875 
[2025-03-21 05:58:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 15 loss:0.06376372277736664 norm:0.00018467739573679864 max memory_allocated 47470.4169921875 
[2025-03-21 06:00:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 16 loss:0.06370477378368378 norm:0.00018504842591937631 max memory_allocated 47470.4169921875 
[2025-03-21 06:01:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 17 loss:0.06368479132652283 norm:0.00018958456348627806 max memory_allocated 47470.4169921875 
[2025-03-21 06:02:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 18 loss:0.06367961317300797 norm:0.0001971102028619498 max memory_allocated 47470.4169921875 
[2025-03-21 06:04:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 19 loss:0.06367170810699463 norm:0.0001993606420001015 max memory_allocated 47470.4169921875 
[2025-03-21 06:06:34 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [19, 20, 21]
[2025-03-21 06:06:34 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [22, 23, 24] ===
[2025-03-21 06:08:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 0 loss:0.12164458632469177 norm:0.0006592865684069693 max memory_allocated 47470.4169921875 
[2025-03-21 06:09:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 1 loss:0.1156269833445549 norm:0.00045715656597167253 max memory_allocated 47470.4169921875 
[2025-03-21 06:10:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 2 loss:0.1083918958902359 norm:0.00037800066638737917 max memory_allocated 47470.4169921875 
[2025-03-21 06:12:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 3 loss:0.10215245187282562 norm:0.0003455565311014652 max memory_allocated 47470.4169921875 
[2025-03-21 06:13:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 4 loss:0.10060249269008636 norm:0.0003299226227682084 max memory_allocated 47470.4169921875 
[2025-03-21 06:14:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 5 loss:0.09996587038040161 norm:0.0003082432085648179 max memory_allocated 47470.4169921875 
[2025-03-21 06:16:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 6 loss:0.09966208040714264 norm:0.0002827106509357691 max memory_allocated 47470.4169921875 
[2025-03-21 06:17:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 7 loss:0.09936562925577164 norm:0.0002637431025505066 max memory_allocated 47470.4169921875 
[2025-03-21 06:18:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 8 loss:0.09913752228021622 norm:0.00026661274023354053 max memory_allocated 47470.4169921875 
[2025-03-21 06:20:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 9 loss:0.09902610629796982 norm:0.0002628242946229875 max memory_allocated 47470.4169921875 
[2025-03-21 06:21:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 10 loss:0.09889090061187744 norm:0.00025805406039580703 max memory_allocated 47470.4169921875 
[2025-03-21 06:22:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 11 loss:0.09877065569162369 norm:0.00025862461188808084 max memory_allocated 47470.4169921875 
[2025-03-21 06:24:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 12 loss:0.09872647374868393 norm:0.00026749528478831053 max memory_allocated 47470.4169921875 
[2025-03-21 06:25:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 13 loss:0.098649762570858 norm:0.0002615316479932517 max memory_allocated 47470.4169921875 
[2025-03-21 06:26:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 14 loss:0.09859660267829895 norm:0.0002538177650421858 max memory_allocated 47470.4169921875 
[2025-03-21 06:28:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 15 loss:0.09854793548583984 norm:0.0002585737966001034 max memory_allocated 47470.4169921875 
[2025-03-21 06:29:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 16 loss:0.09848520159721375 norm:0.00023727724328637123 max memory_allocated 47470.4169921875 
[2025-03-21 06:30:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 17 loss:0.09845959395170212 norm:0.0002485004661139101 max memory_allocated 47470.4169921875 
[2025-03-21 06:32:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 18 loss:0.0983925387263298 norm:0.00023649299691896886 max memory_allocated 47470.4169921875 
[2025-03-21 06:33:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 19 loss:0.09837322682142258 norm:0.0002545815077610314 max memory_allocated 47470.4169921875 
[2025-03-21 06:36:50 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [22, 23, 24]
[2025-03-21 06:36:50 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [25, 26] ===
[2025-03-21 06:37:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 0 loss:0.1412808746099472 norm:0.0005574722890742123 max memory_allocated 47470.4169921875 
[2025-03-21 06:38:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 1 loss:0.1364310085773468 norm:0.00036774613545276225 max memory_allocated 47470.4169921875 
[2025-03-21 06:39:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 2 loss:0.1297789067029953 norm:0.0002933296491391957 max memory_allocated 47470.4169921875 
[2025-03-21 06:40:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 3 loss:0.1256570816040039 norm:0.0002399553486611694 max memory_allocated 47470.4169921875 
[2025-03-21 06:41:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 4 loss:0.1250910609960556 norm:0.00022545820684172213 max memory_allocated 47470.4169921875 
[2025-03-21 06:42:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 5 loss:0.12475816905498505 norm:0.00020886374113615602 max memory_allocated 47470.4169921875 
[2025-03-21 06:43:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 6 loss:0.12447948753833771 norm:0.00018517971329856664 max memory_allocated 47470.4169921875 
[2025-03-21 06:44:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 7 loss:0.12436100840568542 norm:0.0002118345000781119 max memory_allocated 47470.4169921875 
[2025-03-21 06:45:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 8 loss:0.12417095899581909 norm:0.0001747959468048066 max memory_allocated 47470.4169921875 
[2025-03-21 06:45:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 9 loss:0.124078668653965 norm:0.00016270557534880936 max memory_allocated 47470.4169921875 
[2025-03-21 06:46:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 10 loss:0.12401522696018219 norm:0.00017969075997825712 max memory_allocated 47470.4169921875 
[2025-03-21 06:47:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 11 loss:0.12393294274806976 norm:0.00016899245383683592 max memory_allocated 47470.4169921875 
[2025-03-21 06:48:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 12 loss:0.12384208291769028 norm:0.00016974270693026483 max memory_allocated 47470.4169921875 
[2025-03-21 06:49:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 13 loss:0.12379702925682068 norm:0.00017138590919785202 max memory_allocated 47470.4169921875 
[2025-03-21 06:50:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 14 loss:0.12374310195446014 norm:0.00016999688523355871 max memory_allocated 47470.4169921875 
[2025-03-21 06:51:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 15 loss:0.12371599674224854 norm:0.00017013811157085001 max memory_allocated 47470.4169921875 
[2025-03-21 06:52:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 16 loss:0.12367967516183853 norm:0.00018758235091809183 max memory_allocated 47470.4169921875 
[2025-03-21 06:53:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 17 loss:0.12366552650928497 norm:0.00018192750576417893 max memory_allocated 47470.4169921875 
[2025-03-21 06:54:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 18 loss:0.12363067269325256 norm:0.0001689564378466457 max memory_allocated 47470.4169921875 
[2025-03-21 06:54:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 19 loss:0.12364254146814346 norm:0.00018842263671103865 max memory_allocated 47470.4169921875 
[2025-03-21 06:56:59 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [25, 26]
[2025-03-21 06:56:59 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [27] ===
[2025-03-21 06:57:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 0 loss:0.14481554925441742 norm:0.0008094415534287691 max memory_allocated 47470.4169921875 
[2025-03-21 06:57:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 1 loss:0.1418761909008026 norm:0.00045643901103176177 max memory_allocated 47470.4169921875 
[2025-03-21 06:58:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 2 loss:0.13792908191680908 norm:0.0002948905748780817 max memory_allocated 47470.4169921875 
[2025-03-21 06:58:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 3 loss:0.13589172065258026 norm:0.00022189713490661234 max memory_allocated 47470.4169921875 
[2025-03-21 06:59:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 4 loss:0.13565009832382202 norm:0.00022341558360494673 max memory_allocated 47470.4169921875 
[2025-03-21 06:59:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 5 loss:0.13553816080093384 norm:0.00018744858971331269 max memory_allocated 47470.4169921875 
[2025-03-21 07:00:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 6 loss:0.1354503035545349 norm:0.00016402530309278518 max memory_allocated 47470.4169921875 
[2025-03-21 07:00:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 7 loss:0.13536939024925232 norm:0.00014931503392290324 max memory_allocated 47470.4169921875 
[2025-03-21 07:01:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 8 loss:0.1353168785572052 norm:0.0001354877167614177 max memory_allocated 47470.4169921875 
[2025-03-21 07:01:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 9 loss:0.13527736067771912 norm:0.00013405713252723217 max memory_allocated 47470.4169921875 
[2025-03-21 07:02:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 10 loss:0.1352250576019287 norm:0.0001249687047675252 max memory_allocated 47470.4169921875 
[2025-03-21 07:02:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 11 loss:0.1351872831583023 norm:0.00011464278941275552 max memory_allocated 47470.4169921875 
[2025-03-21 07:02:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 12 loss:0.13517507910728455 norm:0.00011241488391533494 max memory_allocated 47470.4169921875 
[2025-03-21 07:03:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 13 loss:0.13515283167362213 norm:0.00011422846728237346 max memory_allocated 47470.4169921875 
[2025-03-21 07:03:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 14 loss:0.135134756565094 norm:0.00011493869533296674 max memory_allocated 47470.4169921875 
[2025-03-21 07:04:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 15 loss:0.1351085603237152 norm:0.00010704681335482746 max memory_allocated 47470.4169921875 
[2025-03-21 07:04:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 16 loss:0.13510338962078094 norm:0.00011054641072405502 max memory_allocated 47470.4169921875 
[2025-03-21 07:05:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 17 loss:0.13509270548820496 norm:0.00011043748236261308 max memory_allocated 47470.4169921875 
[2025-03-21 07:05:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 18 loss:0.1350647509098053 norm:0.00012081626482540742 max memory_allocated 47470.4169921875 
[2025-03-21 07:06:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 19 loss:0.1350664645433426 norm:0.00011119123519165441 max memory_allocated 47470.4169921875 
[2025-03-21 07:07:04 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [27]
[2025-03-21 07:07:04 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [28] ===
[2025-03-21 07:07:04 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 07:07:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 0 loss:0.16918957233428955 norm:0.006104136351495981 max memory_allocated 47470.4169921875 
[2025-03-21 07:08:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 1 loss:0.16490139067173004 norm:0.004910002928227186 max memory_allocated 47470.4169921875 
[2025-03-21 07:08:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 2 loss:0.159956693649292 norm:0.0038615844678133726 max memory_allocated 47470.4169921875 
[2025-03-21 07:08:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 3 loss:0.15785866975784302 norm:0.003246777690947056 max memory_allocated 47470.4169921875 
[2025-03-21 07:09:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 4 loss:0.1574375182390213 norm:0.0029337084852159023 max memory_allocated 47470.4169921875 
[2025-03-21 07:09:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 5 loss:0.15718160569667816 norm:0.0025466708466410637 max memory_allocated 47470.4169921875 
[2025-03-21 07:10:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 6 loss:0.156905859708786 norm:0.0023037891369313 max memory_allocated 47470.4169921875 
[2025-03-21 07:10:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 7 loss:0.15683618187904358 norm:0.0020165937021374702 max memory_allocated 47470.4169921875 
[2025-03-21 07:11:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 8 loss:0.15675637125968933 norm:0.0020572661887854338 max memory_allocated 47470.4169921875 
[2025-03-21 07:11:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 9 loss:0.15677078068256378 norm:0.0018625144148245454 max memory_allocated 47470.4169921875 
[2025-03-21 07:12:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 10 loss:0.15659065544605255 norm:0.0017444787081331015 max memory_allocated 47470.4169921875 
[2025-03-21 07:12:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 11 loss:0.15647390484809875 norm:0.0017758816247805953 max memory_allocated 47470.4169921875 
[2025-03-21 07:13:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 12 loss:0.15646414458751678 norm:0.0016000655014067888 max memory_allocated 47470.4169921875 
[2025-03-21 07:13:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 13 loss:0.15638454258441925 norm:0.0016733714146539569 max memory_allocated 47470.4169921875 
[2025-03-21 07:13:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 14 loss:0.1564914584159851 norm:0.001403042464517057 max memory_allocated 47470.4169921875 
[2025-03-21 07:14:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 15 loss:0.15630963444709778 norm:0.0014534953515976667 max memory_allocated 47470.4169921875 
[2025-03-21 07:14:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 16 loss:0.15624460577964783 norm:0.0015301929088309407 max memory_allocated 47470.4169921875 
[2025-03-21 07:15:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 17 loss:0.1562803089618683 norm:0.0014184240717440844 max memory_allocated 47470.4169921875 
[2025-03-21 07:15:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 18 loss:0.15620723366737366 norm:0.0014896327629685402 max memory_allocated 47470.4169921875 
[2025-03-21 07:16:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 19 loss:0.1562870740890503 norm:0.0013803348410874605 max memory_allocated 47470.4169921875 
[2025-03-21 07:17:10 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [28]
[2025-03-21 07:17:10 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [29] ===
[2025-03-21 07:17:11 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 07:17:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 0 loss:0.19516032934188843 norm:0.006017183419317007 max memory_allocated 47470.4169921875 
[2025-03-21 07:18:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 1 loss:0.1905977874994278 norm:0.004653572104871273 max memory_allocated 47470.4169921875 
[2025-03-21 07:18:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 2 loss:0.18538948893547058 norm:0.003691913327202201 max memory_allocated 47470.4169921875 
[2025-03-21 07:19:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 3 loss:0.18346361815929413 norm:0.00312318024225533 max memory_allocated 47470.4169921875 
[2025-03-21 07:19:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 4 loss:0.18312329053878784 norm:0.002993213012814522 max memory_allocated 47470.4169921875 
[2025-03-21 07:19:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 5 loss:0.18289361894130707 norm:0.002439172938466072 max memory_allocated 47470.4169921875 
[2025-03-21 07:20:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 6 loss:0.18268990516662598 norm:0.002375249518081546 max memory_allocated 47470.4169921875 
[2025-03-21 07:20:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 7 loss:0.18264035880565643 norm:0.0021543349139392376 max memory_allocated 47470.4169921875 
[2025-03-21 07:21:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 8 loss:0.18247002363204956 norm:0.001902397838421166 max memory_allocated 47470.4169921875 
[2025-03-21 07:21:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 9 loss:0.18230722844600677 norm:0.0018942700698971748 max memory_allocated 47470.4169921875 
[2025-03-21 07:22:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 10 loss:0.18230655789375305 norm:0.0016661732224747539 max memory_allocated 47470.4169921875 
[2025-03-21 07:22:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 11 loss:0.1822061836719513 norm:0.0017558159306645393 max memory_allocated 47470.4169921875 
[2025-03-21 07:23:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 12 loss:0.18236485123634338 norm:0.0013000753242522478 max memory_allocated 47470.4169921875 
[2025-03-21 07:23:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 13 loss:0.18225237727165222 norm:0.0015680146170780063 max memory_allocated 47470.4169921875 
[2025-03-21 07:24:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 14 loss:0.18216615915298462 norm:0.001512995338998735 max memory_allocated 47470.4169921875 
[2025-03-21 07:24:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 15 loss:0.18210630118846893 norm:0.001509390422143042 max memory_allocated 47470.4169921875 
[2025-03-21 07:24:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 16 loss:0.18202848732471466 norm:0.001485685002990067 max memory_allocated 47470.4169921875 
[2025-03-21 07:25:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 17 loss:0.1821143478155136 norm:0.0014484624844044447 max memory_allocated 47470.4169921875 
[2025-03-21 07:25:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 18 loss:0.18202781677246094 norm:0.0014629174256697297 max memory_allocated 47470.4169921875 
[2025-03-21 07:26:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 19 loss:0.1821502447128296 norm:0.001357685076072812 max memory_allocated 47470.4169921875 
[2025-03-21 07:27:12 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [29]
[2025-03-21 07:27:12 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [30] ===
[2025-03-21 07:27:13 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 07:27:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 0 loss:0.24787375330924988 norm:0.010532071813941002 max memory_allocated 47470.4169921875 
[2025-03-21 07:28:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 1 loss:0.24038857221603394 norm:0.007783627137541771 max memory_allocated 47470.4169921875 
[2025-03-21 07:28:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 2 loss:0.2320624589920044 norm:0.00771002471446991 max memory_allocated 47470.4169921875 
[2025-03-21 07:29:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 3 loss:0.22937150299549103 norm:0.005758629180490971 max memory_allocated 47470.4169921875 
[2025-03-21 07:29:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 4 loss:0.22844457626342773 norm:0.005628556478768587 max memory_allocated 47470.4169921875 
[2025-03-21 07:29:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 5 loss:0.2276800274848938 norm:0.004437252879142761 max memory_allocated 47470.4169921875 
[2025-03-21 07:30:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 6 loss:0.22743268311023712 norm:0.004082143306732178 max memory_allocated 47470.4169921875 
[2025-03-21 07:30:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 7 loss:0.22718298435211182 norm:0.003837211523205042 max memory_allocated 47470.4169921875 
[2025-03-21 07:31:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 8 loss:0.22688589990139008 norm:0.0035442651715129614 max memory_allocated 47470.4169921875 
[2025-03-21 07:31:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 9 loss:0.22678981721401215 norm:0.0036181588657200336 max memory_allocated 47470.4169921875 
[2025-03-21 07:32:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 10 loss:0.22651755809783936 norm:0.003204495646059513 max memory_allocated 47470.4169921875 
[2025-03-21 07:32:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 11 loss:0.22636081278324127 norm:0.003024136880412698 max memory_allocated 47470.4169921875 
[2025-03-21 07:33:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 12 loss:0.22605392336845398 norm:0.0029369182884693146 max memory_allocated 47470.4169921875 
[2025-03-21 07:33:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 13 loss:0.2261338084936142 norm:0.0028121976647526026 max memory_allocated 47470.4169921875 
[2025-03-21 07:34:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 14 loss:0.22595453262329102 norm:0.002801699796691537 max memory_allocated 47470.4169921875 
[2025-03-21 07:34:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 15 loss:0.22600117325782776 norm:0.0024113799445331097 max memory_allocated 47470.4169921875 
[2025-03-21 07:35:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 16 loss:0.22588036954402924 norm:0.0027243569493293762 max memory_allocated 47470.4169921875 
[2025-03-21 07:35:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 17 loss:0.22585554420948029 norm:0.00246071326546371 max memory_allocated 47470.4169921875 
[2025-03-21 07:35:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 18 loss:0.22582828998565674 norm:0.0025855926796793938 max memory_allocated 47470.4169921875 
[2025-03-21 07:36:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 19 loss:0.22580432891845703 norm:0.002471448387950659 max memory_allocated 47470.4169921875 
[2025-03-21 07:37:13 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [30]
[2025-03-21 07:37:13 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [31] ===
[2025-03-21 07:37:13 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 07:37:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 0 loss:0.4052203893661499 norm:0.03019200637936592 max memory_allocated 47470.4169921875 
[2025-03-21 07:38:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 1 loss:0.3827112913131714 norm:0.021540286019444466 max memory_allocated 47470.4169921875 
[2025-03-21 07:38:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 2 loss:0.36899465322494507 norm:0.016511140391230583 max memory_allocated 47470.4169921875 
[2025-03-21 07:39:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 3 loss:0.36293718218803406 norm:0.013624317944049835 max memory_allocated 47470.4169921875 
[2025-03-21 07:39:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 4 loss:0.35995298624038696 norm:0.011615952476859093 max memory_allocated 47470.4169921875 
[2025-03-21 07:40:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 5 loss:0.3582844138145447 norm:0.010766527615487576 max memory_allocated 47470.4169921875 
[2025-03-21 07:40:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 6 loss:0.3572816252708435 norm:0.009423407725989819 max memory_allocated 47470.4169921875 
[2025-03-21 07:40:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 7 loss:0.35638558864593506 norm:0.008671091869473457 max memory_allocated 47470.4169921875 
[2025-03-21 07:41:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 8 loss:0.356150358915329 norm:0.008472740650177002 max memory_allocated 47470.4169921875 
[2025-03-21 07:41:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 9 loss:0.3557915687561035 norm:0.008139286190271378 max memory_allocated 47470.4169921875 
[2025-03-21 07:42:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 10 loss:0.3551631271839142 norm:0.008063254877924919 max memory_allocated 47470.4169921875 
[2025-03-21 07:42:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 11 loss:0.354873389005661 norm:0.008294054307043552 max memory_allocated 47470.4169921875 
[2025-03-21 07:43:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 12 loss:0.35515886545181274 norm:0.008874448016285896 max memory_allocated 47470.4169921875 
[2025-03-21 07:43:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 13 loss:0.35498300194740295 norm:0.008656172081828117 max memory_allocated 47470.4169921875 
[2025-03-21 07:44:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 14 loss:0.3551393449306488 norm:0.008867144584655762 max memory_allocated 47470.4169921875 
[2025-03-21 07:44:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 15 loss:0.35488656163215637 norm:0.008555621840059757 max memory_allocated 47470.4169921875 
[2025-03-21 07:45:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 16 loss:0.3552270531654358 norm:0.009083878248929977 max memory_allocated 47470.4169921875 
[2025-03-21 07:45:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 17 loss:0.3550226092338562 norm:0.008029435761272907 max memory_allocated 47470.4169921875 
[2025-03-21 07:45:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 18 loss:0.35455322265625 norm:0.008050435222685337 max memory_allocated 47470.4169921875 
[2025-03-21 07:46:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 19 loss:0.3542490005493164 norm:0.007741271983832121 max memory_allocated 47470.4169921875 
[2025-03-21 07:47:23 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 15, block: [31]
[2025-03-21 07:47:24 root] (main_calib_config3_attn.py 379): INFO 19107.528581142426
[2025-03-21 07:47:44 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-21 07:48:31 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.767379283905029
[2025-03-21 07:48:31 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-21 07:49:42 root] (main_calib_config3_attn.py 161): INFO c4 : 7.204414367675781
[2025-03-21 08:52:52 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.767379283905029, 'c4': 7.204414367675781, 'results': {'winogrande': {'acc': 0.6716653512233622, 'acc_stderr': 0.01319829944971789}, 'piqa': {'acc': 0.779107725788901, 'acc_stderr': 0.009679088048842217, 'acc_norm': 0.7731229597388466, 'acc_norm_stderr': 0.009771584259215179}, 'arc_challenge': {'acc': 0.3796928327645051, 'acc_stderr': 0.014182119866974872, 'acc_norm': 0.4035836177474403, 'acc_norm_stderr': 0.014337158914268447}, 'arc_easy': {'acc': 0.664983164983165, 'acc_stderr': 0.009685160765932361, 'acc_norm': 0.5189393939393939, 'acc_norm_stderr': 0.010252420496894496}, 'boolq': {'acc': 0.7302752293577982, 'acc_stderr': 0.007762403976363497}, 'hellaswag': {'acc': 0.5595498904600678, 'acc_stderr': 0.004954265595373456, 'acc_norm': 0.7219677355108544, 'acc_norm_stderr': 0.004471137333619624}}, 'versions': {'winogrande': 0, 'piqa': 0, 'arc_challenge': 0, 'arc_easy': 0, 'boolq': 1, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-21 08:52:52 root] (main_calib_config3_attn.py 175): INFO 37.97,66.50,73.03,55.95,77.91,67.17
