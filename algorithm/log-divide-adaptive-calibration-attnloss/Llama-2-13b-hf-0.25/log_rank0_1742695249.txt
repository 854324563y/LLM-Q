[2025-03-23 02:00:49 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/Llama-2-13b-hf-0.25', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/Llama-2-13b-hf/quant_map_Llama-2-13b-hf_0.25.pkl', blocks_pkl='./log-divide/Llama-2-13b-hf-w4a4/Llama-2-13b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-23 02:00:52 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-23 02:00:53 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-23 02:00:53 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-23 02:00:53 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/Llama-2-13b-hf/quant_map_Llama-2-13b-hf_0.25.pkl
[2025-03-23 02:00:53 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/Llama-2-13b-hf-w4a4/Llama-2-13b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 9), (9, 12), (12, 15), (15, 18), (18, 21), (21, 24), (24, 27), (27, 30), (30, 33), (33, 36), (36, 37), (37, 38), (38, 39), (39, 40)]
[2025-03-23 02:00:53 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17], [18, 19, 20], [21, 22, 23], [24, 25, 26], [27, 28, 29], [30, 31, 32], [33, 34, 35], [36], [37], [38], [39]]
[2025-03-23 02:01:05 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-23 02:01:06 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:01:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.010323721915483475 norm:0.014946574345231056 max memory_allocated 44358.7939453125 
[2025-03-23 02:02:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.005699658766388893 norm:0.008258647285401821 max memory_allocated 44358.7939453125 
[2025-03-23 02:03:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.003884692210704088 norm:0.005237219389528036 max memory_allocated 44358.7939453125 
[2025-03-23 02:04:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0032581498380750418 norm:0.004073811694979668 max memory_allocated 44358.7939453125 
[2025-03-23 02:04:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.0030715144239366055 norm:0.0035019423812627792 max memory_allocated 44358.7939453125 
[2025-03-23 02:05:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0028920730110257864 norm:0.002865179907530546 max memory_allocated 44358.7939453125 
[2025-03-23 02:06:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.002874556463211775 norm:0.002493254840373993 max memory_allocated 44358.7939453125 
[2025-03-23 02:06:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.002789960475638509 norm:0.0020435629412531853 max memory_allocated 44358.7939453125 
[2025-03-23 02:07:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.00268835318274796 norm:0.0018256905023008585 max memory_allocated 44358.7939453125 
[2025-03-23 02:08:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.0025748922489583492 norm:0.0016920724883675575 max memory_allocated 44358.7939453125 
[2025-03-23 02:09:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.0026289094239473343 norm:0.0015630859415978193 max memory_allocated 44358.7939453125 
[2025-03-23 02:09:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0025102468207478523 norm:0.0015044466126710176 max memory_allocated 44358.7939453125 
[2025-03-23 02:10:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.002550604287534952 norm:0.0013970572035759687 max memory_allocated 44358.7939453125 
[2025-03-23 02:11:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.0025005959905683994 norm:0.0012865593889728189 max memory_allocated 44358.7939453125 
[2025-03-23 02:11:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0025623603723943233 norm:0.0013490616111084819 max memory_allocated 44358.7939453125 
[2025-03-23 02:12:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.002429389161989093 norm:0.0011444888077676296 max memory_allocated 44358.7939453125 
[2025-03-23 02:13:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.002466819016262889 norm:0.0011540588457137346 max memory_allocated 44358.7939453125 
[2025-03-23 02:14:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.00246571097522974 norm:0.0011418343055993319 max memory_allocated 44358.7939453125 
[2025-03-23 02:14:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.002463587326928973 norm:0.0011645155027508736 max memory_allocated 44358.7939453125 
[2025-03-23 02:15:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0024297861382365227 norm:0.000997025053948164 max memory_allocated 44358.7939453125 
[2025-03-23 02:16:27 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-23 02:16:27 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-23 02:16:30 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:17:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.023174772039055824 norm:0.014597122557461262 max memory_allocated 44358.7939453125 
[2025-03-23 02:17:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.01426510140299797 norm:0.008723451755940914 max memory_allocated 44358.7939453125 
[2025-03-23 02:18:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.010302052833139896 norm:0.005979773588478565 max memory_allocated 44358.7939453125 
[2025-03-23 02:19:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.009157965891063213 norm:0.00479549914598465 max memory_allocated 44358.7939453125 
[2025-03-23 02:20:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.008694926276803017 norm:0.0040064649656414986 max memory_allocated 44358.7939453125 
[2025-03-23 02:20:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.008436834439635277 norm:0.0034612405579537153 max memory_allocated 44358.7939453125 
[2025-03-23 02:21:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.008240675553679466 norm:0.0030728676356375217 max memory_allocated 44358.7939453125 
[2025-03-23 02:22:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.00813225843012333 norm:0.0026813477743417025 max memory_allocated 44358.7939453125 
[2025-03-23 02:23:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.00799476820975542 norm:0.002302328823134303 max memory_allocated 44358.7939453125 
[2025-03-23 02:23:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.007842025719583035 norm:0.001994355348870158 max memory_allocated 44358.7939453125 
[2025-03-23 02:24:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.007722079288214445 norm:0.0017214968102052808 max memory_allocated 44358.7939453125 
[2025-03-23 02:25:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.007632075343281031 norm:0.001562691992148757 max memory_allocated 44358.7939453125 
[2025-03-23 02:25:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.0076034655794501305 norm:0.001489682705141604 max memory_allocated 44358.7939453125 
[2025-03-23 02:26:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.007583592087030411 norm:0.0015626555541530252 max memory_allocated 44358.7939453125 
[2025-03-23 02:27:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.007636805064976215 norm:0.0014097276143729687 max memory_allocated 44358.7939453125 
[2025-03-23 02:28:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.007521842606365681 norm:0.0013956823386251926 max memory_allocated 44358.7939453125 
[2025-03-23 02:28:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.007569389883428812 norm:0.0012344273272901773 max memory_allocated 44358.7939453125 
[2025-03-23 02:29:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.007467172108590603 norm:0.0012915923725813627 max memory_allocated 44358.7939453125 
[2025-03-23 02:30:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.00754505442455411 norm:0.0011837572092190385 max memory_allocated 44358.7939453125 
[2025-03-23 02:30:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.007440377492457628 norm:0.0012108136434108019 max memory_allocated 44358.7939453125 
[2025-03-23 02:31:51 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-23 02:31:51 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
[2025-03-23 02:31:54 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:32:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.029852252453565598 norm:0.014195245690643787 max memory_allocated 44358.7939453125 
[2025-03-23 02:33:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.02051466889679432 norm:0.009242987260222435 max memory_allocated 44358.7939453125 
[2025-03-23 02:34:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.01570083387196064 norm:0.006097255274653435 max memory_allocated 44358.7939453125 
[2025-03-23 02:34:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.014247966930270195 norm:0.004842390771955252 max memory_allocated 44358.7939453125 
[2025-03-23 02:35:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.013643987476825714 norm:0.003970078658312559 max memory_allocated 44358.7939453125 
[2025-03-23 02:36:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.013236429542303085 norm:0.00343128084205091 max memory_allocated 44358.7939453125 
[2025-03-23 02:36:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.012895829044282436 norm:0.0029997588135302067 max memory_allocated 44358.7939453125 
[2025-03-23 02:37:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.012623484246432781 norm:0.002647638786584139 max memory_allocated 44358.7939453125 
[2025-03-23 02:38:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.01243017241358757 norm:0.002315588528290391 max memory_allocated 44358.7939453125 
[2025-03-23 02:39:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.012293829582631588 norm:0.0019637099467217922 max memory_allocated 44358.7939453125 
[2025-03-23 02:39:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.012182499282062054 norm:0.001689218683168292 max memory_allocated 44358.7939453125 
[2025-03-23 02:40:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.012194829061627388 norm:0.0016289004124701023 max memory_allocated 44358.7939453125 
[2025-03-23 02:41:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.012157576158642769 norm:0.0016141985543072224 max memory_allocated 44358.7939453125 
[2025-03-23 02:41:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.012149511836469173 norm:0.0015849788906052709 max memory_allocated 44358.7939453125 
[2025-03-23 02:42:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.012143515050411224 norm:0.0015529196243733168 max memory_allocated 44358.7939453125 
[2025-03-23 02:43:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.012163687497377396 norm:0.0015619844198226929 max memory_allocated 44358.7939453125 
[2025-03-23 02:44:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.01216251403093338 norm:0.0013411547988653183 max memory_allocated 44358.7939453125 
[2025-03-23 02:44:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.012109652161598206 norm:0.0013820065651088953 max memory_allocated 44358.7939453125 
[2025-03-23 02:45:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.012057051062583923 norm:0.0012276350753381848 max memory_allocated 44358.7939453125 
[2025-03-23 02:46:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.012067765928804874 norm:0.0012922545429319143 max memory_allocated 44358.7939453125 
[2025-03-23 02:47:13 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2]
[2025-03-23 02:47:13 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4, 5] ===
[2025-03-23 02:49:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 0 loss:0.08137516677379608 norm:0.004112974274903536 max memory_allocated 62759.0654296875 
[2025-03-23 02:52:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 1 loss:0.05505110323429108 norm:0.021297985687851906 max memory_allocated 62759.0654296875 
[2025-03-23 02:54:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 2 loss:0.04120941087603569 norm:0.00628896476700902 max memory_allocated 62759.0654296875 
[2025-03-23 02:56:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 3 loss:0.03596086800098419 norm:0.001580437645316124 max memory_allocated 62759.0654296875 
[2025-03-23 02:58:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 4 loss:0.03371936455368996 norm:0.00167197291739285 max memory_allocated 62759.0654296875 
[2025-03-23 03:00:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 5 loss:0.03230646997690201 norm:0.002002925146371126 max memory_allocated 62759.0654296875 
[2025-03-23 03:02:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 6 loss:0.03130955249071121 norm:0.002358009573072195 max memory_allocated 62759.0654296875 
[2025-03-23 03:04:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 7 loss:0.03018665686249733 norm:0.0011644965270534158 max memory_allocated 62759.0654296875 
[2025-03-23 03:06:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 8 loss:0.030063191428780556 norm:0.0020158516708761454 max memory_allocated 62759.0654296875 
[2025-03-23 03:09:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 9 loss:0.028980981558561325 norm:0.0010356539860367775 max memory_allocated 62759.0654296875 
[2025-03-23 03:11:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 10 loss:0.028297215700149536 norm:0.0010010928381234407 max memory_allocated 62759.0654296875 
[2025-03-23 03:13:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 11 loss:0.028025425970554352 norm:0.0009809761540964246 max memory_allocated 62759.0654296875 
[2025-03-23 03:15:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 12 loss:0.027884384617209435 norm:0.0008470249595120549 max memory_allocated 62759.0654296875 
[2025-03-23 03:17:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 13 loss:0.02852129563689232 norm:0.0010943915694952011 max memory_allocated 62759.0654296875 
[2025-03-23 03:19:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 14 loss:0.027844984084367752 norm:0.0010348857613280416 max memory_allocated 62759.0654296875 
[2025-03-23 03:21:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 15 loss:0.027918828651309013 norm:0.0009689056314527988 max memory_allocated 62759.0654296875 
[2025-03-23 03:23:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 16 loss:0.028153039515018463 norm:0.0011014931369572878 max memory_allocated 62759.0654296875 
[2025-03-23 03:26:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 17 loss:0.02791959047317505 norm:0.0008493795758113265 max memory_allocated 62759.0654296875 
[2025-03-23 03:28:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 18 loss:0.027986660599708557 norm:0.000960670760832727 max memory_allocated 62759.0654296875 
[2025-03-23 03:30:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 19 loss:0.027876798063516617 norm:0.0009214110905304551 max memory_allocated 62759.0654296875 
[2025-03-23 03:33:17 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [3, 4, 5]
[2025-03-23 03:33:17 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [6, 7, 8] ===
[2025-03-23 03:35:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 0 loss:0.08382347971200943 norm:0.0016570196021348238 max memory_allocated 62759.0654296875 
[2025-03-23 03:37:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 1 loss:0.05951090529561043 norm:0.0006629070267081261 max memory_allocated 62759.0654296875 
[2025-03-23 03:39:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 2 loss:0.04677898436784744 norm:0.00043121108319610357 max memory_allocated 62759.0654296875 
[2025-03-23 03:42:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 3 loss:0.04110366106033325 norm:0.0003468839277047664 max memory_allocated 62759.0654296875 
[2025-03-23 03:44:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 4 loss:0.03818041458725929 norm:0.000311297073494643 max memory_allocated 62759.0654296875 
[2025-03-23 03:46:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 5 loss:0.036250896751880646 norm:0.0002818330249283463 max memory_allocated 62759.0654296875 
[2025-03-23 03:48:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 6 loss:0.03488792106509209 norm:0.00027725458494387567 max memory_allocated 62759.0654296875 
[2025-03-23 03:50:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 7 loss:0.033926691859960556 norm:0.00026049138978123665 max memory_allocated 62759.0654296875 
[2025-03-23 03:52:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 8 loss:0.03327295184135437 norm:0.00025566789554432034 max memory_allocated 62759.0654296875 
[2025-03-23 03:54:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 9 loss:0.03277302533388138 norm:0.00025120278587564826 max memory_allocated 62759.0654296875 
[2025-03-23 03:57:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 10 loss:0.03245930373668671 norm:0.0002477300586178899 max memory_allocated 62759.0654296875 
[2025-03-23 03:59:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 11 loss:0.03220818191766739 norm:0.00024861309793777764 max memory_allocated 62759.0654296875 
[2025-03-23 04:01:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 12 loss:0.032006677240133286 norm:0.00025802815798670053 max memory_allocated 62759.0654296875 
[2025-03-23 04:03:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 13 loss:0.03183867409825325 norm:0.0002541017020121217 max memory_allocated 62759.0654296875 
[2025-03-23 04:05:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 14 loss:0.03171771019697189 norm:0.00025491893757134676 max memory_allocated 62759.0654296875 
[2025-03-23 04:07:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 15 loss:0.03161177411675453 norm:0.00024343703989870846 max memory_allocated 62759.0654296875 
[2025-03-23 04:09:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 16 loss:0.031495314091444016 norm:0.0002465146535541862 max memory_allocated 62759.0654296875 
[2025-03-23 04:11:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 17 loss:0.03142261877655983 norm:0.00023917786893434823 max memory_allocated 62759.0654296875 
[2025-03-23 04:14:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 18 loss:0.031363219022750854 norm:0.0002378751232754439 max memory_allocated 62759.0654296875 
[2025-03-23 04:16:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 19 loss:0.03132147341966629 norm:0.00023630402574781328 max memory_allocated 62759.0654296875 
[2025-03-23 04:19:06 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [6, 7, 8]
[2025-03-23 04:19:06 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [9, 10, 11] ===
[2025-03-23 04:21:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 0 loss:0.11832799017429352 norm:0.0035034073516726494 max memory_allocated 62760.5341796875 
[2025-03-23 04:23:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 1 loss:0.07989618182182312 norm:0.001399251283146441 max memory_allocated 62760.5341796875 
[2025-03-23 04:25:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 2 loss:0.06130297854542732 norm:0.0007489406270906329 max memory_allocated 62760.5341796875 
[2025-03-23 04:27:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 3 loss:0.052872832864522934 norm:0.0005189223447814584 max memory_allocated 62760.5341796875 
[2025-03-23 04:30:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 4 loss:0.048847272992134094 norm:0.0004293799283914268 max memory_allocated 62760.5341796875 
[2025-03-23 04:32:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 5 loss:0.04644392058253288 norm:0.00038287509232759476 max memory_allocated 62760.5341796875 
[2025-03-23 04:34:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 6 loss:0.044747572392225266 norm:0.00035498448414728045 max memory_allocated 62760.5341796875 
[2025-03-23 04:36:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 7 loss:0.043525829911231995 norm:0.00034886691719293594 max memory_allocated 62760.5341796875 
[2025-03-23 04:38:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 8 loss:0.042629994451999664 norm:0.0003241214726585895 max memory_allocated 62760.5341796875 
[2025-03-23 04:40:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 9 loss:0.041965603828430176 norm:0.0003049198421649635 max memory_allocated 62760.5341796875 
[2025-03-23 04:42:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 10 loss:0.041503939777612686 norm:0.0002945205196738243 max memory_allocated 62760.5341796875 
[2025-03-23 04:45:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 11 loss:0.04111913591623306 norm:0.0002974159433506429 max memory_allocated 62760.5341796875 
[2025-03-23 04:47:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 12 loss:0.04082070663571358 norm:0.0002867175790015608 max memory_allocated 62760.5341796875 
[2025-03-23 04:49:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 13 loss:0.04064612463116646 norm:0.00027853078790940344 max memory_allocated 62760.5341796875 
[2025-03-23 04:51:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 14 loss:0.04043808579444885 norm:0.00027735810726881027 max memory_allocated 62760.5341796875 
[2025-03-23 04:53:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 15 loss:0.040290717035532 norm:0.0002645116182975471 max memory_allocated 62760.5341796875 
[2025-03-23 04:55:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 16 loss:0.04018578678369522 norm:0.00026693238760344684 max memory_allocated 62760.5341796875 
[2025-03-23 04:57:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 17 loss:0.04005143791437149 norm:0.00026919448282569647 max memory_allocated 62760.5341796875 
[2025-03-23 04:59:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 18 loss:0.039924293756484985 norm:0.00026042424724437296 max memory_allocated 62760.5341796875 
[2025-03-23 05:02:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 19 loss:0.03987152874469757 norm:0.0002677694137673825 max memory_allocated 62760.5341796875 
[2025-03-23 05:04:58 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [9, 10, 11]
[2025-03-23 05:04:58 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [12, 13, 14] ===
[2025-03-23 05:07:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 0 loss:0.10433775186538696 norm:0.0017754783621057868 max memory_allocated 62762.7685546875 
[2025-03-23 05:09:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 1 loss:0.07809969782829285 norm:0.000845983624458313 max memory_allocated 62762.7685546875 
[2025-03-23 05:11:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 2 loss:0.06283873319625854 norm:0.000523335940670222 max memory_allocated 62762.7685546875 
[2025-03-23 05:13:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 3 loss:0.0555260069668293 norm:0.0003793902578763664 max memory_allocated 62762.7685546875 
[2025-03-23 05:16:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 4 loss:0.05191972851753235 norm:0.0003125601797364652 max memory_allocated 62762.7685546875 
[2025-03-23 05:18:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 5 loss:0.04973328858613968 norm:0.0002811956510413438 max memory_allocated 62762.7685546875 
[2025-03-23 05:20:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 6 loss:0.04825209081172943 norm:0.00026146273012273014 max memory_allocated 62762.7685546875 
[2025-03-23 05:22:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 7 loss:0.04723310470581055 norm:0.00024781597312539816 max memory_allocated 62762.7685546875 
[2025-03-23 05:24:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 8 loss:0.04646945744752884 norm:0.00023593346122652292 max memory_allocated 62762.7685546875 
[2025-03-23 05:26:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 9 loss:0.04599028825759888 norm:0.0002289803815074265 max memory_allocated 62762.7685546875 
[2025-03-23 05:28:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 10 loss:0.04562173783779144 norm:0.00022301213175524026 max memory_allocated 62762.7685546875 
[2025-03-23 05:30:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 11 loss:0.04530762508511543 norm:0.00021657210891135037 max memory_allocated 62762.7685546875 
[2025-03-23 05:33:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 12 loss:0.04508108273148537 norm:0.00021174649009481072 max memory_allocated 62762.7685546875 
[2025-03-23 05:35:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 13 loss:0.04486006870865822 norm:0.0002029094466706738 max memory_allocated 62762.7685546875 
[2025-03-23 05:37:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 14 loss:0.04468616470694542 norm:0.0002005907881539315 max memory_allocated 62762.7685546875 
[2025-03-23 05:39:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 15 loss:0.04454479366540909 norm:0.00019506693934090436 max memory_allocated 62762.7685546875 
[2025-03-23 05:41:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 16 loss:0.04445094242691994 norm:0.00018621489289216697 max memory_allocated 62762.7685546875 
[2025-03-23 05:43:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 17 loss:0.04434191808104515 norm:0.00018871427164413035 max memory_allocated 62762.7685546875 
[2025-03-23 05:45:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 18 loss:0.044292204082012177 norm:0.00018764850392472 max memory_allocated 62762.7685546875 
[2025-03-23 05:48:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 19 loss:0.04422619938850403 norm:0.0001875588350230828 max memory_allocated 62762.7685546875 
[2025-03-23 05:50:58 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [12, 13, 14]
[2025-03-23 05:50:58 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [15, 16, 17] ===
[2025-03-23 05:53:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 0 loss:0.0938825011253357 norm:0.0013293788069859147 max memory_allocated 62762.7685546875 
[2025-03-23 05:55:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 1 loss:0.07485038787126541 norm:0.000606901419814676 max memory_allocated 62762.7685546875 
[2025-03-23 05:57:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 2 loss:0.06167130172252655 norm:0.00038377358578145504 max memory_allocated 62762.7685546875 
[2025-03-23 05:59:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 3 loss:0.055749427527189255 norm:0.0002875790523830801 max memory_allocated 62762.7685546875 
[2025-03-23 06:01:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 4 loss:0.0527983084321022 norm:0.00024936802219599485 max memory_allocated 62762.7685546875 
[2025-03-23 06:04:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 5 loss:0.05084114521741867 norm:0.0002267491800012067 max memory_allocated 62762.7685546875 
[2025-03-23 06:06:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 6 loss:0.04950125515460968 norm:0.00021220477356109768 max memory_allocated 62762.7685546875 
[2025-03-23 06:08:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 7 loss:0.04852961376309395 norm:0.00020357573521323502 max memory_allocated 62762.7685546875 
[2025-03-23 06:10:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 8 loss:0.04788362979888916 norm:0.00019639349193312228 max memory_allocated 62762.7685546875 
[2025-03-23 06:12:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 9 loss:0.047407396137714386 norm:0.00019100021745543927 max memory_allocated 62762.7685546875 
[2025-03-23 06:14:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 10 loss:0.047043249011039734 norm:0.00018507050117477775 max memory_allocated 62762.7685546875 
[2025-03-23 06:16:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 11 loss:0.04676354303956032 norm:0.00018035306129604578 max memory_allocated 62762.7685546875 
[2025-03-23 06:18:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 12 loss:0.04654155299067497 norm:0.0001787262735888362 max memory_allocated 62762.7685546875 
[2025-03-23 06:21:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 13 loss:0.046328987926244736 norm:0.00017317532910965383 max memory_allocated 62762.7685546875 
[2025-03-23 06:23:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 14 loss:0.046172622591257095 norm:0.00016790298104751855 max memory_allocated 62762.7685546875 
[2025-03-23 06:25:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 15 loss:0.0460580512881279 norm:0.0001652570499572903 max memory_allocated 62762.7685546875 
[2025-03-23 06:27:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 16 loss:0.04595460370182991 norm:0.00016054655134212226 max memory_allocated 62762.7685546875 
[2025-03-23 06:29:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 17 loss:0.04585892707109451 norm:0.0001605212310096249 max memory_allocated 62762.7685546875 
[2025-03-23 06:31:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 18 loss:0.045777514576911926 norm:0.00015747912402730435 max memory_allocated 62762.7685546875 
[2025-03-23 06:33:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 19 loss:0.045707203447818756 norm:0.00015524473565164953 max memory_allocated 62762.7685546875 
[2025-03-23 06:36:45 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [15, 16, 17]
[2025-03-23 06:36:45 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [18, 19, 20] ===
[2025-03-23 06:39:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 0 loss:0.08902721107006073 norm:0.0008713505230844021 max memory_allocated 62762.7685546875 
[2025-03-23 06:41:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 1 loss:0.07408834993839264 norm:0.0004068975686095655 max memory_allocated 62762.7685546875 
[2025-03-23 06:43:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 2 loss:0.06258399784564972 norm:0.00027760874945670366 max memory_allocated 62762.7685546875 
[2025-03-23 06:45:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 3 loss:0.05844256281852722 norm:0.00022560660727322102 max memory_allocated 62762.7685546875 
[2025-03-23 06:47:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 4 loss:0.05600111559033394 norm:0.00019775453256443143 max memory_allocated 62762.7685546875 
[2025-03-23 06:50:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 5 loss:0.054215434938669205 norm:0.00018309996812604368 max memory_allocated 62762.7685546875 
[2025-03-23 06:52:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 6 loss:0.05291258916258812 norm:0.0001719603023957461 max memory_allocated 62762.7685546875 
[2025-03-23 06:54:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 7 loss:0.05210043117403984 norm:0.0001638812682358548 max memory_allocated 62762.7685546875 
[2025-03-23 06:56:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 8 loss:0.05155256763100624 norm:0.00015603868814650923 max memory_allocated 62762.7685546875 
[2025-03-23 06:58:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 9 loss:0.05117763951420784 norm:0.0001504182437201962 max memory_allocated 62762.7685546875 
[2025-03-23 07:00:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 10 loss:0.05090733617544174 norm:0.00014357642794493586 max memory_allocated 62762.7685546875 
[2025-03-23 07:02:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 11 loss:0.050702109932899475 norm:0.000140103860758245 max memory_allocated 62762.7685546875 
[2025-03-23 07:04:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 12 loss:0.050529301166534424 norm:0.00013643497368320823 max memory_allocated 62762.7685546875 
[2025-03-23 07:07:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 13 loss:0.05041218176484108 norm:0.00013365058111958206 max memory_allocated 62762.7685546875 
[2025-03-23 07:09:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 14 loss:0.050273824483156204 norm:0.00013217516243457794 max memory_allocated 62762.7685546875 
[2025-03-23 07:11:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 15 loss:0.05015259608626366 norm:0.0001301317533943802 max memory_allocated 62762.7685546875 
[2025-03-23 07:13:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 16 loss:0.05007846653461456 norm:0.0001268764608539641 max memory_allocated 62762.7685546875 
[2025-03-23 07:15:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 17 loss:0.0500066913664341 norm:0.00012625542876776308 max memory_allocated 62762.7685546875 
[2025-03-23 07:17:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 18 loss:0.049913693219423294 norm:0.00012373521167319268 max memory_allocated 62762.7685546875 
[2025-03-23 07:19:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 19 loss:0.04983018338680267 norm:0.00012070990487700328 max memory_allocated 62762.7685546875 
[2025-03-23 07:22:54 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [18, 19, 20]
[2025-03-23 07:22:54 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [21, 22, 23] ===
[2025-03-23 07:25:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 0 loss:0.10843033343553543 norm:0.0008773640729486942 max memory_allocated 62762.7685546875 
[2025-03-23 07:27:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 1 loss:0.09144233912229538 norm:0.00045711052371189 max memory_allocated 62762.7685546875 
[2025-03-23 07:29:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 2 loss:0.0770845115184784 norm:0.0003027745697181672 max memory_allocated 62762.7685546875 
[2025-03-23 07:31:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 3 loss:0.07261974364519119 norm:0.0002507423341739923 max memory_allocated 62762.7685546875 
[2025-03-23 07:33:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 4 loss:0.06992600858211517 norm:0.0002285970258526504 max memory_allocated 62762.7685546875 
[2025-03-23 07:35:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 5 loss:0.06779511272907257 norm:0.00022104839445091784 max memory_allocated 62762.7685546875 
[2025-03-23 07:38:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 6 loss:0.06642496585845947 norm:0.00021227561228442937 max memory_allocated 62762.7685546875 
[2025-03-23 07:40:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 7 loss:0.06558003276586533 norm:0.00021026376634836197 max memory_allocated 62762.7685546875 
[2025-03-23 07:42:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 8 loss:0.06503926217556 norm:0.00020307546947151423 max memory_allocated 62762.7685546875 
[2025-03-23 07:44:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 9 loss:0.06468462944030762 norm:0.00020119157852604985 max memory_allocated 62762.7685546875 
[2025-03-23 07:46:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 10 loss:0.06435465812683105 norm:0.00020089259487576783 max memory_allocated 62762.7685546875 
[2025-03-23 07:48:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 11 loss:0.06411217153072357 norm:0.0002096985699608922 max memory_allocated 62762.7685546875 
[2025-03-23 07:50:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 12 loss:0.0638638511300087 norm:0.00018107898358721286 max memory_allocated 62762.7685546875 
[2025-03-23 07:53:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 13 loss:0.0636645182967186 norm:0.00016131317534018308 max memory_allocated 62762.7685546875 
[2025-03-23 07:55:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 14 loss:0.06348283588886261 norm:0.00017055936041288078 max memory_allocated 62762.7685546875 
[2025-03-23 07:57:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 15 loss:0.0633208379149437 norm:0.00016461929772049189 max memory_allocated 62762.7685546875 
[2025-03-23 07:59:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 16 loss:0.06317633390426636 norm:0.00015186145901679993 max memory_allocated 62762.7685546875 
[2025-03-23 08:01:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 17 loss:0.06305062025785446 norm:0.00015544469351880252 max memory_allocated 62762.7685546875 
[2025-03-23 08:03:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 18 loss:0.0629274994134903 norm:0.00015082914615049958 max memory_allocated 62762.7685546875 
[2025-03-23 08:05:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 19 loss:0.06282114237546921 norm:0.00014550266496371478 max memory_allocated 62762.7685546875 
[2025-03-23 08:08:45 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [21, 22, 23]
[2025-03-23 08:08:45 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [24, 25, 26] ===
[2025-03-23 08:11:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 0 loss:0.1294272541999817 norm:0.0008205755730159581 max memory_allocated 62762.7685546875 
[2025-03-23 08:13:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 1 loss:0.11189468950033188 norm:0.0004087577108293772 max memory_allocated 62762.7685546875 
[2025-03-23 08:15:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 2 loss:0.0956626608967781 norm:0.00027476472314447165 max memory_allocated 62762.7685546875 
[2025-03-23 08:17:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 3 loss:0.091397225856781 norm:0.00024238781770691276 max memory_allocated 62762.7685546875 
[2025-03-23 08:19:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 4 loss:0.0884070098400116 norm:0.00021778029622510076 max memory_allocated 62762.7685546875 
[2025-03-23 08:21:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 5 loss:0.08604966849088669 norm:0.00020786536333616823 max memory_allocated 62762.7685546875 
[2025-03-23 08:23:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 6 loss:0.0847475454211235 norm:0.0001987981959246099 max memory_allocated 62762.7685546875 
[2025-03-23 08:26:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 7 loss:0.08402270078659058 norm:0.0001927644043462351 max memory_allocated 62762.7685546875 
[2025-03-23 08:28:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 8 loss:0.08357107639312744 norm:0.00018084226758219302 max memory_allocated 62762.7685546875 
[2025-03-23 08:30:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 9 loss:0.08318215608596802 norm:0.00017607564222998917 max memory_allocated 62762.7685546875 
[2025-03-23 08:32:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 10 loss:0.08286529779434204 norm:0.00017450589803047478 max memory_allocated 62762.7685546875 
[2025-03-23 08:34:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 11 loss:0.08256158232688904 norm:0.00016708532348275185 max memory_allocated 62762.7685546875 
[2025-03-23 08:36:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 12 loss:0.082326740026474 norm:0.00016412726836279035 max memory_allocated 62762.7685546875 
[2025-03-23 08:38:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 13 loss:0.08213487267494202 norm:0.00016409462841693312 max memory_allocated 62762.7685546875 
[2025-03-23 08:41:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 14 loss:0.08192352950572968 norm:0.0001607906015124172 max memory_allocated 62762.7685546875 
[2025-03-23 08:43:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 15 loss:0.08172767609357834 norm:0.00015531785902567208 max memory_allocated 62762.7685546875 
[2025-03-23 08:45:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 16 loss:0.08157649636268616 norm:0.00015692543820478022 max memory_allocated 62762.7685546875 
[2025-03-23 08:47:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 17 loss:0.08142058551311493 norm:0.00015334888303186744 max memory_allocated 62762.7685546875 
[2025-03-23 08:49:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 18 loss:0.0812925398349762 norm:0.00014952305355109274 max memory_allocated 62762.7685546875 
[2025-03-23 08:51:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 19 loss:0.0811895877122879 norm:0.0001549113803775981 max memory_allocated 62762.7685546875 
[2025-03-23 08:54:32 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [24, 25, 26]
[2025-03-23 08:54:32 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [27, 28, 29] ===
[2025-03-23 08:57:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 0 loss:0.15983819961547852 norm:0.0005705727962777019 max memory_allocated 62762.9404296875 
[2025-03-23 08:59:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 1 loss:0.14078140258789062 norm:0.0003777424863073975 max memory_allocated 62762.9404296875 
[2025-03-23 09:01:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 2 loss:0.12214697897434235 norm:0.0002788317506201565 max memory_allocated 62762.9404296875 
[2025-03-23 09:03:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 3 loss:0.11722481995820999 norm:0.0002447213919367641 max memory_allocated 62762.9404296875 
[2025-03-23 09:05:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 4 loss:0.11369621753692627 norm:0.00022220908431336284 max memory_allocated 62762.9404296875 
[2025-03-23 09:07:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 5 loss:0.11132882535457611 norm:0.00021511684462893754 max memory_allocated 62762.9404296875 
[2025-03-23 09:09:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 6 loss:0.11023504287004471 norm:0.00021002537687309086 max memory_allocated 62762.9404296875 
[2025-03-23 09:12:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 7 loss:0.10958559811115265 norm:0.00019491888815537095 max memory_allocated 62762.9404296875 
[2025-03-23 09:14:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 8 loss:0.10909494012594223 norm:0.0001875908492365852 max memory_allocated 62762.9404296875 
[2025-03-23 09:16:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 9 loss:0.10866546630859375 norm:0.00018875126261264086 max memory_allocated 62762.9404296875 
[2025-03-23 09:18:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 10 loss:0.10832566767930984 norm:0.0001884974626591429 max memory_allocated 62762.9404296875 
[2025-03-23 09:20:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 11 loss:0.10799868404865265 norm:0.00018424839072395116 max memory_allocated 62762.9404296875 
[2025-03-23 09:22:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 12 loss:0.10773113369941711 norm:0.0001883811637526378 max memory_allocated 62762.9404296875 
[2025-03-23 09:24:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 13 loss:0.10747608542442322 norm:0.00018802465638145804 max memory_allocated 62762.9404296875 
[2025-03-23 09:27:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 14 loss:0.107252337038517 norm:0.0001830827386584133 max memory_allocated 62762.9404296875 
[2025-03-23 09:29:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 15 loss:0.10703948140144348 norm:0.000186096818652004 max memory_allocated 62762.9404296875 
[2025-03-23 09:31:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 16 loss:0.10682431608438492 norm:0.00016639367095194757 max memory_allocated 62762.9404296875 
[2025-03-23 09:33:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 17 loss:0.10664011538028717 norm:0.00017339864280074835 max memory_allocated 62762.9404296875 
[2025-03-23 09:35:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 18 loss:0.10649710148572922 norm:0.00014972250210121274 max memory_allocated 62762.9404296875 
[2025-03-23 09:37:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 19 loss:0.10638782382011414 norm:0.00015035559772513807 max memory_allocated 62762.9404296875 
[2025-03-23 09:40:37 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [27, 28, 29]
[2025-03-23 09:40:37 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [30, 31, 32] ===
[2025-03-23 09:43:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 0 loss:0.20934613049030304 norm:0.0007622141856700182 max memory_allocated 62762.9404296875 
[2025-03-23 09:45:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 1 loss:0.1841055154800415 norm:0.00043250632006675005 max memory_allocated 62762.9404296875 
[2025-03-23 09:47:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 2 loss:0.15957006812095642 norm:0.0002628735965117812 max memory_allocated 62762.9404296875 
[2025-03-23 09:49:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 3 loss:0.15277782082557678 norm:0.00022311676002573222 max memory_allocated 62762.9404296875 
[2025-03-23 09:51:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 4 loss:0.148530513048172 norm:0.0002079698460875079 max memory_allocated 62762.9404296875 
[2025-03-23 09:53:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 5 loss:0.14651983976364136 norm:0.0001932802697410807 max memory_allocated 62762.9404296875 
[2025-03-23 09:56:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 6 loss:0.14565637707710266 norm:0.00018597945745568722 max memory_allocated 62762.9404296875 
[2025-03-23 09:58:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 7 loss:0.14499975740909576 norm:0.00017748291429597884 max memory_allocated 62762.9404296875 
[2025-03-23 10:00:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 8 loss:0.14446493983268738 norm:0.0001704929891275242 max memory_allocated 62762.9404296875 
[2025-03-23 10:02:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 9 loss:0.14401592314243317 norm:0.00016722574946470559 max memory_allocated 62762.9404296875 
[2025-03-23 10:04:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 10 loss:0.14359495043754578 norm:0.00016187030996661633 max memory_allocated 62762.9404296875 
[2025-03-23 10:06:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 11 loss:0.14323902130126953 norm:0.00015659394557587802 max memory_allocated 62762.9404296875 
[2025-03-23 10:08:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 12 loss:0.14291955530643463 norm:0.00015104351041372865 max memory_allocated 62762.9404296875 
[2025-03-23 10:11:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 13 loss:0.14263181388378143 norm:0.00014800243661738932 max memory_allocated 62762.9404296875 
[2025-03-23 10:13:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 14 loss:0.14238578081130981 norm:0.00014473489136435091 max memory_allocated 62762.9404296875 
[2025-03-23 10:15:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 15 loss:0.14215704798698425 norm:0.0001424159127054736 max memory_allocated 62762.9404296875 
[2025-03-23 10:17:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 16 loss:0.14193512499332428 norm:0.00014239642769098282 max memory_allocated 62762.9404296875 
[2025-03-23 10:19:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 17 loss:0.14174365997314453 norm:0.00014234553964342922 max memory_allocated 62762.9404296875 
[2025-03-23 10:21:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 18 loss:0.14156819880008698 norm:0.00014289814862422645 max memory_allocated 62762.9404296875 
[2025-03-23 10:23:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 19 loss:0.14140526950359344 norm:0.0001414288126397878 max memory_allocated 62762.9404296875 
[2025-03-23 10:26:37 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [30, 31, 32]
[2025-03-23 10:26:37 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [33, 34, 35] ===
[2025-03-23 10:29:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 0 loss:0.2700704336166382 norm:0.0006930884555913508 max memory_allocated 62762.9404296875 
[2025-03-23 10:31:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 1 loss:0.23909562826156616 norm:0.0004266324685886502 max memory_allocated 62762.9404296875 
[2025-03-23 10:33:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 2 loss:0.21067725121974945 norm:0.00028765614842996 max memory_allocated 62762.9404296875 
[2025-03-23 10:35:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 3 loss:0.20220127701759338 norm:0.00024949180078692734 max memory_allocated 62762.9404296875 
[2025-03-23 10:37:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 4 loss:0.1976248025894165 norm:0.00023076657089404762 max memory_allocated 62762.9404296875 
[2025-03-23 10:39:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 5 loss:0.19577327370643616 norm:0.00021603534696623683 max memory_allocated 62762.9404296875 
[2025-03-23 10:41:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 6 loss:0.19478166103363037 norm:0.00020661752205342054 max memory_allocated 62762.9404296875 
[2025-03-23 10:43:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 7 loss:0.19395282864570618 norm:0.00019863058696500957 max memory_allocated 62762.9404296875 
[2025-03-23 10:46:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 8 loss:0.1932336837053299 norm:0.00019139779033139348 max memory_allocated 62762.9404296875 
[2025-03-23 10:48:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 9 loss:0.19261951744556427 norm:0.0001860859483713284 max memory_allocated 62762.9404296875 
[2025-03-23 10:50:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 10 loss:0.19205695390701294 norm:0.00018157099839299917 max memory_allocated 62762.9404296875 
[2025-03-23 10:52:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 11 loss:0.1915523111820221 norm:0.00017529759497847408 max memory_allocated 62762.9404296875 
[2025-03-23 10:54:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 12 loss:0.19110418856143951 norm:0.00017249774828087538 max memory_allocated 62762.9404296875 
[2025-03-23 10:56:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 13 loss:0.19071336090564728 norm:0.00016674220387358218 max memory_allocated 62762.9404296875 
[2025-03-23 10:58:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 14 loss:0.19035956263542175 norm:0.00016610941383987665 max memory_allocated 62762.9404296875 
[2025-03-23 11:01:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 15 loss:0.19001442193984985 norm:0.00016255959053523839 max memory_allocated 62762.9404296875 
[2025-03-23 11:03:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 16 loss:0.18973016738891602 norm:0.00016192527255043387 max memory_allocated 62762.9404296875 
[2025-03-23 11:05:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 17 loss:0.1894768327474594 norm:0.00015980619355104864 max memory_allocated 62762.9404296875 
[2025-03-23 11:07:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 18 loss:0.18926101922988892 norm:0.00015603900828864425 max memory_allocated 62762.9404296875 
[2025-03-23 11:09:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 19 loss:0.1890481859445572 norm:0.0001562842953717336 max memory_allocated 62762.9404296875 
[2025-03-23 11:12:19 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [33, 34, 35]
[2025-03-23 11:12:19 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [36] ===
[2025-03-23 11:12:22 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 11:13:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 0 loss:0.23293252289295197 norm:0.009021265432238579 max memory_allocated 62762.9404296875 
[2025-03-23 11:13:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 1 loss:0.21895143389701843 norm:0.006761455908417702 max memory_allocated 62762.9404296875 
[2025-03-23 11:14:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 2 loss:0.20746150612831116 norm:0.004800348076969385 max memory_allocated 62762.9404296875 
[2025-03-23 11:15:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 3 loss:0.20380057394504547 norm:0.003911237698048353 max memory_allocated 62762.9404296875 
[2025-03-23 11:15:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 4 loss:0.2022765427827835 norm:0.003216202836483717 max memory_allocated 62762.9404296875 
[2025-03-23 11:16:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 5 loss:0.20161980390548706 norm:0.0026462639216333628 max memory_allocated 62762.9404296875 
[2025-03-23 11:17:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 6 loss:0.2011280059814453 norm:0.002257442567497492 max memory_allocated 62762.9404296875 
[2025-03-23 11:18:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 7 loss:0.2007790505886078 norm:0.002137805102393031 max memory_allocated 62762.9404296875 
[2025-03-23 11:18:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 8 loss:0.20050163567066193 norm:0.002126969862729311 max memory_allocated 62762.9404296875 
[2025-03-23 11:19:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 9 loss:0.20021827518939972 norm:0.0019588603172451258 max memory_allocated 62762.9404296875 
[2025-03-23 11:20:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 10 loss:0.19998830556869507 norm:0.0019143236568197608 max memory_allocated 62762.9404296875 
[2025-03-23 11:21:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 11 loss:0.19973506033420563 norm:0.0017372058937326074 max memory_allocated 62762.9404296875 
[2025-03-23 11:21:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 12 loss:0.19952324032783508 norm:0.001637644600123167 max memory_allocated 62762.9404296875 
[2025-03-23 11:22:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 13 loss:0.1994239240884781 norm:0.001692894846200943 max memory_allocated 62762.9404296875 
[2025-03-23 11:23:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 14 loss:0.1992669254541397 norm:0.0017281874315813184 max memory_allocated 62762.9404296875 
[2025-03-23 11:23:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 15 loss:0.19910180568695068 norm:0.0016120397485792637 max memory_allocated 62762.9404296875 
[2025-03-23 11:24:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 16 loss:0.19895172119140625 norm:0.0015631517162546515 max memory_allocated 62762.9404296875 
[2025-03-23 11:25:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 17 loss:0.1988433599472046 norm:0.0015041010919958353 max memory_allocated 62762.9404296875 
[2025-03-23 11:26:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 18 loss:0.1987420916557312 norm:0.0014890236780047417 max memory_allocated 62762.9404296875 
[2025-03-23 11:26:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 19 loss:0.19866511225700378 norm:0.0015078387223184109 max memory_allocated 62762.9404296875 
[2025-03-23 11:27:40 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [36]
[2025-03-23 11:27:40 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [37] ===
[2025-03-23 11:27:43 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 11:28:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 0 loss:0.2580084800720215 norm:0.009170254692435265 max memory_allocated 62762.9404296875 
[2025-03-23 11:29:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 1 loss:0.24335555732250214 norm:0.007361755706369877 max memory_allocated 62762.9404296875 
[2025-03-23 11:29:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 2 loss:0.2307528406381607 norm:0.005122989881783724 max memory_allocated 62762.9404296875 
[2025-03-23 11:30:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 3 loss:0.226744145154953 norm:0.004202383104711771 max memory_allocated 62762.9404296875 
[2025-03-23 11:31:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 4 loss:0.2252262681722641 norm:0.003507409244775772 max memory_allocated 62762.9404296875 
[2025-03-23 11:32:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 5 loss:0.22456853091716766 norm:0.002939923433586955 max memory_allocated 62762.9404296875 
[2025-03-23 11:32:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 6 loss:0.2240903675556183 norm:0.0024930189829319715 max memory_allocated 62762.9404296875 
[2025-03-23 11:33:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 7 loss:0.22369442880153656 norm:0.002269313670694828 max memory_allocated 62762.9404296875 
[2025-03-23 11:34:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 8 loss:0.22342132031917572 norm:0.0022652053739875555 max memory_allocated 62762.9404296875 
[2025-03-23 11:34:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 9 loss:0.22317038476467133 norm:0.0021983396727591753 max memory_allocated 62762.9404296875 
[2025-03-23 11:35:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 10 loss:0.22293201088905334 norm:0.002133514964953065 max memory_allocated 62762.9404296875 
[2025-03-23 11:36:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 11 loss:0.22277431190013885 norm:0.002150642918422818 max memory_allocated 62762.9404296875 
[2025-03-23 11:37:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 12 loss:0.22260701656341553 norm:0.0020290121901780367 max memory_allocated 62762.9404296875 
[2025-03-23 11:37:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 13 loss:0.22246283292770386 norm:0.0019742469303309917 max memory_allocated 62762.9404296875 
[2025-03-23 11:38:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 14 loss:0.22236062586307526 norm:0.0019488777033984661 max memory_allocated 62762.9404296875 
[2025-03-23 11:39:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 15 loss:0.22220803797245026 norm:0.001879481365904212 max memory_allocated 62762.9404296875 
[2025-03-23 11:39:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 16 loss:0.22205530107021332 norm:0.0017891956958919764 max memory_allocated 62762.9404296875 
[2025-03-23 11:40:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 17 loss:0.22199051082134247 norm:0.001874554785899818 max memory_allocated 62762.9404296875 
[2025-03-23 11:41:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 18 loss:0.22189131379127502 norm:0.0018189294496551156 max memory_allocated 62762.9404296875 
[2025-03-23 11:42:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 19 loss:0.22173316776752472 norm:0.001664987183175981 max memory_allocated 62762.9404296875 
[2025-03-23 11:43:04 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 15, block: [37]
[2025-03-23 11:43:05 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 16 with layers [38] ===
[2025-03-23 11:43:08 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 11:43:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 0 loss:0.3023860454559326 norm:0.015070604160428047 max memory_allocated 62762.9404296875 
[2025-03-23 11:44:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 1 loss:0.28296637535095215 norm:0.010513965040445328 max memory_allocated 62762.9404296875 
[2025-03-23 11:45:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 2 loss:0.2680397033691406 norm:0.007159772794693708 max memory_allocated 62762.9404296875 
[2025-03-23 11:46:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 3 loss:0.2635680139064789 norm:0.00602363795042038 max memory_allocated 62762.9404296875 
[2025-03-23 11:46:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 4 loss:0.26182955503463745 norm:0.005194059107452631 max memory_allocated 62762.9404296875 
[2025-03-23 11:47:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 5 loss:0.2608935236930847 norm:0.004529749974608421 max memory_allocated 62762.9404296875 
[2025-03-23 11:48:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 6 loss:0.26030251383781433 norm:0.004107234999537468 max memory_allocated 62762.9404296875 
[2025-03-23 11:48:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 7 loss:0.25982382893562317 norm:0.0037493526469916105 max memory_allocated 62762.9404296875 
[2025-03-23 11:49:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 8 loss:0.2594878673553467 norm:0.003582569770514965 max memory_allocated 62762.9404296875 
[2025-03-23 11:50:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 9 loss:0.259313702583313 norm:0.0037313629873096943 max memory_allocated 62762.9404296875 
[2025-03-23 11:51:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 10 loss:0.2591150999069214 norm:0.0036220315378159285 max memory_allocated 62762.9404296875 
[2025-03-23 11:51:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 11 loss:0.2588058114051819 norm:0.003319940296933055 max memory_allocated 62762.9404296875 
[2025-03-23 11:52:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 12 loss:0.25872039794921875 norm:0.0034312757197767496 max memory_allocated 62762.9404296875 
[2025-03-23 11:53:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 13 loss:0.25880366563796997 norm:0.0035841597709804773 max memory_allocated 62762.9404296875 
[2025-03-23 11:53:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 14 loss:0.25874194502830505 norm:0.0032863866072148085 max memory_allocated 62762.9404296875 
[2025-03-23 11:54:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 15 loss:0.2583300471305847 norm:0.003142950590699911 max memory_allocated 62762.9404296875 
[2025-03-23 11:55:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 16 loss:0.25816330313682556 norm:0.0030365826096385717 max memory_allocated 62762.9404296875 
[2025-03-23 11:56:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 17 loss:0.25800544023513794 norm:0.0027715207543224096 max memory_allocated 62762.9404296875 
[2025-03-23 11:56:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 18 loss:0.2579122483730316 norm:0.002845336217433214 max memory_allocated 62762.9404296875 
[2025-03-23 11:57:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 19 loss:0.25783106684684753 norm:0.002743263728916645 max memory_allocated 62762.9404296875 
[2025-03-23 11:58:28 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 16, block: [38]
[2025-03-23 11:58:28 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 17 with layers [39] ===
[2025-03-23 11:58:38 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 11:59:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 0 loss:0.4337228536605835 norm:0.04293124005198479 max memory_allocated 62762.9404296875 
[2025-03-23 12:00:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 1 loss:0.3877596855163574 norm:0.027377646416425705 max memory_allocated 62762.9404296875 
[2025-03-23 12:00:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 2 loss:0.35579949617385864 norm:0.018140112981200218 max memory_allocated 62762.9404296875 
[2025-03-23 12:01:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 3 loss:0.3468247354030609 norm:0.014641344547271729 max memory_allocated 62762.9404296875 
[2025-03-23 12:02:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 4 loss:0.3431919813156128 norm:0.012848800048232079 max memory_allocated 62762.9404296875 
[2025-03-23 12:02:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 5 loss:0.34059667587280273 norm:0.011763432994484901 max memory_allocated 62762.9404296875 
[2025-03-23 12:03:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 6 loss:0.33864885568618774 norm:0.010484648868441582 max memory_allocated 62762.9404296875 
[2025-03-23 12:04:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 7 loss:0.33725401759147644 norm:0.00939885713160038 max memory_allocated 62762.9404296875 
[2025-03-23 12:05:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 8 loss:0.3359939455986023 norm:0.008861425332725048 max memory_allocated 62762.9404296875 
[2025-03-23 12:05:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 9 loss:0.33510488271713257 norm:0.008151126094162464 max memory_allocated 62762.9404296875 
[2025-03-23 12:06:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 10 loss:0.3343203365802765 norm:0.00792510062456131 max memory_allocated 62762.9404296875 
[2025-03-23 12:07:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 11 loss:0.33353671431541443 norm:0.007329073268920183 max memory_allocated 62762.9404296875 
[2025-03-23 12:08:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 12 loss:0.3327910304069519 norm:0.006695338990539312 max memory_allocated 62762.9404296875 
[2025-03-23 12:08:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 13 loss:0.3325710892677307 norm:0.00690507423132658 max memory_allocated 62762.9404296875 
[2025-03-23 12:09:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 14 loss:0.33226603269577026 norm:0.007223221473395824 max memory_allocated 62762.9404296875 
[2025-03-23 12:10:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 15 loss:0.33179226517677307 norm:0.006679660640656948 max memory_allocated 62762.9404296875 
[2025-03-23 12:10:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 16 loss:0.3310709297657013 norm:0.00612089317291975 max memory_allocated 62762.9404296875 
[2025-03-23 12:11:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 17 loss:0.33101680874824524 norm:0.00648144306614995 max memory_allocated 62762.9404296875 
[2025-03-23 12:12:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 18 loss:0.3308221101760864 norm:0.006167788058519363 max memory_allocated 62762.9404296875 
[2025-03-23 12:13:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 19 loss:0.3305920362472534 norm:0.006256584078073502 max memory_allocated 62762.9404296875 
[2025-03-23 12:13:58 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 17, block: [39]
[2025-03-23 12:13:58 root] (main_calib_config3_attn.py 379): INFO 36785.329635858536
[2025-03-23 12:14:10 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-23 12:15:33 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 4.999273777008057
[2025-03-23 12:15:33 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-23 12:17:43 root] (main_calib_config3_attn.py 161): INFO c4 : 6.630703449249268
[2025-03-23 13:11:35 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 4.999273777008057, 'c4': 6.630703449249268, 'results': {'arc_easy': {'acc': 0.7175925925925926, 'acc_stderr': 0.009237303403479339, 'acc_norm': 0.571969696969697, 'acc_norm_stderr': 0.010152943316426265}, 'arc_challenge': {'acc': 0.4308873720136519, 'acc_stderr': 0.014471133392642475, 'acc_norm': 0.43856655290102387, 'acc_norm_stderr': 0.014500682618212864}, 'boolq': {'acc': 0.6697247706422018, 'acc_stderr': 0.008225810914277265}, 'hellaswag': {'acc': 0.59061939852619, 'acc_stderr': 0.00490714622934755, 'acc_norm': 0.7584146584345748, 'acc_norm_stderr': 0.004271694301395475}, 'winogrande': {'acc': 0.6842936069455406, 'acc_stderr': 0.01306309474300081}, 'piqa': {'acc': 0.7840043525571273, 'acc_stderr': 0.009601236303553555, 'acc_norm': 0.7861806311207835, 'acc_norm_stderr': 0.009565994206915607}}, 'versions': {'arc_easy': 0, 'arc_challenge': 0, 'boolq': 1, 'hellaswag': 0, 'winogrande': 0, 'piqa': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-23 13:11:35 root] (main_calib_config3_attn.py 175): INFO 43.09,71.76,66.97,59.06,78.40,68.43
