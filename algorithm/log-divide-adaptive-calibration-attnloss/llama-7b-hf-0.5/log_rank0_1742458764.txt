[2025-03-20 08:19:24 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/llama-7b-hf-0.5', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.5.pkl', blocks_pkl='./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-20 08:19:37 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-20 08:19:38 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-20 08:19:38 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-20 08:19:38 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.5.pkl
[2025-03-20 08:19:38 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 5), (5, 8), (8, 11), (11, 14), (14, 16), (16, 19), (19, 22), (22, 25), (25, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-20 08:19:38 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2, 3, 4], [5, 6, 7], [8, 9, 10], [11, 12, 13], [14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26], [27], [28], [29], [30], [31]]
[2025-03-20 08:19:39 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-20 08:19:39 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 08:20:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.008299101144075394 norm:0.008304367773234844 max memory_allocated 34630.880859375 
[2025-03-20 08:20:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.00468095950782299 norm:0.004716747906059027 max memory_allocated 34630.880859375 
[2025-03-20 08:21:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.003186067333444953 norm:0.003338690847158432 max memory_allocated 34630.880859375 
[2025-03-20 08:21:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.002749051433056593 norm:0.0025900378823280334 max memory_allocated 34630.880859375 
[2025-03-20 08:21:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.0025920337066054344 norm:0.002060113474726677 max memory_allocated 34630.880859375 
[2025-03-20 08:22:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.002495458582416177 norm:0.001708409283310175 max memory_allocated 34630.880859375 
[2025-03-20 08:22:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.0023493319749832153 norm:0.0014036324573680758 max memory_allocated 34630.880859375 
[2025-03-20 08:23:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0022928393445909023 norm:0.0012429896742105484 max memory_allocated 34630.880859375 
[2025-03-20 08:23:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.002270141616463661 norm:0.0011056055082008243 max memory_allocated 34630.880859375 
[2025-03-20 08:24:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.0022598695941269398 norm:0.0009630210697650909 max memory_allocated 34630.880859375 
[2025-03-20 08:24:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.0022213803604245186 norm:0.0008310173288919032 max memory_allocated 34630.880859375 
[2025-03-20 08:25:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.002208704361692071 norm:0.0007442905916832387 max memory_allocated 34630.880859375 
[2025-03-20 08:25:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.0021787898149341345 norm:0.0006848469492979348 max memory_allocated 34630.880859375 
[2025-03-20 08:26:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.002166719175875187 norm:0.0006346411537379026 max memory_allocated 34630.880859375 
[2025-03-20 08:26:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0021649699192494154 norm:0.0006019063876010478 max memory_allocated 34630.880859375 
[2025-03-20 08:26:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.0021517628338187933 norm:0.0005807378329336643 max memory_allocated 34630.880859375 
[2025-03-20 08:27:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.002162510994821787 norm:0.00054928072495386 max memory_allocated 34630.880859375 
[2025-03-20 08:27:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0021110086236149073 norm:0.0005161428125575185 max memory_allocated 34630.880859375 
[2025-03-20 08:28:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.0021300525404512882 norm:0.0005115970852784812 max memory_allocated 34630.880859375 
[2025-03-20 08:28:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0021196925081312656 norm:0.00047172425547614694 max memory_allocated 34630.880859375 
[2025-03-20 08:29:16 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-20 08:29:17 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-20 08:29:18 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 08:29:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.020938845351338387 norm:0.019620202481746674 max memory_allocated 35097.7724609375 
[2025-03-20 08:30:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.011885969899594784 norm:0.012124286964535713 max memory_allocated 35097.7724609375 
[2025-03-20 08:30:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.008178908377885818 norm:0.007033476140350103 max memory_allocated 35097.7724609375 
[2025-03-20 08:31:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.007127808406949043 norm:0.005063171964138746 max memory_allocated 35097.7724609375 
[2025-03-20 08:31:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.006740684155374765 norm:0.004374946001917124 max memory_allocated 35097.7724609375 
[2025-03-20 08:32:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.006470345892012119 norm:0.003912490326911211 max memory_allocated 35097.7724609375 
[2025-03-20 08:32:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.006284440867602825 norm:0.0034934028517454863 max memory_allocated 35097.7724609375 
[2025-03-20 08:32:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.006091439630836248 norm:0.003160703694447875 max memory_allocated 35097.7724609375 
[2025-03-20 08:33:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.005943569354712963 norm:0.002939288504421711 max memory_allocated 35097.7724609375 
[2025-03-20 08:33:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.005811761133372784 norm:0.00268561695702374 max memory_allocated 35097.7724609375 
[2025-03-20 08:34:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.0057282657362520695 norm:0.002459143288433552 max memory_allocated 35097.7724609375 
[2025-03-20 08:34:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.005678041372448206 norm:0.0022749979980289936 max memory_allocated 35097.7724609375 
[2025-03-20 08:35:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.005594112444669008 norm:0.0020475685596466064 max memory_allocated 35097.7724609375 
[2025-03-20 08:35:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.005554138217121363 norm:0.0018606105586513877 max memory_allocated 35097.7724609375 
[2025-03-20 08:36:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.005520168226212263 norm:0.0016667345771566033 max memory_allocated 35097.7724609375 
[2025-03-20 08:36:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.005509582813829184 norm:0.0015227318508550525 max memory_allocated 35097.7724609375 
[2025-03-20 08:37:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.005478305276483297 norm:0.0013553934404626489 max memory_allocated 35097.7724609375 
[2025-03-20 08:37:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.005445536691695452 norm:0.001222546212375164 max memory_allocated 35097.7724609375 
[2025-03-20 08:37:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.005448834504932165 norm:0.0011527086608111858 max memory_allocated 35097.7724609375 
[2025-03-20 08:38:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.005452893674373627 norm:0.0011692126281559467 max memory_allocated 35097.7724609375 
[2025-03-20 08:38:57 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-20 08:38:57 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2, 3, 4] ===
[2025-03-20 08:38:57 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 08:40:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 0 loss:0.0449671670794487 norm:0.02368992753326893 max memory_allocated 47468.5419921875 
[2025-03-20 08:41:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 1 loss:0.032949283719062805 norm:0.01681465283036232 max memory_allocated 47468.5419921875 
[2025-03-20 08:43:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 2 loss:0.02590794675052166 norm:0.011012260802090168 max memory_allocated 47468.5419921875 
[2025-03-20 08:44:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 3 loss:0.02285076305270195 norm:0.006844864692538977 max memory_allocated 47468.5419921875 
[2025-03-20 08:45:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 4 loss:0.02116980031132698 norm:0.005602840334177017 max memory_allocated 47468.5419921875 
[2025-03-20 08:47:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 5 loss:0.019992606714367867 norm:0.004579381551593542 max memory_allocated 47468.5419921875 
[2025-03-20 08:48:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 6 loss:0.019266195595264435 norm:0.004352133255451918 max memory_allocated 47468.5419921875 
[2025-03-20 08:49:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 7 loss:0.018826568499207497 norm:0.004006421659141779 max memory_allocated 47468.5419921875 
[2025-03-20 08:51:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 8 loss:0.01873106323182583 norm:0.004200102761387825 max memory_allocated 47468.5419921875 
[2025-03-20 08:52:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 9 loss:0.018671810626983643 norm:0.003906980622559786 max memory_allocated 47468.5419921875 
[2025-03-20 08:53:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 10 loss:0.018336504697799683 norm:0.00339162047021091 max memory_allocated 47468.5419921875 
[2025-03-20 08:55:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 11 loss:0.01811852492392063 norm:0.0031330520287156105 max memory_allocated 47468.5419921875 
[2025-03-20 08:56:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 12 loss:0.018188029527664185 norm:0.0027586398646235466 max memory_allocated 47468.5419921875 
[2025-03-20 08:57:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 13 loss:0.018427614122629166 norm:0.0024245218373835087 max memory_allocated 47468.5419921875 
[2025-03-20 08:59:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 14 loss:0.0181100033223629 norm:0.0023442208766937256 max memory_allocated 47468.5419921875 
[2025-03-20 09:00:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 15 loss:0.018297377973794937 norm:0.002468834165483713 max memory_allocated 47468.5419921875 
[2025-03-20 09:01:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 16 loss:0.01853039860725403 norm:0.002907897811383009 max memory_allocated 47468.5419921875 
[2025-03-20 09:03:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 17 loss:0.018150871619582176 norm:0.0023194013629108667 max memory_allocated 47468.5419921875 
[2025-03-20 09:04:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 18 loss:0.017893904820084572 norm:0.0018436168320477009 max memory_allocated 47468.5419921875 
[2025-03-20 09:05:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 19 loss:0.06754408776760101 norm:0.13854995369911194 max memory_allocated 47468.5419921875 
[2025-03-20 09:07:43 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2, 3, 4]
[2025-03-20 09:07:43 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [5, 6, 7] ===
[2025-03-20 09:09:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 0 loss:0.0897664949297905 norm:0.004434674512594938 max memory_allocated 47468.7294921875 
[2025-03-20 09:10:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 1 loss:0.0700346902012825 norm:0.0009279766818508506 max memory_allocated 47468.7294921875 
[2025-03-20 09:11:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 2 loss:0.059622135013341904 norm:0.0004829646204598248 max memory_allocated 47468.7294921875 
[2025-03-20 09:13:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 3 loss:0.054726555943489075 norm:0.0003998189349658787 max memory_allocated 47468.7294921875 
[2025-03-20 09:14:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 4 loss:0.05234547704458237 norm:0.00040394841926172376 max memory_allocated 47468.7294921875 
[2025-03-20 09:15:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 5 loss:0.050608888268470764 norm:0.0003583601792342961 max memory_allocated 47468.7294921875 
[2025-03-20 09:17:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 6 loss:0.04950207471847534 norm:0.000326680950820446 max memory_allocated 47468.7294921875 
[2025-03-20 09:18:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 7 loss:0.04878700524568558 norm:0.00032883358653634787 max memory_allocated 47468.7294921875 
[2025-03-20 09:19:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 8 loss:0.048290498554706573 norm:0.00032918169745244086 max memory_allocated 47468.7294921875 
[2025-03-20 09:21:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 9 loss:0.04799472168087959 norm:0.0003466239431872964 max memory_allocated 47468.7294921875 
[2025-03-20 09:22:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 10 loss:0.047831516712903976 norm:0.0003493035037536174 max memory_allocated 47468.7294921875 
[2025-03-20 09:23:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 11 loss:0.047703832387924194 norm:0.0003592310822568834 max memory_allocated 47468.7294921875 
[2025-03-20 09:25:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 12 loss:0.047525640577077866 norm:0.0003479442384559661 max memory_allocated 47468.7294921875 
[2025-03-20 09:26:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 13 loss:0.04749644175171852 norm:0.00036382314283400774 max memory_allocated 47468.7294921875 
[2025-03-20 09:27:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 14 loss:0.04735884070396423 norm:0.0003565439837984741 max memory_allocated 47468.7294921875 
[2025-03-20 09:29:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 15 loss:0.047260187566280365 norm:0.00033267721300944686 max memory_allocated 47468.7294921875 
[2025-03-20 09:30:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 16 loss:0.047178853303194046 norm:0.0003486786154098809 max memory_allocated 47468.7294921875 
[2025-03-20 09:31:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 17 loss:0.04717220366001129 norm:0.000341973383910954 max memory_allocated 47468.7294921875 
[2025-03-20 09:33:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 18 loss:0.04713454097509384 norm:0.0003356471424922347 max memory_allocated 47468.7294921875 
[2025-03-20 09:34:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 19 loss:0.04714083671569824 norm:0.0003609975101426244 max memory_allocated 47468.7294921875 
[2025-03-20 09:36:22 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [5, 6, 7]
[2025-03-20 09:36:22 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [8, 9, 10] ===
[2025-03-20 09:37:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 0 loss:0.09410152584314346 norm:0.0013019673060625792 max memory_allocated 47468.9169921875 
[2025-03-20 09:39:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 1 loss:0.07546564936637878 norm:0.0005833820323459804 max memory_allocated 47468.9169921875 
[2025-03-20 09:40:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 2 loss:0.06383731216192245 norm:0.00037132998113520443 max memory_allocated 47468.9169921875 
[2025-03-20 09:41:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 3 loss:0.058788739144802094 norm:0.0003089993551839143 max memory_allocated 47468.9169921875 
[2025-03-20 09:43:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 4 loss:0.05618426948785782 norm:0.0002900171675719321 max memory_allocated 47468.9169921875 
[2025-03-20 09:44:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 5 loss:0.05451611056923866 norm:0.00027502220473252237 max memory_allocated 47468.9169921875 
[2025-03-20 09:45:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 6 loss:0.053397662937641144 norm:0.00025554661988280714 max memory_allocated 47468.9169921875 
[2025-03-20 09:47:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 7 loss:0.05276777595281601 norm:0.0002463374985381961 max memory_allocated 47468.9169921875 
[2025-03-20 09:48:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 8 loss:0.05228652432560921 norm:0.00023299857275560498 max memory_allocated 47468.9169921875 
[2025-03-20 09:49:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 9 loss:0.05196501314640045 norm:0.0002313574805157259 max memory_allocated 47468.9169921875 
[2025-03-20 09:51:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 10 loss:0.05175459384918213 norm:0.0002451852196827531 max memory_allocated 47468.9169921875 
[2025-03-20 09:52:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 11 loss:0.05163460969924927 norm:0.00024478480918332934 max memory_allocated 47468.9169921875 
[2025-03-20 09:53:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 12 loss:0.051530759781599045 norm:0.00023491846513934433 max memory_allocated 47468.9169921875 
[2025-03-20 09:55:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 13 loss:0.05140917748212814 norm:0.00023230815713759512 max memory_allocated 47468.9169921875 
[2025-03-20 09:56:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 14 loss:0.051377274096012115 norm:0.00023354616132564843 max memory_allocated 47468.9169921875 
[2025-03-20 09:57:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 15 loss:0.05134059861302376 norm:0.00024001272686291486 max memory_allocated 47468.9169921875 
[2025-03-20 09:59:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 16 loss:0.05127997323870659 norm:0.0002347227418795228 max memory_allocated 47468.9169921875 
[2025-03-20 10:00:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 17 loss:0.05127670615911484 norm:0.00024326724815182388 max memory_allocated 47468.9169921875 
[2025-03-20 10:01:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 18 loss:0.05123177915811539 norm:0.00023786461679264903 max memory_allocated 47468.9169921875 
[2025-03-20 10:03:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 19 loss:0.051184240728616714 norm:0.00023655897530261427 max memory_allocated 47468.9169921875 
[2025-03-20 10:04:58 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [8, 9, 10]
[2025-03-20 10:04:58 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [11, 12, 13] ===
[2025-03-20 10:06:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 0 loss:0.09346053004264832 norm:0.0012850952334702015 max memory_allocated 47469.1044921875 
[2025-03-20 10:07:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 1 loss:0.07782431691884995 norm:0.0005495405057445168 max memory_allocated 47469.1044921875 
[2025-03-20 10:09:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 2 loss:0.06641869992017746 norm:0.00032637373078614473 max memory_allocated 47469.1044921875 
[2025-03-20 10:10:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 3 loss:0.0615575835108757 norm:0.00024121197930071503 max memory_allocated 47469.1044921875 
[2025-03-20 10:11:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 4 loss:0.05909205973148346 norm:0.00022078133770264685 max memory_allocated 47469.1044921875 
[2025-03-20 10:13:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 5 loss:0.057567812502384186 norm:0.0002033599157584831 max memory_allocated 47469.1044921875 
[2025-03-20 10:14:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 6 loss:0.056604355573654175 norm:0.00019997142953798175 max memory_allocated 47469.1044921875 
[2025-03-20 10:15:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 7 loss:0.05601269379258156 norm:0.00019612303003668785 max memory_allocated 47469.1044921875 
[2025-03-20 10:17:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 8 loss:0.05560971051454544 norm:0.00019151247397530824 max memory_allocated 47469.1044921875 
[2025-03-20 10:18:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 9 loss:0.055305223912000656 norm:0.0001899788185255602 max memory_allocated 47469.1044921875 
[2025-03-20 10:19:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 10 loss:0.05510084331035614 norm:0.0001884600060293451 max memory_allocated 47469.1044921875 
[2025-03-20 10:21:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 11 loss:0.05500052869319916 norm:0.0001928817218868062 max memory_allocated 47469.1044921875 
[2025-03-20 10:22:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 12 loss:0.05487307161092758 norm:0.00018795175128616393 max memory_allocated 47469.1044921875 
[2025-03-20 10:23:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 13 loss:0.05477181822061539 norm:0.00018213891598861665 max memory_allocated 47469.1044921875 
[2025-03-20 10:25:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 14 loss:0.054720405489206314 norm:0.00017972190107684582 max memory_allocated 47469.1044921875 
[2025-03-20 10:26:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 15 loss:0.05464572831988335 norm:0.0001786941138561815 max memory_allocated 47469.1044921875 
[2025-03-20 10:27:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 16 loss:0.05462063476443291 norm:0.00018122723849955946 max memory_allocated 47469.1044921875 
[2025-03-20 10:29:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 17 loss:0.05459100008010864 norm:0.00018019489652942866 max memory_allocated 47469.1044921875 
[2025-03-20 10:30:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 18 loss:0.05454511195421219 norm:0.00018012204964179546 max memory_allocated 47469.1044921875 
[2025-03-20 10:31:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 19 loss:0.05451710522174835 norm:0.00017939411918632686 max memory_allocated 47469.1044921875 
[2025-03-20 10:33:34 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [11, 12, 13]
[2025-03-20 10:33:34 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [14, 15] ===
[2025-03-20 10:34:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 0 loss:0.08482739329338074 norm:0.0006074613193050027 max memory_allocated 47469.1044921875 
[2025-03-20 10:35:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 1 loss:0.0731801986694336 norm:0.0003191184368915856 max memory_allocated 47469.1044921875 
[2025-03-20 10:36:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 2 loss:0.06431415677070618 norm:0.00021269604621920735 max memory_allocated 47469.1044921875 
[2025-03-20 10:37:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 3 loss:0.061264898627996445 norm:0.0001920681243063882 max memory_allocated 47469.1044921875 
[2025-03-20 10:38:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 4 loss:0.059634871780872345 norm:0.000173100212123245 max memory_allocated 47469.1044921875 
[2025-03-20 10:39:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 5 loss:0.05861824378371239 norm:0.00015919728321023285 max memory_allocated 47469.1044921875 
[2025-03-20 10:39:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 6 loss:0.05807795748114586 norm:0.00015169489779509604 max memory_allocated 47469.1044921875 
[2025-03-20 10:40:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 7 loss:0.0577530600130558 norm:0.00014914781786501408 max memory_allocated 47469.1044921875 
[2025-03-20 10:41:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 8 loss:0.05753029137849808 norm:0.00014995003584772348 max memory_allocated 47469.1044921875 
[2025-03-20 10:42:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 9 loss:0.057424239814281464 norm:0.00015753762272652239 max memory_allocated 47469.1044921875 
[2025-03-20 10:43:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 10 loss:0.057214803993701935 norm:0.0001333321852143854 max memory_allocated 47469.1044921875 
[2025-03-20 10:44:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 11 loss:0.05710066854953766 norm:0.00012533055269159377 max memory_allocated 47469.1044921875 
[2025-03-20 10:45:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 12 loss:0.057006292045116425 norm:0.0001227380707859993 max memory_allocated 47469.1044921875 
[2025-03-20 10:46:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 13 loss:0.056954093277454376 norm:0.00012709893053397536 max memory_allocated 47469.1044921875 
[2025-03-20 10:47:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 14 loss:0.056881554424762726 norm:0.00012353887723293155 max memory_allocated 47469.1044921875 
[2025-03-20 10:47:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 15 loss:0.05680222809314728 norm:0.00012544705532491207 max memory_allocated 47469.1044921875 
[2025-03-20 10:48:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 16 loss:0.05676686018705368 norm:0.00012506084749475121 max memory_allocated 47469.1044921875 
[2025-03-20 10:49:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 17 loss:0.05671634525060654 norm:0.0001232241775142029 max memory_allocated 47469.1044921875 
[2025-03-20 10:50:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 18 loss:0.05666264519095421 norm:0.0001284814061364159 max memory_allocated 47469.1044921875 
[2025-03-20 10:51:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 19 loss:0.05662057548761368 norm:0.00012159958714619279 max memory_allocated 47469.1044921875 
[2025-03-20 10:52:41 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [14, 15]
[2025-03-20 10:52:41 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [16, 17, 18] ===
[2025-03-20 10:54:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 0 loss:0.1334916055202484 norm:0.0012694818433374166 max memory_allocated 47469.4169921875 
[2025-03-20 10:55:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 1 loss:0.1125180795788765 norm:0.0005609389627352357 max memory_allocated 47469.4169921875 
[2025-03-20 10:56:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 2 loss:0.0955716073513031 norm:0.0003380077250767499 max memory_allocated 47469.4169921875 
[2025-03-20 10:58:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 3 loss:0.08991817384958267 norm:0.00029263977194204926 max memory_allocated 47469.4169921875 
[2025-03-20 10:59:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 4 loss:0.08667077124118805 norm:0.000269300740910694 max memory_allocated 47469.4169921875 
[2025-03-20 11:00:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 5 loss:0.08502823114395142 norm:0.00026254006661474705 max memory_allocated 47469.4169921875 
[2025-03-20 11:02:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 6 loss:0.084127277135849 norm:0.0002505685552023351 max memory_allocated 47469.4169921875 
[2025-03-20 11:03:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 7 loss:0.08351238071918488 norm:0.0002394340990576893 max memory_allocated 47469.4169921875 
[2025-03-20 11:04:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 8 loss:0.08313024044036865 norm:0.00023845827672630548 max memory_allocated 47469.4169921875 
[2025-03-20 11:06:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 9 loss:0.08276426047086716 norm:0.0002299831248819828 max memory_allocated 47469.4169921875 
[2025-03-20 11:07:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 10 loss:0.08249486237764359 norm:0.00022978604829404503 max memory_allocated 47469.4169921875 
[2025-03-20 11:08:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 11 loss:0.08229336142539978 norm:0.00022713678481522948 max memory_allocated 47469.4169921875 
[2025-03-20 11:10:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 12 loss:0.08206339180469513 norm:0.00022681309201288968 max memory_allocated 47469.4169921875 
[2025-03-20 11:11:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 13 loss:0.08191357553005219 norm:0.00022470211843028665 max memory_allocated 47469.4169921875 
[2025-03-20 11:12:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 14 loss:0.08172458410263062 norm:0.00022463779896497726 max memory_allocated 47469.4169921875 
[2025-03-20 11:14:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 15 loss:0.08160097897052765 norm:0.0002232678816653788 max memory_allocated 47469.4169921875 
[2025-03-20 11:15:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 16 loss:0.08144544810056686 norm:0.00021975253184791654 max memory_allocated 47469.4169921875 
[2025-03-20 11:16:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 17 loss:0.08134139329195023 norm:0.0002215843414887786 max memory_allocated 47469.4169921875 
[2025-03-20 11:18:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 18 loss:0.08125920593738556 norm:0.00021897509577684104 max memory_allocated 47469.4169921875 
[2025-03-20 11:19:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 19 loss:0.08118291199207306 norm:0.00021615684090647846 max memory_allocated 47469.4169921875 
[2025-03-20 11:21:22 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [16, 17, 18]
[2025-03-20 11:21:23 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [19, 20, 21] ===
[2025-03-20 11:22:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 0 loss:0.20176434516906738 norm:0.0011136825196444988 max memory_allocated 47469.6044921875 
[2025-03-20 11:24:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 1 loss:0.17342528700828552 norm:0.0006436908151954412 max memory_allocated 47469.6044921875 
[2025-03-20 11:25:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 2 loss:0.14907613396644592 norm:0.0005059015238657594 max memory_allocated 47469.6044921875 
[2025-03-20 11:26:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 3 loss:0.14139795303344727 norm:0.0004821266047656536 max memory_allocated 47469.6044921875 
[2025-03-20 11:28:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 4 loss:0.1372714340686798 norm:0.00042931761709041893 max memory_allocated 47469.6044921875 
[2025-03-20 11:29:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 5 loss:0.1355610340833664 norm:0.0004203614080324769 max memory_allocated 47469.6044921875 
[2025-03-20 11:30:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 6 loss:0.13452349603176117 norm:0.0003978600725531578 max memory_allocated 47469.6044921875 
[2025-03-20 11:32:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 7 loss:0.133726567029953 norm:0.00041203765431419015 max memory_allocated 47469.6044921875 
[2025-03-20 11:33:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 8 loss:0.13302986323833466 norm:0.00037277708179317415 max memory_allocated 47469.6044921875 
[2025-03-20 11:34:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 9 loss:0.13238441944122314 norm:0.00036244725924916565 max memory_allocated 47469.6044921875 
[2025-03-20 11:36:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 10 loss:0.13190214335918427 norm:0.00035978021332994103 max memory_allocated 47469.6044921875 
[2025-03-20 11:37:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 11 loss:0.13146011531352997 norm:0.00035030784783884883 max memory_allocated 47469.6044921875 
[2025-03-20 11:38:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 12 loss:0.13117101788520813 norm:0.0003505989152472466 max memory_allocated 47469.6044921875 
[2025-03-20 11:40:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 13 loss:0.13084708154201508 norm:0.00034747994504868984 max memory_allocated 47469.6044921875 
[2025-03-20 11:41:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 14 loss:0.13051074743270874 norm:0.0003493226831778884 max memory_allocated 47469.6044921875 
[2025-03-20 11:42:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 15 loss:0.13025686144828796 norm:0.0003275387571193278 max memory_allocated 47469.6044921875 
[2025-03-20 11:44:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 16 loss:0.1300099939107895 norm:0.00033255910966545343 max memory_allocated 47469.6044921875 
[2025-03-20 11:45:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 17 loss:0.12979060411453247 norm:0.0003364240110386163 max memory_allocated 47469.6044921875 
[2025-03-20 11:46:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 18 loss:0.12961752712726593 norm:0.0003404780582059175 max memory_allocated 47469.6044921875 
[2025-03-20 11:48:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 19 loss:0.129446342587471 norm:0.0003308759769424796 max memory_allocated 47469.6044921875 
[2025-03-20 11:49:56 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [19, 20, 21]
[2025-03-20 11:49:56 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [22, 23, 24] ===
[2025-03-20 11:51:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 0 loss:0.2937529683113098 norm:0.0014147982001304626 max memory_allocated 47469.7919921875 
[2025-03-20 11:52:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 1 loss:0.25676658749580383 norm:0.0008020435925573111 max memory_allocated 47469.7919921875 
[2025-03-20 11:54:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 2 loss:0.2231798768043518 norm:0.0005470897885970771 max memory_allocated 47469.7919921875 
[2025-03-20 11:55:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 3 loss:0.21306239068508148 norm:0.0004802518233191222 max memory_allocated 47469.7919921875 
[2025-03-20 11:56:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 4 loss:0.20908427238464355 norm:0.00045631485409103334 max memory_allocated 47469.7919921875 
[2025-03-20 11:58:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 5 loss:0.2073250114917755 norm:0.00044311533565633 max memory_allocated 47469.7919921875 
[2025-03-20 11:59:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 6 loss:0.20615525543689728 norm:0.00042247906094416976 max memory_allocated 47469.7919921875 
[2025-03-20 12:00:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 7 loss:0.20517216622829437 norm:0.000414082664065063 max memory_allocated 47469.7919921875 
[2025-03-20 12:02:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 8 loss:0.20444568991661072 norm:0.0004019832704216242 max memory_allocated 47469.7919921875 
[2025-03-20 12:03:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 9 loss:0.20374898612499237 norm:0.0004023942456115037 max memory_allocated 47469.7919921875 
[2025-03-20 12:04:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 10 loss:0.20304051041603088 norm:0.000413326924899593 max memory_allocated 47469.7919921875 
[2025-03-20 12:06:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 11 loss:0.2025296688079834 norm:0.0004105214902665466 max memory_allocated 47469.7919921875 
[2025-03-20 12:07:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 12 loss:0.20204421877861023 norm:0.00041468627750873566 max memory_allocated 47469.7919921875 
[2025-03-20 12:08:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 13 loss:0.20165270566940308 norm:0.0004109719884581864 max memory_allocated 47469.7919921875 
[2025-03-20 12:10:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 14 loss:0.20126424729824066 norm:0.00039135219412855804 max memory_allocated 47469.7919921875 
[2025-03-20 12:11:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 15 loss:0.20089641213417053 norm:0.0003947274817619473 max memory_allocated 47469.7919921875 
[2025-03-20 12:12:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 16 loss:0.20058146119117737 norm:0.00040096338489092886 max memory_allocated 47469.7919921875 
[2025-03-20 12:14:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 17 loss:0.2002890557050705 norm:0.00040953702409751713 max memory_allocated 47469.7919921875 
[2025-03-20 12:15:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 18 loss:0.20007532835006714 norm:0.0004052637086715549 max memory_allocated 47469.7919921875 
[2025-03-20 12:16:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 19 loss:0.19979451596736908 norm:0.00039803548133932054 max memory_allocated 47469.7919921875 
[2025-03-20 12:18:31 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [22, 23, 24]
[2025-03-20 12:18:31 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [25, 26] ===
[2025-03-20 12:19:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 0 loss:0.31884443759918213 norm:0.0011443295516073704 max memory_allocated 47469.7919921875 
[2025-03-20 12:20:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 1 loss:0.29043540358543396 norm:0.000590976036619395 max memory_allocated 47469.7919921875 
[2025-03-20 12:21:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 2 loss:0.26412978768348694 norm:0.00037311037885956466 max memory_allocated 47469.7919921875 
[2025-03-20 12:22:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 3 loss:0.2570403218269348 norm:0.0003424052847549319 max memory_allocated 47469.7919921875 
[2025-03-20 12:23:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 4 loss:0.25520673394203186 norm:0.0003232197486795485 max memory_allocated 47469.7919921875 
[2025-03-20 12:23:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 5 loss:0.25422102212905884 norm:0.00029770078253932297 max memory_allocated 47469.7919921875 
[2025-03-20 12:24:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 6 loss:0.25352826714515686 norm:0.0002886949514504522 max memory_allocated 47469.7919921875 
[2025-03-20 12:25:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 7 loss:0.2528434693813324 norm:0.00028890513931401074 max memory_allocated 47469.7919921875 
[2025-03-20 12:26:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 8 loss:0.2522174119949341 norm:0.0002671353577170521 max memory_allocated 47469.7919921875 
[2025-03-20 12:27:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 9 loss:0.25172752141952515 norm:0.00027316188788972795 max memory_allocated 47469.7919921875 
[2025-03-20 12:28:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 10 loss:0.2513010799884796 norm:0.0002716279122978449 max memory_allocated 47469.7919921875 
[2025-03-20 12:29:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 11 loss:0.2509011924266815 norm:0.0002723531797528267 max memory_allocated 47469.7919921875 
[2025-03-20 12:30:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 12 loss:0.2505541145801544 norm:0.0002642592880874872 max memory_allocated 47469.7919921875 
[2025-03-20 12:31:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 13 loss:0.2502797245979309 norm:0.000275703176157549 max memory_allocated 47469.7919921875 
[2025-03-20 12:32:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 14 loss:0.250031054019928 norm:0.0002711396664381027 max memory_allocated 47469.7919921875 
[2025-03-20 12:32:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 15 loss:0.24975012242794037 norm:0.0002654799318406731 max memory_allocated 47469.7919921875 
[2025-03-20 12:33:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 16 loss:0.2495242953300476 norm:0.00026410550344735384 max memory_allocated 47469.7919921875 
[2025-03-20 12:34:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 17 loss:0.24931327998638153 norm:0.0002623698383104056 max memory_allocated 47469.7919921875 
[2025-03-20 12:35:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 18 loss:0.24915577471256256 norm:0.00025815071421675384 max memory_allocated 47469.7919921875 
[2025-03-20 12:36:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 19 loss:0.24901585280895233 norm:0.00025165220722556114 max memory_allocated 47469.7919921875 
[2025-03-20 12:37:37 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [25, 26]
[2025-03-20 12:37:37 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [27] ===
[2025-03-20 12:38:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 0 loss:0.30408957600593567 norm:0.0009553045383654535 max memory_allocated 47469.7919921875 
[2025-03-20 12:38:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 1 loss:0.2897634208202362 norm:0.0005464149289764464 max memory_allocated 47469.7919921875 
[2025-03-20 12:39:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 2 loss:0.27647367119789124 norm:0.00034165813121944666 max memory_allocated 47469.7919921875 
[2025-03-20 12:39:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 3 loss:0.27327704429626465 norm:0.00026799069019034505 max memory_allocated 47469.7919921875 
[2025-03-20 12:39:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 4 loss:0.27253541350364685 norm:0.0002769988786894828 max memory_allocated 47469.7919921875 
[2025-03-20 12:40:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 5 loss:0.2721400260925293 norm:0.0002499603433534503 max memory_allocated 47469.7919921875 
[2025-03-20 12:40:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 6 loss:0.27179232239723206 norm:0.00023309860262088478 max memory_allocated 47469.7919921875 
[2025-03-20 12:41:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 7 loss:0.27151209115982056 norm:0.00023084302665665746 max memory_allocated 47469.7919921875 
[2025-03-20 12:41:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 8 loss:0.2712363600730896 norm:0.00020693741680588573 max memory_allocated 47469.7919921875 
[2025-03-20 12:42:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 9 loss:0.2710605561733246 norm:0.00019653738127090037 max memory_allocated 47469.7919921875 
[2025-03-20 12:42:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 10 loss:0.2708975076675415 norm:0.0001950595178641379 max memory_allocated 47469.7919921875 
[2025-03-20 12:43:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 11 loss:0.2707235813140869 norm:0.00018913160602096468 max memory_allocated 47469.7919921875 
[2025-03-20 12:43:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 12 loss:0.27057555317878723 norm:0.00017631796072237194 max memory_allocated 47469.7919921875 
[2025-03-20 12:43:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 13 loss:0.2704877555370331 norm:0.00017089919128920883 max memory_allocated 47469.7919921875 
[2025-03-20 12:44:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 14 loss:0.2704014182090759 norm:0.00017419182404410094 max memory_allocated 47469.7919921875 
[2025-03-20 12:44:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 15 loss:0.2702864110469818 norm:0.00017897452926263213 max memory_allocated 47469.7919921875 
[2025-03-20 12:45:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 16 loss:0.2701941430568695 norm:0.0001682565052760765 max memory_allocated 47469.7919921875 
[2025-03-20 12:45:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 17 loss:0.27011188864707947 norm:0.00016415936988778412 max memory_allocated 47469.7919921875 
[2025-03-20 12:46:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 18 loss:0.2700231969356537 norm:0.0001686422765487805 max memory_allocated 47469.7919921875 
[2025-03-20 12:46:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 19 loss:0.2699360251426697 norm:0.00017287991067860276 max memory_allocated 47469.7919921875 
[2025-03-20 12:47:13 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [27]
[2025-03-20 12:47:13 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [28] ===
[2025-03-20 12:47:13 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 12:47:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 0 loss:0.35048916935920715 norm:0.012862492352724075 max memory_allocated 47469.7919921875 
[2025-03-20 12:48:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 1 loss:0.33366110920906067 norm:0.010188383050262928 max memory_allocated 47469.7919921875 
[2025-03-20 12:48:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 2 loss:0.3184414505958557 norm:0.007165136747062206 max memory_allocated 47469.7919921875 
[2025-03-20 12:49:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 3 loss:0.3147225081920624 norm:0.006190033163875341 max memory_allocated 47469.7919921875 
[2025-03-20 12:49:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 4 loss:0.3137829899787903 norm:0.005348402541130781 max memory_allocated 47469.7919921875 
[2025-03-20 12:49:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 5 loss:0.31308096647262573 norm:0.004600548185408115 max memory_allocated 47469.7919921875 
[2025-03-20 12:50:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 6 loss:0.3124969005584717 norm:0.0039124274626374245 max memory_allocated 47469.7919921875 
[2025-03-20 12:50:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 7 loss:0.3121308386325836 norm:0.0036444186698645353 max memory_allocated 47469.7919921875 
[2025-03-20 12:51:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 8 loss:0.31195536255836487 norm:0.003776960074901581 max memory_allocated 47469.7919921875 
[2025-03-20 12:51:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 9 loss:0.31160008907318115 norm:0.0036518489941954613 max memory_allocated 47469.7919921875 
[2025-03-20 12:52:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 10 loss:0.311418354511261 norm:0.0031985545065253973 max memory_allocated 47469.7919921875 
[2025-03-20 12:52:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 11 loss:0.3111285865306854 norm:0.0031229977030307055 max memory_allocated 47469.7919921875 
[2025-03-20 12:53:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 12 loss:0.3108639717102051 norm:0.002917306497693062 max memory_allocated 47469.7919921875 
[2025-03-20 12:53:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 13 loss:0.31071406602859497 norm:0.0028633279725909233 max memory_allocated 47469.7919921875 
[2025-03-20 12:54:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 14 loss:0.31051722168922424 norm:0.0027637311723083258 max memory_allocated 47469.7919921875 
[2025-03-20 12:54:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 15 loss:0.3104740083217621 norm:0.002799452282488346 max memory_allocated 47469.7919921875 
[2025-03-20 12:54:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 16 loss:0.31029650568962097 norm:0.0027063945308327675 max memory_allocated 47469.7919921875 
[2025-03-20 12:55:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 17 loss:0.310170441865921 norm:0.0025966630782932043 max memory_allocated 47469.7919921875 
[2025-03-20 12:55:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 18 loss:0.31004756689071655 norm:0.002504018135368824 max memory_allocated 47469.7919921875 
[2025-03-20 12:56:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 19 loss:0.30997326970100403 norm:0.0025251717306673527 max memory_allocated 47469.7919921875 
[2025-03-20 12:56:52 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [28]
[2025-03-20 12:56:52 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [29] ===
[2025-03-20 12:56:52 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 12:57:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 0 loss:0.4038683772087097 norm:0.013132664375007153 max memory_allocated 47469.7919921875 
[2025-03-20 12:57:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 1 loss:0.3843069076538086 norm:0.009311106987297535 max memory_allocated 47469.7919921875 
[2025-03-20 12:58:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 2 loss:0.36748483777046204 norm:0.006696170661598444 max memory_allocated 47469.7919921875 
[2025-03-20 12:58:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 3 loss:0.36392977833747864 norm:0.005747932009398937 max memory_allocated 47469.7919921875 
[2025-03-20 12:59:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 4 loss:0.36298471689224243 norm:0.004981435835361481 max memory_allocated 47469.7919921875 
[2025-03-20 12:59:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 5 loss:0.3622359335422516 norm:0.004329030402004719 max memory_allocated 47469.7919921875 
[2025-03-20 13:00:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 6 loss:0.3616774082183838 norm:0.00389009527862072 max memory_allocated 47469.7919921875 
[2025-03-20 13:00:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 7 loss:0.36125296354293823 norm:0.003579007461667061 max memory_allocated 47469.7919921875 
[2025-03-20 13:00:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 8 loss:0.36105936765670776 norm:0.003686807816848159 max memory_allocated 47469.7919921875 
[2025-03-20 13:01:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 9 loss:0.3607161045074463 norm:0.0034263215493410826 max memory_allocated 47469.7919921875 
[2025-03-20 13:01:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 10 loss:0.3604578673839569 norm:0.003220566548407078 max memory_allocated 47469.7919921875 
[2025-03-20 13:02:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 11 loss:0.36010944843292236 norm:0.0030145312193781137 max memory_allocated 47469.7919921875 
[2025-03-20 13:02:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 12 loss:0.35994258522987366 norm:0.002999864984303713 max memory_allocated 47469.7919921875 
[2025-03-20 13:03:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 13 loss:0.35962972044944763 norm:0.0027790390886366367 max memory_allocated 47469.7919921875 
[2025-03-20 13:03:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 14 loss:0.3595028221607208 norm:0.002821779577061534 max memory_allocated 47469.7919921875 
[2025-03-20 13:04:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 15 loss:0.35924720764160156 norm:0.002678620396181941 max memory_allocated 47469.7919921875 
[2025-03-20 13:04:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 16 loss:0.3592155873775482 norm:0.0028291563503444195 max memory_allocated 47469.7919921875 
[2025-03-20 13:05:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 17 loss:0.35903820395469666 norm:0.0026573967188596725 max memory_allocated 47469.7919921875 
[2025-03-20 13:05:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 18 loss:0.35895809531211853 norm:0.002700060373172164 max memory_allocated 47469.7919921875 
[2025-03-20 13:05:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 19 loss:0.3588272035121918 norm:0.002574659651145339 max memory_allocated 47469.7919921875 
[2025-03-20 13:06:29 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [29]
[2025-03-20 13:06:29 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [30] ===
[2025-03-20 13:06:30 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 13:06:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 0 loss:0.5430575609207153 norm:0.03408866003155708 max memory_allocated 47469.7919921875 
[2025-03-20 13:07:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 1 loss:0.5016558170318604 norm:0.022171778604388237 max memory_allocated 47469.7919921875 
[2025-03-20 13:07:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 2 loss:0.4672461450099945 norm:0.014575899578630924 max memory_allocated 47469.7919921875 
[2025-03-20 13:08:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 3 loss:0.46001237630844116 norm:0.013355872593820095 max memory_allocated 47469.7919921875 
[2025-03-20 13:08:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 4 loss:0.45759332180023193 norm:0.011860341764986515 max memory_allocated 47469.7919921875 
[2025-03-20 13:09:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 5 loss:0.45636874437332153 norm:0.012638958171010017 max memory_allocated 47469.7919921875 
[2025-03-20 13:09:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 6 loss:0.45526790618896484 norm:0.01338950078934431 max memory_allocated 47469.7919921875 
[2025-03-20 13:10:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 7 loss:0.4539989233016968 norm:0.01081820484250784 max memory_allocated 47469.7919921875 
[2025-03-20 13:10:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 8 loss:0.45312365889549255 norm:0.010829304344952106 max memory_allocated 47469.7919921875 
[2025-03-20 13:11:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 9 loss:0.4530929625034332 norm:0.010341884568333626 max memory_allocated 47469.7919921875 
[2025-03-20 13:11:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 10 loss:0.45221081376075745 norm:0.01073765754699707 max memory_allocated 47469.7919921875 
[2025-03-20 13:11:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 11 loss:0.4521922469139099 norm:0.010606633499264717 max memory_allocated 47469.7919921875 
[2025-03-20 13:12:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 12 loss:0.45156872272491455 norm:0.009650563821196556 max memory_allocated 47469.7919921875 
[2025-03-20 13:12:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 13 loss:0.45178696513175964 norm:0.009855670854449272 max memory_allocated 47469.7919921875 
[2025-03-20 13:13:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 14 loss:0.4508415460586548 norm:0.008628731593489647 max memory_allocated 47469.7919921875 
[2025-03-20 13:13:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 15 loss:0.44995880126953125 norm:0.008519550785422325 max memory_allocated 47469.7919921875 
[2025-03-20 13:14:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 16 loss:0.4505745470523834 norm:0.008949698880314827 max memory_allocated 47469.7919921875 
[2025-03-20 13:14:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 17 loss:0.45032015442848206 norm:0.008938092738389969 max memory_allocated 47469.7919921875 
[2025-03-20 13:15:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 18 loss:0.4496840834617615 norm:0.008239762857556343 max memory_allocated 47469.7919921875 
[2025-03-20 13:15:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 19 loss:0.44986075162887573 norm:0.008400584571063519 max memory_allocated 47469.7919921875 
[2025-03-20 13:16:07 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [30]
[2025-03-20 13:16:07 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [31] ===
[2025-03-20 13:16:07 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 13:16:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 0 loss:0.9433187246322632 norm:0.08359862118959427 max memory_allocated 47469.7919921875 
[2025-03-20 13:17:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 1 loss:0.8517034649848938 norm:0.057374678552150726 max memory_allocated 47469.7919921875 
[2025-03-20 13:17:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 2 loss:0.7852401733398438 norm:0.03787464275956154 max memory_allocated 47469.7919921875 
[2025-03-20 13:17:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 3 loss:0.7698637247085571 norm:0.034753311425447464 max memory_allocated 47469.7919921875 
[2025-03-20 13:18:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 4 loss:0.7615488171577454 norm:0.03114965371787548 max memory_allocated 47469.7919921875 
[2025-03-20 13:18:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 5 loss:0.7556163668632507 norm:0.027845513075590134 max memory_allocated 47469.7919921875 
[2025-03-20 13:19:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 6 loss:0.751182496547699 norm:0.02531774900853634 max memory_allocated 47469.7919921875 
[2025-03-20 13:19:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 7 loss:0.7480065822601318 norm:0.023655246943235397 max memory_allocated 47469.7919921875 
[2025-03-20 13:20:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 8 loss:0.7454413175582886 norm:0.023407718166708946 max memory_allocated 47469.7919921875 
[2025-03-20 13:20:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 9 loss:0.7434289455413818 norm:0.021673722192645073 max memory_allocated 47469.7919921875 
[2025-03-20 13:21:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 10 loss:0.7419644594192505 norm:0.022266915068030357 max memory_allocated 47469.7919921875 
[2025-03-20 13:21:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 11 loss:0.7411102652549744 norm:0.02127653732895851 max memory_allocated 47469.7919921875 
[2025-03-20 13:22:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 12 loss:0.7396374344825745 norm:0.02136208675801754 max memory_allocated 47469.7919921875 
[2025-03-20 13:22:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 13 loss:0.7386963367462158 norm:0.020746368914842606 max memory_allocated 47469.7919921875 
[2025-03-20 13:22:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 14 loss:0.7377170324325562 norm:0.02092866599559784 max memory_allocated 47469.7919921875 
[2025-03-20 13:23:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 15 loss:0.7367104887962341 norm:0.019819073379039764 max memory_allocated 47469.7919921875 
[2025-03-20 13:23:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 16 loss:0.7362081408500671 norm:0.01953900046646595 max memory_allocated 47469.7919921875 
[2025-03-20 13:24:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 17 loss:0.7357640862464905 norm:0.01921127736568451 max memory_allocated 47469.7919921875 
[2025-03-20 13:24:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 18 loss:0.7341763377189636 norm:0.01786740869283676 max memory_allocated 47469.7919921875 
[2025-03-20 13:25:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 19 loss:0.7337924242019653 norm:0.017766393721103668 max memory_allocated 47469.7919921875 
[2025-03-20 13:25:47 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 15, block: [31]
[2025-03-20 13:25:47 root] (main_calib_config3_attn.py 379): INFO 18369.361836194992
[2025-03-20 13:25:52 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-20 13:26:38 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.867445945739746
[2025-03-20 13:26:38 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-20 13:27:50 root] (main_calib_config3_attn.py 161): INFO c4 : 7.331217288970947
[2025-03-20 13:28:00 datasets.load] (load.py 1272): WARNING Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/hellaswag/512a66dd8b1b1643ab4a48aa4f150d04c91680da6a4096498a5e5f799623d5ae (last modified on Tue Feb 18 03:27:10 2025) since it couldn't be found locally at hellaswag., or remotely on the Hugging Face Hub.
[2025-03-20 14:03:53 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.867445945739746, 'c4': 7.331217288970947, 'results': {'hellaswag': {'acc': 0.5548695478988249, 'acc_stderr': 0.004959645263390241, 'acc_norm': 0.7175861382194781, 'acc_norm_stderr': 0.004492535748097639}, 'arc_challenge': {'acc': 0.38310580204778155, 'acc_stderr': 0.014206472661672881, 'acc_norm': 0.40017064846416384, 'acc_norm_stderr': 0.014317197787809174}, 'winogrande': {'acc': 0.659037095501184, 'acc_stderr': 0.013322681435934805}, 'piqa': {'acc': 0.7818280739934712, 'acc_stderr': 0.009636081958374381, 'acc_norm': 0.7763873775843307, 'acc_norm_stderr': 0.009721489519176283}, 'boolq': {'acc': 0.7122324159021407, 'acc_stderr': 0.007918161273721634}, 'arc_easy': {'acc': 0.6717171717171717, 'acc_stderr': 0.009635749509262163, 'acc_norm': 0.5227272727272727, 'acc_norm_stderr': 0.010249179090605987}}, 'versions': {'hellaswag': 0, 'arc_challenge': 0, 'winogrande': 0, 'piqa': 0, 'boolq': 1, 'arc_easy': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-20 14:03:53 root] (main_calib_config3_attn.py 175): INFO 38.31,67.17,71.22,55.49,78.18,65.90
