[2025-03-23 01:51:19 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/llama-7b-hf-0.8', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.8.pkl', blocks_pkl='./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-23 01:52:47 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-23 01:52:47 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-23 01:52:48 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-23 01:52:48 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.8.pkl
[2025-03-23 01:52:48 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 5), (5, 8), (8, 11), (11, 14), (14, 16), (16, 19), (19, 22), (22, 25), (25, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-23 01:52:48 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2, 3, 4], [5, 6, 7], [8, 9, 10], [11, 12, 13], [14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26], [27], [28], [29], [30], [31]]
[2025-03-23 01:52:50 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-23 01:52:50 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 01:53:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.0015039292629808187 norm:0.004173396620899439 max memory_allocated 34630.880859375 
[2025-03-23 01:53:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.0010539920767769217 norm:0.0034411668311804533 max memory_allocated 34630.880859375 
[2025-03-23 01:54:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.0009752794867381454 norm:0.0027457557152956724 max memory_allocated 34630.880859375 
[2025-03-23 01:54:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0009138669120147824 norm:0.002266368130221963 max memory_allocated 34630.880859375 
[2025-03-23 01:55:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.000831884506624192 norm:0.0018389764009043574 max memory_allocated 34630.880859375 
[2025-03-23 01:55:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0007909375126473606 norm:0.0014875817578285933 max memory_allocated 34630.880859375 
[2025-03-23 01:56:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.0007522910600528121 norm:0.001261349767446518 max memory_allocated 34630.880859375 
[2025-03-23 01:56:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0007347083301283419 norm:0.001066758530214429 max memory_allocated 34630.880859375 
[2025-03-23 01:57:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0007114814943633974 norm:0.0009247283451259136 max memory_allocated 34630.880859375 
[2025-03-23 01:57:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.0006905011832714081 norm:0.0008135936222970486 max memory_allocated 34630.880859375 
[2025-03-23 01:58:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.000685409118887037 norm:0.0007142670801840723 max memory_allocated 34630.880859375 
[2025-03-23 01:58:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0006857078988105059 norm:0.000640287296846509 max memory_allocated 34630.880859375 
[2025-03-23 01:59:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.000697033538017422 norm:0.0005534597439691424 max memory_allocated 34630.880859375 
[2025-03-23 01:59:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.000663626124151051 norm:0.00046510001993738115 max memory_allocated 34630.880859375 
[2025-03-23 02:00:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.000659625104162842 norm:0.00043632485903799534 max memory_allocated 34630.880859375 
[2025-03-23 02:00:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.0006570514524355531 norm:0.0004173520428594202 max memory_allocated 34630.880859375 
[2025-03-23 02:01:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.0006539371097460389 norm:0.0003918739384971559 max memory_allocated 34630.880859375 
[2025-03-23 02:01:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0006497016875073314 norm:0.0003546859079506248 max memory_allocated 34630.880859375 
[2025-03-23 02:02:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.0006530337268486619 norm:0.00035835261223837733 max memory_allocated 34630.880859375 
[2025-03-23 02:02:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0006538446177728474 norm:0.0003577436145860702 max memory_allocated 34630.880859375 
[2025-03-23 02:03:07 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-23 02:03:07 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-23 02:03:08 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:03:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.008636320009827614 norm:0.014411086216568947 max memory_allocated 35097.7724609375 
[2025-03-23 02:04:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.004251571372151375 norm:0.00895947776734829 max memory_allocated 35097.7724609375 
[2025-03-23 02:04:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.0033815293572843075 norm:0.005240718834102154 max memory_allocated 35097.7724609375 
[2025-03-23 02:05:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.0030872100032866 norm:0.0041357409209012985 max memory_allocated 35097.7724609375 
[2025-03-23 02:05:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.002903147367760539 norm:0.0035915677435696125 max memory_allocated 35097.7724609375 
[2025-03-23 02:06:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.0028149092104285955 norm:0.003336334601044655 max memory_allocated 35097.7724609375 
[2025-03-23 02:06:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.0027174740098416805 norm:0.0030002957209944725 max memory_allocated 35097.7724609375 
[2025-03-23 02:07:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.002649410394951701 norm:0.0027834626380354166 max memory_allocated 35097.7724609375 
[2025-03-23 02:07:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.002552745630964637 norm:0.0025699445977807045 max memory_allocated 35097.7724609375 
[2025-03-23 02:07:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.002559749875217676 norm:0.002405837643891573 max memory_allocated 35097.7724609375 
[2025-03-23 02:08:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.0025010781828314066 norm:0.0021813714411109686 max memory_allocated 35097.7724609375 
[2025-03-23 02:08:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.0024426765739917755 norm:0.001875575166195631 max memory_allocated 35097.7724609375 
[2025-03-23 02:09:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.0024260315112769604 norm:0.0017122164135798812 max memory_allocated 35097.7724609375 
[2025-03-23 02:09:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.0023762392811477184 norm:0.0014631273224949837 max memory_allocated 35097.7724609375 
[2025-03-23 02:10:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.0023714182898402214 norm:0.0013152554165571928 max memory_allocated 35097.7724609375 
[2025-03-23 02:10:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.002381681464612484 norm:0.0011589106870815158 max memory_allocated 35097.7724609375 
[2025-03-23 02:11:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.0023231501691043377 norm:0.0010243186261504889 max memory_allocated 35097.7724609375 
[2025-03-23 02:11:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.002310299314558506 norm:0.000902370666153729 max memory_allocated 35097.7724609375 
[2025-03-23 02:12:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.002329425886273384 norm:0.0008374153985641897 max memory_allocated 35097.7724609375 
[2025-03-23 02:12:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.002341741928830743 norm:0.0008887869189493358 max memory_allocated 35097.7724609375 
[2025-03-23 02:13:24 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-23 02:13:24 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2, 3, 4] ===
[2025-03-23 02:13:24 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:14:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 0 loss:0.021701175719499588 norm:0.013407133519649506 max memory_allocated 47468.5419921875 
[2025-03-23 02:16:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 1 loss:0.23844404518604279 norm:0.6158443093299866 max memory_allocated 47468.5419921875 
[2025-03-23 02:17:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 2 loss:0.032539837062358856 norm:0.022090794518589973 max memory_allocated 47468.5419921875 
[2025-03-23 02:19:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 3 loss:0.017290636897087097 norm:0.009130025282502174 max memory_allocated 47468.5419921875 
[2025-03-23 02:20:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 4 loss:0.013796078972518444 norm:0.0049828654155135155 max memory_allocated 47468.5419921875 
[2025-03-23 02:22:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 5 loss:0.012485523708164692 norm:0.003285784274339676 max memory_allocated 47468.5419921875 
[2025-03-23 02:23:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 6 loss:0.011818578466773033 norm:0.0025190708693116903 max memory_allocated 47468.5419921875 
[2025-03-23 02:24:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 7 loss:0.011417966336011887 norm:0.0022235282231122255 max memory_allocated 47468.5419921875 
[2025-03-23 02:26:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 8 loss:0.011093937791883945 norm:0.0020532177295535803 max memory_allocated 47468.5419921875 
[2025-03-23 02:27:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 9 loss:0.010852489620447159 norm:0.001977981999516487 max memory_allocated 47468.5419921875 
[2025-03-23 02:29:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 10 loss:0.010640439577400684 norm:0.0019967928528785706 max memory_allocated 47468.5419921875 
[2025-03-23 02:30:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 11 loss:0.010458243079483509 norm:0.002003795001655817 max memory_allocated 47468.5419921875 
[2025-03-23 02:32:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 12 loss:0.010312477126717567 norm:0.00204419600777328 max memory_allocated 47468.5419921875 
[2025-03-23 02:33:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 13 loss:0.010186906903982162 norm:0.0020875437185168266 max memory_allocated 47468.5419921875 
[2025-03-23 02:35:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 14 loss:0.010060194879770279 norm:0.0021672588773071766 max memory_allocated 47468.5419921875 
[2025-03-23 02:36:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 15 loss:0.009950120933353901 norm:0.0022131362929940224 max memory_allocated 47468.5419921875 
[2025-03-23 02:37:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 16 loss:0.009870663285255432 norm:0.0023337542079389095 max memory_allocated 47468.5419921875 
[2025-03-23 02:39:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 17 loss:0.009789640083909035 norm:0.002403680235147476 max memory_allocated 47468.5419921875 
[2025-03-23 02:40:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 18 loss:0.00970601849257946 norm:0.002562078647315502 max memory_allocated 47468.5419921875 
[2025-03-23 02:42:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 19 loss:0.009633130393922329 norm:0.00272896746173501 max memory_allocated 47468.5419921875 
[2025-03-23 02:43:57 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2, 3, 4]
[2025-03-23 02:43:57 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [5, 6, 7] ===
[2025-03-23 02:45:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 0 loss:0.020101996138691902 norm:0.000445595127530396 max memory_allocated 47468.7294921875 
[2025-03-23 02:46:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 1 loss:0.01775660365819931 norm:0.00027181179029867053 max memory_allocated 47468.7294921875 
[2025-03-23 02:48:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 2 loss:0.016479134559631348 norm:0.00019968142441939563 max memory_allocated 47468.7294921875 
[2025-03-23 02:49:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 3 loss:0.01502727996557951 norm:0.00016619014786556363 max memory_allocated 47468.7294921875 
[2025-03-23 02:51:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 4 loss:0.01384611614048481 norm:0.0001625628356123343 max memory_allocated 47468.7294921875 
[2025-03-23 02:52:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 5 loss:0.013072353787720203 norm:0.00015074078692123294 max memory_allocated 47468.7294921875 
[2025-03-23 02:54:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 6 loss:0.012623685412108898 norm:0.0001364630152238533 max memory_allocated 47468.7294921875 
[2025-03-23 02:55:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 7 loss:0.012436497956514359 norm:0.00014038136578164995 max memory_allocated 47468.7294921875 
[2025-03-23 02:56:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 8 loss:0.0123281329870224 norm:0.00013344036415219307 max memory_allocated 47468.7294921875 
[2025-03-23 02:58:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 9 loss:0.012267599813640118 norm:0.00013336591655388474 max memory_allocated 47468.7294921875 
[2025-03-23 02:59:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 10 loss:0.012241410091519356 norm:0.00014750157424714416 max memory_allocated 47468.7294921875 
[2025-03-23 03:01:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 11 loss:0.012198706157505512 norm:0.0001392988779116422 max memory_allocated 47468.7294921875 
[2025-03-23 03:02:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 12 loss:0.012168947607278824 norm:0.00013311098155099899 max memory_allocated 47468.7294921875 
[2025-03-23 03:04:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 13 loss:0.012157503515481949 norm:0.0001438673207303509 max memory_allocated 47468.7294921875 
[2025-03-23 03:05:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 14 loss:0.012157143093645573 norm:0.00014656467828899622 max memory_allocated 47468.7294921875 
[2025-03-23 03:06:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 15 loss:0.012121676467359066 norm:0.00013586170098278672 max memory_allocated 47468.7294921875 
[2025-03-23 03:08:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 16 loss:0.01211757306009531 norm:0.00013927930558566004 max memory_allocated 47468.7294921875 
[2025-03-23 03:09:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 17 loss:0.012090814299881458 norm:0.0001432392600690946 max memory_allocated 47468.7294921875 
[2025-03-23 03:11:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 18 loss:0.01207638718187809 norm:0.00013740919530391693 max memory_allocated 47468.7294921875 
[2025-03-23 03:12:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 19 loss:0.012091848067939281 norm:0.0001425103982910514 max memory_allocated 47468.7294921875 
[2025-03-23 03:14:28 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [5, 6, 7]
[2025-03-23 03:14:28 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [8, 9, 10] ===
[2025-03-23 03:16:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 0 loss:0.026366496458649635 norm:0.0006090497481636703 max memory_allocated 47468.9169921875 
[2025-03-23 03:17:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 1 loss:0.02218770422041416 norm:0.0003433252568356693 max memory_allocated 47468.9169921875 
[2025-03-23 03:18:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 2 loss:0.020254967734217644 norm:0.00024837887031026185 max memory_allocated 47468.9169921875 
[2025-03-23 03:20:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 3 loss:0.01860637217760086 norm:0.0001993216574192047 max memory_allocated 47468.9169921875 
[2025-03-23 03:21:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 4 loss:0.017306629568338394 norm:0.0001802280603442341 max memory_allocated 47468.9169921875 
[2025-03-23 03:23:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 5 loss:0.016402509063482285 norm:0.0001644860312808305 max memory_allocated 47468.9169921875 
[2025-03-23 03:24:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 6 loss:0.015850316733121872 norm:0.00015555095160380006 max memory_allocated 47468.9169921875 
[2025-03-23 03:26:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 7 loss:0.015535490587353706 norm:0.00014702464977744967 max memory_allocated 47468.9169921875 
[2025-03-23 03:27:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 8 loss:0.015350539237260818 norm:0.00014189883950166404 max memory_allocated 47468.9169921875 
[2025-03-23 03:28:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 9 loss:0.01525036245584488 norm:0.00016301876166835427 max memory_allocated 47468.9169921875 
[2025-03-23 03:30:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 10 loss:0.015090436674654484 norm:0.00012795082875527442 max memory_allocated 47468.9169921875 
[2025-03-23 03:31:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 11 loss:0.014975854195654392 norm:0.00012046360643580556 max memory_allocated 47468.9169921875 
[2025-03-23 03:33:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 12 loss:0.014929219149053097 norm:0.00012324491399340332 max memory_allocated 47468.9169921875 
[2025-03-23 03:34:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 13 loss:0.014890087768435478 norm:0.00012027384946122766 max memory_allocated 47468.9169921875 
[2025-03-23 03:36:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 14 loss:0.014859376475214958 norm:0.00011413164611440152 max memory_allocated 47468.9169921875 
[2025-03-23 03:37:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 15 loss:0.014821861870586872 norm:0.00011217513383598998 max memory_allocated 47468.9169921875 
[2025-03-23 03:38:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 16 loss:0.01477275975048542 norm:0.00010955811740132049 max memory_allocated 47468.9169921875 
[2025-03-23 03:40:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 17 loss:0.014772161841392517 norm:0.00011665046622511 max memory_allocated 47468.9169921875 
[2025-03-23 03:41:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 18 loss:0.014754390344023705 norm:0.00011409946455387399 max memory_allocated 47468.9169921875 
[2025-03-23 03:43:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 19 loss:0.014750270172953606 norm:0.00011315698066027835 max memory_allocated 47468.9169921875 
[2025-03-23 03:45:04 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [8, 9, 10]
[2025-03-23 03:45:04 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [11, 12, 13] ===
[2025-03-23 03:46:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 0 loss:0.02510102093219757 norm:0.0004000600310973823 max memory_allocated 47469.1044921875 
[2025-03-23 03:48:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 1 loss:0.02223089337348938 norm:0.00026025957777164876 max memory_allocated 47469.1044921875 
[2025-03-23 03:49:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 2 loss:0.020586233586072922 norm:0.0002044581196969375 max memory_allocated 47469.1044921875 
[2025-03-23 03:50:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 3 loss:0.01925569400191307 norm:0.00017314343131147325 max memory_allocated 47469.1044921875 
[2025-03-23 03:52:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 4 loss:0.01815088838338852 norm:0.00015933225222397596 max memory_allocated 47469.1044921875 
[2025-03-23 03:53:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 5 loss:0.017275311052799225 norm:0.00014502239355351776 max memory_allocated 47469.1044921875 
[2025-03-23 03:55:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 6 loss:0.016787219792604446 norm:0.00014070671750232577 max memory_allocated 47469.1044921875 
[2025-03-23 03:56:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 7 loss:0.016437668353319168 norm:0.00014002781244926155 max memory_allocated 47469.1044921875 
[2025-03-23 03:58:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 8 loss:0.016228847205638885 norm:0.0001245607709279284 max memory_allocated 47469.1044921875 
[2025-03-23 03:59:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 9 loss:0.016056369990110397 norm:0.00011625316255958751 max memory_allocated 47469.1044921875 
[2025-03-23 04:00:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 10 loss:0.01594037562608719 norm:0.00010912502330029383 max memory_allocated 47469.1044921875 
[2025-03-23 04:02:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 11 loss:0.015851665288209915 norm:0.00010732741066021845 max memory_allocated 47469.1044921875 
[2025-03-23 04:03:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 12 loss:0.015783848240971565 norm:0.0001000052216113545 max memory_allocated 47469.1044921875 
[2025-03-23 04:05:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 13 loss:0.01576213538646698 norm:0.00010010487312683836 max memory_allocated 47469.1044921875 
[2025-03-23 04:06:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 14 loss:0.01573699526488781 norm:0.0001021805073833093 max memory_allocated 47469.1044921875 
[2025-03-23 04:08:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 15 loss:0.01569952256977558 norm:0.00010127654240932316 max memory_allocated 47469.1044921875 
[2025-03-23 04:09:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 16 loss:0.0156734436750412 norm:0.0001001932832878083 max memory_allocated 47469.1044921875 
[2025-03-23 04:10:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 17 loss:0.01564430445432663 norm:0.0001093677565222606 max memory_allocated 47469.1044921875 
[2025-03-23 04:12:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 18 loss:0.01562519744038582 norm:0.0001139868181780912 max memory_allocated 47469.1044921875 
[2025-03-23 04:13:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 19 loss:0.01559481956064701 norm:0.00010036534513346851 max memory_allocated 47469.1044921875 
[2025-03-23 04:15:41 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [11, 12, 13]
[2025-03-23 04:15:41 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [14, 15] ===
[2025-03-23 04:16:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 0 loss:0.024156231433153152 norm:0.00034844354377128184 max memory_allocated 47469.1044921875 
[2025-03-23 04:17:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 1 loss:0.022193072363734245 norm:0.0002083139552269131 max memory_allocated 47469.1044921875 
[2025-03-23 04:18:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 2 loss:0.020868616178631783 norm:0.00015329984307754785 max memory_allocated 47469.1044921875 
[2025-03-23 04:19:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 3 loss:0.019595876336097717 norm:0.00013381443568505347 max memory_allocated 47469.1044921875 
[2025-03-23 04:20:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 4 loss:0.018560538068413734 norm:0.00011911238834727556 max memory_allocated 47469.1044921875 
[2025-03-23 04:21:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 5 loss:0.017957737669348717 norm:0.00010890250268857926 max memory_allocated 47469.1044921875 
[2025-03-23 04:22:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 6 loss:0.01771244965493679 norm:9.759908425621688e-05 max memory_allocated 47469.1044921875 
[2025-03-23 04:23:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 7 loss:0.017548661679029465 norm:9.08238289412111e-05 max memory_allocated 47469.1044921875 
[2025-03-23 04:24:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 8 loss:0.017439376562833786 norm:8.272432023659348e-05 max memory_allocated 47469.1044921875 
[2025-03-23 04:25:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 9 loss:0.01737358048558235 norm:8.188970241462812e-05 max memory_allocated 47469.1044921875 
[2025-03-23 04:26:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 10 loss:0.01728648692369461 norm:7.474694575648755e-05 max memory_allocated 47469.1044921875 
[2025-03-23 04:27:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 11 loss:0.017233068123459816 norm:6.997770833550021e-05 max memory_allocated 47469.1044921875 
[2025-03-23 04:28:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 12 loss:0.017196420580148697 norm:6.387704343069345e-05 max memory_allocated 47469.1044921875 
[2025-03-23 04:29:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 13 loss:0.017189372330904007 norm:6.950456736376509e-05 max memory_allocated 47469.1044921875 
[2025-03-23 04:30:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 14 loss:0.01715625822544098 norm:6.672466406598687e-05 max memory_allocated 47469.1044921875 
[2025-03-23 04:31:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 15 loss:0.01713416911661625 norm:6.290659075602889e-05 max memory_allocated 47469.1044921875 
[2025-03-23 04:31:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 16 loss:0.01710916869342327 norm:6.0513022617669776e-05 max memory_allocated 47469.1044921875 
[2025-03-23 04:32:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 17 loss:0.017078038305044174 norm:5.8009747590404004e-05 max memory_allocated 47469.1044921875 
[2025-03-23 04:33:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 18 loss:0.01705685816705227 norm:5.7650526287034154e-05 max memory_allocated 47469.1044921875 
[2025-03-23 04:34:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 19 loss:0.0170399509370327 norm:5.7605117035564035e-05 max memory_allocated 47469.1044921875 
[2025-03-23 04:36:04 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [14, 15]
[2025-03-23 04:36:04 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [16, 17, 18] ===
[2025-03-23 04:37:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 0 loss:0.03560461476445198 norm:0.0003950395912397653 max memory_allocated 47471.4169921875 
[2025-03-23 04:39:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 1 loss:0.03250101953744888 norm:0.00026982431882061064 max memory_allocated 47471.4169921875 
[2025-03-23 04:40:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 2 loss:0.030275512486696243 norm:0.00021317051141522825 max memory_allocated 47471.4169921875 
[2025-03-23 04:41:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 3 loss:0.028217151761054993 norm:0.00019063660874962807 max memory_allocated 47471.4169921875 
[2025-03-23 04:43:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 4 loss:0.02659110352396965 norm:0.00017613274394534528 max memory_allocated 47471.4169921875 
[2025-03-23 04:44:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 5 loss:0.025864655151963234 norm:0.00016438393504358828 max memory_allocated 47471.4169921875 
[2025-03-23 04:46:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 6 loss:0.025525858625769615 norm:0.00015668479318264872 max memory_allocated 47471.4169921875 
[2025-03-23 04:47:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 7 loss:0.025267895311117172 norm:0.0001419522741343826 max memory_allocated 47471.4169921875 
[2025-03-23 04:49:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 8 loss:0.02509809285402298 norm:0.0001431663113180548 max memory_allocated 47471.4169921875 
[2025-03-23 04:50:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 9 loss:0.02493269182741642 norm:0.00013612033217214048 max memory_allocated 47471.4169921875 
[2025-03-23 04:51:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 10 loss:0.024843202903866768 norm:0.0001320728042628616 max memory_allocated 47471.4169921875 
[2025-03-23 04:53:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 11 loss:0.024776948615908623 norm:0.00013221865810919553 max memory_allocated 47471.4169921875 
[2025-03-23 04:54:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 12 loss:0.024706613272428513 norm:0.00012814982619602233 max memory_allocated 47471.4169921875 
[2025-03-23 04:56:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 13 loss:0.024703290313482285 norm:0.00013588604633696377 max memory_allocated 47471.4169921875 
[2025-03-23 04:57:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 14 loss:0.02465245872735977 norm:0.00012657658953685313 max memory_allocated 47471.4169921875 
[2025-03-23 04:59:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 15 loss:0.024595584720373154 norm:0.00012345521827228367 max memory_allocated 47471.4169921875 
[2025-03-23 05:00:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 16 loss:0.02455066703259945 norm:0.00012286579294595867 max memory_allocated 47471.4169921875 
[2025-03-23 05:01:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 17 loss:0.024515872821211815 norm:0.00012666663678828627 max memory_allocated 47471.4169921875 
[2025-03-23 05:03:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 18 loss:0.024491004645824432 norm:0.00012900400906801224 max memory_allocated 47471.4169921875 
[2025-03-23 05:04:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 19 loss:0.02446647174656391 norm:0.00012722224346362054 max memory_allocated 47471.4169921875 
[2025-03-23 05:06:41 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [16, 17, 18]
[2025-03-23 05:06:41 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [19, 20, 21] ===
[2025-03-23 05:08:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 0 loss:0.05570945888757706 norm:0.0005043871351517737 max memory_allocated 47471.4169921875 
[2025-03-23 05:09:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 1 loss:0.0522148422896862 norm:0.00029868684941902757 max memory_allocated 47471.4169921875 
[2025-03-23 05:11:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 2 loss:0.04857691749930382 norm:0.00024728692369535565 max memory_allocated 47471.4169921875 
[2025-03-23 05:12:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 3 loss:0.044586844742298126 norm:0.00021181907504796982 max memory_allocated 47471.4169921875 
[2025-03-23 05:13:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 4 loss:0.04298551008105278 norm:0.00019545800751075149 max memory_allocated 47471.4169921875 
[2025-03-23 05:15:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 5 loss:0.04261063039302826 norm:0.0001865185477072373 max memory_allocated 47471.4169921875 
[2025-03-23 05:16:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 6 loss:0.04235417768359184 norm:0.0001706794137135148 max memory_allocated 47471.4169921875 
[2025-03-23 05:18:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 7 loss:0.04213681071996689 norm:0.0001599147799424827 max memory_allocated 47471.4169921875 
[2025-03-23 05:19:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 8 loss:0.041961025446653366 norm:0.00014601220027543604 max memory_allocated 47471.4169921875 
[2025-03-23 05:21:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 9 loss:0.04184451699256897 norm:0.0001451359421480447 max memory_allocated 47471.4169921875 
[2025-03-23 05:22:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 10 loss:0.04171033576130867 norm:0.0001355356944259256 max memory_allocated 47471.4169921875 
[2025-03-23 05:23:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 11 loss:0.041632022708654404 norm:0.0001324568729614839 max memory_allocated 47471.4169921875 
[2025-03-23 05:25:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 12 loss:0.041582874953746796 norm:0.0001373515260638669 max memory_allocated 47471.4169921875 
[2025-03-23 05:26:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 13 loss:0.04152212291955948 norm:0.00012949535448569804 max memory_allocated 47471.4169921875 
[2025-03-23 05:28:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 14 loss:0.041436873376369476 norm:0.00012748476001434028 max memory_allocated 47471.4169921875 
[2025-03-23 05:29:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 15 loss:0.04140329360961914 norm:0.00013483126531355083 max memory_allocated 47471.4169921875 
[2025-03-23 05:31:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 16 loss:0.041358642280101776 norm:0.00013291175127960742 max memory_allocated 47471.4169921875 
[2025-03-23 05:32:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 17 loss:0.04133445397019386 norm:0.00013216276420280337 max memory_allocated 47471.4169921875 
[2025-03-23 05:34:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 18 loss:0.041303008794784546 norm:0.00012139708996983245 max memory_allocated 47471.4169921875 
[2025-03-23 05:35:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 19 loss:0.041294317692518234 norm:0.00011704587086569518 max memory_allocated 47471.4169921875 
[2025-03-23 05:37:15 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [19, 20, 21]
[2025-03-23 05:37:15 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [22, 23, 24] ===
[2025-03-23 05:38:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 0 loss:0.08186709880828857 norm:0.0005147576448507607 max memory_allocated 47471.4169921875 
[2025-03-23 05:40:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 1 loss:0.07742266356945038 norm:0.0003785320441238582 max memory_allocated 47471.4169921875 
[2025-03-23 05:41:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 2 loss:0.07196833193302155 norm:0.0003165052330587059 max memory_allocated 47471.4169921875 
[2025-03-23 05:43:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 3 loss:0.06739693135023117 norm:0.0002898850943893194 max memory_allocated 47471.4169921875 
[2025-03-23 05:44:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 4 loss:0.06636762619018555 norm:0.00026211177464574575 max memory_allocated 47471.4169921875 
[2025-03-23 05:45:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 5 loss:0.06596127152442932 norm:0.00025183905381709337 max memory_allocated 47471.4169921875 
[2025-03-23 05:47:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 6 loss:0.06567345559597015 norm:0.0002270731347380206 max memory_allocated 47471.4169921875 
[2025-03-23 05:48:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 7 loss:0.06547366082668304 norm:0.00021917860431130975 max memory_allocated 47471.4169921875 
[2025-03-23 05:50:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 8 loss:0.06530903279781342 norm:0.00021307611314114183 max memory_allocated 47471.4169921875 
[2025-03-23 05:51:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 9 loss:0.06517153233289719 norm:0.0002095852978527546 max memory_allocated 47471.4169921875 
[2025-03-23 05:53:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 10 loss:0.06499889492988586 norm:0.00019621392129920423 max memory_allocated 47471.4169921875 
[2025-03-23 05:54:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 11 loss:0.0648929700255394 norm:0.00019548939599189907 max memory_allocated 47471.4169921875 
[2025-03-23 05:55:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 12 loss:0.06478358805179596 norm:0.00018609376274980605 max memory_allocated 47471.4169921875 
[2025-03-23 05:57:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 13 loss:0.0647052526473999 norm:0.00017550474149174988 max memory_allocated 47471.4169921875 
[2025-03-23 05:58:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 14 loss:0.06465774774551392 norm:0.00017776258755475283 max memory_allocated 47471.4169921875 
[2025-03-23 06:00:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 15 loss:0.06459292769432068 norm:0.00017811451107263565 max memory_allocated 47471.4169921875 
[2025-03-23 06:01:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 16 loss:0.06454552710056305 norm:0.00017482720431871712 max memory_allocated 47471.4169921875 
[2025-03-23 06:03:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 17 loss:0.06451679021120071 norm:0.0001734921825118363 max memory_allocated 47471.4169921875 
[2025-03-23 06:04:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 18 loss:0.06447358429431915 norm:0.00017518570530228317 max memory_allocated 47471.4169921875 
[2025-03-23 06:06:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 19 loss:0.06445951014757156 norm:0.00016670956392772496 max memory_allocated 47471.4169921875 
[2025-03-23 06:07:50 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [22, 23, 24]
[2025-03-23 06:07:51 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [25, 26] ===
[2025-03-23 06:08:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 0 loss:0.09465145319700241 norm:0.0005306798266246915 max memory_allocated 47471.4169921875 
[2025-03-23 06:09:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 1 loss:0.09090175479650497 norm:0.0003520085010677576 max memory_allocated 47471.4169921875 
[2025-03-23 06:10:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 2 loss:0.08596356213092804 norm:0.00028149099671281874 max memory_allocated 47471.4169921875 
[2025-03-23 06:11:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 3 loss:0.08304360508918762 norm:0.00022773434466216713 max memory_allocated 47471.4169921875 
[2025-03-23 06:12:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 4 loss:0.08264558017253876 norm:0.00020877078350167722 max memory_allocated 47471.4169921875 
[2025-03-23 06:13:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 5 loss:0.08232884109020233 norm:0.00020278847659938037 max memory_allocated 47471.4169921875 
[2025-03-23 06:14:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 6 loss:0.08206579834222794 norm:0.00016858977323863655 max memory_allocated 47471.4169921875 
[2025-03-23 06:15:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 7 loss:0.08190155774354935 norm:0.00016496401804033667 max memory_allocated 47471.4169921875 
[2025-03-23 06:16:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 8 loss:0.08175252377986908 norm:0.00015875903773121536 max memory_allocated 47471.4169921875 
[2025-03-23 06:17:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 9 loss:0.08165661245584488 norm:0.00016047380631789565 max memory_allocated 47471.4169921875 
[2025-03-23 06:18:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 10 loss:0.08158228546380997 norm:0.00015359841927420348 max memory_allocated 47471.4169921875 
[2025-03-23 06:19:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 11 loss:0.08152216672897339 norm:0.00016000322648324072 max memory_allocated 47471.4169921875 
[2025-03-23 06:20:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 12 loss:0.08146201819181442 norm:0.00015502444875892252 max memory_allocated 47471.4169921875 
[2025-03-23 06:21:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 13 loss:0.08140905201435089 norm:0.00015781175170559436 max memory_allocated 47471.4169921875 
[2025-03-23 06:22:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 14 loss:0.08135612308979034 norm:0.00014209540677256882 max memory_allocated 47471.4169921875 
[2025-03-23 06:23:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 15 loss:0.08131372928619385 norm:0.0001340600720141083 max memory_allocated 47471.4169921875 
[2025-03-23 06:24:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 16 loss:0.08126184344291687 norm:0.00013690139167010784 max memory_allocated 47471.4169921875 
[2025-03-23 06:25:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 17 loss:0.0812540277838707 norm:0.00014677652507089078 max memory_allocated 47471.4169921875 
[2025-03-23 06:26:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 18 loss:0.08124292641878128 norm:0.00014264682249631733 max memory_allocated 47471.4169921875 
[2025-03-23 06:27:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 19 loss:0.08122033625841141 norm:0.00014872002066113055 max memory_allocated 47471.4169921875 
[2025-03-23 06:28:16 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [25, 26]
[2025-03-23 06:28:16 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [27] ===
[2025-03-23 06:28:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 0 loss:0.09611029177904129 norm:0.00037077057640999556 max memory_allocated 47471.4169921875 
[2025-03-23 06:29:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 1 loss:0.0934695303440094 norm:0.00026528610032983124 max memory_allocated 47471.4169921875 
[2025-03-23 06:29:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 2 loss:0.09070863574743271 norm:0.00022693588107358664 max memory_allocated 47471.4169921875 
[2025-03-23 06:30:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 3 loss:0.08935485780239105 norm:0.00019140449876431376 max memory_allocated 47471.4169921875 
[2025-03-23 06:30:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 4 loss:0.08914831280708313 norm:0.00018343780538998544 max memory_allocated 47471.4169921875 
[2025-03-23 06:31:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 5 loss:0.08903852850198746 norm:0.0001664538576733321 max memory_allocated 47471.4169921875 
[2025-03-23 06:31:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 6 loss:0.08890419453382492 norm:0.00014053055201657116 max memory_allocated 47471.4169921875 
[2025-03-23 06:32:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 7 loss:0.08883556723594666 norm:0.00012493539543356746 max memory_allocated 47471.4169921875 
[2025-03-23 06:32:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 8 loss:0.08883717656135559 norm:0.000133864872623235 max memory_allocated 47471.4169921875 
[2025-03-23 06:33:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 9 loss:0.08878778666257858 norm:0.00012342499394435436 max memory_allocated 47471.4169921875 
[2025-03-23 06:33:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 10 loss:0.08871588110923767 norm:0.0001241550053237006 max memory_allocated 47471.4169921875 
[2025-03-23 06:34:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 11 loss:0.08866996318101883 norm:0.00012197883188491687 max memory_allocated 47471.4169921875 
[2025-03-23 06:34:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 12 loss:0.0886276587843895 norm:0.00011458770313765854 max memory_allocated 47471.4169921875 
[2025-03-23 06:35:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 13 loss:0.08861034363508224 norm:0.00012609039549715817 max memory_allocated 47471.4169921875 
[2025-03-23 06:35:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 14 loss:0.08859644830226898 norm:0.00011955828085774556 max memory_allocated 47471.4169921875 
[2025-03-23 06:35:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 15 loss:0.08859897404909134 norm:0.00011858822836074978 max memory_allocated 47471.4169921875 
[2025-03-23 06:36:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 16 loss:0.08859144151210785 norm:0.00011614445975283161 max memory_allocated 47471.4169921875 
[2025-03-23 06:36:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 17 loss:0.0885581225156784 norm:0.0001187344387290068 max memory_allocated 47471.4169921875 
[2025-03-23 06:37:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 18 loss:0.08856759965419769 norm:0.00011703609197866172 max memory_allocated 47471.4169921875 
[2025-03-23 06:37:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 19 loss:0.0885639637708664 norm:0.00011994764645351097 max memory_allocated 47471.4169921875 
[2025-03-23 06:38:41 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [27]
[2025-03-23 06:38:41 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [28] ===
[2025-03-23 06:38:41 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 06:39:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 0 loss:0.11490438133478165 norm:0.003428538329899311 max memory_allocated 47471.4169921875 
[2025-03-23 06:39:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 1 loss:0.11166021972894669 norm:0.0031177690252661705 max memory_allocated 47471.4169921875 
[2025-03-23 06:40:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 2 loss:0.10754585266113281 norm:0.002605143003165722 max memory_allocated 47471.4169921875 
[2025-03-23 06:40:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 3 loss:0.10584479570388794 norm:0.002268529264256358 max memory_allocated 47471.4169921875 
[2025-03-23 06:41:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 4 loss:0.10547690093517303 norm:0.0019199058879166842 max memory_allocated 47471.4169921875 
[2025-03-23 06:41:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 5 loss:0.10526197403669357 norm:0.001646811026148498 max memory_allocated 47471.4169921875 
[2025-03-23 06:42:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 6 loss:0.10510745644569397 norm:0.001562244025990367 max memory_allocated 47471.4169921875 
[2025-03-23 06:42:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 7 loss:0.10506089776754379 norm:0.0013607987202703953 max memory_allocated 47471.4169921875 
[2025-03-23 06:43:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 8 loss:0.10497202724218369 norm:0.0012010680511593819 max memory_allocated 47471.4169921875 
[2025-03-23 06:43:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 9 loss:0.10478323698043823 norm:0.0012565780198201537 max memory_allocated 47471.4169921875 
[2025-03-23 06:44:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 10 loss:0.10470860451459885 norm:0.0011739019537344575 max memory_allocated 47471.4169921875 
[2025-03-23 06:44:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 11 loss:0.10461963713169098 norm:0.001177063095383346 max memory_allocated 47471.4169921875 
[2025-03-23 06:44:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 12 loss:0.10457734018564224 norm:0.0011301776394248009 max memory_allocated 47471.4169921875 
[2025-03-23 06:45:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 13 loss:0.10457763075828552 norm:0.0012136769946664572 max memory_allocated 47471.4169921875 
[2025-03-23 06:45:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 14 loss:0.10461805760860443 norm:0.0010305590694770217 max memory_allocated 47471.4169921875 
[2025-03-23 06:46:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 15 loss:0.10451706498861313 norm:0.0011300405021756887 max memory_allocated 47471.4169921875 
[2025-03-23 06:46:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 16 loss:0.10452865064144135 norm:0.0010654330253601074 max memory_allocated 47471.4169921875 
[2025-03-23 06:47:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 17 loss:0.10442980378866196 norm:0.0011203987523913383 max memory_allocated 47471.4169921875 
[2025-03-23 06:47:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 18 loss:0.10443129390478134 norm:0.0010077566839754581 max memory_allocated 47471.4169921875 
[2025-03-23 06:48:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 19 loss:0.10448836535215378 norm:0.0012027067132294178 max memory_allocated 47471.4169921875 
[2025-03-23 06:48:57 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [28]
[2025-03-23 06:48:57 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [29] ===
[2025-03-23 06:48:58 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 06:49:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 0 loss:0.13378563523292542 norm:0.003628587117418647 max memory_allocated 47471.4169921875 
[2025-03-23 06:49:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 1 loss:0.13018925487995148 norm:0.003063290612772107 max memory_allocated 47471.4169921875 
[2025-03-23 06:50:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 2 loss:0.12578308582305908 norm:0.0025484715588390827 max memory_allocated 47471.4169921875 
[2025-03-23 06:50:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 3 loss:0.12429653108119965 norm:0.0023232088424265385 max memory_allocated 47471.4169921875 
[2025-03-23 06:51:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 4 loss:0.12399163842201233 norm:0.0020093026105314493 max memory_allocated 47471.4169921875 
[2025-03-23 06:51:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 5 loss:0.12377749383449554 norm:0.0017547679599374533 max memory_allocated 47471.4169921875 
[2025-03-23 06:52:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 6 loss:0.12362890690565109 norm:0.0015632645227015018 max memory_allocated 47471.4169921875 
[2025-03-23 06:52:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 7 loss:0.12355786561965942 norm:0.001470197457820177 max memory_allocated 47471.4169921875 
[2025-03-23 06:53:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 8 loss:0.12341389060020447 norm:0.0013969439314678311 max memory_allocated 47471.4169921875 
[2025-03-23 06:53:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 9 loss:0.12337440252304077 norm:0.001331200124695897 max memory_allocated 47471.4169921875 
[2025-03-23 06:54:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 10 loss:0.12336208671331406 norm:0.0013465990778058767 max memory_allocated 47471.4169921875 
[2025-03-23 06:54:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 11 loss:0.12336613982915878 norm:0.0012358942767605186 max memory_allocated 47471.4169921875 
[2025-03-23 06:55:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 12 loss:0.12337227910757065 norm:0.0011991019127890468 max memory_allocated 47471.4169921875 
[2025-03-23 06:55:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 13 loss:0.12319578975439072 norm:0.0011184178292751312 max memory_allocated 47471.4169921875 
[2025-03-23 06:56:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 14 loss:0.12316342443227768 norm:0.0011666204081848264 max memory_allocated 47471.4169921875 
[2025-03-23 06:56:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 15 loss:0.12313884496688843 norm:0.0010616641957312822 max memory_allocated 47471.4169921875 
[2025-03-23 06:57:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 16 loss:0.12315773963928223 norm:0.0011372384615242481 max memory_allocated 47471.4169921875 
[2025-03-23 06:57:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 17 loss:0.12321358174085617 norm:0.0010871444828808308 max memory_allocated 47471.4169921875 
[2025-03-23 06:58:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 18 loss:0.12327462434768677 norm:0.0011663127224892378 max memory_allocated 47471.4169921875 
[2025-03-23 06:58:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 19 loss:0.12316755950450897 norm:0.001029436243698001 max memory_allocated 47471.4169921875 
[2025-03-23 06:59:16 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [29]
[2025-03-23 06:59:16 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [30] ===
[2025-03-23 06:59:16 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 06:59:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 0 loss:0.16693750023841858 norm:0.015368849970400333 max memory_allocated 47471.4169921875 
[2025-03-23 07:00:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 1 loss:0.16087760031223297 norm:0.010929368436336517 max memory_allocated 47471.4169921875 
[2025-03-23 07:00:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 2 loss:0.1554054468870163 norm:0.007330091670155525 max memory_allocated 47471.4169921875 
[2025-03-23 07:01:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 3 loss:0.15349063277244568 norm:0.005869964137673378 max memory_allocated 47471.4169921875 
[2025-03-23 07:01:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 4 loss:0.15297019481658936 norm:0.005114883184432983 max memory_allocated 47471.4169921875 
[2025-03-23 07:02:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 5 loss:0.15254086256027222 norm:0.004441678524017334 max memory_allocated 47471.4169921875 
[2025-03-23 07:02:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 6 loss:0.15215948224067688 norm:0.0038974450435489416 max memory_allocated 47471.4169921875 
[2025-03-23 07:03:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 7 loss:0.15191173553466797 norm:0.003501990344375372 max memory_allocated 47471.4169921875 
[2025-03-23 07:03:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 8 loss:0.1517324298620224 norm:0.003286193124949932 max memory_allocated 47471.4169921875 
[2025-03-23 07:04:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 9 loss:0.15167978405952454 norm:0.003179793944582343 max memory_allocated 47471.4169921875 
[2025-03-23 07:04:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 10 loss:0.15174368023872375 norm:0.0031251460313796997 max memory_allocated 47471.4169921875 
[2025-03-23 07:05:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 11 loss:0.1515340507030487 norm:0.0028346413746476173 max memory_allocated 47471.4169921875 
[2025-03-23 07:05:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 12 loss:0.15167772769927979 norm:0.003121177665889263 max memory_allocated 47471.4169921875 
[2025-03-23 07:06:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 13 loss:0.15148110687732697 norm:0.0029366365633904934 max memory_allocated 47471.4169921875 
[2025-03-23 07:06:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 14 loss:0.15138307213783264 norm:0.0030013257637619972 max memory_allocated 47471.4169921875 
[2025-03-23 07:07:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 15 loss:0.1514032781124115 norm:0.002756963251158595 max memory_allocated 47471.4169921875 
[2025-03-23 07:07:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 16 loss:0.1513075828552246 norm:0.002780341310426593 max memory_allocated 47471.4169921875 
[2025-03-23 07:07:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 17 loss:0.15136924386024475 norm:0.0026394198648631573 max memory_allocated 47471.4169921875 
[2025-03-23 07:08:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 18 loss:0.151307612657547 norm:0.0027727284468710423 max memory_allocated 47471.4169921875 
[2025-03-23 07:08:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 19 loss:0.15121948719024658 norm:0.0026027404237538576 max memory_allocated 47471.4169921875 
[2025-03-23 07:09:31 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [30]
[2025-03-23 07:09:31 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [31] ===
[2025-03-23 07:09:31 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 07:10:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 0 loss:0.2695770561695099 norm:0.024595104157924652 max memory_allocated 47471.4169921875 
[2025-03-23 07:10:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 1 loss:0.25451427698135376 norm:0.017062878236174583 max memory_allocated 47471.4169921875 
[2025-03-23 07:11:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 2 loss:0.2453509271144867 norm:0.012699536979198456 max memory_allocated 47471.4169921875 
[2025-03-23 07:11:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 3 loss:0.2406328320503235 norm:0.010207422077655792 max memory_allocated 47471.4169921875 
[2025-03-23 07:11:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 4 loss:0.23861806094646454 norm:0.008870838209986687 max memory_allocated 47471.4169921875 
[2025-03-23 07:12:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 5 loss:0.237214133143425 norm:0.007892736233770847 max memory_allocated 47471.4169921875 
[2025-03-23 07:12:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 6 loss:0.2363908588886261 norm:0.007267520297318697 max memory_allocated 47471.4169921875 
[2025-03-23 07:13:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 7 loss:0.23563039302825928 norm:0.0068630981259047985 max memory_allocated 47471.4169921875 
[2025-03-23 07:13:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 8 loss:0.23512402176856995 norm:0.00619481923058629 max memory_allocated 47471.4169921875 
[2025-03-23 07:14:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 9 loss:0.23455744981765747 norm:0.006201812997460365 max memory_allocated 47471.4169921875 
[2025-03-23 07:14:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 10 loss:0.23441243171691895 norm:0.006149467546492815 max memory_allocated 47471.4169921875 
[2025-03-23 07:15:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 11 loss:0.23417772352695465 norm:0.0064081791788339615 max memory_allocated 47471.4169921875 
[2025-03-23 07:15:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 12 loss:0.2341282069683075 norm:0.006202744785696268 max memory_allocated 47471.4169921875 
[2025-03-23 07:16:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 13 loss:0.23415055871009827 norm:0.006398479454219341 max memory_allocated 47471.4169921875 
[2025-03-23 07:16:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 14 loss:0.23385119438171387 norm:0.005996032617986202 max memory_allocated 47471.4169921875 
[2025-03-23 07:17:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 15 loss:0.23386184871196747 norm:0.006138456054031849 max memory_allocated 47471.4169921875 
[2025-03-23 07:17:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 16 loss:0.2337634563446045 norm:0.005775327794253826 max memory_allocated 47471.4169921875 
[2025-03-23 07:18:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 17 loss:0.23384955525398254 norm:0.006258614361286163 max memory_allocated 47471.4169921875 
[2025-03-23 07:18:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 18 loss:0.23362019658088684 norm:0.005599986761808395 max memory_allocated 47471.4169921875 
[2025-03-23 07:19:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 19 loss:0.23358874022960663 norm:0.005931323394179344 max memory_allocated 47471.4169921875 
[2025-03-23 07:19:47 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 15, block: [31]
[2025-03-23 07:19:48 root] (main_calib_config3_attn.py 379): INFO 19620.666949033737
[2025-03-23 07:19:59 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-23 07:20:49 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.736148357391357
[2025-03-23 07:20:49 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-23 07:22:08 root] (main_calib_config3_attn.py 161): INFO c4 : 7.161301612854004
[2025-03-23 08:06:10 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.736148357391357, 'c4': 7.161301612854004, 'results': {'winogrande': {'acc': 0.6677190213101816, 'acc_stderr': 0.013238316554236521}, 'hellaswag': {'acc': 0.5611431985660227, 'acc_stderr': 0.004952332378120329, 'acc_norm': 0.7233618801035651, 'acc_norm_stderr': 0.004464217420693369}, 'arc_challenge': {'acc': 0.38310580204778155, 'acc_stderr': 0.01420647266167288, 'acc_norm': 0.4052901023890785, 'acc_norm_stderr': 0.01434686906022932}, 'arc_easy': {'acc': 0.6670875420875421, 'acc_stderr': 0.009669958978395326, 'acc_norm': 0.5151515151515151, 'acc_norm_stderr': 0.010255071794531504}, 'boolq': {'acc': 0.7241590214067278, 'acc_stderr': 0.007816978272864553}, 'piqa': {'acc': 0.7763873775843307, 'acc_stderr': 0.009721489519176297, 'acc_norm': 0.7731229597388466, 'acc_norm_stderr': 0.009771584259215179}}, 'versions': {'winogrande': 0, 'hellaswag': 0, 'arc_challenge': 0, 'arc_easy': 0, 'boolq': 1, 'piqa': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-23 08:06:10 root] (main_calib_config3_attn.py 175): INFO 38.31,66.71,72.42,56.11,77.64,66.77
