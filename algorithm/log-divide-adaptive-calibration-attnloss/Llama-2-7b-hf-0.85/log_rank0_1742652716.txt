[2025-03-22 14:11:56 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/Llama-2-7b-hf-0.85', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.85.pkl', blocks_pkl='./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-22 14:12:04 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-22 14:12:04 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-22 14:12:04 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-22 14:12:04 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.85.pkl
[2025-03-22 14:12:04 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 9), (9, 11), (11, 14), (14, 17), (17, 20), (20, 23), (23, 26), (26, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-22 14:12:04 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12, 13], [14, 15, 16], [17, 18, 19], [20, 21, 22], [23, 24, 25], [26, 27, 28], [29], [30], [31]]
[2025-03-22 14:12:06 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-22 14:12:06 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:12:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.0011067207669839263 norm:0.0036583752371370792 max memory_allocated 34633.880859375 
[2025-03-22 14:13:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.0005666714278049767 norm:0.0011039660312235355 max memory_allocated 34633.880859375 
[2025-03-22 14:13:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.0005197914433665574 norm:0.0015015597455203533 max memory_allocated 34633.880859375 
[2025-03-22 14:14:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0005060415132902563 norm:0.0013923197984695435 max memory_allocated 34633.880859375 
[2025-03-22 14:14:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.0004995978670194745 norm:0.001293764216825366 max memory_allocated 34633.880859375 
[2025-03-22 14:15:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.00048666849033907056 norm:0.0012387096649035811 max memory_allocated 34633.880859375 
[2025-03-22 14:15:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.00047583613195456564 norm:0.0011846448760479689 max memory_allocated 34633.880859375 
[2025-03-22 14:15:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.00047266646288335323 norm:0.0011327340034767985 max memory_allocated 34633.880859375 
[2025-03-22 14:16:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.00046891081728972495 norm:0.001082420814782381 max memory_allocated 34633.880859375 
[2025-03-22 14:16:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.0004607400915119797 norm:0.0009997868910431862 max memory_allocated 34633.880859375 
[2025-03-22 14:17:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.00045603938633576035 norm:0.0009566172957420349 max memory_allocated 34633.880859375 
[2025-03-22 14:17:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.00045482689165510237 norm:0.0009653010638430715 max memory_allocated 34633.880859375 
[2025-03-22 14:18:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.00044140592217445374 norm:0.0008704447536729276 max memory_allocated 34633.880859375 
[2025-03-22 14:18:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.0004395324212964624 norm:0.0008502872078679502 max memory_allocated 34633.880859375 
[2025-03-22 14:19:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0004361443570815027 norm:0.0007827631197869778 max memory_allocated 34633.880859375 
[2025-03-22 14:19:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.0004297640989534557 norm:0.000751568702980876 max memory_allocated 34633.880859375 
[2025-03-22 14:20:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.0004265529860276729 norm:0.0007296350668184459 max memory_allocated 34633.880859375 
[2025-03-22 14:20:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0004181562107987702 norm:0.0006583408685401082 max memory_allocated 34633.880859375 
[2025-03-22 14:21:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.00041499410872347653 norm:0.00062232103664428 max memory_allocated 34633.880859375 
[2025-03-22 14:21:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0004207551828585565 norm:0.0006334297941066325 max memory_allocated 34633.880859375 
[2025-03-22 14:22:21 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-22 14:22:21 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-22 14:22:21 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:22:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.02003769390285015 norm:0.013160455040633678 max memory_allocated 35100.7724609375 
[2025-03-22 14:23:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.012362469919025898 norm:0.011206201277673244 max memory_allocated 35100.7724609375 
[2025-03-22 14:23:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.008469787426292896 norm:0.01031782478094101 max memory_allocated 35100.7724609375 
[2025-03-22 14:24:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.007520848885178566 norm:0.006589976139366627 max memory_allocated 35100.7724609375 
[2025-03-22 14:24:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.011094295419752598 norm:0.010930893942713737 max memory_allocated 35100.7724609375 
[2025-03-22 14:25:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.007363270036876202 norm:0.00897710956633091 max memory_allocated 35100.7724609375 
[2025-03-22 14:25:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.007373843342065811 norm:0.008776616305112839 max memory_allocated 35100.7724609375 
[2025-03-22 14:26:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.007082865107804537 norm:0.008266758173704147 max memory_allocated 35100.7724609375 
[2025-03-22 14:26:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.006916246842592955 norm:0.0074635716155171394 max memory_allocated 35100.7724609375 
[2025-03-22 14:27:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.007092623971402645 norm:0.007699545938521624 max memory_allocated 35100.7724609375 
[2025-03-22 14:27:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.006890181452035904 norm:0.0074950591661036015 max memory_allocated 35100.7724609375 
[2025-03-22 14:28:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.006947284564375877 norm:0.006905488669872284 max memory_allocated 35100.7724609375 
[2025-03-22 14:28:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.006934711709618568 norm:0.006829177029430866 max memory_allocated 35100.7724609375 
[2025-03-22 14:29:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.006844611838459969 norm:0.006683822721242905 max memory_allocated 35100.7724609375 
[2025-03-22 14:29:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.006799548398703337 norm:0.006356899160891771 max memory_allocated 35100.7724609375 
[2025-03-22 14:30:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.007110020145773888 norm:0.0064654541201889515 max memory_allocated 35100.7724609375 
[2025-03-22 14:30:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.006974945310503244 norm:0.006377415265887976 max memory_allocated 35100.7724609375 
[2025-03-22 14:31:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.006762364413589239 norm:0.005949830170720816 max memory_allocated 35100.7724609375 
[2025-03-22 14:31:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.006823861971497536 norm:0.005827560555189848 max memory_allocated 35100.7724609375 
[2025-03-22 14:31:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.006905728485435247 norm:0.005889146123081446 max memory_allocated 35100.7724609375 
[2025-03-22 14:32:36 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-22 14:32:36 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
[2025-03-22 14:32:37 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:33:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.009486556053161621 norm:0.004328734241425991 max memory_allocated 35100.8349609375 
[2025-03-22 14:33:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.007110386621206999 norm:0.003393953898921609 max memory_allocated 35100.8349609375 
[2025-03-22 14:34:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.006624253466725349 norm:0.0028022341430187225 max memory_allocated 35100.8349609375 
[2025-03-22 14:34:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.006256887689232826 norm:0.0023987917229533195 max memory_allocated 35100.8349609375 
[2025-03-22 14:35:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.00588590744882822 norm:0.0021241253707557917 max memory_allocated 35100.8349609375 
[2025-03-22 14:35:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.005583995021879673 norm:0.0018855022499337792 max memory_allocated 35100.8349609375 
[2025-03-22 14:36:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.005423497408628464 norm:0.001673086080700159 max memory_allocated 35100.8349609375 
[2025-03-22 14:36:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.005349523387849331 norm:0.001472387695685029 max memory_allocated 35100.8349609375 
[2025-03-22 14:36:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.00531905610114336 norm:0.0012958841398358345 max memory_allocated 35100.8349609375 
[2025-03-22 14:37:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.005280263721942902 norm:0.0011194933904334903 max memory_allocated 35100.8349609375 
[2025-03-22 14:37:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.005266404245048761 norm:0.0009498129365965724 max memory_allocated 35100.8349609375 
[2025-03-22 14:38:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.005242655985057354 norm:0.0007826655637472868 max memory_allocated 35100.8349609375 
[2025-03-22 14:38:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.005223918706178665 norm:0.0006370461778715253 max memory_allocated 35100.8349609375 
[2025-03-22 14:39:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.005213680677115917 norm:0.0005485742585733533 max memory_allocated 35100.8349609375 
[2025-03-22 14:39:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.005282140336930752 norm:0.0007101909723132849 max memory_allocated 35100.8349609375 
[2025-03-22 14:40:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.005244611296802759 norm:0.0006332472548820078 max memory_allocated 35100.8349609375 
[2025-03-22 14:40:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.005219793412834406 norm:0.0004784749180544168 max memory_allocated 35100.8349609375 
[2025-03-22 14:41:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.005209644790738821 norm:0.0005271804402582347 max memory_allocated 35100.8349609375 
[2025-03-22 14:41:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.005244262982159853 norm:0.0005192480166442692 max memory_allocated 35100.8349609375 
[2025-03-22 14:42:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.00522004347294569 norm:0.0004523741372395307 max memory_allocated 35100.8349609375 
[2025-03-22 14:42:53 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2]
[2025-03-22 14:42:53 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4, 5] ===
[2025-03-22 14:44:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 0 loss:0.016001930460333824 norm:0.00029338288004510105 max memory_allocated 47477.6044921875 
[2025-03-22 14:45:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 1 loss:0.013937031850218773 norm:0.00019673097995109856 max memory_allocated 47477.6044921875 
[2025-03-22 14:47:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 2 loss:0.012844718061387539 norm:0.00016722561849746853 max memory_allocated 47477.6044921875 
[2025-03-22 14:48:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 3 loss:0.011896956712007523 norm:0.00015279938816092908 max memory_allocated 47477.6044921875 
[2025-03-22 14:50:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 4 loss:0.010933799669146538 norm:0.00014236672723200172 max memory_allocated 47477.6044921875 
[2025-03-22 14:51:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 5 loss:0.010246116667985916 norm:0.00013614405179396272 max memory_allocated 47477.6044921875 
[2025-03-22 14:53:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 6 loss:0.009841124527156353 norm:0.00015789449389558285 max memory_allocated 47477.6044921875 
[2025-03-22 14:54:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 7 loss:0.009623111225664616 norm:0.00017260578169953078 max memory_allocated 47477.6044921875 
[2025-03-22 14:55:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 8 loss:0.009530325420200825 norm:0.0002345792017877102 max memory_allocated 47477.6044921875 
[2025-03-22 14:57:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 9 loss:0.009481864050030708 norm:0.00012931802484672517 max memory_allocated 47477.6044921875 
[2025-03-22 14:58:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 10 loss:0.009454187005758286 norm:0.00020844305981881917 max memory_allocated 47477.6044921875 
[2025-03-22 15:00:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 11 loss:0.009434700012207031 norm:0.00020871796004939824 max memory_allocated 47477.6044921875 
[2025-03-22 15:01:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 12 loss:0.009408293291926384 norm:0.00023819674970582128 max memory_allocated 47477.6044921875 
[2025-03-22 15:02:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 13 loss:0.009406229481101036 norm:0.0002283073408761993 max memory_allocated 47477.6044921875 
[2025-03-22 15:04:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 14 loss:0.009395258501172066 norm:0.00023730889370199293 max memory_allocated 47477.6044921875 
[2025-03-22 15:05:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 15 loss:0.009363390505313873 norm:0.00021656768512912095 max memory_allocated 47477.6044921875 
[2025-03-22 15:07:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 16 loss:0.009361579082906246 norm:0.00022875278955325484 max memory_allocated 47477.6044921875 
[2025-03-22 15:08:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 17 loss:0.009352571330964565 norm:0.00020857826166320592 max memory_allocated 47477.6044921875 
[2025-03-22 15:10:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 18 loss:0.009344632737338543 norm:0.00023179908748716116 max memory_allocated 47477.6044921875 
[2025-03-22 15:11:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 19 loss:0.009323246777057648 norm:0.00024968310026451945 max memory_allocated 47477.6044921875 
[2025-03-22 15:13:30 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [3, 4, 5]
[2025-03-22 15:13:30 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [6, 7, 8] ===
[2025-03-22 15:15:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 0 loss:0.019460149109363556 norm:0.00028331391513347626 max memory_allocated 47477.7919921875 
[2025-03-22 15:16:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 1 loss:0.01783924177289009 norm:0.0001838636671891436 max memory_allocated 47477.7919921875 
[2025-03-22 15:17:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 2 loss:0.016629045829176903 norm:0.0001537927455501631 max memory_allocated 47477.7919921875 
[2025-03-22 15:19:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 3 loss:0.015214068815112114 norm:0.00013211416080594063 max memory_allocated 47477.7919921875 
[2025-03-22 15:20:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 4 loss:0.014142892323434353 norm:0.00012559120659716427 max memory_allocated 47477.7919921875 
[2025-03-22 15:22:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 5 loss:0.013539044186472893 norm:0.00013128329010214657 max memory_allocated 47477.7919921875 
[2025-03-22 15:23:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 6 loss:0.013174465857446194 norm:0.00011779845954151824 max memory_allocated 47477.7919921875 
[2025-03-22 15:25:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 7 loss:0.012992553412914276 norm:0.00011708788952091709 max memory_allocated 47477.7919921875 
[2025-03-22 15:26:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 8 loss:0.012884333729743958 norm:0.00010650725744199008 max memory_allocated 47477.7919921875 
[2025-03-22 15:27:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 9 loss:0.012860648334026337 norm:0.00011443786206655204 max memory_allocated 47477.7919921875 
[2025-03-22 15:29:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 10 loss:0.01281413808465004 norm:0.00010977518104482442 max memory_allocated 47477.7919921875 
[2025-03-22 15:30:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 11 loss:0.012798639014363289 norm:0.00010636248043738306 max memory_allocated 47477.7919921875 
[2025-03-22 15:32:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 12 loss:0.012772159650921822 norm:0.00011047926091123372 max memory_allocated 47477.7919921875 
[2025-03-22 15:33:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 13 loss:0.012762792408466339 norm:0.00010414872667752206 max memory_allocated 47477.7919921875 
[2025-03-22 15:35:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 14 loss:0.012757468968629837 norm:0.00010212485358351842 max memory_allocated 47477.7919921875 
[2025-03-22 15:36:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 15 loss:0.012729035690426826 norm:9.654861059971154e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:37:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 16 loss:0.012719091959297657 norm:0.00010517676855670288 max memory_allocated 47477.7919921875 
[2025-03-22 15:39:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 17 loss:0.012710154987871647 norm:0.00010007002856582403 max memory_allocated 47477.7919921875 
[2025-03-22 15:40:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 18 loss:0.012697271071374416 norm:0.00010023931827163324 max memory_allocated 47477.7919921875 
[2025-03-22 15:42:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 19 loss:0.01268346793949604 norm:0.00010038648906629533 max memory_allocated 47477.7919921875 
[2025-03-22 15:44:06 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [6, 7, 8]
[2025-03-22 15:44:06 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [9, 10] ===
[2025-03-22 15:45:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 0 loss:0.018490910530090332 norm:0.0003761462867259979 max memory_allocated 47477.7919921875 
[2025-03-22 15:46:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 1 loss:0.016345981508493423 norm:0.00023597384279128164 max memory_allocated 47477.7919921875 
[2025-03-22 15:47:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 2 loss:0.015183959156274796 norm:0.00017704065248835832 max memory_allocated 47477.7919921875 
[2025-03-22 15:47:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 3 loss:0.014222460798919201 norm:0.0001616302179172635 max memory_allocated 47477.7919921875 
[2025-03-22 15:48:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 4 loss:0.013500518165528774 norm:0.0001450252311769873 max memory_allocated 47477.7919921875 
[2025-03-22 15:49:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 5 loss:0.012995547614991665 norm:0.00012951345706824213 max memory_allocated 47477.7919921875 
[2025-03-22 15:50:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 6 loss:0.01269465871155262 norm:0.00012120730389142409 max memory_allocated 47477.7919921875 
[2025-03-22 15:51:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 7 loss:0.012517670169472694 norm:0.00011003649706253782 max memory_allocated 47477.7919921875 
[2025-03-22 15:52:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 8 loss:0.01242807973176241 norm:0.00011099511903012171 max memory_allocated 47477.7919921875 
[2025-03-22 15:53:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 9 loss:0.012298328801989555 norm:0.00010246159217786044 max memory_allocated 47477.7919921875 
[2025-03-22 15:54:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 10 loss:0.012211901135742664 norm:9.704384865472093e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:55:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 11 loss:0.01217394694685936 norm:9.918283467413858e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:56:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 12 loss:0.012151350267231464 norm:0.00010296935215592384 max memory_allocated 47477.7919921875 
[2025-03-22 15:57:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 13 loss:0.01212462130934 norm:0.00010997871868312359 max memory_allocated 47477.7919921875 
[2025-03-22 15:58:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 14 loss:0.012100853025913239 norm:0.00010786881466628984 max memory_allocated 47477.7919921875 
[2025-03-22 15:59:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 15 loss:0.012087828479707241 norm:0.0001092364764190279 max memory_allocated 47477.7919921875 
[2025-03-22 16:00:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 16 loss:0.01207148376852274 norm:0.00011780278873629868 max memory_allocated 47477.7919921875 
[2025-03-22 16:01:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 17 loss:0.012046164833009243 norm:0.00010575448686722666 max memory_allocated 47477.7919921875 
[2025-03-22 16:02:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 18 loss:0.012040723115205765 norm:0.0001077748165698722 max memory_allocated 47477.7919921875 
[2025-03-22 16:03:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 19 loss:0.012023841962218285 norm:0.0001063729650923051 max memory_allocated 47477.7919921875 
[2025-03-22 16:04:30 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [9, 10]
[2025-03-22 16:04:30 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [11, 12, 13] ===
[2025-03-22 16:06:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 0 loss:0.020156562328338623 norm:0.0002723748912103474 max memory_allocated 47478.1044921875 
[2025-03-22 16:07:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 1 loss:0.018623551353812218 norm:0.00016165061970241368 max memory_allocated 47478.1044921875 
[2025-03-22 16:08:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 2 loss:0.01750723458826542 norm:0.00012201887875562534 max memory_allocated 47478.1044921875 
[2025-03-22 16:10:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 3 loss:0.016443738713860512 norm:0.00010559675865806639 max memory_allocated 47478.1044921875 
[2025-03-22 16:11:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 4 loss:0.015569182112812996 norm:9.628654515836388e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:13:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 5 loss:0.014960583299398422 norm:9.082583710551262e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:14:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 6 loss:0.014588248915970325 norm:8.305133087560534e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:16:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 7 loss:0.01438430417329073 norm:8.122869621729478e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:17:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 8 loss:0.014257943257689476 norm:7.671265484532341e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:18:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 9 loss:0.01418349053710699 norm:7.431232370436192e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:20:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 10 loss:0.014130422845482826 norm:6.98731018928811e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:21:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 11 loss:0.01409180648624897 norm:6.44383835606277e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:23:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 12 loss:0.014071465469896793 norm:6.576953455805779e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:24:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 13 loss:0.014052385464310646 norm:7.244168227771297e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:26:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 14 loss:0.014025833457708359 norm:6.292467878665775e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:27:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 15 loss:0.013996337540447712 norm:6.09206072113011e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:28:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 16 loss:0.013982238247990608 norm:6.126375956228003e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:30:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 17 loss:0.013977841474115849 norm:6.091426985221915e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:31:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 18 loss:0.013960294425487518 norm:5.8061457821168005e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:33:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 19 loss:0.013937106356024742 norm:5.719610635424033e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:35:09 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [11, 12, 13]
[2025-03-22 16:35:09 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [14, 15, 16] ===
[2025-03-22 16:36:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 0 loss:0.02295610122382641 norm:0.0004923806991428137 max memory_allocated 47479.2919921875 
[2025-03-22 16:38:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 1 loss:0.0199055764824152 norm:0.00029460585210472345 max memory_allocated 47479.2919921875 
[2025-03-22 16:39:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 2 loss:0.01837468147277832 norm:0.00021683642989955842 max memory_allocated 47479.2919921875 
[2025-03-22 16:40:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 3 loss:0.017220094799995422 norm:0.00017510709585621953 max memory_allocated 47479.2919921875 
[2025-03-22 16:42:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 4 loss:0.0162784606218338 norm:0.0001566254795761779 max memory_allocated 47479.2919921875 
[2025-03-22 16:43:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 5 loss:0.01555335521697998 norm:0.00014302063209470361 max memory_allocated 47479.2919921875 
[2025-03-22 16:45:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 6 loss:0.015034129843115807 norm:0.00013522186782211065 max memory_allocated 47479.2919921875 
[2025-03-22 16:46:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 7 loss:0.014681828208267689 norm:0.0001251379871973768 max memory_allocated 47479.2919921875 
[2025-03-22 16:48:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 8 loss:0.014448483474552631 norm:0.00011733880819519982 max memory_allocated 47479.2919921875 
[2025-03-22 16:49:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 9 loss:0.014281826093792915 norm:0.00011040499521186575 max memory_allocated 47479.2919921875 
[2025-03-22 16:50:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 10 loss:0.014172397553920746 norm:0.00010929525888059288 max memory_allocated 47479.2919921875 
[2025-03-22 16:52:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 11 loss:0.014052665792405605 norm:0.00010777577699627727 max memory_allocated 47479.2919921875 
[2025-03-22 16:53:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 12 loss:0.013978075236082077 norm:9.978379966923967e-05 max memory_allocated 47479.2919921875 
[2025-03-22 16:55:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 13 loss:0.013906842097640038 norm:9.39888195716776e-05 max memory_allocated 47479.2919921875 
[2025-03-22 16:56:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 14 loss:0.013828780502080917 norm:9.445989417145029e-05 max memory_allocated 47479.2919921875 
[2025-03-22 16:58:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 15 loss:0.013780511915683746 norm:9.278235665988177e-05 max memory_allocated 47479.2919921875 
[2025-03-22 16:59:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 16 loss:0.013734890148043633 norm:9.35188727453351e-05 max memory_allocated 47479.2919921875 
[2025-03-22 17:00:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 17 loss:0.013708426617085934 norm:8.946629532147199e-05 max memory_allocated 47479.2919921875 
[2025-03-22 17:02:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 18 loss:0.013656764291226864 norm:7.748053758405149e-05 max memory_allocated 47479.2919921875 
[2025-03-22 17:03:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 19 loss:0.013624075800180435 norm:7.963386451592669e-05 max memory_allocated 47479.2919921875 
[2025-03-22 17:05:46 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [14, 15, 16]
[2025-03-22 17:05:46 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [17, 18, 19] ===
[2025-03-22 17:07:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 0 loss:0.023737389594316483 norm:0.0003347010933794081 max memory_allocated 47479.2919921875 
[2025-03-22 17:08:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 1 loss:0.021764060482382774 norm:0.0002138450654456392 max memory_allocated 47479.2919921875 
[2025-03-22 17:10:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 2 loss:0.020537780597805977 norm:0.00016225881699938327 max memory_allocated 47479.2919921875 
[2025-03-22 17:11:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 3 loss:0.019455116242170334 norm:0.00013870724069420248 max memory_allocated 47479.2919921875 
[2025-03-22 17:13:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 4 loss:0.018398568034172058 norm:0.00013013507123105228 max memory_allocated 47479.2919921875 
[2025-03-22 17:14:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 5 loss:0.017599601298570633 norm:0.00011824762623291463 max memory_allocated 47479.2919921875 
[2025-03-22 17:15:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 6 loss:0.01716742478311062 norm:0.00010998039942933246 max memory_allocated 47479.2919921875 
[2025-03-22 17:17:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 7 loss:0.016957290470600128 norm:0.00010292093793395907 max memory_allocated 47479.2919921875 
[2025-03-22 17:18:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 8 loss:0.01682249642908573 norm:0.00010041392670245841 max memory_allocated 47479.2919921875 
[2025-03-22 17:20:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 9 loss:0.01671138033270836 norm:9.003966260934249e-05 max memory_allocated 47479.2919921875 
[2025-03-22 17:21:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 10 loss:0.016632433980703354 norm:8.48455965751782e-05 max memory_allocated 47479.2919921875 
[2025-03-22 17:23:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 11 loss:0.01657148450613022 norm:8.298462489619851e-05 max memory_allocated 47479.2919921875 
[2025-03-22 17:24:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 12 loss:0.016499686986207962 norm:8.169858483597636e-05 max memory_allocated 47479.2919921875 
[2025-03-22 17:25:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 13 loss:0.01645634137094021 norm:8.121512655634433e-05 max memory_allocated 47479.2919921875 
[2025-03-22 17:27:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 14 loss:0.016400208696722984 norm:7.464217924280092e-05 max memory_allocated 47479.2919921875 
[2025-03-22 17:28:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 15 loss:0.01636224240064621 norm:7.070341962389648e-05 max memory_allocated 47479.2919921875 
[2025-03-22 17:30:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 16 loss:0.016332797706127167 norm:7.127709977794439e-05 max memory_allocated 47479.2919921875 
[2025-03-22 17:31:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 17 loss:0.016287149861454964 norm:6.558851600857452e-05 max memory_allocated 47479.2919921875 
[2025-03-22 17:33:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 18 loss:0.016272416338324547 norm:6.89217122271657e-05 max memory_allocated 47479.2919921875 
[2025-03-22 17:34:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 19 loss:0.01625131070613861 norm:6.729119195370004e-05 max memory_allocated 47479.2919921875 
[2025-03-22 17:36:22 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [17, 18, 19]
[2025-03-22 17:36:22 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [20, 21, 22] ===
[2025-03-22 17:37:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 0 loss:0.032396409660577774 norm:0.000716283917427063 max memory_allocated 47479.2919921875 
[2025-03-22 17:39:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 1 loss:0.029497390612959862 norm:0.00035672125522978604 max memory_allocated 47479.2919921875 
[2025-03-22 17:40:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 2 loss:0.027830548584461212 norm:0.0002457191876601428 max memory_allocated 47479.2919921875 
[2025-03-22 17:42:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 3 loss:0.02619575336575508 norm:0.0001911698782350868 max memory_allocated 47479.2919921875 
[2025-03-22 17:43:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 4 loss:0.02476595528423786 norm:0.00017433182802051306 max memory_allocated 47479.2919921875 
[2025-03-22 17:45:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 5 loss:0.0239990446716547 norm:0.0001585846475791186 max memory_allocated 47479.2919921875 
[2025-03-22 17:46:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 6 loss:0.023724013939499855 norm:0.000155736313899979 max memory_allocated 47479.2919921875 
[2025-03-22 17:47:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 7 loss:0.023549310863018036 norm:0.0001390228426316753 max memory_allocated 47479.2919921875 
[2025-03-22 17:49:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 8 loss:0.023413628339767456 norm:0.00013752990344073623 max memory_allocated 47479.2919921875 
[2025-03-22 17:50:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 9 loss:0.023302368819713593 norm:0.0001309137005591765 max memory_allocated 47479.2919921875 
[2025-03-22 17:52:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 10 loss:0.023215528577566147 norm:0.0001266530598513782 max memory_allocated 47479.2919921875 
[2025-03-22 17:53:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 11 loss:0.023128321394324303 norm:0.00012155399599578232 max memory_allocated 47479.2919921875 
[2025-03-22 17:55:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 12 loss:0.023053308948874474 norm:0.00011568988702492788 max memory_allocated 47479.2919921875 
[2025-03-22 17:56:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 13 loss:0.02297976240515709 norm:0.00010956850019283593 max memory_allocated 47479.2919921875 
[2025-03-22 17:57:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 14 loss:0.02291460894048214 norm:0.00010577894863672554 max memory_allocated 47479.2919921875 
[2025-03-22 17:59:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 15 loss:0.022859705612063408 norm:0.0001103212998714298 max memory_allocated 47479.2919921875 
[2025-03-22 18:00:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 16 loss:0.02278728038072586 norm:0.00010406884393887594 max memory_allocated 47479.2919921875 
[2025-03-22 18:02:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 17 loss:0.022745700553059578 norm:9.945857163984329e-05 max memory_allocated 47479.2919921875 
[2025-03-22 18:03:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 18 loss:0.02271243929862976 norm:0.000102193997008726 max memory_allocated 47479.2919921875 
[2025-03-22 18:05:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 19 loss:0.02266724221408367 norm:9.556184522807598e-05 max memory_allocated 47479.2919921875 
[2025-03-22 18:07:02 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [20, 21, 22]
[2025-03-22 18:07:02 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [23, 24, 25] ===
[2025-03-22 18:08:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 0 loss:0.0476500429213047 norm:0.0015619833720847964 max memory_allocated 47479.2919921875 
[2025-03-22 18:10:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 1 loss:0.04296428710222244 norm:0.000496805994771421 max memory_allocated 47479.2919921875 
[2025-03-22 18:11:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 2 loss:0.04036463424563408 norm:0.0003484920016489923 max memory_allocated 47479.2919921875 
[2025-03-22 18:12:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 3 loss:0.03776931017637253 norm:0.0002783848613034934 max memory_allocated 47479.2919921875 
[2025-03-22 18:14:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 4 loss:0.03598921000957489 norm:0.00025266536977142096 max memory_allocated 47479.2919921875 
[2025-03-22 18:15:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 5 loss:0.035449571907520294 norm:0.00023181027790997177 max memory_allocated 47479.2919921875 
[2025-03-22 18:17:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 6 loss:0.035242803394794464 norm:0.00021960305457469076 max memory_allocated 47479.2919921875 
[2025-03-22 18:18:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 7 loss:0.0350736565887928 norm:0.00020901314564980567 max memory_allocated 47479.2919921875 
[2025-03-22 18:20:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 8 loss:0.03488605096936226 norm:0.00019379728473722935 max memory_allocated 47479.2919921875 
[2025-03-22 18:21:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 9 loss:0.034772682934999466 norm:0.00018967047799378633 max memory_allocated 47479.2919921875 
[2025-03-22 18:22:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 10 loss:0.03464452922344208 norm:0.0001748396607581526 max memory_allocated 47479.2919921875 
[2025-03-22 18:24:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 11 loss:0.034523576498031616 norm:0.00016971112927421927 max memory_allocated 47479.2919921875 
[2025-03-22 18:25:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 12 loss:0.03444066271185875 norm:0.0001679737470112741 max memory_allocated 47479.2919921875 
[2025-03-22 18:27:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 13 loss:0.03435875475406647 norm:0.0001571237953612581 max memory_allocated 47479.2919921875 
[2025-03-22 18:28:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 14 loss:0.03427758812904358 norm:0.0001555228664074093 max memory_allocated 47479.2919921875 
[2025-03-22 18:29:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 15 loss:0.03420885652303696 norm:0.0001578202354721725 max memory_allocated 47479.2919921875 
[2025-03-22 18:31:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 16 loss:0.03415282070636749 norm:0.00015362646081484854 max memory_allocated 47479.2919921875 
[2025-03-22 18:32:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 17 loss:0.03410400450229645 norm:0.000145859201438725 max memory_allocated 47479.2919921875 
[2025-03-22 18:34:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 18 loss:0.034052200615406036 norm:0.00013838603626936674 max memory_allocated 47479.2919921875 
[2025-03-22 18:35:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 19 loss:0.03399231657385826 norm:0.0001343524781987071 max memory_allocated 47479.2919921875 
[2025-03-22 18:37:37 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [23, 24, 25]
[2025-03-22 18:37:37 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [26, 27, 28] ===
[2025-03-22 18:37:39 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 18:39:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 0 loss:0.06546294689178467 norm:0.003944519907236099 max memory_allocated 47479.2919921875 
[2025-03-22 18:40:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 1 loss:0.060765430331230164 norm:0.0028814105316996574 max memory_allocated 47479.2919921875 
[2025-03-22 18:42:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 2 loss:0.05745217204093933 norm:0.0023171729408204556 max memory_allocated 47479.2919921875 
[2025-03-22 18:43:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 3 loss:0.05415219068527222 norm:0.0018610339611768723 max memory_allocated 47479.2919921875 
[2025-03-22 18:44:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 4 loss:0.052754417061805725 norm:0.001538466545753181 max memory_allocated 47479.2919921875 
[2025-03-22 18:46:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 5 loss:0.05221988260746002 norm:0.0012775689829140902 max memory_allocated 47479.2919921875 
[2025-03-22 18:47:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 6 loss:0.05188709497451782 norm:0.0011335358722135425 max memory_allocated 47479.2919921875 
[2025-03-22 18:49:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 7 loss:0.05167541280388832 norm:0.0011182388989254832 max memory_allocated 47479.2919921875 
[2025-03-22 18:50:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 8 loss:0.05148822441697121 norm:0.0011412749299779534 max memory_allocated 47479.2919921875 
[2025-03-22 18:52:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 9 loss:0.051433008164167404 norm:0.000934074108954519 max memory_allocated 47479.2919921875 
[2025-03-22 18:53:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 10 loss:0.05117771402001381 norm:0.0008671742980368435 max memory_allocated 47479.2919921875 
[2025-03-22 18:54:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 11 loss:0.05099569633603096 norm:0.0009102439507842064 max memory_allocated 47479.2919921875 
[2025-03-22 18:56:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 12 loss:0.050881028175354004 norm:0.0008854527841322124 max memory_allocated 47479.2919921875 
[2025-03-22 18:57:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 13 loss:0.050780363380908966 norm:0.0008953125798143446 max memory_allocated 47479.2919921875 
[2025-03-22 18:59:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 14 loss:0.05071551725268364 norm:0.0008193592657335103 max memory_allocated 47479.2919921875 
[2025-03-22 19:00:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 15 loss:0.05056484043598175 norm:0.0008220222662203014 max memory_allocated 47479.2919921875 
[2025-03-22 19:02:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 16 loss:0.05047819763422012 norm:0.0008351131691597402 max memory_allocated 47479.2919921875 
[2025-03-22 19:03:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 17 loss:0.05042772367596626 norm:0.000792891310993582 max memory_allocated 47479.2919921875 
[2025-03-22 19:04:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 18 loss:0.050392843782901764 norm:0.0007587617146782577 max memory_allocated 47479.2919921875 
[2025-03-22 19:06:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 19 loss:0.050329625606536865 norm:0.0007313770474866033 max memory_allocated 47479.2919921875 
[2025-03-22 19:08:16 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [26, 27, 28]
[2025-03-22 19:08:16 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [29] ===
[2025-03-22 19:08:16 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:08:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 0 loss:0.06353781372308731 norm:0.008304680697619915 max memory_allocated 47479.2919921875 
[2025-03-22 19:09:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 1 loss:0.05974356830120087 norm:0.006334560923278332 max memory_allocated 47479.2919921875 
[2025-03-22 19:09:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 2 loss:0.05725478008389473 norm:0.003991628997027874 max memory_allocated 47479.2919921875 
[2025-03-22 19:10:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 3 loss:0.055781520903110504 norm:0.0030229308176785707 max memory_allocated 47479.2919921875 
[2025-03-22 19:10:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 4 loss:0.055359527468681335 norm:0.0025645638816058636 max memory_allocated 47479.2919921875 
[2025-03-22 19:11:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 5 loss:0.055131666362285614 norm:0.002192281885072589 max memory_allocated 47479.2919921875 
[2025-03-22 19:11:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 6 loss:0.054971590638160706 norm:0.001876004389487207 max memory_allocated 47479.2919921875 
[2025-03-22 19:12:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 7 loss:0.05486927181482315 norm:0.0016049651894718409 max memory_allocated 47479.2919921875 
[2025-03-22 19:12:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 8 loss:0.05482109636068344 norm:0.0015015521785244346 max memory_allocated 47479.2919921875 
[2025-03-22 19:13:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 9 loss:0.05478948354721069 norm:0.0014865247067064047 max memory_allocated 47479.2919921875 
[2025-03-22 19:13:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 10 loss:0.054852645844221115 norm:0.0013986467383801937 max memory_allocated 47479.2919921875 
[2025-03-22 19:14:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 11 loss:0.054713666439056396 norm:0.0014159760903567076 max memory_allocated 47479.2919921875 
[2025-03-22 19:14:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 12 loss:0.054668184369802475 norm:0.0012582259951159358 max memory_allocated 47479.2919921875 
[2025-03-22 19:15:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 13 loss:0.054657064378261566 norm:0.001336153014563024 max memory_allocated 47479.2919921875 
[2025-03-22 19:15:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 14 loss:0.05462723597884178 norm:0.0012611900456249714 max memory_allocated 47479.2919921875 
[2025-03-22 19:15:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 15 loss:0.05457465723156929 norm:0.0012268864084035158 max memory_allocated 47479.2919921875 
[2025-03-22 19:16:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 16 loss:0.0545574426651001 norm:0.0011037115473300219 max memory_allocated 47479.2919921875 
[2025-03-22 19:16:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 17 loss:0.05453534796833992 norm:0.0011283353669568896 max memory_allocated 47479.2919921875 
[2025-03-22 19:17:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 18 loss:0.05454311892390251 norm:0.0010870778933167458 max memory_allocated 47479.2919921875 
[2025-03-22 19:17:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 19 loss:0.054518017917871475 norm:0.001108557335101068 max memory_allocated 47479.2919921875 
[2025-03-22 19:18:32 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [29]
[2025-03-22 19:18:32 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [30] ===
[2025-03-22 19:18:32 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:19:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 0 loss:0.0941237062215805 norm:0.010770373046398163 max memory_allocated 47479.2919921875 
[2025-03-22 19:19:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 1 loss:0.08157910406589508 norm:0.006724203936755657 max memory_allocated 47479.2919921875 
[2025-03-22 19:20:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 2 loss:0.07517792284488678 norm:0.004784207325428724 max memory_allocated 47479.2919921875 
[2025-03-22 19:20:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 3 loss:0.07273782044649124 norm:0.0038791692350059748 max memory_allocated 47479.2919921875 
[2025-03-22 19:20:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 4 loss:0.07220738381147385 norm:0.0034616547636687756 max memory_allocated 47479.2919921875 
[2025-03-22 19:21:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 5 loss:0.0717908963561058 norm:0.0030608540400862694 max memory_allocated 47479.2919921875 
[2025-03-22 19:21:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 6 loss:0.07157275080680847 norm:0.002920820377767086 max memory_allocated 47479.2919921875 
[2025-03-22 19:22:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 7 loss:0.07132947444915771 norm:0.0026500390376895666 max memory_allocated 47479.2919921875 
[2025-03-22 19:22:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 8 loss:0.0711761936545372 norm:0.0024170351680368185 max memory_allocated 47479.2919921875 
[2025-03-22 19:23:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 9 loss:0.07107298821210861 norm:0.00224266923032701 max memory_allocated 47479.2919921875 
[2025-03-22 19:23:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 10 loss:0.07096230983734131 norm:0.0021128382068127394 max memory_allocated 47479.2919921875 
[2025-03-22 19:24:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 11 loss:0.0710524246096611 norm:0.0021786484867334366 max memory_allocated 47479.2919921875 
[2025-03-22 19:24:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 12 loss:0.07077194005250931 norm:0.0019122756784781814 max memory_allocated 47479.2919921875 
[2025-03-22 19:25:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 13 loss:0.07081592082977295 norm:0.00195698207244277 max memory_allocated 47479.2919921875 
[2025-03-22 19:25:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 14 loss:0.07065977901220322 norm:0.0017421082593500614 max memory_allocated 47479.2919921875 
[2025-03-22 19:26:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 15 loss:0.0706443190574646 norm:0.0016167588764801621 max memory_allocated 47479.2919921875 
[2025-03-22 19:26:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 16 loss:0.070529043674469 norm:0.001565699465572834 max memory_allocated 47479.2919921875 
[2025-03-22 19:27:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 17 loss:0.07065532356500626 norm:0.0017114186193794012 max memory_allocated 47479.2919921875 
[2025-03-22 19:27:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 18 loss:0.07054394483566284 norm:0.0015229361597448587 max memory_allocated 47479.2919921875 
[2025-03-22 19:28:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 19 loss:0.07051870226860046 norm:0.001526760053820908 max memory_allocated 47479.2919921875 
[2025-03-22 19:28:48 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [30]
[2025-03-22 19:28:48 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [31] ===
[2025-03-22 19:28:48 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:29:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 0 loss:0.12596410512924194 norm:0.013334857299923897 max memory_allocated 47479.2919921875 
[2025-03-22 19:29:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 1 loss:0.11978407204151154 norm:0.009956005029380322 max memory_allocated 47479.2919921875 
[2025-03-22 19:30:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 2 loss:0.1156623587012291 norm:0.007525291293859482 max memory_allocated 47479.2919921875 
[2025-03-22 19:30:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 3 loss:0.11351676285266876 norm:0.006058854516595602 max memory_allocated 47479.2919921875 
[2025-03-22 19:31:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 4 loss:0.11257442831993103 norm:0.005105328280478716 max memory_allocated 47479.2919921875 
[2025-03-22 19:31:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 5 loss:0.11213783174753189 norm:0.004564996343106031 max memory_allocated 47479.2919921875 
[2025-03-22 19:32:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 6 loss:0.11182568967342377 norm:0.004149727523326874 max memory_allocated 47479.2919921875 
[2025-03-22 19:32:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 7 loss:0.11170705407857895 norm:0.003750699106603861 max memory_allocated 47479.2919921875 
[2025-03-22 19:33:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 8 loss:0.11168825626373291 norm:0.0035323486663401127 max memory_allocated 47479.2919921875 
[2025-03-22 19:33:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 9 loss:0.11165215820074081 norm:0.003401870606467128 max memory_allocated 47479.2919921875 
[2025-03-22 19:34:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 10 loss:0.11145050823688507 norm:0.003277326002717018 max memory_allocated 47479.2919921875 
[2025-03-22 19:34:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 11 loss:0.1113923117518425 norm:0.0032600685954093933 max memory_allocated 47479.2919921875 
[2025-03-22 19:35:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 12 loss:0.11127041280269623 norm:0.0031222663819789886 max memory_allocated 47479.2919921875 
[2025-03-22 19:35:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 13 loss:0.11111879348754883 norm:0.0032789488323032856 max memory_allocated 47479.2919921875 
[2025-03-22 19:36:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 14 loss:0.11099936068058014 norm:0.00308605981990695 max memory_allocated 47479.2919921875 
[2025-03-22 19:36:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 15 loss:0.11107901483774185 norm:0.003008102299645543 max memory_allocated 47479.2919921875 
[2025-03-22 19:37:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 16 loss:0.11119955778121948 norm:0.003128873184323311 max memory_allocated 47479.2919921875 
[2025-03-22 19:37:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 17 loss:0.11139200627803802 norm:0.0031855637207627296 max memory_allocated 47479.2919921875 
[2025-03-22 19:37:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 18 loss:0.11127384752035141 norm:0.0030865033622831106 max memory_allocated 47479.2919921875 
[2025-03-22 19:38:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 19 loss:0.11122255027294159 norm:0.0030585480853915215 max memory_allocated 47479.2919921875 
[2025-03-22 19:39:05 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [31]
[2025-03-22 19:39:05 root] (main_calib_config3_attn.py 379): INFO 19621.402075767517
[2025-03-22 19:39:11 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-22 19:40:01 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.526843070983887
[2025-03-22 19:40:02 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-22 19:41:20 root] (main_calib_config3_attn.py 161): INFO c4 : 7.0459723472595215
[2025-03-22 20:43:22 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.526843070983887, 'c4': 7.0459723472595215, 'results': {'hellaswag': {'acc': 0.5630352519418442, 'acc_stderr': 0.004949969363017659, 'acc_norm': 0.7221668990240988, 'acc_norm_stderr': 0.0044701520816751265}, 'piqa': {'acc': 0.7829162132752993, 'acc_stderr': 0.00961870841575678, 'acc_norm': 0.7687704026115343, 'acc_norm_stderr': 0.009837063180625334}, 'winogrande': {'acc': 0.6614048934490924, 'acc_stderr': 0.013300169865842416}, 'arc_easy': {'acc': 0.696969696969697, 'acc_stderr': 0.00943014066927896, 'acc_norm': 0.5298821548821548, 'acc_norm_stderr': 0.010241444322886433}, 'boolq': {'acc': 0.689908256880734, 'acc_stderr': 0.008089716685417732}, 'arc_challenge': {'acc': 0.40017064846416384, 'acc_stderr': 0.01431719778780918, 'acc_norm': 0.3967576791808874, 'acc_norm_stderr': 0.014296513020180639}}, 'versions': {'hellaswag': 0, 'piqa': 0, 'winogrande': 0, 'arc_easy': 0, 'boolq': 1, 'arc_challenge': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 20:43:22 root] (main_calib_config3_attn.py 175): INFO 40.02,69.70,68.99,56.30,78.29,66.14
