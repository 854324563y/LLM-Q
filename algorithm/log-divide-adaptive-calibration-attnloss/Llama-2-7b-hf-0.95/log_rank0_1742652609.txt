[2025-03-22 14:10:09 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/Llama-2-7b-hf-0.95', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.95.pkl', blocks_pkl='./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-22 14:11:27 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-22 14:11:27 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-22 14:11:27 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-22 14:11:27 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.95.pkl
[2025-03-22 14:11:27 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 9), (9, 11), (11, 14), (14, 17), (17, 20), (20, 23), (23, 26), (26, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-22 14:11:27 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12, 13], [14, 15, 16], [17, 18, 19], [20, 21, 22], [23, 24, 25], [26, 27, 28], [29], [30], [31]]
[2025-03-22 14:11:30 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-22 14:11:30 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:12:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.0008076177327893674 norm:0.003495570505037904 max memory_allocated 34633.880859375 
[2025-03-22 14:12:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.0003132517449557781 norm:0.0007592297624796629 max memory_allocated 34633.880859375 
[2025-03-22 14:13:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.0002935497323051095 norm:0.0010795246344059706 max memory_allocated 34633.880859375 
[2025-03-22 14:13:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0002893857017625123 norm:0.0012385924346745014 max memory_allocated 34633.880859375 
[2025-03-22 14:13:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.0002803966635838151 norm:0.0011512087658047676 max memory_allocated 34633.880859375 
[2025-03-22 14:14:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0002738257753662765 norm:0.0010595571948215365 max memory_allocated 34633.880859375 
[2025-03-22 14:14:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.0002615017583593726 norm:0.0010118980426341295 max memory_allocated 34633.880859375 
[2025-03-22 14:15:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.00025420007295906544 norm:0.0009210575371980667 max memory_allocated 34633.880859375 
[2025-03-22 14:15:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.00025057164020836353 norm:0.0008735700394026935 max memory_allocated 34633.880859375 
[2025-03-22 14:16:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.00024860264966264367 norm:0.0008529399638064206 max memory_allocated 34633.880859375 
[2025-03-22 14:16:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.000253295642323792 norm:0.0008706229273229837 max memory_allocated 34633.880859375 
[2025-03-22 14:17:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.00025125069078058004 norm:0.0008648623479530215 max memory_allocated 34633.880859375 
[2025-03-22 14:17:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.00024537122226320207 norm:0.0007791893440298736 max memory_allocated 34633.880859375 
[2025-03-22 14:18:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.00023500363749917597 norm:0.0007180923130363226 max memory_allocated 34633.880859375 
[2025-03-22 14:18:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.00023319524188991636 norm:0.0007005921215750277 max memory_allocated 34633.880859375 
[2025-03-22 14:19:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.00023293704725801945 norm:0.0006774567300453782 max memory_allocated 34633.880859375 
[2025-03-22 14:19:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.0002270189143018797 norm:0.0006298421649262309 max memory_allocated 34633.880859375 
[2025-03-22 14:20:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0002301371714565903 norm:0.000638447527308017 max memory_allocated 34633.880859375 
[2025-03-22 14:20:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.000223225710215047 norm:0.0005532801733352244 max memory_allocated 34633.880859375 
[2025-03-22 14:21:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0002217364526586607 norm:0.0005328069673851132 max memory_allocated 34633.880859375 
[2025-03-22 14:21:48 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-22 14:21:49 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-22 14:21:49 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:22:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.019757337868213654 norm:0.012942059896886349 max memory_allocated 35100.7724609375 
[2025-03-22 14:22:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.012177531607449055 norm:0.010283012874424458 max memory_allocated 35100.7724609375 
[2025-03-22 14:23:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.01124980952590704 norm:0.013024384155869484 max memory_allocated 35100.7724609375 
[2025-03-22 14:23:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.0076217143796384335 norm:0.01268990058451891 max memory_allocated 35100.7724609375 
[2025-03-22 14:24:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.0071760062128305435 norm:0.01013384573161602 max memory_allocated 35100.7724609375 
[2025-03-22 14:24:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.006995280273258686 norm:0.008867045864462852 max memory_allocated 35100.7724609375 
[2025-03-22 14:25:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.006895840633660555 norm:0.008247249759733677 max memory_allocated 35100.7724609375 
[2025-03-22 14:25:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.008275897242128849 norm:0.009204640984535217 max memory_allocated 35100.7724609375 
[2025-03-22 14:26:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.007024569436907768 norm:0.00815583672374487 max memory_allocated 35100.7724609375 
[2025-03-22 14:26:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.006912987679243088 norm:0.007665751501917839 max memory_allocated 35100.7724609375 
[2025-03-22 14:27:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.006934169214218855 norm:0.007836348377168179 max memory_allocated 35100.7724609375 
[2025-03-22 14:27:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.0067934803664684296 norm:0.007197875063866377 max memory_allocated 35100.7724609375 
[2025-03-22 14:28:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.006881227251142263 norm:0.006819264031946659 max memory_allocated 35100.7724609375 
[2025-03-22 14:28:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.0067235068418085575 norm:0.006634749472141266 max memory_allocated 35100.7724609375 
[2025-03-22 14:29:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.006845774129033089 norm:0.00651869410648942 max memory_allocated 35100.7724609375 
[2025-03-22 14:29:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.007731155026704073 norm:0.007590817287564278 max memory_allocated 35100.7724609375 
[2025-03-22 14:30:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.007313354406505823 norm:0.006632641889154911 max memory_allocated 35100.7724609375 
[2025-03-22 14:30:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.00743022421374917 norm:0.0065866378135979176 max memory_allocated 35100.7724609375 
[2025-03-22 14:31:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.007782242260873318 norm:0.00671777781099081 max memory_allocated 35100.7724609375 
[2025-03-22 14:31:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.006944482214748859 norm:0.006420334335416555 max memory_allocated 35100.7724609375 
[2025-03-22 14:32:05 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-22 14:32:05 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
[2025-03-22 14:32:06 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:32:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.007611358538269997 norm:0.00392718892544508 max memory_allocated 35100.8349609375 
[2025-03-22 14:33:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.005418931134045124 norm:0.0031462290789932013 max memory_allocated 35100.8349609375 
[2025-03-22 14:33:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.005060478579252958 norm:0.002672110451385379 max memory_allocated 35100.8349609375 
[2025-03-22 14:34:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.00482851592823863 norm:0.002371164271607995 max memory_allocated 35100.8349609375 
[2025-03-22 14:34:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.004639598540961742 norm:0.002146812155842781 max memory_allocated 35100.8349609375 
[2025-03-22 14:35:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.004480364732444286 norm:0.0019594780169427395 max memory_allocated 35100.8349609375 
[2025-03-22 14:35:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.004348516929894686 norm:0.0017490367172285914 max memory_allocated 35100.8349609375 
[2025-03-22 14:35:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.004294057376682758 norm:0.0015669907443225384 max memory_allocated 35100.8349609375 
[2025-03-22 14:36:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.004265587776899338 norm:0.0013896062737330794 max memory_allocated 35100.8349609375 
[2025-03-22 14:36:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.004245021380484104 norm:0.0012153055286034942 max memory_allocated 35100.8349609375 
[2025-03-22 14:37:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.004228477831929922 norm:0.001043150550685823 max memory_allocated 35100.8349609375 
[2025-03-22 14:37:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.00423815893009305 norm:0.0009078678558580577 max memory_allocated 35100.8349609375 
[2025-03-22 14:38:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.004211679566651583 norm:0.0007616886869072914 max memory_allocated 35100.8349609375 
[2025-03-22 14:38:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.004166976548731327 norm:0.0006554755382239819 max memory_allocated 35100.8349609375 
[2025-03-22 14:39:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.004178852774202824 norm:0.0006763527635484934 max memory_allocated 35100.8349609375 
[2025-03-22 14:39:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.004216093569993973 norm:0.0007357156137004495 max memory_allocated 35100.8349609375 
[2025-03-22 14:40:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.004149789921939373 norm:0.0004410026012919843 max memory_allocated 35100.8349609375 
[2025-03-22 14:40:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.004132193047553301 norm:0.0003775868099182844 max memory_allocated 35100.8349609375 
[2025-03-22 14:41:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.0041529154404997826 norm:0.00043637261842377484 max memory_allocated 35100.8349609375 
[2025-03-22 14:41:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.004174808040261269 norm:0.0004831932601518929 max memory_allocated 35100.8349609375 
[2025-03-22 14:42:23 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2]
[2025-03-22 14:42:23 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4, 5] ===
[2025-03-22 14:43:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 0 loss:0.010271207429468632 norm:0.00023331513511948287 max memory_allocated 47477.6044921875 
[2025-03-22 14:45:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 1 loss:0.008919649757444859 norm:0.00017766332894098014 max memory_allocated 47477.6044921875 
[2025-03-22 14:46:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 2 loss:0.00831078551709652 norm:0.0001340029266430065 max memory_allocated 47477.6044921875 
[2025-03-22 14:48:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 3 loss:0.007775363512337208 norm:0.00011924267164431512 max memory_allocated 47477.6044921875 
[2025-03-22 14:49:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 4 loss:0.007278576958924532 norm:0.0001062566225300543 max memory_allocated 47477.6044921875 
[2025-03-22 14:51:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 5 loss:0.006864675786346197 norm:0.00010421098704682663 max memory_allocated 47477.6044921875 
[2025-03-22 14:52:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 6 loss:0.006629300303757191 norm:0.00011584910680539906 max memory_allocated 47477.6044921875 
[2025-03-22 14:53:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 7 loss:0.006511924788355827 norm:0.00014943470887374133 max memory_allocated 47477.6044921875 
[2025-03-22 14:55:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 8 loss:0.006430595647543669 norm:0.00019025770598091185 max memory_allocated 47477.6044921875 
[2025-03-22 14:56:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 9 loss:0.006401182152330875 norm:0.00018905475735664368 max memory_allocated 47477.6044921875 
[2025-03-22 14:58:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 10 loss:0.00635745283216238 norm:0.00021134053531568497 max memory_allocated 47477.6044921875 
[2025-03-22 14:59:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 11 loss:0.006330298259854317 norm:0.00019556120969355106 max memory_allocated 47477.6044921875 
[2025-03-22 15:01:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 12 loss:0.006315939594060183 norm:0.00019181700190529227 max memory_allocated 47477.6044921875 
[2025-03-22 15:02:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 13 loss:0.006304946728050709 norm:0.00020812034199479967 max memory_allocated 47477.6044921875 
[2025-03-22 15:03:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 14 loss:0.006307715084403753 norm:0.00021066574845463037 max memory_allocated 47477.6044921875 
[2025-03-22 15:05:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 15 loss:0.006289458367973566 norm:0.00018789594469126314 max memory_allocated 47477.6044921875 
[2025-03-22 15:06:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 16 loss:0.006266647018492222 norm:0.0002046698791673407 max memory_allocated 47477.6044921875 
[2025-03-22 15:08:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 17 loss:0.006255979649722576 norm:0.0002023658307734877 max memory_allocated 47477.6044921875 
[2025-03-22 15:09:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 18 loss:0.0062487199902534485 norm:0.00019282489665783942 max memory_allocated 47477.6044921875 
[2025-03-22 15:11:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 19 loss:0.006238653790205717 norm:0.000207542470889166 max memory_allocated 47477.6044921875 
[2025-03-22 15:13:03 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [3, 4, 5]
[2025-03-22 15:13:04 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [6, 7, 8] ===
[2025-03-22 15:14:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 0 loss:0.013966590166091919 norm:0.00041278000571765006 max memory_allocated 47478.7919921875 
[2025-03-22 15:16:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 1 loss:0.011714749969542027 norm:0.00025100482162088156 max memory_allocated 47478.7919921875 
[2025-03-22 15:17:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 2 loss:0.010737510398030281 norm:0.00020805517851840705 max memory_allocated 47478.7919921875 
[2025-03-22 15:18:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 3 loss:0.009951516054570675 norm:0.00018510302470531315 max memory_allocated 47478.7919921875 
[2025-03-22 15:20:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 4 loss:0.00929301418364048 norm:0.00015497308049816638 max memory_allocated 47478.7919921875 
[2025-03-22 15:21:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 5 loss:0.008765681646764278 norm:0.00014433897740673274 max memory_allocated 47478.7919921875 
[2025-03-22 15:23:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 6 loss:0.008461765944957733 norm:0.0001584656274644658 max memory_allocated 47478.7919921875 
[2025-03-22 15:24:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 7 loss:0.008259117603302002 norm:0.0001621479750610888 max memory_allocated 47478.7919921875 
[2025-03-22 15:26:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 8 loss:0.00809401087462902 norm:0.00014948357420507818 max memory_allocated 47478.7919921875 
[2025-03-22 15:27:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 9 loss:0.007982905022799969 norm:0.00012644179514609277 max memory_allocated 47478.7919921875 
[2025-03-22 15:28:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 10 loss:0.00790722668170929 norm:0.00013389017840381712 max memory_allocated 47478.7919921875 
[2025-03-22 15:30:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 11 loss:0.00783696211874485 norm:0.00012719526421278715 max memory_allocated 47478.7919921875 
[2025-03-22 15:31:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 12 loss:0.0077968258410692215 norm:0.0001234169176314026 max memory_allocated 47478.7919921875 
[2025-03-22 15:33:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 13 loss:0.007761270739138126 norm:0.000127088584122248 max memory_allocated 47478.7919921875 
[2025-03-22 15:34:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 14 loss:0.007713239639997482 norm:0.00011260120663791895 max memory_allocated 47478.7919921875 
[2025-03-22 15:36:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 15 loss:0.007693556137382984 norm:0.0001098731518140994 max memory_allocated 47478.7919921875 
[2025-03-22 15:37:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 16 loss:0.007708052173256874 norm:0.00012507378414738923 max memory_allocated 47478.7919921875 
[2025-03-22 15:38:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 17 loss:0.0076613519340753555 norm:0.00012682058149948716 max memory_allocated 47478.7919921875 
[2025-03-22 15:40:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 18 loss:0.0076534077525138855 norm:0.00012106339272577316 max memory_allocated 47478.7919921875 
[2025-03-22 15:41:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 19 loss:0.007625225931406021 norm:0.0001219349360326305 max memory_allocated 47478.7919921875 
[2025-03-22 15:43:44 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [6, 7, 8]
[2025-03-22 15:43:44 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [9, 10] ===
[2025-03-22 15:44:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 0 loss:0.01176990196108818 norm:0.0003438935964368284 max memory_allocated 47478.7919921875 
[2025-03-22 15:45:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 1 loss:0.010375029407441616 norm:0.00018753352924250066 max memory_allocated 47478.7919921875 
[2025-03-22 15:46:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 2 loss:0.009677750989794731 norm:0.0001330214145127684 max memory_allocated 47478.7919921875 
[2025-03-22 15:47:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 3 loss:0.0091498252004385 norm:0.00011765376257244498 max memory_allocated 47478.7919921875 
[2025-03-22 15:48:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 4 loss:0.008692381903529167 norm:0.00010343272879254073 max memory_allocated 47478.7919921875 
[2025-03-22 15:49:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 5 loss:0.008339022286236286 norm:9.485207556281239e-05 max memory_allocated 47478.7919921875 
[2025-03-22 15:50:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 6 loss:0.008131774142384529 norm:8.667899237480015e-05 max memory_allocated 47478.7919921875 
[2025-03-22 15:51:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 7 loss:0.00800015963613987 norm:8.28730917419307e-05 max memory_allocated 47478.7919921875 
[2025-03-22 15:52:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 8 loss:0.007912838831543922 norm:7.28075101505965e-05 max memory_allocated 47478.7919921875 
[2025-03-22 15:53:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 9 loss:0.007858934812247753 norm:7.442225614795461e-05 max memory_allocated 47478.7919921875 
[2025-03-22 15:54:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 10 loss:0.007808915339410305 norm:6.966634828131646e-05 max memory_allocated 47478.7919921875 
[2025-03-22 15:55:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 11 loss:0.007781314197927713 norm:6.435941031668335e-05 max memory_allocated 47478.7919921875 
[2025-03-22 15:56:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 12 loss:0.007751483004540205 norm:6.506718636956066e-05 max memory_allocated 47478.7919921875 
[2025-03-22 15:57:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 13 loss:0.0077363355085253716 norm:6.207598926266655e-05 max memory_allocated 47478.7919921875 
[2025-03-22 15:58:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 14 loss:0.0077187600545585155 norm:6.29388596280478e-05 max memory_allocated 47478.7919921875 
[2025-03-22 15:59:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 15 loss:0.0076907929033041 norm:6.081441097194329e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:00:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 16 loss:0.007681343238800764 norm:6.040290099917911e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:01:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 17 loss:0.007690218277275562 norm:7.091171573847532e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:01:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 18 loss:0.007685172837227583 norm:6.842179573141038e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:02:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 19 loss:0.0076695154421031475 norm:6.645148096140474e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:04:08 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [9, 10]
[2025-03-22 16:04:08 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [11, 12, 13] ===
[2025-03-22 16:05:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 0 loss:0.015503390692174435 norm:0.00044801938929595053 max memory_allocated 47478.7919921875 
[2025-03-22 16:07:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 1 loss:0.012979944236576557 norm:0.00026479025837033987 max memory_allocated 47478.7919921875 
[2025-03-22 16:08:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 2 loss:0.011873234063386917 norm:0.00021056158584542572 max memory_allocated 47478.7919921875 
[2025-03-22 16:09:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 3 loss:0.011108780279755592 norm:0.00017257692525163293 max memory_allocated 47478.7919921875 
[2025-03-22 16:11:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 4 loss:0.010545245371758938 norm:0.00015981035539880395 max memory_allocated 47478.7919921875 
[2025-03-22 16:12:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 5 loss:0.010091077536344528 norm:0.00014901859685778618 max memory_allocated 47478.7919921875 
[2025-03-22 16:14:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 6 loss:0.009724018163979053 norm:0.00013445131480693817 max memory_allocated 47478.7919921875 
[2025-03-22 16:15:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 7 loss:0.00943243782967329 norm:0.00012466138286981732 max memory_allocated 47478.7919921875 
[2025-03-22 16:17:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 8 loss:0.00923286285251379 norm:0.00011196373088750988 max memory_allocated 47478.7919921875 
[2025-03-22 16:18:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 9 loss:0.009108447469770908 norm:0.00011247039219597355 max memory_allocated 47478.7919921875 
[2025-03-22 16:20:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 10 loss:0.00898059643805027 norm:0.00010674563964130357 max memory_allocated 47478.7919921875 
[2025-03-22 16:21:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 11 loss:0.00888742320239544 norm:9.985405631596223e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:22:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 12 loss:0.008813993073999882 norm:9.301682439399883e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:24:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 13 loss:0.008745304308831692 norm:9.239968494512141e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:25:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 14 loss:0.008709685876965523 norm:9.124359348788857e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:27:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 15 loss:0.008662768639624119 norm:8.16539250081405e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:28:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 16 loss:0.008618620224297047 norm:7.34775239834562e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:30:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 17 loss:0.00858469307422638 norm:7.485580863431096e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:31:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 18 loss:0.0085544902831316 norm:7.617878873134032e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:32:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 19 loss:0.008533449843525887 norm:7.13435874786228e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:34:47 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [11, 12, 13]
[2025-03-22 16:34:47 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [14, 15, 16] ===
[2025-03-22 16:36:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 0 loss:0.015815148130059242 norm:0.00048372664605267346 max memory_allocated 47478.7919921875 
[2025-03-22 16:37:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 1 loss:0.013229820877313614 norm:0.0002777849731501192 max memory_allocated 47478.7919921875 
[2025-03-22 16:39:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 2 loss:0.012053880840539932 norm:0.00020798465993721038 max memory_allocated 47478.7919921875 
[2025-03-22 16:40:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 3 loss:0.011277062818408012 norm:0.00016829778905957937 max memory_allocated 47478.7919921875 
[2025-03-22 16:42:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 4 loss:0.010713271796703339 norm:0.00014843555982224643 max memory_allocated 47478.7919921875 
[2025-03-22 16:43:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 5 loss:0.010228946805000305 norm:0.00013738228881265968 max memory_allocated 47478.7919921875 
[2025-03-22 16:44:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 6 loss:0.009871142916381359 norm:0.00012796746159438044 max memory_allocated 47478.7919921875 
[2025-03-22 16:46:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 7 loss:0.009606275707483292 norm:0.00011833712051156908 max memory_allocated 47478.7919921875 
[2025-03-22 16:47:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 8 loss:0.009404374286532402 norm:0.00010875668522203341 max memory_allocated 47478.7919921875 
[2025-03-22 16:49:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 9 loss:0.009247838519513607 norm:0.00010435218428028747 max memory_allocated 47478.7919921875 
[2025-03-22 16:50:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 10 loss:0.009148732759058475 norm:0.0001047863916028291 max memory_allocated 47478.7919921875 
[2025-03-22 16:52:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 11 loss:0.009064569137990475 norm:9.706365381134674e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:53:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 12 loss:0.008982405997812748 norm:8.92625394044444e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:54:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 13 loss:0.008898593485355377 norm:8.813871681923047e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:56:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 14 loss:0.008829365484416485 norm:8.5470310295932e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:57:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 15 loss:0.008766578510403633 norm:8.508094470016658e-05 max memory_allocated 47478.7919921875 
[2025-03-22 16:59:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 16 loss:0.008722444996237755 norm:8.271345723187551e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:00:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 17 loss:0.008679411374032497 norm:7.371751416940242e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:02:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 18 loss:0.008637032471597195 norm:6.882027082610875e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:03:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 19 loss:0.008604316040873528 norm:6.684365507680923e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:05:28 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [14, 15, 16]
[2025-03-22 17:05:28 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [17, 18, 19] ===
[2025-03-22 17:07:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 0 loss:0.015450038947165012 norm:0.00032714090775698423 max memory_allocated 47478.7919921875 
[2025-03-22 17:08:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 1 loss:0.013963758014142513 norm:0.00020560137636493891 max memory_allocated 47478.7919921875 
[2025-03-22 17:09:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 2 loss:0.013103680685162544 norm:0.0001560200471431017 max memory_allocated 47478.7919921875 
[2025-03-22 17:11:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 3 loss:0.012441541999578476 norm:0.00012989522656425834 max memory_allocated 47478.7919921875 
[2025-03-22 17:12:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 4 loss:0.01180456206202507 norm:0.00011485717550385743 max memory_allocated 47478.7919921875 
[2025-03-22 17:14:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 5 loss:0.011324061080813408 norm:0.00010925871902145445 max memory_allocated 47478.7919921875 
[2025-03-22 17:15:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 6 loss:0.011024040170013905 norm:0.00010048136755358428 max memory_allocated 47478.7919921875 
[2025-03-22 17:17:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 7 loss:0.010855662636458874 norm:9.618328476790339e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:18:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 8 loss:0.010729472152888775 norm:8.582015288993716e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:19:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 9 loss:0.010650662705302238 norm:8.268878445960581e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:21:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 10 loss:0.010582013987004757 norm:7.943222590256482e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:22:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 11 loss:0.010512661188840866 norm:7.466172974091023e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:24:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 12 loss:0.010445979423820972 norm:7.161004759836942e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:25:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 13 loss:0.010397628881037235 norm:7.019992335699499e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:27:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 14 loss:0.01036140974611044 norm:6.712652975693345e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:28:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 15 loss:0.01031587366014719 norm:6.427064363379031e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:29:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 16 loss:0.010285774245858192 norm:6.0362766816979274e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:31:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 17 loss:0.010267507284879684 norm:5.560867430176586e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:32:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 18 loss:0.010239498689770699 norm:5.528198380488902e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:34:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 19 loss:0.01020877342671156 norm:4.996902134735137e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:36:12 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [17, 18, 19]
[2025-03-22 17:36:12 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [20, 21, 22] ===
[2025-03-22 17:37:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 0 loss:0.0212448388338089 norm:0.0004687224864028394 max memory_allocated 47478.7919921875 
[2025-03-22 17:39:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 1 loss:0.019194655120372772 norm:0.0002930389891844243 max memory_allocated 47478.7919921875 
[2025-03-22 17:40:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 2 loss:0.018048668280243874 norm:0.00021524775365833193 max memory_allocated 47478.7919921875 
[2025-03-22 17:42:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 3 loss:0.01707460731267929 norm:0.0001781633181963116 max memory_allocated 47478.7919921875 
[2025-03-22 17:43:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 4 loss:0.016165276989340782 norm:0.00016010140825528651 max memory_allocated 47478.7919921875 
[2025-03-22 17:44:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 5 loss:0.015629665926098824 norm:0.00014747680688742548 max memory_allocated 47478.7919921875 
[2025-03-22 17:46:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 6 loss:0.015425547957420349 norm:0.00014163056039251387 max memory_allocated 47478.7919921875 
[2025-03-22 17:47:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 7 loss:0.015267566777765751 norm:0.00013421566109173 max memory_allocated 47478.7919921875 
[2025-03-22 17:49:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 8 loss:0.0151450764387846 norm:0.00012690361472778022 max memory_allocated 47478.7919921875 
[2025-03-22 17:50:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 9 loss:0.015026101842522621 norm:0.00011716509470716119 max memory_allocated 47478.7919921875 
[2025-03-22 17:52:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 10 loss:0.01493922434747219 norm:0.00011694432032527402 max memory_allocated 47478.7919921875 
[2025-03-22 17:53:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 11 loss:0.014852277934551239 norm:0.00010598845256026834 max memory_allocated 47478.7919921875 
[2025-03-22 17:54:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 12 loss:0.014776292257010937 norm:0.00010274656233377755 max memory_allocated 47478.7919921875 
[2025-03-22 17:56:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 13 loss:0.014703230932354927 norm:9.445843170396984e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:57:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 14 loss:0.014650302939116955 norm:9.428990597371012e-05 max memory_allocated 47478.7919921875 
[2025-03-22 17:59:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 15 loss:0.014594024047255516 norm:9.035129914991558e-05 max memory_allocated 47478.7919921875 
[2025-03-22 18:00:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 16 loss:0.014541281387209892 norm:8.93138931132853e-05 max memory_allocated 47478.7919921875 
[2025-03-22 18:02:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 17 loss:0.014499624259769917 norm:9.098548616748303e-05 max memory_allocated 47478.7919921875 
[2025-03-22 18:03:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 18 loss:0.01445702463388443 norm:8.587230695411563e-05 max memory_allocated 47478.7919921875 
[2025-03-22 18:04:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 19 loss:0.014409391209483147 norm:7.174689380917698e-05 max memory_allocated 47478.7919921875 
[2025-03-22 18:06:53 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [20, 21, 22]
[2025-03-22 18:06:53 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [23, 24, 25] ===
[2025-03-22 18:08:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 0 loss:0.029601985588669777 norm:0.000482542731333524 max memory_allocated 47478.8544921875 
[2025-03-22 18:09:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 1 loss:0.027337733656167984 norm:0.00032915541669353843 max memory_allocated 47478.8544921875 
[2025-03-22 18:11:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 2 loss:0.0258475411683321 norm:0.00027631595730781555 max memory_allocated 47478.8544921875 
[2025-03-22 18:12:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 3 loss:0.024308212101459503 norm:0.00023537810193374753 max memory_allocated 47478.8544921875 
[2025-03-22 18:14:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 4 loss:0.023222144693136215 norm:0.0002158670249627903 max memory_allocated 47478.8544921875 
[2025-03-22 18:15:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 5 loss:0.022863022983074188 norm:0.0002002042019739747 max memory_allocated 47478.8544921875 
[2025-03-22 18:17:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 6 loss:0.022655067965388298 norm:0.0001912014849949628 max memory_allocated 47478.8544921875 
[2025-03-22 18:18:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 7 loss:0.02253381721675396 norm:0.0001895554014481604 max memory_allocated 47478.8544921875 
[2025-03-22 18:19:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 8 loss:0.02239484339952469 norm:0.00017336299060843885 max memory_allocated 47478.8544921875 
[2025-03-22 18:21:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 9 loss:0.02225860208272934 norm:0.0001572385081090033 max memory_allocated 47478.8544921875 
[2025-03-22 18:22:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 10 loss:0.02214031107723713 norm:0.00014878599904477596 max memory_allocated 47478.8544921875 
[2025-03-22 18:24:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 11 loss:0.022054968401789665 norm:0.00015303477994166315 max memory_allocated 47478.8544921875 
[2025-03-22 18:25:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 12 loss:0.021944720298051834 norm:0.0001467953552491963 max memory_allocated 47478.8544921875 
[2025-03-22 18:27:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 13 loss:0.02186710201203823 norm:0.00013962907542008907 max memory_allocated 47478.8544921875 
[2025-03-22 18:28:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 14 loss:0.021766463294625282 norm:0.0001229934423463419 max memory_allocated 47478.8544921875 
[2025-03-22 18:29:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 15 loss:0.021675076335668564 norm:0.00012216318282298744 max memory_allocated 47478.8544921875 
[2025-03-22 18:31:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 16 loss:0.02163565717637539 norm:0.00012972729746252298 max memory_allocated 47478.8544921875 
[2025-03-22 18:32:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 17 loss:0.02158963494002819 norm:0.00012149551912443712 max memory_allocated 47478.8544921875 
[2025-03-22 18:34:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 18 loss:0.021525539457798004 norm:0.00011402192467357963 max memory_allocated 47478.8544921875 
[2025-03-22 18:35:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 19 loss:0.02147669531404972 norm:0.00011527090100571513 max memory_allocated 47478.8544921875 
[2025-03-22 18:37:34 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [23, 24, 25]
[2025-03-22 18:37:34 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [26, 27, 28] ===
[2025-03-22 18:37:35 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 18:39:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 0 loss:0.042912788689136505 norm:0.0028934155125170946 max memory_allocated 47479.0419921875 
[2025-03-22 18:40:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 1 loss:0.03982766717672348 norm:0.002133444417268038 max memory_allocated 47479.0419921875 
[2025-03-22 18:41:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 2 loss:0.037598639726638794 norm:0.001776340650394559 max memory_allocated 47479.0419921875 
[2025-03-22 18:43:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 3 loss:0.03535345196723938 norm:0.001476043020375073 max memory_allocated 47479.0419921875 
[2025-03-22 18:44:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 4 loss:0.03443290293216705 norm:0.0012283588293939829 max memory_allocated 47479.0419921875 
[2025-03-22 18:46:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 5 loss:0.034067302942276 norm:0.0009950321400538087 max memory_allocated 47479.0419921875 
[2025-03-22 18:47:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 6 loss:0.03384293243288994 norm:0.0008537782705388963 max memory_allocated 47479.0419921875 
[2025-03-22 18:49:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 7 loss:0.03368161991238594 norm:0.0008053563069552183 max memory_allocated 47479.0419921875 
[2025-03-22 18:50:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 8 loss:0.03352588415145874 norm:0.0007820007158443332 max memory_allocated 47479.0419921875 
[2025-03-22 18:52:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 9 loss:0.033405594527721405 norm:0.0007229418843053281 max memory_allocated 47479.0419921875 
[2025-03-22 18:53:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 10 loss:0.03330395743250847 norm:0.0007327147759497166 max memory_allocated 47479.0419921875 
[2025-03-22 18:54:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 11 loss:0.03319301828742027 norm:0.0006984744686633348 max memory_allocated 47479.0419921875 
[2025-03-22 18:56:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 12 loss:0.033099837601184845 norm:0.0006555501604452729 max memory_allocated 47479.0419921875 
[2025-03-22 18:57:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 13 loss:0.033019617199897766 norm:0.0006179327028803527 max memory_allocated 47479.0419921875 
[2025-03-22 18:59:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 14 loss:0.032910823822021484 norm:0.0006092118564993143 max memory_allocated 47479.0419921875 
[2025-03-22 19:00:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 15 loss:0.03279488533735275 norm:0.0005252568516880274 max memory_allocated 47479.0419921875 
[2025-03-22 19:02:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 16 loss:0.03272517770528793 norm:0.0005385479889810085 max memory_allocated 47479.0419921875 
[2025-03-22 19:03:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 17 loss:0.03268786519765854 norm:0.0005444809794425964 max memory_allocated 47479.0419921875 
[2025-03-22 19:04:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 18 loss:0.03262326493859291 norm:0.0005125357420183718 max memory_allocated 47479.0419921875 
[2025-03-22 19:06:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 19 loss:0.03260061517357826 norm:0.0005108478362672031 max memory_allocated 47479.0419921875 
[2025-03-22 19:08:17 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [26, 27, 28]
[2025-03-22 19:08:17 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [29] ===
[2025-03-22 19:08:17 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:08:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 0 loss:0.03916814178228378 norm:0.0020777424797415733 max memory_allocated 47479.0419921875 
[2025-03-22 19:09:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 1 loss:0.03753214329481125 norm:0.0015520674642175436 max memory_allocated 47479.0419921875 
[2025-03-22 19:09:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 2 loss:0.03638292849063873 norm:0.0012345683062449098 max memory_allocated 47479.0419921875 
[2025-03-22 19:10:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 3 loss:0.03559005260467529 norm:0.0010310523211956024 max memory_allocated 47479.0419921875 
[2025-03-22 19:10:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 4 loss:0.03538466617465019 norm:0.0009768428280949593 max memory_allocated 47479.0419921875 
[2025-03-22 19:11:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 5 loss:0.03527279943227768 norm:0.000831879791803658 max memory_allocated 47479.0419921875 
[2025-03-22 19:11:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 6 loss:0.03517308831214905 norm:0.0007032642024569213 max memory_allocated 47479.0419921875 
[2025-03-22 19:12:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 7 loss:0.03508734703063965 norm:0.0006487753125838935 max memory_allocated 47479.0419921875 
[2025-03-22 19:12:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 8 loss:0.03503717854619026 norm:0.0005306124221533537 max memory_allocated 47479.0419921875 
[2025-03-22 19:13:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 9 loss:0.034974902868270874 norm:0.0005835721967741847 max memory_allocated 47479.0419921875 
[2025-03-22 19:13:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 10 loss:0.034977879375219345 norm:0.0005268251989036798 max memory_allocated 47479.0419921875 
[2025-03-22 19:14:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 11 loss:0.034977469593286514 norm:0.0006281324313022196 max memory_allocated 47479.0419921875 
[2025-03-22 19:14:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 12 loss:0.0348799042403698 norm:0.00048446047003380954 max memory_allocated 47479.0419921875 
[2025-03-22 19:15:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 13 loss:0.03482462465763092 norm:0.0004981178790330887 max memory_allocated 47479.0419921875 
[2025-03-22 19:15:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 14 loss:0.03480784222483635 norm:0.00043303208076395094 max memory_allocated 47479.0419921875 
[2025-03-22 19:16:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 15 loss:0.03477092459797859 norm:0.0004607999580912292 max memory_allocated 47479.0419921875 
[2025-03-22 19:16:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 16 loss:0.0347447469830513 norm:0.0004120866069570184 max memory_allocated 47479.0419921875 
[2025-03-22 19:16:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 17 loss:0.03473423048853874 norm:0.0004267208860255778 max memory_allocated 47479.0419921875 
[2025-03-22 19:17:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 18 loss:0.034702010452747345 norm:0.0004012563149444759 max memory_allocated 47479.0419921875 
[2025-03-22 19:17:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 19 loss:0.03469682112336159 norm:0.00039803338586352766 max memory_allocated 47479.0419921875 
[2025-03-22 19:18:34 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [29]
[2025-03-22 19:18:35 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [30] ===
[2025-03-22 19:18:35 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:19:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 0 loss:0.07042048871517181 norm:0.010199358686804771 max memory_allocated 47479.0419921875 
[2025-03-22 19:19:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 1 loss:0.05862146243453026 norm:0.006261101923882961 max memory_allocated 47479.0419921875 
[2025-03-22 19:20:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 2 loss:0.05246296525001526 norm:0.004377660341560841 max memory_allocated 47479.0419921875 
[2025-03-22 19:20:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 3 loss:0.05016814544796944 norm:0.003504092339426279 max memory_allocated 47479.0419921875 
[2025-03-22 19:21:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 4 loss:0.04971544072031975 norm:0.0030615455470979214 max memory_allocated 47479.0419921875 
[2025-03-22 19:21:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 5 loss:0.049429554492235184 norm:0.002762348623946309 max memory_allocated 47479.0419921875 
[2025-03-22 19:21:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 6 loss:0.0492473803460598 norm:0.002627831883728504 max memory_allocated 47479.0419921875 
[2025-03-22 19:22:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 7 loss:0.049112409353256226 norm:0.0023871520534157753 max memory_allocated 47479.0419921875 
[2025-03-22 19:22:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 8 loss:0.04895201697945595 norm:0.002114717848598957 max memory_allocated 47479.0419921875 
[2025-03-22 19:23:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 9 loss:0.048853859305381775 norm:0.0019502121722325683 max memory_allocated 47479.0419921875 
[2025-03-22 19:23:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 10 loss:0.04879188910126686 norm:0.0018305995035916567 max memory_allocated 47479.0419921875 
[2025-03-22 19:24:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 11 loss:0.04874390736222267 norm:0.001775462762452662 max memory_allocated 47479.0419921875 
[2025-03-22 19:24:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 12 loss:0.04872801899909973 norm:0.0017266403883695602 max memory_allocated 47479.0419921875 
[2025-03-22 19:25:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 13 loss:0.048770926892757416 norm:0.0016892882995307446 max memory_allocated 47479.0419921875 
[2025-03-22 19:25:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 14 loss:0.04861653596162796 norm:0.001502365805208683 max memory_allocated 47479.0419921875 
[2025-03-22 19:26:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 15 loss:0.04860929027199745 norm:0.001491930102929473 max memory_allocated 47479.0419921875 
[2025-03-22 19:26:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 16 loss:0.04880170896649361 norm:0.001667119562625885 max memory_allocated 47479.0419921875 
[2025-03-22 19:27:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 17 loss:0.04852406308054924 norm:0.001368399360217154 max memory_allocated 47479.0419921875 
[2025-03-22 19:27:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 18 loss:0.04851054400205612 norm:0.001337626134045422 max memory_allocated 47479.0419921875 
[2025-03-22 19:28:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 19 loss:0.04895387589931488 norm:0.001609044848009944 max memory_allocated 47479.0419921875 
[2025-03-22 19:28:53 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [30]
[2025-03-22 19:28:53 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [31] ===
[2025-03-22 19:28:53 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:29:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 0 loss:0.08237564563751221 norm:0.006992229726165533 max memory_allocated 47479.0419921875 
[2025-03-22 19:29:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 1 loss:0.078689344227314 norm:0.005033080466091633 max memory_allocated 47479.0419921875 
[2025-03-22 19:30:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 2 loss:0.07621879875659943 norm:0.004107197746634483 max memory_allocated 47479.0419921875 
[2025-03-22 19:30:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 3 loss:0.07519315183162689 norm:0.0036284124944359064 max memory_allocated 47479.0419921875 
[2025-03-22 19:31:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 4 loss:0.0748341754078865 norm:0.0032918178476393223 max memory_allocated 47479.0419921875 
[2025-03-22 19:31:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 5 loss:0.07453297078609467 norm:0.0029356207232922316 max memory_allocated 47479.0419921875 
[2025-03-22 19:32:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 6 loss:0.07434111833572388 norm:0.0027210493572056293 max memory_allocated 47479.0419921875 
[2025-03-22 19:32:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 7 loss:0.07421998679637909 norm:0.002601061249151826 max memory_allocated 47479.0419921875 
[2025-03-22 19:33:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 8 loss:0.07411546260118484 norm:0.0025012807454913855 max memory_allocated 47479.0419921875 
[2025-03-22 19:33:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 9 loss:0.07399468868970871 norm:0.0024185343645513058 max memory_allocated 47479.0419921875 
[2025-03-22 19:34:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 10 loss:0.07392372190952301 norm:0.002277328632771969 max memory_allocated 47479.0419921875 
[2025-03-22 19:34:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 11 loss:0.07392187416553497 norm:0.00246195076033473 max memory_allocated 47479.0419921875 
[2025-03-22 19:35:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 12 loss:0.07385791838169098 norm:0.0022875552531331778 max memory_allocated 47479.0419921875 
[2025-03-22 19:35:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 13 loss:0.07378941029310226 norm:0.002248370787128806 max memory_allocated 47479.0419921875 
[2025-03-22 19:36:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 14 loss:0.07375513762235641 norm:0.0021625151857733727 max memory_allocated 47479.0419921875 
[2025-03-22 19:36:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 15 loss:0.07366956025362015 norm:0.0020473264157772064 max memory_allocated 47479.0419921875 
[2025-03-22 19:37:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 16 loss:0.0736146941781044 norm:0.0019231070764362812 max memory_allocated 47479.0419921875 
[2025-03-22 19:37:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 17 loss:0.07361612468957901 norm:0.0020677971187978983 max memory_allocated 47479.0419921875 
[2025-03-22 19:38:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 18 loss:0.07365909218788147 norm:0.002181645482778549 max memory_allocated 47479.0419921875 
[2025-03-22 19:38:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 19 loss:0.0736488476395607 norm:0.002148517407476902 max memory_allocated 47479.0419921875 
[2025-03-22 19:39:10 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [31]
[2025-03-22 19:39:10 root] (main_calib_config3_attn.py 379): INFO 19663.301944732666
[2025-03-22 19:39:15 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-22 19:40:06 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.508194923400879
[2025-03-22 19:40:06 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-22 19:41:24 root] (main_calib_config3_attn.py 161): INFO c4 : 7.0200347900390625
[2025-03-22 20:41:42 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.508194923400879, 'c4': 7.0200347900390625, 'results': {'boolq': {'acc': 0.7143730886850153, 'acc_stderr': 0.00790050121831875}, 'piqa': {'acc': 0.7812840043525572, 'acc_stderr': 0.009644731932667561, 'acc_norm': 0.7709466811751904, 'acc_norm_stderr': 0.009804509865175505}, 'hellaswag': {'acc': 0.5660227046405099, 'acc_stderr': 0.004946089230153021, 'acc_norm': 0.7258514240191197, 'acc_norm_stderr': 0.004451725530626376}, 'arc_challenge': {'acc': 0.4061433447098976, 'acc_stderr': 0.01435165669009786, 'acc_norm': 0.4035836177474403, 'acc_norm_stderr': 0.014337158914268445}, 'arc_easy': {'acc': 0.6957070707070707, 'acc_stderr': 0.009441202922359185, 'acc_norm': 0.5311447811447811, 'acc_norm_stderr': 0.010239860250021748}, 'winogrande': {'acc': 0.6692975532754538, 'acc_stderr': 0.01322243588700269}}, 'versions': {'boolq': 1, 'piqa': 0, 'hellaswag': 0, 'arc_challenge': 0, 'arc_easy': 0, 'winogrande': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 20:41:42 root] (main_calib_config3_attn.py 175): INFO 40.61,69.57,71.44,56.60,78.13,66.93
