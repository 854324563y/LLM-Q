[2025-03-23 02:01:29 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/Llama-2-13b-hf-1.0', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/Llama-2-13b-hf/quant_map_Llama-2-13b-hf_1.0.pkl', blocks_pkl='./log-divide/Llama-2-13b-hf-w4a4/Llama-2-13b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-23 02:01:31 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-23 02:01:31 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-23 02:01:31 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-23 02:01:31 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/Llama-2-13b-hf/quant_map_Llama-2-13b-hf_1.0.pkl
[2025-03-23 02:01:31 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/Llama-2-13b-hf-w4a4/Llama-2-13b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 9), (9, 12), (12, 15), (15, 18), (18, 21), (21, 24), (24, 27), (27, 30), (30, 33), (33, 36), (36, 37), (37, 38), (38, 39), (39, 40)]
[2025-03-23 02:01:31 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17], [18, 19, 20], [21, 22, 23], [24, 25, 26], [27, 28, 29], [30, 31, 32], [33, 34, 35], [36], [37], [38], [39]]
[2025-03-23 02:01:33 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-23 02:01:34 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:02:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.0010575376218184829 norm:0.004043658263981342 max memory_allocated 44358.7939453125 
[2025-03-23 02:03:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.0002648137742653489 norm:0.00055480602895841 max memory_allocated 44358.7939453125 
[2025-03-23 02:03:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.00022921941126696765 norm:0.0008194604888558388 max memory_allocated 44358.7939453125 
[2025-03-23 02:04:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.00021621680934913456 norm:0.0013302480801939964 max memory_allocated 44358.7939453125 
[2025-03-23 02:05:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.00020632297673728317 norm:0.0013497192412614822 max memory_allocated 44358.7939453125 
[2025-03-23 02:05:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.00019878253806382418 norm:0.0012940901797264814 max memory_allocated 44358.7939453125 
[2025-03-23 02:06:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.00018802584963850677 norm:0.0011386909754946828 max memory_allocated 44358.7939453125 
[2025-03-23 02:07:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.00019550725119188428 norm:0.0012025529285892844 max memory_allocated 44358.7939453125 
[2025-03-23 02:08:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0001857266470324248 norm:0.0010867886012420058 max memory_allocated 44358.7939453125 
[2025-03-23 02:08:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.00017709135136101395 norm:0.0009620860801078379 max memory_allocated 44358.7939453125 
[2025-03-23 02:09:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.00017845979891717434 norm:0.000933528586756438 max memory_allocated 44358.7939453125 
[2025-03-23 02:10:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0001676796964602545 norm:0.0008330001146532595 max memory_allocated 44358.7939453125 
[2025-03-23 02:11:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.00017246084462385625 norm:0.0008292822749353945 max memory_allocated 44358.7939453125 
[2025-03-23 02:11:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.00016666790179442614 norm:0.0007681440911255777 max memory_allocated 44358.7939453125 
[2025-03-23 02:12:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.00016172743926290423 norm:0.0006819840637035668 max memory_allocated 44358.7939453125 
[2025-03-23 02:13:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.00016147899441421032 norm:0.000634732365142554 max memory_allocated 44358.7939453125 
[2025-03-23 02:13:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.00015517948486376554 norm:0.0006016865954734385 max memory_allocated 44358.7939453125 
[2025-03-23 02:14:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.00015319345402531326 norm:0.0005589532083831728 max memory_allocated 44358.7939453125 
[2025-03-23 02:15:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.00015439765411429107 norm:0.0005349004059098661 max memory_allocated 44358.7939453125 
[2025-03-23 02:16:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.00015455298125743866 norm:0.00050703453598544 max memory_allocated 44358.7939453125 
[2025-03-23 02:17:10 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-23 02:17:10 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-23 02:17:11 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:17:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.0010465166997164488 norm:0.0024424174334853888 max memory_allocated 44358.7939453125 
[2025-03-23 02:18:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.0006846641190350056 norm:0.0007754231919534504 max memory_allocated 44358.7939453125 
[2025-03-23 02:19:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.0006784777506254613 norm:0.001317791873589158 max memory_allocated 44358.7939453125 
[2025-03-23 02:20:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.0006684331456199288 norm:0.0012514741392806172 max memory_allocated 44358.7939453125 
[2025-03-23 02:20:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.0006577010499313474 norm:0.0011732412967830896 max memory_allocated 44358.7939453125 
[2025-03-23 02:21:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.0006469505606219172 norm:0.001089396420866251 max memory_allocated 44358.7939453125 
[2025-03-23 02:22:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.0006385810556821525 norm:0.0010306843323633075 max memory_allocated 44358.7939453125 
[2025-03-23 02:23:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.0006274365005083382 norm:0.0009627318358980119 max memory_allocated 44358.7939453125 
[2025-03-23 02:23:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.0006175637827254832 norm:0.0009063609759323299 max memory_allocated 44358.7939453125 
[2025-03-23 02:24:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.0006088565569370985 norm:0.0008584195747971535 max memory_allocated 44358.7939453125 
[2025-03-23 02:25:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.0006006296607665718 norm:0.0008292121929116547 max memory_allocated 44358.7939453125 
[2025-03-23 02:25:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.0005902001284994185 norm:0.0007766959024593234 max memory_allocated 44358.7939453125 
[2025-03-23 02:26:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.0005804337561130524 norm:0.0007383671472780406 max memory_allocated 44358.7939453125 
[2025-03-23 02:27:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.0005738838808611035 norm:0.0007129036239348352 max memory_allocated 44358.7939453125 
[2025-03-23 02:28:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.0005616752314381301 norm:0.000652928079944104 max memory_allocated 44358.7939453125 
[2025-03-23 02:28:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.0005521345301531255 norm:0.0006214709719642997 max memory_allocated 44358.7939453125 
[2025-03-23 02:29:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.0005428596632555127 norm:0.0005910914624109864 max memory_allocated 44358.7939453125 
[2025-03-23 02:30:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.0005339788622222841 norm:0.0005532066570594907 max memory_allocated 44358.7939453125 
[2025-03-23 02:31:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.0005230639362707734 norm:0.0005164037575013936 max memory_allocated 44358.7939453125 
[2025-03-23 02:31:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.0005161772714927793 norm:0.00048723703366704285 max memory_allocated 44358.7939453125 
[2025-03-23 02:32:43 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-23 02:32:43 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
[2025-03-23 02:32:43 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:33:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.0016886687371879816 norm:0.002058317419141531 max memory_allocated 44358.7939453125 
[2025-03-23 02:34:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.0013759273570030928 norm:0.0007257199613377452 max memory_allocated 44358.7939453125 
[2025-03-23 02:34:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.0013557138154283166 norm:0.001126338029280305 max memory_allocated 44358.7939453125 
[2025-03-23 02:35:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.0013340163277462125 norm:0.001278855255804956 max memory_allocated 44358.7939453125 
[2025-03-23 02:36:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.0013040085323154926 norm:0.0012215602910146117 max memory_allocated 44358.7939453125 
[2025-03-23 02:37:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.0012633568840101361 norm:0.0011436383938416839 max memory_allocated 44358.7939453125 
[2025-03-23 02:37:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.0012145148357376456 norm:0.00107970938552171 max memory_allocated 44358.7939453125 
[2025-03-23 02:38:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.0011548562906682491 norm:0.0010286252945661545 max memory_allocated 44358.7939453125 
[2025-03-23 02:39:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.0010952884331345558 norm:0.0009618093608878553 max memory_allocated 44358.7939453125 
[2025-03-23 02:40:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.0010492042638361454 norm:0.0009009463828988373 max memory_allocated 44358.7939453125 
[2025-03-23 02:40:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.0010235903318971395 norm:0.000875515048392117 max memory_allocated 44358.7939453125 
[2025-03-23 02:41:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.0010084109380841255 norm:0.0008021201938390732 max memory_allocated 44358.7939453125 
[2025-03-23 02:42:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.00100642628967762 norm:0.0007703242008574307 max memory_allocated 44358.7939453125 
[2025-03-23 02:42:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.0010029859840869904 norm:0.0007131922175176442 max memory_allocated 44358.7939453125 
[2025-03-23 02:43:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.0010015808511525393 norm:0.000680129393003881 max memory_allocated 44358.7939453125 
[2025-03-23 02:44:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.0009973194682970643 norm:0.0006169878179207444 max memory_allocated 44358.7939453125 
[2025-03-23 02:45:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.0009940318996086717 norm:0.0005652482504956424 max memory_allocated 44358.7939453125 
[2025-03-23 02:45:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.0009898002026602626 norm:0.0005037655355408788 max memory_allocated 44358.7939453125 
[2025-03-23 02:46:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.000989772961474955 norm:0.0004719213175121695 max memory_allocated 44358.7939453125 
[2025-03-23 02:47:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.0009865295141935349 norm:0.00042884930735453963 max memory_allocated 44358.7939453125 
[2025-03-23 02:48:19 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2]
[2025-03-23 02:48:19 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4, 5] ===
[2025-03-23 02:50:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 0 loss:0.004363672807812691 norm:0.0001990408345591277 max memory_allocated 62760.0654296875 
[2025-03-23 02:52:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 1 loss:0.004096413962543011 norm:0.00011801690561696887 max memory_allocated 62760.0654296875 
[2025-03-23 02:54:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 2 loss:0.003972490783780813 norm:0.00011174354585818946 max memory_allocated 62760.0654296875 
[2025-03-23 02:57:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 3 loss:0.003842664649710059 norm:0.00011166561307618394 max memory_allocated 62760.0654296875 
[2025-03-23 02:59:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 4 loss:0.0036920567508786917 norm:0.00020065564604010433 max memory_allocated 62760.0654296875 
[2025-03-23 03:01:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 5 loss:0.0035034650936722755 norm:0.00019577318744268268 max memory_allocated 62760.0654296875 
[2025-03-23 03:03:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 6 loss:0.003248354885727167 norm:0.00010096158075612038 max memory_allocated 62760.0654296875 
[2025-03-23 03:05:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 7 loss:0.0030845701694488525 norm:0.00012510699161794037 max memory_allocated 62760.0654296875 
[2025-03-23 03:07:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 8 loss:0.0029785980004817247 norm:0.00015248864656314254 max memory_allocated 62760.0654296875 
[2025-03-23 03:10:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 9 loss:0.002887253649532795 norm:0.00012834425433538854 max memory_allocated 62760.0654296875 
[2025-03-23 03:12:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 10 loss:0.002840327564626932 norm:0.00012719824735540897 max memory_allocated 62760.0654296875 
[2025-03-23 03:14:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 11 loss:0.002807868644595146 norm:0.00011930032633244991 max memory_allocated 62760.0654296875 
[2025-03-23 03:16:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 12 loss:0.002785617485642433 norm:0.00011227771756239235 max memory_allocated 62760.0654296875 
[2025-03-23 03:18:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 13 loss:0.0027803690172731876 norm:0.00010304240277037024 max memory_allocated 62760.0654296875 
[2025-03-23 03:20:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 14 loss:0.0027826563455164433 norm:0.00011600610741879791 max memory_allocated 62760.0654296875 
[2025-03-23 03:23:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 15 loss:0.0027734925970435143 norm:9.48765518842265e-05 max memory_allocated 62760.0654296875 
[2025-03-23 03:25:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 16 loss:0.002774301916360855 norm:8.300218178192154e-05 max memory_allocated 62760.0654296875 
[2025-03-23 03:27:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 17 loss:0.002773081883788109 norm:9.47004446061328e-05 max memory_allocated 62760.0654296875 
[2025-03-23 03:29:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 18 loss:0.0027767722494900227 norm:0.00010077810293296352 max memory_allocated 62760.0654296875 
[2025-03-23 03:31:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 19 loss:0.0027739261277019978 norm:9.342977864434943e-05 max memory_allocated 62760.0654296875 
[2025-03-23 03:34:40 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [3, 4, 5]
[2025-03-23 03:34:40 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [6, 7, 8] ===
[2025-03-23 03:36:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 0 loss:0.005899805575609207 norm:0.00010047961404779926 max memory_allocated 62760.2998046875 
[2025-03-23 03:39:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 1 loss:0.005521573591977358 norm:6.663668318651617e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:41:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 2 loss:0.0053021786734461784 norm:5.064816650701687e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:43:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 3 loss:0.005092773586511612 norm:4.5865501306252554e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:45:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 4 loss:0.004839141853153706 norm:4.222612551529892e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:47:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 5 loss:0.00456374604254961 norm:4.24278769060038e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:49:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 6 loss:0.004319390282034874 norm:4.242729119141586e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:52:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 7 loss:0.004146449733525515 norm:4.0864924812922254e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:54:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 8 loss:0.004036521073430777 norm:4.168076702626422e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:56:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 9 loss:0.003971975762397051 norm:4.2415376810822636e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:58:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 10 loss:0.003935576882213354 norm:4.311537486501038e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:00:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 11 loss:0.003916017711162567 norm:3.888824721798301e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:02:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 12 loss:0.003903638571500778 norm:3.95175811718218e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:05:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 13 loss:0.0038965572603046894 norm:3.90103341487702e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:07:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 14 loss:0.003895214293152094 norm:3.6641220503952354e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:09:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 15 loss:0.0038903753738850355 norm:3.809778354479931e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:11:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 16 loss:0.0038900936488062143 norm:3.798681063926779e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:13:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 17 loss:0.0038878985214978456 norm:3.77519718313124e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:15:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 18 loss:0.003881253534927964 norm:3.659052890725434e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:18:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 19 loss:0.0038765964563935995 norm:3.3877327950904146e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:20:56 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [6, 7, 8]
[2025-03-23 04:20:56 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [9, 10, 11] ===
[2025-03-23 04:23:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 0 loss:0.007622820790857077 norm:0.00013040498015470803 max memory_allocated 62760.2998046875 
[2025-03-23 04:25:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 1 loss:0.007011725567281246 norm:8.760283526498824e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:27:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 2 loss:0.006644555367529392 norm:6.428843335015699e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:29:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 3 loss:0.006326914764940739 norm:5.461544787976891e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:31:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 4 loss:0.006007832009345293 norm:5.0950126023963094e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:34:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 5 loss:0.005692383274435997 norm:4.874367004958913e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:36:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 6 loss:0.0054388451389968395 norm:4.5060394768370315e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:38:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 7 loss:0.005268943030387163 norm:4.079182690475136e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:40:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 8 loss:0.00516847288236022 norm:3.7272311601554975e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:42:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 9 loss:0.0051186298951506615 norm:3.876288246829063e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:44:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 10 loss:0.005083003547042608 norm:3.6325403925729915e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:47:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 11 loss:0.005065491423010826 norm:3.598294279072434e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:49:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 12 loss:0.005061195231974125 norm:3.606658719945699e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:51:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 13 loss:0.0050572617910802364 norm:3.6680336052086204e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:53:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 14 loss:0.0050556231290102005 norm:3.974094215664081e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:55:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 15 loss:0.005051829386502504 norm:3.766014924622141e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:57:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 16 loss:0.005056392401456833 norm:3.928816659026779e-05 max memory_allocated 62760.2998046875 
[2025-03-23 05:00:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 17 loss:0.005048351362347603 norm:3.764493885682896e-05 max memory_allocated 62760.2998046875 
[2025-03-23 05:02:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 18 loss:0.005043442361056805 norm:3.528215893311426e-05 max memory_allocated 62760.2998046875 
[2025-03-23 05:04:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 19 loss:0.005040272139012814 norm:3.930788443540223e-05 max memory_allocated 62760.2998046875 
[2025-03-23 05:07:12 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [9, 10, 11]
[2025-03-23 05:07:12 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [12, 13, 14] ===
[2025-03-23 05:09:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 0 loss:0.007887573912739754 norm:9.190601849695668e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:11:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 1 loss:0.007341998629271984 norm:5.811127630295232e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:13:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 2 loss:0.007047790102660656 norm:4.1630100895417854e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:16:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 3 loss:0.006800592411309481 norm:3.232764356653206e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:18:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 4 loss:0.006537477020174265 norm:2.7985075575998053e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:20:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 5 loss:0.006273300386965275 norm:2.5184966943925247e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:22:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 6 loss:0.006042972672730684 norm:2.4081648007268086e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:24:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 7 loss:0.005874721799045801 norm:2.277096064062789e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:26:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 8 loss:0.005764978937804699 norm:2.225252683274448e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:28:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 9 loss:0.00569860590621829 norm:2.2359650756698102e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:31:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 10 loss:0.0056578489020466805 norm:2.262210000480991e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:33:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 11 loss:0.0056349728256464005 norm:2.2274522052612156e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:35:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 12 loss:0.005624220706522465 norm:2.240632602479309e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:37:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 13 loss:0.005618035793304443 norm:2.2087009710958228e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:39:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 14 loss:0.005615667439997196 norm:2.2265669031185098e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:41:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 15 loss:0.0056113749742507935 norm:2.2317915863823146e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:44:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 16 loss:0.005609325133264065 norm:2.2245938453124836e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:46:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 17 loss:0.005606497637927532 norm:2.2564068785868585e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:48:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 18 loss:0.005607409868389368 norm:2.30888654186856e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:50:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 19 loss:0.005605411250144243 norm:2.2801999875809997e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:53:30 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [12, 13, 14]
[2025-03-23 05:53:31 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [15, 16, 17] ===
[2025-03-23 05:55:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 0 loss:0.008111763745546341 norm:6.6858105128631e-05 max memory_allocated 62763.0029296875 
[2025-03-23 05:57:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 1 loss:0.007704088930040598 norm:4.292586163501255e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:00:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 2 loss:0.007458468433469534 norm:3.198783451807685e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:02:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 3 loss:0.007236616685986519 norm:2.7428570319898427e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:04:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 4 loss:0.006992696318775415 norm:2.484718061168678e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:06:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 5 loss:0.0067389728501439095 norm:2.293196666869335e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:08:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 6 loss:0.0065114726312458515 norm:2.3021199012873694e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:10:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 7 loss:0.006345234811306 norm:2.287426104885526e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:13:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 8 loss:0.006235033273696899 norm:2.2121059373603202e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:15:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 9 loss:0.006169563625007868 norm:2.1741561795352027e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:17:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 10 loss:0.006127361673861742 norm:2.1393625502241775e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:19:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 11 loss:0.006102368701249361 norm:2.1627631213050336e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:21:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 12 loss:0.006088689435273409 norm:2.1896772523177788e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:23:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 13 loss:0.006077220197767019 norm:2.118147494911682e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:26:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 14 loss:0.0060728476382792 norm:2.128400774381589e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:28:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 15 loss:0.006066952832043171 norm:2.1149626263650134e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:30:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 16 loss:0.006064298562705517 norm:2.1219761038082652e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:32:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 17 loss:0.0060621448792517185 norm:2.1378975361585617e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:34:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 18 loss:0.006060222629457712 norm:2.1131909306859598e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:36:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 19 loss:0.0060571348294615746 norm:2.1062478481326252e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:40:12 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [15, 16, 17]
[2025-03-23 06:40:12 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [18, 19, 20] ===
[2025-03-23 06:42:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 0 loss:0.009339121170341969 norm:5.44772447028663e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:44:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 1 loss:0.008983943611383438 norm:3.6881716368952766e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:46:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 2 loss:0.008751542307436466 norm:2.8050462788087316e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:48:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 3 loss:0.00850633718073368 norm:2.3282167603611015e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:51:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 4 loss:0.008211581036448479 norm:2.0028490325785242e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:53:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 5 loss:0.007878834381699562 norm:1.9020446416107006e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:55:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 6 loss:0.007593801710754633 norm:2.0574610971380025e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:57:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 7 loss:0.007410737685859203 norm:1.9811013771686703e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:59:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 8 loss:0.007315861061215401 norm:1.954448271135334e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:01:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 9 loss:0.007271561771631241 norm:1.9059780242969282e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:04:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 10 loss:0.0072489045560359955 norm:1.8708164134295657e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:06:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 11 loss:0.007238961756229401 norm:1.9004377463716082e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:08:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 12 loss:0.007235806901007891 norm:1.8725833797361702e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:10:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 13 loss:0.00723084295168519 norm:1.836824776546564e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:12:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 14 loss:0.007227099500596523 norm:1.8765869754133746e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:14:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 15 loss:0.0072247060015797615 norm:1.868647632363718e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:17:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 16 loss:0.0072218445129692554 norm:1.847992643888574e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:19:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 17 loss:0.007220400497317314 norm:1.872951543191448e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:21:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 18 loss:0.007216668222099543 norm:1.849868567660451e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:23:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 19 loss:0.007215749006718397 norm:1.904824239318259e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:26:39 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [18, 19, 20]
[2025-03-23 07:26:39 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [21, 22, 23] ===
[2025-03-23 07:28:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 0 loss:0.011972280219197273 norm:6.419698183890432e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:31:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 1 loss:0.011650467291474342 norm:5.3772706451127306e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:33:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 2 loss:0.011400047689676285 norm:4.5530909119406715e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:35:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 3 loss:0.011074068024754524 norm:3.573370122467168e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:37:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 4 loss:0.0106191486120224 norm:3.6585264751920477e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:39:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 5 loss:0.010163298808038235 norm:4.774024273501709e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:41:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 6 loss:0.009855994023382664 norm:3.436322367633693e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:44:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 7 loss:0.009718944318592548 norm:3.151000419165939e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:46:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 8 loss:0.009668596088886261 norm:2.9574344807770103e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:48:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 9 loss:0.009646289050579071 norm:2.7647694878396578e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:50:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 10 loss:0.009638408198952675 norm:3.919822847819887e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:52:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 11 loss:0.009623452089726925 norm:2.7924123060074635e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:54:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 12 loss:0.009615194983780384 norm:2.836353814927861e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:57:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 13 loss:0.009617832489311695 norm:4.4607368181459606e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:59:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 14 loss:0.009608323685824871 norm:2.707287421799265e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:01:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 15 loss:0.009606043808162212 norm:2.3394892195938155e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:03:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 16 loss:0.009603378362953663 norm:2.3910477466415614e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:05:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 17 loss:0.009601525962352753 norm:2.26422052946873e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:07:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 18 loss:0.009599083103239536 norm:2.4255417883978225e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:10:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 19 loss:0.009596390649676323 norm:2.410769411653746e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:12:59 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [21, 22, 23]
[2025-03-23 08:12:59 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [24, 25, 26] ===
[2025-03-23 08:15:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 0 loss:0.015858057886362076 norm:3.3778931538108736e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:17:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 1 loss:0.0155702019110322 norm:2.5118180928984657e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:19:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 2 loss:0.015271913260221481 norm:2.128564301528968e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:21:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 3 loss:0.014797746203839779 norm:2.0698580556199886e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:23:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 4 loss:0.014085564762353897 norm:2.0295412468840368e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:26:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 5 loss:0.01351876650005579 norm:2.00402682821732e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:28:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 6 loss:0.013315603137016296 norm:2.0375253370730206e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:30:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 7 loss:0.013265782035887241 norm:2.0121768102399074e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:32:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 8 loss:0.013253039680421352 norm:2.023457818722818e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:34:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 9 loss:0.013245442882180214 norm:2.025721551035531e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:36:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 10 loss:0.013238932006061077 norm:1.9776653061853722e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:39:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 11 loss:0.01323496364057064 norm:1.988168696698267e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:41:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 12 loss:0.01323165837675333 norm:2.0898009097436443e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:43:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 13 loss:0.013228817842900753 norm:2.089553527184762e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:45:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 14 loss:0.013227973133325577 norm:2.065861554001458e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:47:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 15 loss:0.013225598260760307 norm:2.0725887225125916e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:49:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 16 loss:0.013222454115748405 norm:2.117136500601191e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:52:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 17 loss:0.013219330459833145 norm:2.18550212593982e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:54:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 18 loss:0.013216669671237469 norm:2.1075136828585528e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:56:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 19 loss:0.013215357437729836 norm:2.0287168808863498e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:59:19 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [24, 25, 26]
[2025-03-23 08:59:20 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [27, 28, 29] ===
[2025-03-23 09:01:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 0 loss:0.021401816979050636 norm:4.117473872611299e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:03:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 1 loss:0.021055961027741432 norm:3.0228191462811083e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:05:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 2 loss:0.020631063729524612 norm:2.7792812034022063e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:08:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 3 loss:0.019828904420137405 norm:2.6753930796985514e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:10:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 4 loss:0.018809476867318153 norm:2.4958311769296415e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:12:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 5 loss:0.018377477303147316 norm:2.5243989512091503e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:14:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 6 loss:0.01830778270959854 norm:2.366245462326333e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:16:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 7 loss:0.01829524338245392 norm:2.312539618287701e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:18:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 8 loss:0.018286772072315216 norm:2.3768057872075588e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:21:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 9 loss:0.018281469121575356 norm:2.382479033258278e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:23:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 10 loss:0.01827823743224144 norm:2.351961848034989e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:25:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 11 loss:0.018275028094649315 norm:2.3882503228378482e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:27:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 12 loss:0.018271436914801598 norm:2.3507796868216246e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:29:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 13 loss:0.01826801337301731 norm:2.284782203787472e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:31:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 14 loss:0.018263256177306175 norm:2.309685805812478e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:34:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 15 loss:0.01825915463268757 norm:2.3101918486645445e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:36:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 16 loss:0.018258338794112206 norm:2.301033600815572e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:38:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 17 loss:0.018254093825817108 norm:2.2533346054842696e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:40:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 18 loss:0.018252043053507805 norm:2.2096353859524243e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:42:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 19 loss:0.018250782042741776 norm:2.250728357466869e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:45:41 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [27, 28, 29]
[2025-03-23 09:45:41 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [30, 31, 32] ===
[2025-03-23 09:48:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 0 loss:0.029241790995001793 norm:5.8142992202192545e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:50:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 1 loss:0.028687477111816406 norm:4.6473636757582426e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:52:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 2 loss:0.027939509600400925 norm:3.6661986086983234e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:54:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 3 loss:0.026579130440950394 norm:2.9921615350758657e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:56:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 4 loss:0.025500303134322166 norm:2.8052387278876267e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:58:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 5 loss:0.02531140111386776 norm:2.9103506676619872e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:00:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 6 loss:0.025287771597504616 norm:2.8365211619529873e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:03:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 7 loss:0.025276528671383858 norm:2.9038097636657767e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:05:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 8 loss:0.02526862360537052 norm:2.924381260527298e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:07:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 9 loss:0.02526051364839077 norm:2.8284595828154124e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:09:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 10 loss:0.025251243263483047 norm:2.7414953365223482e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:11:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 11 loss:0.02524768002331257 norm:2.8913418645970523e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:13:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 12 loss:0.025241639465093613 norm:2.8364715035422705e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:16:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 13 loss:0.025236759334802628 norm:2.7920041247853078e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:18:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 14 loss:0.025232089683413506 norm:2.6548303139861673e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:20:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 15 loss:0.025228550657629967 norm:2.7534015316632576e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:22:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 16 loss:0.02522421069443226 norm:2.720197880989872e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:24:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 17 loss:0.025219950824975967 norm:2.7207053790334612e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:26:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 18 loss:0.02521633915603161 norm:2.6212605007458478e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:29:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 19 loss:0.02521343156695366 norm:2.632363248267211e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:31:58 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [30, 31, 32]
[2025-03-23 10:31:58 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [33, 34, 35] ===
[2025-03-23 10:34:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 0 loss:0.04005405679345131 norm:6.0058697272324935e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:36:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 1 loss:0.03918211907148361 norm:3.9592570828972384e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:38:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 2 loss:0.037814900279045105 norm:3.0946754122851416e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:40:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 3 loss:0.03586454316973686 norm:2.96526250167517e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:42:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 4 loss:0.03509056940674782 norm:2.8287890017963946e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:45:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 5 loss:0.03503744676709175 norm:2.7715679607354105e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:47:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 6 loss:0.035022519528865814 norm:2.7495661925058812e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:49:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 7 loss:0.03501022234559059 norm:2.7581507310969755e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:51:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 8 loss:0.03499582037329674 norm:2.7444555598776788e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:53:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 9 loss:0.034985557198524475 norm:2.7639249310595915e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:55:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 10 loss:0.03497433662414551 norm:2.747738290054258e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:58:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 11 loss:0.03496662154793739 norm:2.6994917789124884e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:00:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 12 loss:0.0349600613117218 norm:2.6947194783133455e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:02:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 13 loss:0.03495705872774124 norm:2.7275176762486808e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:04:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 14 loss:0.034951694309711456 norm:2.765845238172915e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:06:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 15 loss:0.03494768589735031 norm:2.7116040655528195e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:08:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 16 loss:0.034943751990795135 norm:2.6638630515662953e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:11:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 17 loss:0.03493611142039299 norm:2.695196235436015e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:13:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 18 loss:0.03492739051580429 norm:2.675461655599065e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:15:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 19 loss:0.03492093086242676 norm:2.6532130505074747e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:18:16 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [33, 34, 35]
[2025-03-23 11:18:16 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [36] ===
[2025-03-23 11:18:17 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 11:19:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 0 loss:0.03864593058824539 norm:0.0011485142167657614 max memory_allocated 62763.0029296875 
[2025-03-23 11:19:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 1 loss:0.0381881520152092 norm:0.0008471137844026089 max memory_allocated 62763.0029296875 
[2025-03-23 11:20:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 2 loss:0.03754296153783798 norm:0.0007294559036381543 max memory_allocated 62763.0029296875 
[2025-03-23 11:21:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 3 loss:0.03681172803044319 norm:0.000625927874352783 max memory_allocated 62763.0029296875 
[2025-03-23 11:21:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 4 loss:0.03663284331560135 norm:0.0005443347617983818 max memory_allocated 62763.0029296875 
[2025-03-23 11:22:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 5 loss:0.0366174578666687 norm:0.0004832665144931525 max memory_allocated 62763.0029296875 
[2025-03-23 11:23:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 6 loss:0.036602724343538284 norm:0.0004309411742724478 max memory_allocated 62763.0029296875 
[2025-03-23 11:24:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 7 loss:0.036597780883312225 norm:0.0003662168746814132 max memory_allocated 62763.0029296875 
[2025-03-23 11:24:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 8 loss:0.03660321235656738 norm:0.0003539373283274472 max memory_allocated 62763.0029296875 
[2025-03-23 11:25:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 9 loss:0.0366356261074543 norm:0.0003904696786776185 max memory_allocated 62763.0029296875 
[2025-03-23 11:26:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 10 loss:0.03662463277578354 norm:0.0003677599597722292 max memory_allocated 62763.0029296875 
[2025-03-23 11:27:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 11 loss:0.03658086434006691 norm:0.00022879285097587854 max memory_allocated 62763.0029296875 
[2025-03-23 11:27:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 12 loss:0.03655174747109413 norm:0.00022025691578164697 max memory_allocated 62763.0029296875 
[2025-03-23 11:28:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 13 loss:0.03654871881008148 norm:0.00020686950301751494 max memory_allocated 62763.0029296875 
[2025-03-23 11:29:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 14 loss:0.03655267879366875 norm:0.00020651494560297579 max memory_allocated 62763.0029296875 
[2025-03-23 11:29:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 15 loss:0.03656957298517227 norm:0.0002638906007632613 max memory_allocated 62763.0029296875 
[2025-03-23 11:30:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 16 loss:0.036649346351623535 norm:0.00042442072299309075 max memory_allocated 62763.0029296875 
[2025-03-23 11:31:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 17 loss:0.036817558109760284 norm:0.0006730688619427383 max memory_allocated 62763.0029296875 
[2025-03-23 11:32:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 18 loss:0.03720647096633911 norm:0.0012769214808940887 max memory_allocated 62763.0029296875 
[2025-03-23 11:32:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 19 loss:0.03757975250482559 norm:0.0016399106243625283 max memory_allocated 62763.0029296875 
[2025-03-23 11:33:51 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [36]
[2025-03-23 11:33:51 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [37] ===
[2025-03-23 11:33:51 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 11:34:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 0 loss:0.0440426729619503 norm:0.0013503897935152054 max memory_allocated 62763.0029296875 
[2025-03-23 11:35:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 1 loss:0.043309640139341354 norm:0.0009542417246848345 max memory_allocated 62763.0029296875 
[2025-03-23 11:36:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 2 loss:0.042402561753988266 norm:0.0007861279300414026 max memory_allocated 62763.0029296875 
[2025-03-23 11:36:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 3 loss:0.04163847863674164 norm:0.0006682550301775336 max memory_allocated 62763.0029296875 
[2025-03-23 11:37:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 4 loss:0.04148440435528755 norm:0.0006033406243659556 max memory_allocated 62763.0029296875 
[2025-03-23 11:38:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 5 loss:0.04147535189986229 norm:0.0005603345925919712 max memory_allocated 62763.0029296875 
[2025-03-23 11:39:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 6 loss:0.04145099222660065 norm:0.0004422749625518918 max memory_allocated 62763.0029296875 
[2025-03-23 11:39:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 7 loss:0.041439708322286606 norm:0.0004315344849601388 max memory_allocated 62763.0029296875 
[2025-03-23 11:40:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 8 loss:0.04142531007528305 norm:0.00037115070153959095 max memory_allocated 62763.0029296875 
[2025-03-23 11:41:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 9 loss:0.04141279309988022 norm:0.0003513178089633584 max memory_allocated 62763.0029296875 
[2025-03-23 11:41:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 10 loss:0.041413746774196625 norm:0.00036640511825680733 max memory_allocated 62763.0029296875 
[2025-03-23 11:42:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 11 loss:0.041414521634578705 norm:0.00038128718733787537 max memory_allocated 62763.0029296875 
[2025-03-23 11:43:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 12 loss:0.04139072075486183 norm:0.00030199115280993283 max memory_allocated 62763.0029296875 
[2025-03-23 11:44:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 13 loss:0.04139989987015724 norm:0.00028881931211799383 max memory_allocated 62763.0029296875 
[2025-03-23 11:44:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 14 loss:0.041447024792432785 norm:0.0004369701782707125 max memory_allocated 62763.0029296875 
[2025-03-23 11:45:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 15 loss:0.04154520854353905 norm:0.0006543378112837672 max memory_allocated 62763.0029296875 
[2025-03-23 11:46:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 16 loss:0.04143223911523819 norm:0.00041744153713807464 max memory_allocated 62763.0029296875 
[2025-03-23 11:47:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 17 loss:0.04134948551654816 norm:0.0002375917974859476 max memory_allocated 62763.0029296875 
[2025-03-23 11:47:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 18 loss:0.041324056684970856 norm:0.00023043507826514542 max memory_allocated 62763.0029296875 
[2025-03-23 11:48:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 19 loss:0.04131610319018364 norm:0.00022157093917485327 max memory_allocated 62763.0029296875 
[2025-03-23 11:49:26 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 15, block: [37]
[2025-03-23 11:49:26 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 16 with layers [38] ===
[2025-03-23 11:49:26 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 11:50:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 0 loss:0.05293463170528412 norm:0.001820066012442112 max memory_allocated 62763.0029296875 
[2025-03-23 11:50:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 1 loss:0.05062023550271988 norm:0.0012987158261239529 max memory_allocated 62763.0029296875 
[2025-03-23 11:51:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 2 loss:0.0492112822830677 norm:0.0010037970496341586 max memory_allocated 62763.0029296875 
[2025-03-23 11:52:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 3 loss:0.048394575715065 norm:0.0008463386911898851 max memory_allocated 62763.0029296875 
[2025-03-23 11:53:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 4 loss:0.04822801426053047 norm:0.0008017969084903598 max memory_allocated 62763.0029296875 
[2025-03-23 11:53:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 5 loss:0.04818522557616234 norm:0.0007019786862656474 max memory_allocated 62763.0029296875 
[2025-03-23 11:54:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 6 loss:0.04815538227558136 norm:0.0006492005777545273 max memory_allocated 62763.0029296875 
[2025-03-23 11:55:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 7 loss:0.04814664646983147 norm:0.0006272693281061947 max memory_allocated 62763.0029296875 
[2025-03-23 11:56:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 8 loss:0.048143379390239716 norm:0.0005822674138471484 max memory_allocated 62763.0029296875 
[2025-03-23 11:56:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 9 loss:0.04812271520495415 norm:0.0006171524291858077 max memory_allocated 62763.0029296875 
[2025-03-23 11:57:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 10 loss:0.048100292682647705 norm:0.0005294795264489949 max memory_allocated 62763.0029296875 
[2025-03-23 11:58:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 11 loss:0.04808472841978073 norm:0.00048571161460131407 max memory_allocated 62763.0029296875 
[2025-03-23 11:58:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 12 loss:0.04807385429739952 norm:0.0004831971600651741 max memory_allocated 62763.0029296875 
[2025-03-23 11:59:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 13 loss:0.048073187470436096 norm:0.00046022635069675744 max memory_allocated 62763.0029296875 
[2025-03-23 12:00:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 14 loss:0.048068199306726456 norm:0.00046881387243047357 max memory_allocated 62763.0029296875 
[2025-03-23 12:01:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 15 loss:0.048092667013406754 norm:0.0006049309158697724 max memory_allocated 62763.0029296875 
[2025-03-23 12:01:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 16 loss:0.048117924481630325 norm:0.0006143643404357135 max memory_allocated 62763.0029296875 
[2025-03-23 12:02:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 17 loss:0.048090606927871704 norm:0.0005562449805438519 max memory_allocated 62763.0029296875 
[2025-03-23 12:03:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 18 loss:0.048114579170942307 norm:0.0006038111750967801 max memory_allocated 62763.0029296875 
[2025-03-23 12:04:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 19 loss:0.04806026816368103 norm:0.00046181719517335296 max memory_allocated 62763.0029296875 
[2025-03-23 12:05:00 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 16, block: [38]
[2025-03-23 12:05:00 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 17 with layers [39] ===
[2025-03-23 12:05:01 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 12:05:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 0 loss:0.06593044102191925 norm:0.003981234971433878 max memory_allocated 62763.0029296875 
[2025-03-23 12:06:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 1 loss:0.0641043558716774 norm:0.0032376290764659643 max memory_allocated 62763.0029296875 
[2025-03-23 12:07:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 2 loss:0.0628117099404335 norm:0.0028021596372127533 max memory_allocated 62763.0029296875 
[2025-03-23 12:07:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 3 loss:0.06203272566199303 norm:0.0026629066560417414 max memory_allocated 62763.0029296875 
[2025-03-23 12:08:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 4 loss:0.06177731603384018 norm:0.0024941761512309313 max memory_allocated 62763.0029296875 
[2025-03-23 12:09:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 5 loss:0.061677511781454086 norm:0.002430395223200321 max memory_allocated 62763.0029296875 
[2025-03-23 12:10:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 6 loss:0.061651092022657394 norm:0.002332936739549041 max memory_allocated 62763.0029296875 
[2025-03-23 12:10:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 7 loss:0.061639945954084396 norm:0.0023096667136996984 max memory_allocated 62763.0029296875 
[2025-03-23 12:11:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 8 loss:0.06161302328109741 norm:0.002258470980450511 max memory_allocated 62763.0029296875 
[2025-03-23 12:12:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 9 loss:0.06166933476924896 norm:0.0023297499865293503 max memory_allocated 62763.0029296875 
[2025-03-23 12:13:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 10 loss:0.06188354641199112 norm:0.0028225460555404425 max memory_allocated 62763.0029296875 
[2025-03-23 12:13:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 11 loss:0.06180077791213989 norm:0.00257278885692358 max memory_allocated 62763.0029296875 
[2025-03-23 12:14:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 12 loss:0.06151823326945305 norm:0.0021477846894413233 max memory_allocated 62763.0029296875 
[2025-03-23 12:15:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 13 loss:0.061458516865968704 norm:0.002092628739774227 max memory_allocated 62763.0029296875 
[2025-03-23 12:16:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 14 loss:0.06145011633634567 norm:0.0020624175667762756 max memory_allocated 62763.0029296875 
[2025-03-23 12:16:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 15 loss:0.061429522931575775 norm:0.0020328708924353123 max memory_allocated 62763.0029296875 
[2025-03-23 12:17:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 16 loss:0.06141708791255951 norm:0.0019585806876420975 max memory_allocated 62763.0029296875 
[2025-03-23 12:18:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 17 loss:0.06141170486807823 norm:0.0019532230217009783 max memory_allocated 62763.0029296875 
[2025-03-23 12:18:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 18 loss:0.061525195837020874 norm:0.002099362900480628 max memory_allocated 62763.0029296875 
[2025-03-23 12:19:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 19 loss:0.06180638447403908 norm:0.002668826375156641 max memory_allocated 62763.0029296875 
[2025-03-23 12:20:39 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 17, block: [39]
[2025-03-23 12:20:40 root] (main_calib_config3_attn.py 379): INFO 37148.451874017715
[2025-03-23 12:20:57 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-23 12:22:23 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 4.905276298522949
[2025-03-23 12:22:23 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-23 12:24:36 root] (main_calib_config3_attn.py 161): INFO c4 : 6.4932637214660645
