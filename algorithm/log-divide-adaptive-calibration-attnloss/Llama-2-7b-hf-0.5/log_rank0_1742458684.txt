[2025-03-20 08:18:04 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/Llama-2-7b-hf-0.5', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.5.pkl', blocks_pkl='./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-20 08:21:39 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-20 08:21:40 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-20 08:21:40 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-20 08:21:40 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.5.pkl
[2025-03-20 08:21:40 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 9), (9, 11), (11, 14), (14, 17), (17, 20), (20, 23), (23, 26), (26, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-20 08:21:40 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12, 13], [14, 15, 16], [17, 18, 19], [20, 21, 22], [23, 24, 25], [26, 27, 28], [29], [30], [31]]
[2025-03-20 08:21:41 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-20 08:21:42 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 08:22:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.008308044634759426 norm:0.01283178385347128 max memory_allocated 34633.880859375 
[2025-03-20 08:22:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.004624516237527132 norm:0.007592895999550819 max memory_allocated 34633.880859375 
[2025-03-20 08:23:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.0031226298306137323 norm:0.005292136687785387 max memory_allocated 34633.880859375 
[2025-03-20 08:23:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.00257560214959085 norm:0.004187874495983124 max memory_allocated 34633.880859375 
[2025-03-20 08:23:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.002414178801700473 norm:0.003462668973952532 max memory_allocated 34633.880859375 
[2025-03-20 08:24:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0023077516816556454 norm:0.0029456315096467733 max memory_allocated 34633.880859375 
[2025-03-20 08:24:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.002263115020468831 norm:0.002637614496052265 max memory_allocated 34633.880859375 
[2025-03-20 08:25:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0022089642006903887 norm:0.0023967106826603413 max memory_allocated 34633.880859375 
[2025-03-20 08:25:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0021747350692749023 norm:0.002067222958430648 max memory_allocated 34633.880859375 
[2025-03-20 08:26:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.00211614859290421 norm:0.001800770522095263 max memory_allocated 34633.880859375 
[2025-03-20 08:26:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.0020775131415575743 norm:0.001601334079168737 max memory_allocated 34633.880859375 
[2025-03-20 08:27:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0020921663381159306 norm:0.0014141459250822663 max memory_allocated 34633.880859375 
[2025-03-20 08:27:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.0020837881602346897 norm:0.0012948352377861738 max memory_allocated 34633.880859375 
[2025-03-20 08:28:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.002077151322737336 norm:0.0011660916497930884 max memory_allocated 34633.880859375 
[2025-03-20 08:28:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0020744078792631626 norm:0.0011700369650498033 max memory_allocated 34633.880859375 
[2025-03-20 08:28:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.0020017423667013645 norm:0.0010277517139911652 max memory_allocated 34633.880859375 
[2025-03-20 08:29:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.0019806965719908476 norm:0.0009244356770068407 max memory_allocated 34633.880859375 
[2025-03-20 08:29:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0019748061895370483 norm:0.0008857278735376894 max memory_allocated 34633.880859375 
[2025-03-20 08:30:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.0019506942480802536 norm:0.0008941451669670641 max memory_allocated 34633.880859375 
[2025-03-20 08:30:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0019514854066073895 norm:0.000811669509857893 max memory_allocated 34633.880859375 
[2025-03-20 08:31:16 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-20 08:31:16 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-20 08:31:16 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 08:31:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.02811942994594574 norm:0.024772651493549347 max memory_allocated 35100.7724609375 
[2025-03-20 08:32:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.017022989690303802 norm:0.014217107556760311 max memory_allocated 35100.7724609375 
[2025-03-20 08:32:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.01269824244081974 norm:0.009550893679261208 max memory_allocated 35100.7724609375 
[2025-03-20 08:33:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.011336817406117916 norm:0.012880713678896427 max memory_allocated 35100.7724609375 
[2025-03-20 08:33:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.010511713102459908 norm:0.009888132102787495 max memory_allocated 35100.7724609375 
[2025-03-20 08:34:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.010044561699032784 norm:0.008013379760086536 max memory_allocated 35100.7724609375 
[2025-03-20 08:34:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.010298424400389194 norm:0.008421109989285469 max memory_allocated 35100.7724609375 
[2025-03-20 08:34:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.010494190268218517 norm:0.008478648960590363 max memory_allocated 35100.7724609375 
[2025-03-20 08:35:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.009436463937163353 norm:0.008250758051872253 max memory_allocated 35100.7724609375 
[2025-03-20 08:35:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.009163402020931244 norm:0.007720480673015118 max memory_allocated 35100.7724609375 
[2025-03-20 08:36:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.009019150398671627 norm:0.0072352527640759945 max memory_allocated 35100.7724609375 
[2025-03-20 08:36:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.00953727401793003 norm:0.008603397756814957 max memory_allocated 35100.7724609375 
[2025-03-20 08:37:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.009239481762051582 norm:0.00796583667397499 max memory_allocated 35100.7724609375 
[2025-03-20 08:37:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.009185954928398132 norm:0.008115066215395927 max memory_allocated 35100.7724609375 
[2025-03-20 08:38:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.009045885875821114 norm:0.007353545632213354 max memory_allocated 35100.7724609375 
[2025-03-20 08:38:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.00987364910542965 norm:0.008376100100576878 max memory_allocated 35100.7724609375 
[2025-03-20 08:38:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.009029907174408436 norm:0.007417944725602865 max memory_allocated 35100.7724609375 
[2025-03-20 08:39:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.009116151370108128 norm:0.007183887530118227 max memory_allocated 35100.7724609375 
[2025-03-20 08:39:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.009248221293091774 norm:0.007138457149267197 max memory_allocated 35100.7724609375 
[2025-03-20 08:40:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.009264810010790825 norm:0.007380855269730091 max memory_allocated 35100.7724609375 
[2025-03-20 08:40:49 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-20 08:40:49 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
[2025-03-20 08:40:50 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 08:41:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.021257495507597923 norm:0.0056730289943516254 max memory_allocated 35101.8349609375 
[2025-03-20 08:41:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.014939647167921066 norm:0.0045272777788341045 max memory_allocated 35101.8349609375 
[2025-03-20 08:42:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.012531287968158722 norm:0.0034006773494184017 max memory_allocated 35101.8349609375 
[2025-03-20 08:42:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.011718031018972397 norm:0.002857412211596966 max memory_allocated 35101.8349609375 
[2025-03-20 08:43:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.011234083212912083 norm:0.002435071859508753 max memory_allocated 35101.8349609375 
[2025-03-20 08:43:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.010858512483537197 norm:0.002029543509706855 max memory_allocated 35101.8349609375 
[2025-03-20 08:44:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.010637018829584122 norm:0.001728689530864358 max memory_allocated 35101.8349609375 
[2025-03-20 08:44:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.010528970509767532 norm:0.0014842303935438395 max memory_allocated 35101.8349609375 
[2025-03-20 08:44:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.010478685609996319 norm:0.0012452627997845411 max memory_allocated 35101.8349609375 
[2025-03-20 08:45:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.010425320826470852 norm:0.0011034220224246383 max memory_allocated 35101.8349609375 
[2025-03-20 08:45:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.010466271080076694 norm:0.0010983151150867343 max memory_allocated 35101.8349609375 
[2025-03-20 08:46:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.010426362045109272 norm:0.00104267499409616 max memory_allocated 35101.8349609375 
[2025-03-20 08:46:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.010388918220996857 norm:0.000946508371271193 max memory_allocated 35101.8349609375 
[2025-03-20 08:47:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.010359019041061401 norm:0.0009348602034151554 max memory_allocated 35101.8349609375 
[2025-03-20 08:47:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.010360192507505417 norm:0.0009063107427209616 max memory_allocated 35101.8349609375 
[2025-03-20 08:48:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.010377203114330769 norm:0.0008865046547725797 max memory_allocated 35101.8349609375 
[2025-03-20 08:48:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.010370486415922642 norm:0.0008647935464978218 max memory_allocated 35101.8349609375 
[2025-03-20 08:48:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.010363232344388962 norm:0.0008243594784289598 max memory_allocated 35101.8349609375 
[2025-03-20 08:49:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.010359196923673153 norm:0.0007902201032266021 max memory_allocated 35101.8349609375 
[2025-03-20 08:49:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.010307961143553257 norm:0.0007812827243469656 max memory_allocated 35101.8349609375 
[2025-03-20 08:50:29 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2]
[2025-03-20 08:50:29 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4, 5] ===
[2025-03-20 08:51:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 0 loss:0.0663924440741539 norm:0.004338269587606192 max memory_allocated 47477.6044921875 
[2025-03-20 08:53:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 1 loss:0.04628950357437134 norm:0.001222961931489408 max memory_allocated 47477.6044921875 
[2025-03-20 08:54:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 2 loss:0.03572509065270424 norm:0.0006474219844676554 max memory_allocated 47477.6044921875 
[2025-03-20 08:55:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 3 loss:0.031010132282972336 norm:0.0004506556724663824 max memory_allocated 47477.6044921875 
[2025-03-20 08:57:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 4 loss:0.028726957738399506 norm:0.0003810117195826024 max memory_allocated 47477.6044921875 
[2025-03-20 08:58:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 5 loss:0.027284298092126846 norm:0.00037123271613381803 max memory_allocated 47477.6044921875 
[2025-03-20 08:59:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 6 loss:0.026314718648791313 norm:0.00036537612322717905 max memory_allocated 47477.6044921875 
[2025-03-20 09:01:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 7 loss:0.025714870542287827 norm:0.00036110024666413665 max memory_allocated 47477.6044921875 
[2025-03-20 09:02:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 8 loss:0.025360742583870888 norm:0.00044182458077557385 max memory_allocated 47477.6044921875 
[2025-03-20 09:03:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 9 loss:0.025151295587420464 norm:0.0004410078108776361 max memory_allocated 47477.6044921875 
[2025-03-20 09:05:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 10 loss:0.02510172128677368 norm:0.00040543609065935016 max memory_allocated 47477.6044921875 
[2025-03-20 09:06:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 11 loss:0.024967484176158905 norm:0.00040899834129959345 max memory_allocated 47477.6044921875 
[2025-03-20 09:07:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 12 loss:0.024850981310009956 norm:0.00039419406675733626 max memory_allocated 47477.6044921875 
[2025-03-20 09:09:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 13 loss:0.02480599656701088 norm:0.00041873822920024395 max memory_allocated 47477.6044921875 
[2025-03-20 09:10:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 14 loss:0.02478213980793953 norm:0.0003837640688288957 max memory_allocated 47477.6044921875 
[2025-03-20 09:11:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 15 loss:0.02473582699894905 norm:0.0003828711051028222 max memory_allocated 47477.6044921875 
[2025-03-20 09:13:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 16 loss:0.02472909539937973 norm:0.00035372882848605514 max memory_allocated 47477.6044921875 
[2025-03-20 09:14:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 17 loss:0.024704594165086746 norm:0.00039047712925821543 max memory_allocated 47477.6044921875 
[2025-03-20 09:15:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 18 loss:0.024672217667102814 norm:0.00040626066038385034 max memory_allocated 47477.6044921875 
[2025-03-20 09:17:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 19 loss:0.02468562312424183 norm:0.00042394609772600234 max memory_allocated 47477.6044921875 
[2025-03-20 09:18:55 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [3, 4, 5]
[2025-03-20 09:18:55 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [6, 7, 8] ===
[2025-03-20 09:20:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 0 loss:0.07869870960712433 norm:0.001739996368996799 max memory_allocated 47477.7919921875 
[2025-03-20 09:21:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 1 loss:0.05834269896149635 norm:0.0007057461189106107 max memory_allocated 47477.7919921875 
[2025-03-20 09:23:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 2 loss:0.04583799093961716 norm:0.00044141552643850446 max memory_allocated 47477.7919921875 
[2025-03-20 09:24:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 3 loss:0.04049033671617508 norm:0.00036498764529824257 max memory_allocated 47477.7919921875 
[2025-03-20 09:25:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 4 loss:0.0378284826874733 norm:0.00035167811438441277 max memory_allocated 47477.7919921875 
[2025-03-20 09:27:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 5 loss:0.036033716052770615 norm:0.00032266342896036804 max memory_allocated 47477.7919921875 
[2025-03-20 09:28:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 6 loss:0.03488459810614586 norm:0.00030823578708805144 max memory_allocated 47477.7919921875 
[2025-03-20 09:29:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 7 loss:0.03426329046487808 norm:0.00032120459945872426 max memory_allocated 47477.7919921875 
[2025-03-20 09:31:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 8 loss:0.03380869701504707 norm:0.0003147035022266209 max memory_allocated 47477.7919921875 
[2025-03-20 09:32:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 9 loss:0.03335435688495636 norm:0.00030554726254194975 max memory_allocated 47477.7919921875 
[2025-03-20 09:33:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 10 loss:0.03309785574674606 norm:0.00033312710002064705 max memory_allocated 47477.7919921875 
[2025-03-20 09:34:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 11 loss:0.03288263455033302 norm:0.0002942385617643595 max memory_allocated 47477.7919921875 
[2025-03-20 09:36:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 12 loss:0.03283952921628952 norm:0.00032250655931420624 max memory_allocated 47477.7919921875 
[2025-03-20 09:37:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 13 loss:0.03272026777267456 norm:0.000305987341562286 max memory_allocated 47477.7919921875 
[2025-03-20 09:38:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 14 loss:0.03261464089155197 norm:0.0003115748113486916 max memory_allocated 47477.7919921875 
[2025-03-20 09:40:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 15 loss:0.032572921365499496 norm:0.0003174013691022992 max memory_allocated 47477.7919921875 
[2025-03-20 09:41:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 16 loss:0.032459624111652374 norm:0.0003126738010905683 max memory_allocated 47477.7919921875 
[2025-03-20 09:42:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 17 loss:0.03235602006316185 norm:0.0003138972679153085 max memory_allocated 47477.7919921875 
[2025-03-20 09:44:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 18 loss:0.03235422447323799 norm:0.0003099496243521571 max memory_allocated 47477.7919921875 
[2025-03-20 09:45:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 19 loss:0.03233053535223007 norm:0.0003217291086912155 max memory_allocated 47477.7919921875 
[2025-03-20 09:47:18 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [6, 7, 8]
[2025-03-20 09:47:18 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [9, 10] ===
[2025-03-20 09:48:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 0 loss:0.0628996342420578 norm:0.0015849999617785215 max memory_allocated 47477.7919921875 
[2025-03-20 09:49:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 1 loss:0.049623459577560425 norm:0.0007393793202936649 max memory_allocated 47477.7919921875 
[2025-03-20 09:50:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 2 loss:0.040455061942338943 norm:0.0003565406077541411 max memory_allocated 47477.7919921875 
[2025-03-20 09:50:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 3 loss:0.03708595409989357 norm:0.0002567663905210793 max memory_allocated 47477.7919921875 
[2025-03-20 09:51:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 4 loss:0.035435739904642105 norm:0.00022100632486399263 max memory_allocated 47477.7919921875 
[2025-03-20 09:52:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 5 loss:0.03443874418735504 norm:0.00020582356955856085 max memory_allocated 47477.7919921875 
[2025-03-20 09:53:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 6 loss:0.03380509838461876 norm:0.0001887725666165352 max memory_allocated 47477.7919921875 
[2025-03-20 09:54:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 7 loss:0.03347323089838028 norm:0.00018359256500843912 max memory_allocated 47477.7919921875 
[2025-03-20 09:55:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 8 loss:0.03327352553606033 norm:0.0001856571325333789 max memory_allocated 47477.7919921875 
[2025-03-20 09:56:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 9 loss:0.033072300255298615 norm:0.00018204600200988352 max memory_allocated 47477.7919921875 
[2025-03-20 09:57:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 10 loss:0.03297228366136551 norm:0.00020174695237074047 max memory_allocated 47477.7919921875 
[2025-03-20 09:58:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 11 loss:0.032879892736673355 norm:0.0002028773888014257 max memory_allocated 47477.7919921875 
[2025-03-20 09:58:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 12 loss:0.03273925930261612 norm:0.00019813029211945832 max memory_allocated 47477.7919921875 
[2025-03-20 09:59:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 13 loss:0.03267807513475418 norm:0.00019791639351751655 max memory_allocated 47477.7919921875 
[2025-03-20 10:00:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 14 loss:0.032633569091558456 norm:0.00019656243966892362 max memory_allocated 47477.7919921875 
[2025-03-20 10:01:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 15 loss:0.03258102387189865 norm:0.00018218606419395655 max memory_allocated 47477.7919921875 
[2025-03-20 10:02:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 16 loss:0.03255608305335045 norm:0.00018617375462781638 max memory_allocated 47477.7919921875 
[2025-03-20 10:03:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 17 loss:0.03255540877580643 norm:0.0001893815933726728 max memory_allocated 47477.7919921875 
[2025-03-20 10:04:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 18 loss:0.032513733953237534 norm:0.00018301286036148667 max memory_allocated 47477.7919921875 
[2025-03-20 10:05:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 19 loss:0.032502103596925735 norm:0.00018477111007086933 max memory_allocated 47477.7919921875 
[2025-03-20 10:06:15 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [9, 10]
[2025-03-20 10:06:15 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [11, 12, 13] ===
[2025-03-20 10:07:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 0 loss:0.07884563505649567 norm:0.0016038008034229279 max memory_allocated 47478.1044921875 
[2025-03-20 10:09:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 1 loss:0.06208731606602669 norm:0.0007570921443402767 max memory_allocated 47478.1044921875 
[2025-03-20 10:10:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 2 loss:0.05011308938264847 norm:0.0004086508706677705 max memory_allocated 47478.1044921875 
[2025-03-20 10:11:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 3 loss:0.0449628084897995 norm:0.0002923661668319255 max memory_allocated 47478.1044921875 
[2025-03-20 10:13:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 4 loss:0.04237426817417145 norm:0.0002469221071805805 max memory_allocated 47478.1044921875 
[2025-03-20 10:14:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 5 loss:0.040825601667165756 norm:0.00022817573335487396 max memory_allocated 47478.1044921875 
[2025-03-20 10:15:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 6 loss:0.03984298184514046 norm:0.00021454137458931655 max memory_allocated 47478.1044921875 
[2025-03-20 10:17:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 7 loss:0.03922370821237564 norm:0.00021385647414717823 max memory_allocated 47478.1044921875 
[2025-03-20 10:18:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 8 loss:0.03881742060184479 norm:0.00021364698477555066 max memory_allocated 47478.1044921875 
[2025-03-20 10:19:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 9 loss:0.038503870368003845 norm:0.0002023312554229051 max memory_allocated 47478.1044921875 
[2025-03-20 10:21:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 10 loss:0.03830510377883911 norm:0.0002014694327954203 max memory_allocated 47478.1044921875 
[2025-03-20 10:22:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 11 loss:0.038117632269859314 norm:0.00020118869724683464 max memory_allocated 47478.1044921875 
[2025-03-20 10:23:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 12 loss:0.03799715265631676 norm:0.00019560943474061787 max memory_allocated 47478.1044921875 
[2025-03-20 10:24:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 13 loss:0.03788405656814575 norm:0.00018790348258335143 max memory_allocated 47478.1044921875 
[2025-03-20 10:26:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 14 loss:0.03781957924365997 norm:0.00019380342564545572 max memory_allocated 47478.1044921875 
[2025-03-20 10:27:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 15 loss:0.03773669898509979 norm:0.00018717221973929554 max memory_allocated 47478.1044921875 
[2025-03-20 10:28:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 16 loss:0.03767969086766243 norm:0.00018909158825408667 max memory_allocated 47478.1044921875 
[2025-03-20 10:30:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 17 loss:0.03761236369609833 norm:0.00019045898807235062 max memory_allocated 47478.1044921875 
[2025-03-20 10:31:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 18 loss:0.03756222128868103 norm:0.00019043633074034005 max memory_allocated 47478.1044921875 
[2025-03-20 10:32:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 19 loss:0.0375179797410965 norm:0.00019734087982214987 max memory_allocated 47478.1044921875 
[2025-03-20 10:34:37 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [11, 12, 13]
[2025-03-20 10:34:37 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [14, 15, 16] ===
[2025-03-20 10:36:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 0 loss:0.07635761797428131 norm:0.0009751148754730821 max memory_allocated 47478.2919921875 
[2025-03-20 10:37:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 1 loss:0.061133477836847305 norm:0.0004681287973653525 max memory_allocated 47478.2919921875 
[2025-03-20 10:38:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 2 loss:0.04985661804676056 norm:0.00031621663947589695 max memory_allocated 47478.2919921875 
[2025-03-20 10:40:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 3 loss:0.04540824890136719 norm:0.0002537019026931375 max memory_allocated 47478.2919921875 
[2025-03-20 10:41:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 4 loss:0.043048687279224396 norm:0.00021849083714187145 max memory_allocated 47478.2919921875 
[2025-03-20 10:42:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 5 loss:0.04156337305903435 norm:0.0001976128260139376 max memory_allocated 47478.2919921875 
[2025-03-20 10:44:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 6 loss:0.040608908981084824 norm:0.0001915907923830673 max memory_allocated 47478.2919921875 
[2025-03-20 10:45:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 7 loss:0.039974093437194824 norm:0.00018653499137144536 max memory_allocated 47478.2919921875 
[2025-03-20 10:46:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 8 loss:0.039517585188150406 norm:0.000179651819053106 max memory_allocated 47478.2919921875 
[2025-03-20 10:48:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 9 loss:0.039170801639556885 norm:0.0001733710669213906 max memory_allocated 47478.2919921875 
[2025-03-20 10:49:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 10 loss:0.038896191865205765 norm:0.0001731242664391175 max memory_allocated 47478.2919921875 
[2025-03-20 10:50:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 11 loss:0.038693055510520935 norm:0.00016629594028927386 max memory_allocated 47478.2919921875 
[2025-03-20 10:52:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 12 loss:0.038512129336595535 norm:0.00016311259241774678 max memory_allocated 47478.2919921875 
[2025-03-20 10:53:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 13 loss:0.038372837007045746 norm:0.00016344993491657078 max memory_allocated 47478.2919921875 
[2025-03-20 10:54:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 14 loss:0.038269344717264175 norm:0.00015687342965975404 max memory_allocated 47478.2919921875 
[2025-03-20 10:56:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 15 loss:0.03816619887948036 norm:0.00015108486695680767 max memory_allocated 47478.2919921875 
[2025-03-20 10:57:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 16 loss:0.03809158131480217 norm:0.00015143999189604074 max memory_allocated 47478.2919921875 
[2025-03-20 10:58:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 17 loss:0.037981484085321426 norm:0.00015296995115932077 max memory_allocated 47478.2919921875 
[2025-03-20 11:00:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 18 loss:0.0379132516682148 norm:0.00014966407616157085 max memory_allocated 47478.2919921875 
[2025-03-20 11:01:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 19 loss:0.03788052126765251 norm:0.0001502870727563277 max memory_allocated 47478.2919921875 
[2025-03-20 11:02:59 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [14, 15, 16]
[2025-03-20 11:02:59 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [17, 18, 19] ===
[2025-03-20 11:04:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 0 loss:0.08177352696657181 norm:0.001304547768086195 max memory_allocated 47478.4794921875 
[2025-03-20 11:05:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 1 loss:0.067323237657547 norm:0.0005043970886617899 max memory_allocated 47478.4794921875 
[2025-03-20 11:07:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 2 loss:0.0555594265460968 norm:0.00028680326067842543 max memory_allocated 47478.4794921875 
[2025-03-20 11:08:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 3 loss:0.05168798193335533 norm:0.00021865610324312001 max memory_allocated 47478.4794921875 
[2025-03-20 11:09:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 4 loss:0.049297526478767395 norm:0.00018957337306346744 max memory_allocated 47478.4794921875 
[2025-03-20 11:11:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 5 loss:0.04769870266318321 norm:0.00018086025374941528 max memory_allocated 47478.4794921875 
[2025-03-20 11:12:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 6 loss:0.04684191942214966 norm:0.0001760389714036137 max memory_allocated 47478.4794921875 
[2025-03-20 11:13:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 7 loss:0.04630574211478233 norm:0.00017252462566830218 max memory_allocated 47478.4794921875 
[2025-03-20 11:15:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 8 loss:0.04590648412704468 norm:0.00016828757361508906 max memory_allocated 47478.4794921875 
[2025-03-20 11:16:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 9 loss:0.04557439312338829 norm:0.0001545516715850681 max memory_allocated 47478.4794921875 
[2025-03-20 11:17:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 10 loss:0.04535768926143646 norm:0.00015468434139620513 max memory_allocated 47478.4794921875 
[2025-03-20 11:19:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 11 loss:0.04512002691626549 norm:0.000146147096529603 max memory_allocated 47478.4794921875 
[2025-03-20 11:20:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 12 loss:0.044955965131521225 norm:0.0001484169770264998 max memory_allocated 47478.4794921875 
[2025-03-20 11:21:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 13 loss:0.0447869747877121 norm:0.00014128195471130311 max memory_allocated 47478.4794921875 
[2025-03-20 11:23:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 14 loss:0.04466339945793152 norm:0.00014323112554848194 max memory_allocated 47478.4794921875 
[2025-03-20 11:24:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 15 loss:0.04454294219613075 norm:0.00013887665409129113 max memory_allocated 47478.4794921875 
[2025-03-20 11:25:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 16 loss:0.04447105899453163 norm:0.00013638548261951655 max memory_allocated 47478.4794921875 
[2025-03-20 11:27:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 17 loss:0.04440683498978615 norm:0.00013890795526094735 max memory_allocated 47478.4794921875 
[2025-03-20 11:28:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 18 loss:0.04431071877479553 norm:0.00014256253780331463 max memory_allocated 47478.4794921875 
[2025-03-20 11:29:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 19 loss:0.04425448179244995 norm:0.00014192459639161825 max memory_allocated 47478.4794921875 
[2025-03-20 11:31:25 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [17, 18, 19]
[2025-03-20 11:31:25 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [20, 21, 22] ===
[2025-03-20 11:32:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 0 loss:0.10707785934209824 norm:0.0019602803513407707 max memory_allocated 47478.6669921875 
[2025-03-20 11:34:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 1 loss:0.0895749032497406 norm:0.0007628118619322777 max memory_allocated 47478.6669921875 
[2025-03-20 11:35:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 2 loss:0.07478649169206619 norm:0.0003904956975020468 max memory_allocated 47478.6669921875 
[2025-03-20 11:36:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 3 loss:0.07013052701950073 norm:0.0003001118020620197 max memory_allocated 47478.6669921875 
[2025-03-20 11:38:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 4 loss:0.06715641170740128 norm:0.0002626769710332155 max memory_allocated 47478.6669921875 
[2025-03-20 11:39:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 5 loss:0.06523626297712326 norm:0.00024159203167073429 max memory_allocated 47478.6669921875 
[2025-03-20 11:40:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 6 loss:0.06420625001192093 norm:0.0002340601640753448 max memory_allocated 47478.6669921875 
[2025-03-20 11:42:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 7 loss:0.06364796310663223 norm:0.00022868288215249777 max memory_allocated 47478.6669921875 
[2025-03-20 11:43:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 8 loss:0.06320905685424805 norm:0.00021998134616296738 max memory_allocated 47478.6669921875 
[2025-03-20 11:44:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 9 loss:0.06285835057497025 norm:0.00021351694886106998 max memory_allocated 47478.6669921875 
[2025-03-20 11:46:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 10 loss:0.06253761053085327 norm:0.0002069371985271573 max memory_allocated 47478.6669921875 
[2025-03-20 11:47:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 11 loss:0.06227889657020569 norm:0.0002002928376896307 max memory_allocated 47478.6669921875 
[2025-03-20 11:48:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 12 loss:0.062024857848882675 norm:0.0001984366390388459 max memory_allocated 47478.6669921875 
[2025-03-20 11:50:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 13 loss:0.06180001422762871 norm:0.00019103669910691679 max memory_allocated 47478.6669921875 
[2025-03-20 11:51:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 14 loss:0.06160002201795578 norm:0.00018436727987136692 max memory_allocated 47478.6669921875 
[2025-03-20 11:52:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 15 loss:0.06141189485788345 norm:0.00017988092440646142 max memory_allocated 47478.6669921875 
[2025-03-20 11:54:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 16 loss:0.06125906854867935 norm:0.00017636445409152657 max memory_allocated 47478.6669921875 
[2025-03-20 11:55:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 17 loss:0.061105549335479736 norm:0.00017517630476504564 max memory_allocated 47478.6669921875 
[2025-03-20 11:56:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 18 loss:0.060989927500486374 norm:0.00016995817713905126 max memory_allocated 47478.6669921875 
[2025-03-20 11:58:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 19 loss:0.06090155616402626 norm:0.0001683732698438689 max memory_allocated 47478.6669921875 
[2025-03-20 11:59:50 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [20, 21, 22]
[2025-03-20 11:59:50 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [23, 24, 25] ===
[2025-03-20 12:01:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 0 loss:0.1543557196855545 norm:0.00418917927891016 max memory_allocated 47478.8544921875 
[2025-03-20 12:02:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 1 loss:0.12667854130268097 norm:0.0009566916269250214 max memory_allocated 47478.8544921875 
[2025-03-20 12:03:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 2 loss:0.10575620830059052 norm:0.0004668272740673274 max memory_allocated 47478.8544921875 
[2025-03-20 12:05:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 3 loss:0.09927718341350555 norm:0.00034392820089124143 max memory_allocated 47478.8544921875 
[2025-03-20 12:06:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 4 loss:0.09552711248397827 norm:0.00031353504164144397 max memory_allocated 47478.8544921875 
[2025-03-20 12:07:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 5 loss:0.0936012715101242 norm:0.00029429729329422116 max memory_allocated 47478.8544921875 
[2025-03-20 12:09:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 6 loss:0.09275361895561218 norm:0.00027666299138218164 max memory_allocated 47478.8544921875 
[2025-03-20 12:10:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 7 loss:0.09212911128997803 norm:0.0002696544397622347 max memory_allocated 47478.8544921875 
[2025-03-20 12:11:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 8 loss:0.09164737910032272 norm:0.0002845801063813269 max memory_allocated 47478.8544921875 
[2025-03-20 12:13:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 9 loss:0.09118463844060898 norm:0.0002560803550295532 max memory_allocated 47478.8544921875 
[2025-03-20 12:14:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 10 loss:0.09079375118017197 norm:0.0002505857264623046 max memory_allocated 47478.8544921875 
[2025-03-20 12:15:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 11 loss:0.0904216319322586 norm:0.0002349196292925626 max memory_allocated 47478.8544921875 
[2025-03-20 12:17:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 12 loss:0.09010181576013565 norm:0.00023474193585570902 max memory_allocated 47478.8544921875 
[2025-03-20 12:18:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 13 loss:0.08982478082180023 norm:0.00022177262871991843 max memory_allocated 47478.8544921875 
[2025-03-20 12:19:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 14 loss:0.08960098773241043 norm:0.00022234098287299275 max memory_allocated 47478.8544921875 
[2025-03-20 12:21:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 15 loss:0.08938085287809372 norm:0.00022574118338525295 max memory_allocated 47478.8544921875 
[2025-03-20 12:22:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 16 loss:0.08919643610715866 norm:0.00021537272550631315 max memory_allocated 47478.8544921875 
[2025-03-20 12:23:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 17 loss:0.08901199698448181 norm:0.00020455659250728786 max memory_allocated 47478.8544921875 
[2025-03-20 12:25:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 18 loss:0.08887507021427155 norm:0.00020372560538817197 max memory_allocated 47478.8544921875 
[2025-03-20 12:26:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 19 loss:0.08873669803142548 norm:0.00020111653429921716 max memory_allocated 47478.8544921875 
[2025-03-20 12:28:14 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [23, 24, 25]
[2025-03-20 12:28:14 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [26, 27, 28] ===
[2025-03-20 12:28:15 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 12:29:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 0 loss:0.20458699762821198 norm:0.016413910314440727 max memory_allocated 47479.0419921875 
[2025-03-20 12:31:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 1 loss:0.17674070596694946 norm:0.011636685580015182 max memory_allocated 47479.0419921875 
[2025-03-20 12:32:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 2 loss:0.1516714096069336 norm:0.006989886984229088 max memory_allocated 47479.0419921875 
[2025-03-20 12:33:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 3 loss:0.1434326171875 norm:0.005795433651655912 max memory_allocated 47479.0419921875 
[2025-03-20 12:35:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 4 loss:0.13955892622470856 norm:0.004806481767445803 max memory_allocated 47479.0419921875 
[2025-03-20 12:36:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 5 loss:0.1379389762878418 norm:0.004191133193671703 max memory_allocated 47479.0419921875 
[2025-03-20 12:37:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 6 loss:0.1368570625782013 norm:0.0035745271015912294 max memory_allocated 47479.0419921875 
[2025-03-20 12:39:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 7 loss:0.13601870834827423 norm:0.0030228635296225548 max memory_allocated 47479.0419921875 
[2025-03-20 12:40:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 8 loss:0.1353616714477539 norm:0.002795090666040778 max memory_allocated 47479.0419921875 
[2025-03-20 12:41:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 9 loss:0.13485945761203766 norm:0.0027815967332571745 max memory_allocated 47479.0419921875 
[2025-03-20 12:43:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 10 loss:0.1344083994626999 norm:0.0028702369891107082 max memory_allocated 47479.0419921875 
[2025-03-20 12:44:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 11 loss:0.1340404599905014 norm:0.0025960830971598625 max memory_allocated 47479.0419921875 
[2025-03-20 12:45:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 12 loss:0.13359783589839935 norm:0.002525897463783622 max memory_allocated 47479.0419921875 
[2025-03-20 12:47:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 13 loss:0.13324710726737976 norm:0.002312389900907874 max memory_allocated 47479.0419921875 
[2025-03-20 12:48:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 14 loss:0.13292942941188812 norm:0.002249357523396611 max memory_allocated 47479.0419921875 
[2025-03-20 12:49:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 15 loss:0.1326940655708313 norm:0.002240060130134225 max memory_allocated 47479.0419921875 
[2025-03-20 12:51:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 16 loss:0.13240952789783478 norm:0.0021086412016302347 max memory_allocated 47479.0419921875 
[2025-03-20 12:52:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 17 loss:0.13219530880451202 norm:0.002023866167291999 max memory_allocated 47479.0419921875 
[2025-03-20 12:53:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 18 loss:0.1320018470287323 norm:0.0019807578064501286 max memory_allocated 47479.0419921875 
[2025-03-20 12:55:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 19 loss:0.1318979114294052 norm:0.0019024661742150784 max memory_allocated 47479.0419921875 
[2025-03-20 12:56:44 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [26, 27, 28]
[2025-03-20 12:56:44 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [29] ===
[2025-03-20 12:56:44 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 12:57:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 0 loss:0.16987232863903046 norm:0.010698020458221436 max memory_allocated 47479.0419921875 
[2025-03-20 12:57:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 1 loss:0.15662413835525513 norm:0.00849121157079935 max memory_allocated 47479.0419921875 
[2025-03-20 12:58:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 2 loss:0.1463700532913208 norm:0.005418268498033285 max memory_allocated 47479.0419921875 
[2025-03-20 12:58:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 3 loss:0.14335240423679352 norm:0.004473562818020582 max memory_allocated 47479.0419921875 
[2025-03-20 12:59:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 4 loss:0.14232566952705383 norm:0.0037989558186382055 max memory_allocated 47479.0419921875 
[2025-03-20 12:59:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 5 loss:0.1418597251176834 norm:0.0032270161900669336 max memory_allocated 47479.0419921875 
[2025-03-20 12:59:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 6 loss:0.14148007333278656 norm:0.0027537343557924032 max memory_allocated 47479.0419921875 
[2025-03-20 13:00:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 7 loss:0.1411799192428589 norm:0.002384364604949951 max memory_allocated 47479.0419921875 
[2025-03-20 13:00:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 8 loss:0.14097869396209717 norm:0.0022781272418797016 max memory_allocated 47479.0419921875 
[2025-03-20 13:01:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 9 loss:0.14086967706680298 norm:0.002277134219184518 max memory_allocated 47479.0419921875 
[2025-03-20 13:01:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 10 loss:0.14065496623516083 norm:0.0021441103890538216 max memory_allocated 47479.0419921875 
[2025-03-20 13:02:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 11 loss:0.1406104415655136 norm:0.0021943310275673866 max memory_allocated 47479.0419921875 
[2025-03-20 13:02:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 12 loss:0.14049866795539856 norm:0.002162642078474164 max memory_allocated 47479.0419921875 
[2025-03-20 13:03:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 13 loss:0.14039841294288635 norm:0.0020519020035862923 max memory_allocated 47479.0419921875 
[2025-03-20 13:03:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 14 loss:0.14023034274578094 norm:0.0019536695908755064 max memory_allocated 47479.0419921875 
[2025-03-20 13:03:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 15 loss:0.14023283123970032 norm:0.0019320189021527767 max memory_allocated 47479.0419921875 
[2025-03-20 13:04:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 16 loss:0.14013850688934326 norm:0.001940359128639102 max memory_allocated 47479.0419921875 
[2025-03-20 13:04:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 17 loss:0.14003965258598328 norm:0.0017849478172138333 max memory_allocated 47479.0419921875 
[2025-03-20 13:05:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 18 loss:0.139944389462471 norm:0.0017303084023296833 max memory_allocated 47479.0419921875 
[2025-03-20 13:05:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 19 loss:0.13988430798053741 norm:0.0016677806852385402 max memory_allocated 47479.0419921875 
[2025-03-20 13:06:20 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [29]
[2025-03-20 13:06:21 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [30] ===
[2025-03-20 13:06:22 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 13:06:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 0 loss:0.2445645034313202 norm:0.027417484670877457 max memory_allocated 47479.0419921875 
[2025-03-20 13:07:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 1 loss:0.2116176187992096 norm:0.019254159182310104 max memory_allocated 47479.0419921875 
[2025-03-20 13:07:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 2 loss:0.19153591990470886 norm:0.012155616655945778 max memory_allocated 47479.0419921875 
[2025-03-20 13:08:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 3 loss:0.18689687550067902 norm:0.010559935122728348 max memory_allocated 47479.0419921875 
[2025-03-20 13:08:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 4 loss:0.18495699763298035 norm:0.009719595313072205 max memory_allocated 47479.0419921875 
[2025-03-20 13:09:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 5 loss:0.1841575801372528 norm:0.00865933671593666 max memory_allocated 47479.0419921875 
[2025-03-20 13:09:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 6 loss:0.18374720215797424 norm:0.00843011774122715 max memory_allocated 47479.0419921875 
[2025-03-20 13:10:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 7 loss:0.18298397958278656 norm:0.007816794328391552 max memory_allocated 47479.0419921875 
[2025-03-20 13:10:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 8 loss:0.18265917897224426 norm:0.0077682798728346825 max memory_allocated 47479.0419921875 
[2025-03-20 13:10:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 9 loss:0.18246877193450928 norm:0.007601084653288126 max memory_allocated 47479.0419921875 
[2025-03-20 13:11:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 10 loss:0.18283846974372864 norm:0.007456235121935606 max memory_allocated 47479.0419921875 
[2025-03-20 13:11:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 11 loss:0.18200916051864624 norm:0.0073627894744277 max memory_allocated 47479.0419921875 
[2025-03-20 13:12:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 12 loss:0.18196849524974823 norm:0.00728604244068265 max memory_allocated 47479.0419921875 
[2025-03-20 13:12:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 13 loss:0.18170590698719025 norm:0.007239828817546368 max memory_allocated 47479.0419921875 
[2025-03-20 13:13:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 14 loss:0.18196061253547668 norm:0.00703379325568676 max memory_allocated 47479.0419921875 
[2025-03-20 13:13:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 15 loss:0.18148323893547058 norm:0.006671525072306395 max memory_allocated 47479.0419921875 
[2025-03-20 13:14:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 16 loss:0.18142658472061157 norm:0.00665992172434926 max memory_allocated 47479.0419921875 
[2025-03-20 13:14:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 17 loss:0.1812153160572052 norm:0.006650139577686787 max memory_allocated 47479.0419921875 
[2025-03-20 13:14:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 18 loss:0.18130618333816528 norm:0.006591212935745716 max memory_allocated 47479.0419921875 
[2025-03-20 13:15:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 19 loss:0.1814950704574585 norm:0.00668018963187933 max memory_allocated 47479.0419921875 
[2025-03-20 13:15:57 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [30]
[2025-03-20 13:15:57 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [31] ===
[2025-03-20 13:15:57 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 13:16:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 0 loss:0.2964787781238556 norm:0.01968039944767952 max memory_allocated 47479.0419921875 
[2025-03-20 13:16:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 1 loss:0.2824700176715851 norm:0.014731100760400295 max memory_allocated 47479.0419921875 
[2025-03-20 13:17:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 2 loss:0.27410343289375305 norm:0.011490367352962494 max memory_allocated 47479.0419921875 
[2025-03-20 13:17:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 3 loss:0.26938396692276 norm:0.009367844089865685 max memory_allocated 47479.0419921875 
[2025-03-20 13:18:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 4 loss:0.2672807574272156 norm:0.007897653616964817 max memory_allocated 47479.0419921875 
[2025-03-20 13:18:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 5 loss:0.2663249969482422 norm:0.007104602642357349 max memory_allocated 47479.0419921875 
[2025-03-20 13:19:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 6 loss:0.26561662554740906 norm:0.006519773043692112 max memory_allocated 47479.0419921875 
[2025-03-20 13:19:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 7 loss:0.2650964856147766 norm:0.00609966367483139 max memory_allocated 47479.0419921875 
[2025-03-20 13:20:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 8 loss:0.26482242345809937 norm:0.005840244237333536 max memory_allocated 47479.0419921875 
[2025-03-20 13:20:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 9 loss:0.26470842957496643 norm:0.005978916771709919 max memory_allocated 47479.0419921875 
[2025-03-20 13:20:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 10 loss:0.26434004306793213 norm:0.005905518773943186 max memory_allocated 47479.0419921875 
[2025-03-20 13:21:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 11 loss:0.2643062174320221 norm:0.005915879271924496 max memory_allocated 47479.0419921875 
[2025-03-20 13:21:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 12 loss:0.26391348242759705 norm:0.005874871276319027 max memory_allocated 47479.0419921875 
[2025-03-20 13:22:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 13 loss:0.2637174129486084 norm:0.005949008744210005 max memory_allocated 47479.0419921875 
[2025-03-20 13:22:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 14 loss:0.26356789469718933 norm:0.005678633227944374 max memory_allocated 47479.0419921875 
[2025-03-20 13:23:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 15 loss:0.26345813274383545 norm:0.005551030859351158 max memory_allocated 47479.0419921875 
[2025-03-20 13:23:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 16 loss:0.26332542300224304 norm:0.005517687648534775 max memory_allocated 47479.0419921875 
[2025-03-20 13:24:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 17 loss:0.2634098529815674 norm:0.005951854400336742 max memory_allocated 47479.0419921875 
[2025-03-20 13:24:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 18 loss:0.26327359676361084 norm:0.005518927238881588 max memory_allocated 47479.0419921875 
[2025-03-20 13:25:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 19 loss:0.26317307353019714 norm:0.005627165082842112 max memory_allocated 47479.0419921875 
[2025-03-20 13:25:35 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [31]
[2025-03-20 13:25:35 root] (main_calib_config3_attn.py 379): INFO 18235.778156518936
[2025-03-20 13:25:40 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-20 13:26:26 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.61967658996582
[2025-03-20 13:26:26 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-20 13:27:37 root] (main_calib_config3_attn.py 161): INFO c4 : 7.167895317077637
[2025-03-20 14:04:12 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.61967658996582, 'c4': 7.167895317077637, 'results': {'winogrande': {'acc': 0.6692975532754538, 'acc_stderr': 0.0132224358870027}, 'piqa': {'acc': 0.7752992383025027, 'acc_stderr': 0.009738282586548375, 'acc_norm': 0.7693144722524483, 'acc_norm_stderr': 0.0098289595509831}, 'hellaswag': {'acc': 0.5566620195180243, 'acc_stderr': 0.004957637648426469, 'acc_norm': 0.7201752638916551, 'acc_norm_stderr': 0.004479955169853626}, 'boolq': {'acc': 0.7333333333333333, 'acc_stderr': 0.007734417634064949}, 'arc_easy': {'acc': 0.694023569023569, 'acc_stderr': 0.00945582203642662, 'acc_norm': 0.531986531986532, 'acc_norm_stderr': 0.010238767643185709}, 'arc_challenge': {'acc': 0.40784982935153585, 'acc_stderr': 0.014361097288449705, 'acc_norm': 0.3967576791808874, 'acc_norm_stderr': 0.014296513020180646}}, 'versions': {'winogrande': 0, 'piqa': 0, 'hellaswag': 0, 'boolq': 1, 'arc_easy': 0, 'arc_challenge': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-20 14:04:12 root] (main_calib_config3_attn.py 175): INFO 40.78,69.40,73.33,55.67,77.53,66.93
