[2025-03-21 09:39:11 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/Llama-2-7b-hf-0.75', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.75.pkl', blocks_pkl='./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-21 09:39:28 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-21 09:39:28 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-21 09:39:28 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-21 09:39:28 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.75.pkl
[2025-03-21 09:39:28 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 9), (9, 11), (11, 14), (14, 17), (17, 20), (20, 23), (23, 26), (26, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-21 09:39:28 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12, 13], [14, 15, 16], [17, 18, 19], [20, 21, 22], [23, 24, 25], [26, 27, 28], [29], [30], [31]]
[2025-03-21 09:39:37 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-21 09:39:38 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 09:40:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.002762023126706481 norm:0.00862133875489235 max memory_allocated 34633.880859375 
[2025-03-21 09:40:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.0014251968823373318 norm:0.005211437586694956 max memory_allocated 34633.880859375 
[2025-03-21 09:41:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.0012158052995800972 norm:0.004111037123948336 max memory_allocated 34633.880859375 
[2025-03-21 09:41:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0010655914666131139 norm:0.003168245777487755 max memory_allocated 34633.880859375 
[2025-03-21 09:41:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.001017234637401998 norm:0.002839579712599516 max memory_allocated 34633.880859375 
[2025-03-21 09:42:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0009604785009287298 norm:0.0023744262289255857 max memory_allocated 34633.880859375 
[2025-03-21 09:42:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.0009499102015979588 norm:0.002202455885708332 max memory_allocated 34633.880859375 
[2025-03-21 09:43:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0009393994114361703 norm:0.0019733228255063295 max memory_allocated 34633.880859375 
[2025-03-21 09:43:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0009216570178978145 norm:0.0017936569638550282 max memory_allocated 34633.880859375 
[2025-03-21 09:44:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.0009035616531036794 norm:0.0016353994142264128 max memory_allocated 34633.880859375 
[2025-03-21 09:44:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.0008996875258162618 norm:0.0015074826078489423 max memory_allocated 34633.880859375 
[2025-03-21 09:45:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0008837603381834924 norm:0.001346480450592935 max memory_allocated 34633.880859375 
[2025-03-21 09:45:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.0008799562929198146 norm:0.0012737333308905363 max memory_allocated 34633.880859375 
[2025-03-21 09:46:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.000856531725730747 norm:0.0011674445122480392 max memory_allocated 34633.880859375 
[2025-03-21 09:46:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0008340278291143477 norm:0.0010070153512060642 max memory_allocated 34633.880859375 
[2025-03-21 09:46:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.0008248801459558308 norm:0.0009323639096692204 max memory_allocated 34633.880859375 
[2025-03-21 09:47:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.0008199741714634001 norm:0.0008426188142038882 max memory_allocated 34633.880859375 
[2025-03-21 09:47:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0008405601256527007 norm:0.0007882423815317452 max memory_allocated 34633.880859375 
[2025-03-21 09:48:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.0008198311552405357 norm:0.0007250878261402249 max memory_allocated 34633.880859375 
[2025-03-21 09:48:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0008135517709888518 norm:0.0006574781727977097 max memory_allocated 34633.880859375 
[2025-03-21 09:49:50 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-21 09:49:50 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-21 09:49:50 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 09:50:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.024476654827594757 norm:0.02285216748714447 max memory_allocated 35100.7724609375 
[2025-03-21 09:50:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.013510181568562984 norm:0.012719551101326942 max memory_allocated 35100.7724609375 
[2025-03-21 09:51:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.009010379202663898 norm:0.008860551752150059 max memory_allocated 35100.7724609375 
[2025-03-21 09:51:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.008466183207929134 norm:0.011407675221562386 max memory_allocated 35100.7724609375 
[2025-03-21 09:52:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.007955262437462807 norm:0.008705846965312958 max memory_allocated 35100.7724609375 
[2025-03-21 09:52:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.007518646772950888 norm:0.007335225120186806 max memory_allocated 35100.7724609375 
[2025-03-21 09:53:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.007342655211687088 norm:0.00644112890586257 max memory_allocated 35100.7724609375 
[2025-03-21 09:53:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.007364500779658556 norm:0.006004399619996548 max memory_allocated 35100.7724609375 
[2025-03-21 09:53:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.00765644945204258 norm:0.005907200742512941 max memory_allocated 35100.7724609375 
[2025-03-21 09:54:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.008180016651749611 norm:0.006333693861961365 max memory_allocated 35100.7724609375 
[2025-03-21 09:54:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.008254296146333218 norm:0.006684667896479368 max memory_allocated 35100.7724609375 
[2025-03-21 09:55:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.007523925043642521 norm:0.006978326011449099 max memory_allocated 35100.7724609375 
[2025-03-21 09:55:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.007260838523507118 norm:0.007665522396564484 max memory_allocated 35100.7724609375 
[2025-03-21 09:56:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.006926564034074545 norm:0.007170607801526785 max memory_allocated 35100.7724609375 
[2025-03-21 09:56:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.006800762843340635 norm:0.0068222288973629475 max memory_allocated 35100.7724609375 
[2025-03-21 09:57:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.00678342767059803 norm:0.006549539044499397 max memory_allocated 35100.7724609375 
[2025-03-21 09:57:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.006941499654203653 norm:0.006618296727538109 max memory_allocated 35100.7724609375 
[2025-03-21 09:58:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.006819217465817928 norm:0.007175147533416748 max memory_allocated 35100.7724609375 
[2025-03-21 09:58:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.007370447739958763 norm:0.007351044099777937 max memory_allocated 35100.7724609375 
[2025-03-21 09:58:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.007315666880458593 norm:0.00726061686873436 max memory_allocated 35100.7724609375 
[2025-03-21 10:00:01 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-21 10:00:01 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
[2025-03-21 10:00:02 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 10:00:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.009814069606363773 norm:0.00459960987791419 max memory_allocated 35101.8349609375 
[2025-03-21 10:00:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.007569832727313042 norm:0.0035002739168703556 max memory_allocated 35101.8349609375 
[2025-03-21 10:01:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.0070726266130805016 norm:0.002899687271565199 max memory_allocated 35101.8349609375 
[2025-03-21 10:01:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.006703834515064955 norm:0.002512962557375431 max memory_allocated 35101.8349609375 
[2025-03-21 10:02:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.0063288863748312 norm:0.0022137730848044157 max memory_allocated 35101.8349609375 
[2025-03-21 10:02:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.006021724548190832 norm:0.00197008578106761 max memory_allocated 35101.8349609375 
[2025-03-21 10:03:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.0058534215204417706 norm:0.001734787249006331 max memory_allocated 35101.8349609375 
[2025-03-21 10:03:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.0057785389944911 norm:0.0015151231782510877 max memory_allocated 35101.8349609375 
[2025-03-21 10:04:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.005739269778132439 norm:0.0013027102686464787 max memory_allocated 35101.8349609375 
[2025-03-21 10:04:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.005713324993848801 norm:0.00110505404882133 max memory_allocated 35101.8349609375 
[2025-03-21 10:05:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.00570030277594924 norm:0.0009247534908354282 max memory_allocated 35101.8349609375 
[2025-03-21 10:05:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.005685112439095974 norm:0.0007644058205187321 max memory_allocated 35101.8349609375 
[2025-03-21 10:06:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.005685125477612019 norm:0.0006284299306571484 max memory_allocated 35101.8349609375 
[2025-03-21 10:06:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.005684071686118841 norm:0.0005917653325013816 max memory_allocated 35101.8349609375 
[2025-03-21 10:06:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.005725308321416378 norm:0.0007392157567664981 max memory_allocated 35101.8349609375 
[2025-03-21 10:07:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.005638327449560165 norm:0.000506982731167227 max memory_allocated 35101.8349609375 
[2025-03-21 10:07:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.005662758368998766 norm:0.0005133294616825879 max memory_allocated 35101.8349609375 
[2025-03-21 10:08:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.005633687600493431 norm:0.0005186757771298289 max memory_allocated 35101.8349609375 
[2025-03-21 10:08:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.005672093015164137 norm:0.0005416882922872901 max memory_allocated 35101.8349609375 
[2025-03-21 10:09:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.005675091408193111 norm:0.000562635890673846 max memory_allocated 35101.8349609375 
[2025-03-21 10:10:10 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2]
[2025-03-21 10:10:10 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4, 5] ===
[2025-03-21 10:11:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 0 loss:0.024383988231420517 norm:0.000371431524399668 max memory_allocated 47477.6044921875 
[2025-03-21 10:12:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 1 loss:0.02007877826690674 norm:0.00027274579042568803 max memory_allocated 47477.6044921875 
[2025-03-21 10:14:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 2 loss:0.0173970814794302 norm:0.00029402817017398775 max memory_allocated 47477.6044921875 
[2025-03-21 10:15:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 3 loss:0.015681810677051544 norm:0.00019284297013655305 max memory_allocated 47477.6044921875 
[2025-03-21 10:17:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 4 loss:0.014435147866606712 norm:0.00016787744243629277 max memory_allocated 47477.6044921875 
[2025-03-21 10:18:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 5 loss:0.013667436316609383 norm:0.00020816345931962132 max memory_allocated 47477.6044921875 
[2025-03-21 10:19:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 6 loss:0.013232381083071232 norm:0.0002421076933387667 max memory_allocated 47477.6044921875 
[2025-03-21 10:21:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 7 loss:0.01298036240041256 norm:0.00025742914294824004 max memory_allocated 47477.6044921875 
[2025-03-21 10:22:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 8 loss:0.012869646772742271 norm:0.00024405564181506634 max memory_allocated 47477.6044921875 
[2025-03-21 10:23:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 9 loss:0.012796900235116482 norm:0.00022123170492704958 max memory_allocated 47477.6044921875 
[2025-03-21 10:25:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 10 loss:0.01276116631925106 norm:0.000244585593463853 max memory_allocated 47477.6044921875 
[2025-03-21 10:26:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 11 loss:0.012759323231875896 norm:0.0002579848514869809 max memory_allocated 47477.6044921875 
[2025-03-21 10:27:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 12 loss:0.012742849998176098 norm:0.00024120273883454502 max memory_allocated 47477.6044921875 
[2025-03-21 10:29:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 13 loss:0.01270981039851904 norm:0.00023400119971483946 max memory_allocated 47477.6044921875 
[2025-03-21 10:30:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 14 loss:0.012690717354416847 norm:0.00024111021775752306 max memory_allocated 47477.6044921875 
[2025-03-21 10:31:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 15 loss:0.012682420201599598 norm:0.00022929823899175972 max memory_allocated 47477.6044921875 
[2025-03-21 10:33:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 16 loss:0.012654555961489677 norm:0.00023238672292791307 max memory_allocated 47477.6044921875 
[2025-03-21 10:34:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 17 loss:0.012629983015358448 norm:0.00023419945500791073 max memory_allocated 47477.6044921875 
[2025-03-21 10:35:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 18 loss:0.012620024383068085 norm:0.0002399609365966171 max memory_allocated 47477.6044921875 
[2025-03-21 10:37:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 19 loss:0.01261771097779274 norm:0.00021984452905599028 max memory_allocated 47477.6044921875 
[2025-03-21 10:40:24 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [3, 4, 5]
[2025-03-21 10:40:24 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [6, 7, 8] ===
[2025-03-21 10:41:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 0 loss:0.03392829746007919 norm:0.0005708741373382509 max memory_allocated 47477.7919921875 
[2025-03-21 10:43:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 1 loss:0.027729086577892303 norm:0.0003107245429418981 max memory_allocated 47477.7919921875 
[2025-03-21 10:44:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 2 loss:0.023774001747369766 norm:0.00024869164917618036 max memory_allocated 47477.7919921875 
[2025-03-21 10:45:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 3 loss:0.02128441631793976 norm:0.0002051614865195006 max memory_allocated 47477.7919921875 
[2025-03-21 10:47:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 4 loss:0.019808420911431313 norm:0.00019868373055942357 max memory_allocated 47477.7919921875 
[2025-03-21 10:48:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 5 loss:0.018844973295927048 norm:0.00019638000230770558 max memory_allocated 47477.7919921875 
[2025-03-21 10:49:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 6 loss:0.018268825486302376 norm:0.000190150152775459 max memory_allocated 47477.7919921875 
[2025-03-21 10:51:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 7 loss:0.01788930781185627 norm:0.0001725211477605626 max memory_allocated 47477.7919921875 
[2025-03-21 10:52:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 8 loss:0.017697807401418686 norm:0.00018143493798561394 max memory_allocated 47477.7919921875 
[2025-03-21 10:53:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 9 loss:0.017533045262098312 norm:0.00016305646568071097 max memory_allocated 47477.7919921875 
[2025-03-21 10:55:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 10 loss:0.017412833869457245 norm:0.00015939401055220515 max memory_allocated 47477.7919921875 
[2025-03-21 10:56:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 11 loss:0.017359202727675438 norm:0.00015977588191162795 max memory_allocated 47477.7919921875 
[2025-03-21 10:58:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 12 loss:0.017295939847826958 norm:0.000170219675055705 max memory_allocated 47477.7919921875 
[2025-03-21 10:59:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 13 loss:0.017218798398971558 norm:0.00016258646792266518 max memory_allocated 47477.7919921875 
[2025-03-21 11:00:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 14 loss:0.017180779948830605 norm:0.0001568972074892372 max memory_allocated 47477.7919921875 
[2025-03-21 11:02:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 15 loss:0.017127562314271927 norm:0.00016614867490716279 max memory_allocated 47477.7919921875 
[2025-03-21 11:03:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 16 loss:0.017102012410759926 norm:0.00016306179168168455 max memory_allocated 47477.7919921875 
[2025-03-21 11:04:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 17 loss:0.017076466232538223 norm:0.0001658903347561136 max memory_allocated 47477.7919921875 
[2025-03-21 11:06:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 18 loss:0.017007915303111076 norm:0.00015143460768740624 max memory_allocated 47477.7919921875 
[2025-03-21 11:07:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 19 loss:0.016981547698378563 norm:0.000152715016156435 max memory_allocated 47477.7919921875 
[2025-03-21 11:10:28 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [6, 7, 8]
[2025-03-21 11:10:28 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [9, 10] ===
[2025-03-21 11:11:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 0 loss:0.02940637245774269 norm:0.0004184443096164614 max memory_allocated 47477.7919921875 
[2025-03-21 11:12:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 1 loss:0.0245817843824625 norm:0.000261615845374763 max memory_allocated 47477.7919921875 
[2025-03-21 11:13:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 2 loss:0.021268745884299278 norm:0.00016409798990935087 max memory_allocated 47477.7919921875 
[2025-03-21 11:14:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 3 loss:0.01988425850868225 norm:0.00014450443268287927 max memory_allocated 47477.7919921875 
[2025-03-21 11:15:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 4 loss:0.018940942361950874 norm:0.00013571183080784976 max memory_allocated 47477.7919921875 
[2025-03-21 11:15:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 5 loss:0.018321199342608452 norm:0.00011648931103991345 max memory_allocated 47477.7919921875 
[2025-03-21 11:16:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 6 loss:0.017991865053772926 norm:0.00011255276331212372 max memory_allocated 47477.7919921875 
[2025-03-21 11:17:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 7 loss:0.017823537811636925 norm:0.00010496059985598549 max memory_allocated 47477.7919921875 
[2025-03-21 11:18:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 8 loss:0.017710024490952492 norm:0.00010367095819674432 max memory_allocated 47477.7919921875 
[2025-03-21 11:19:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 9 loss:0.01764308288693428 norm:0.0001029872364597395 max memory_allocated 47477.7919921875 
[2025-03-21 11:20:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 10 loss:0.017567317932844162 norm:9.237114136340097e-05 max memory_allocated 47477.7919921875 
[2025-03-21 11:21:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 11 loss:0.01751573756337166 norm:9.322933328803629e-05 max memory_allocated 47477.7919921875 
[2025-03-21 11:22:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 12 loss:0.017468132078647614 norm:8.920470281736925e-05 max memory_allocated 47477.7919921875 
[2025-03-21 11:23:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 13 loss:0.017435770481824875 norm:8.672357944305986e-05 max memory_allocated 47477.7919921875 
[2025-03-21 11:24:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 14 loss:0.017416400834918022 norm:9.038090502144769e-05 max memory_allocated 47477.7919921875 
[2025-03-21 11:24:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 15 loss:0.017387405037879944 norm:9.004615276353434e-05 max memory_allocated 47477.7919921875 
[2025-03-21 11:25:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 16 loss:0.017367886379361153 norm:9.077788854483515e-05 max memory_allocated 47477.7919921875 
[2025-03-21 11:26:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 17 loss:0.01735294982790947 norm:8.552981307730079e-05 max memory_allocated 47477.7919921875 
[2025-03-21 11:27:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 18 loss:0.017339050769805908 norm:8.736484596738592e-05 max memory_allocated 47477.7919921875 
[2025-03-21 11:28:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 19 loss:0.017328692600131035 norm:9.333089110441506e-05 max memory_allocated 47477.7919921875 
[2025-03-21 11:30:45 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [9, 10]
[2025-03-21 11:30:45 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [11, 12, 13] ===
[2025-03-21 11:32:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 0 loss:0.034205298870801926 norm:0.0005425033741630614 max memory_allocated 47478.1044921875 
[2025-03-21 11:33:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 1 loss:0.02858169563114643 norm:0.0003038273425772786 max memory_allocated 47478.1044921875 
[2025-03-21 11:34:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 2 loss:0.02514117956161499 norm:0.00023581332061439753 max memory_allocated 47478.1044921875 
[2025-03-21 11:36:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 3 loss:0.023269694298505783 norm:0.00020116535597480834 max memory_allocated 47478.1044921875 
[2025-03-21 11:37:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 4 loss:0.022031385451555252 norm:0.00017881463281810284 max memory_allocated 47478.1044921875 
[2025-03-21 11:38:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 5 loss:0.021164339035749435 norm:0.000167405407410115 max memory_allocated 47478.1044921875 
[2025-03-21 11:40:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 6 loss:0.020580753684043884 norm:0.0001562910620123148 max memory_allocated 47478.1044921875 
[2025-03-21 11:41:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 7 loss:0.020185936242341995 norm:0.00014889145677443594 max memory_allocated 47478.1044921875 
[2025-03-21 11:43:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 8 loss:0.01989372819662094 norm:0.00014479330275207758 max memory_allocated 47478.1044921875 
[2025-03-21 11:44:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 9 loss:0.01969275251030922 norm:0.0001378493761876598 max memory_allocated 47478.1044921875 
[2025-03-21 11:45:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 10 loss:0.0195331871509552 norm:0.00012881943257525563 max memory_allocated 47478.1044921875 
[2025-03-21 11:47:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 11 loss:0.01943461038172245 norm:0.00012701202649623156 max memory_allocated 47478.1044921875 
[2025-03-21 11:48:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 12 loss:0.019340744242072105 norm:0.0001246455212822184 max memory_allocated 47478.1044921875 
[2025-03-21 11:49:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 13 loss:0.019253240898251534 norm:0.00011611184163484722 max memory_allocated 47478.1044921875 
[2025-03-21 11:51:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 14 loss:0.019181763753294945 norm:0.00011376353359082714 max memory_allocated 47478.1044921875 
[2025-03-21 11:52:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 15 loss:0.01913473755121231 norm:0.00011578055273275822 max memory_allocated 47478.1044921875 
[2025-03-21 11:53:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 16 loss:0.01908130943775177 norm:0.00010308156197424978 max memory_allocated 47478.1044921875 
[2025-03-21 11:55:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 17 loss:0.01904798485338688 norm:0.00010437982564326376 max memory_allocated 47478.1044921875 
[2025-03-21 11:56:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 18 loss:0.018991492688655853 norm:0.00010098779603140429 max memory_allocated 47478.1044921875 
[2025-03-21 11:57:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 19 loss:0.018965639173984528 norm:0.00010523119271965697 max memory_allocated 47478.1044921875 
[2025-03-21 12:00:43 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [11, 12, 13]
[2025-03-21 12:00:43 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [14, 15, 16] ===
[2025-03-21 12:02:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 0 loss:0.033843304961919785 norm:0.0005114763043820858 max memory_allocated 47478.2919921875 
[2025-03-21 12:03:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 1 loss:0.02874186262488365 norm:0.00030643807258456945 max memory_allocated 47478.2919921875 
[2025-03-21 12:04:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 2 loss:0.025270231068134308 norm:0.0002252449921797961 max memory_allocated 47478.2919921875 
[2025-03-21 12:06:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 3 loss:0.023475753143429756 norm:0.00018296041525900364 max memory_allocated 47478.2919921875 
[2025-03-21 12:07:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 4 loss:0.022251538932323456 norm:0.00016350392252206802 max memory_allocated 47478.2919921875 
[2025-03-21 12:08:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 5 loss:0.021348485723137856 norm:0.00015310176240745932 max memory_allocated 47478.2919921875 
[2025-03-21 12:10:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 6 loss:0.020764585584402084 norm:0.0001419585314579308 max memory_allocated 47478.2919921875 
[2025-03-21 12:11:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 7 loss:0.020390644669532776 norm:0.0001339978480245918 max memory_allocated 47478.2919921875 
[2025-03-21 12:12:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 8 loss:0.020118340849876404 norm:0.00012879793939646333 max memory_allocated 47478.2919921875 
[2025-03-21 12:14:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 9 loss:0.019928082823753357 norm:0.00012395897647365928 max memory_allocated 47478.2919921875 
[2025-03-21 12:15:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 10 loss:0.019783953204751015 norm:0.00011945850565098226 max memory_allocated 47478.2919921875 
[2025-03-21 12:16:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 11 loss:0.01965242438018322 norm:0.00011527059541549534 max memory_allocated 47478.2919921875 
[2025-03-21 12:18:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 12 loss:0.019571345299482346 norm:0.00011190180521225557 max memory_allocated 47478.2919921875 
[2025-03-21 12:19:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 13 loss:0.01947774365544319 norm:0.0001084939285647124 max memory_allocated 47478.2919921875 
[2025-03-21 12:21:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 14 loss:0.019398096948862076 norm:0.00010588228178676218 max memory_allocated 47478.2919921875 
[2025-03-21 12:22:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 15 loss:0.019351154565811157 norm:0.0001071258811862208 max memory_allocated 47478.2919921875 
[2025-03-21 12:23:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 16 loss:0.01927206665277481 norm:0.00010090353316627443 max memory_allocated 47478.2919921875 
[2025-03-21 12:25:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 17 loss:0.019226549193263054 norm:9.56726071308367e-05 max memory_allocated 47478.2919921875 
[2025-03-21 12:26:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 18 loss:0.019183391705155373 norm:8.903998968889937e-05 max memory_allocated 47478.2919921875 
[2025-03-21 12:27:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 19 loss:0.019151443615555763 norm:9.464749018661678e-05 max memory_allocated 47478.2919921875 
[2025-03-21 12:30:46 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [14, 15, 16]
[2025-03-21 12:30:46 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [17, 18, 19] ===
[2025-03-21 12:32:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 0 loss:0.03397412598133087 norm:0.00035717288847081363 max memory_allocated 47478.4794921875 
[2025-03-21 12:33:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 1 loss:0.030987873673439026 norm:0.00022543522936757654 max memory_allocated 47478.4794921875 
[2025-03-21 12:34:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 2 loss:0.029033588245511055 norm:0.00016921930364333093 max memory_allocated 47478.4794921875 
[2025-03-21 12:36:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 3 loss:0.027152203023433685 norm:0.00014307127275969833 max memory_allocated 47478.4794921875 
[2025-03-21 12:37:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 4 loss:0.02557026967406273 norm:0.00013301247963681817 max memory_allocated 47478.4794921875 
[2025-03-21 12:38:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 5 loss:0.024621890857815742 norm:0.0001237944234162569 max memory_allocated 47478.4794921875 
[2025-03-21 12:40:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 6 loss:0.02415541559457779 norm:0.00011634540715022013 max memory_allocated 47478.4794921875 
[2025-03-21 12:41:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 7 loss:0.023920513689517975 norm:0.00011057169467676431 max memory_allocated 47478.4794921875 
[2025-03-21 12:42:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 8 loss:0.023756764829158783 norm:0.0001043365482473746 max memory_allocated 47478.4794921875 
[2025-03-21 12:44:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 9 loss:0.02362828701734543 norm:9.827477333601564e-05 max memory_allocated 47478.4794921875 
[2025-03-21 12:45:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 10 loss:0.023562781512737274 norm:9.308003063779324e-05 max memory_allocated 47478.4794921875 
[2025-03-21 12:47:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 11 loss:0.023497246205806732 norm:9.208708797814324e-05 max memory_allocated 47478.4794921875 
[2025-03-21 12:48:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 12 loss:0.02343941666185856 norm:9.045153274200857e-05 max memory_allocated 47478.4794921875 
[2025-03-21 12:49:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 13 loss:0.02338193543255329 norm:8.831635204842314e-05 max memory_allocated 47478.4794921875 
[2025-03-21 12:51:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 14 loss:0.023344065994024277 norm:8.747931133257225e-05 max memory_allocated 47478.4794921875 
[2025-03-21 12:52:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 15 loss:0.02329379692673683 norm:8.63810273585841e-05 max memory_allocated 47478.4794921875 
[2025-03-21 12:53:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 16 loss:0.023249134421348572 norm:8.141819853335619e-05 max memory_allocated 47478.4794921875 
[2025-03-21 12:55:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 17 loss:0.023215672001242638 norm:7.783338514855132e-05 max memory_allocated 47478.4794921875 
[2025-03-21 12:56:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 18 loss:0.02320181205868721 norm:8.061211701715365e-05 max memory_allocated 47478.4794921875 
[2025-03-21 12:57:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 19 loss:0.02317032590508461 norm:7.454755541402847e-05 max memory_allocated 47478.4794921875 
[2025-03-21 13:00:54 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [17, 18, 19]
[2025-03-21 13:00:54 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [20, 21, 22] ===
[2025-03-21 13:02:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 0 loss:0.04471242055296898 norm:0.0007145719137042761 max memory_allocated 47478.6669921875 
[2025-03-21 13:03:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 1 loss:0.041289303451776505 norm:0.0003671722370199859 max memory_allocated 47478.6669921875 
[2025-03-21 13:05:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 2 loss:0.039017099887132645 norm:0.000250380631769076 max memory_allocated 47478.6669921875 
[2025-03-21 13:06:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 3 loss:0.036718495190143585 norm:0.0001931341685121879 max memory_allocated 47478.6669921875 
[2025-03-21 13:07:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 4 loss:0.03460947424173355 norm:0.00017636787379160523 max memory_allocated 47478.6669921875 
[2025-03-21 13:09:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 5 loss:0.03348805010318756 norm:0.00016562777454964817 max memory_allocated 47478.6669921875 
[2025-03-21 13:10:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 6 loss:0.033126115798950195 norm:0.00016490169218741357 max memory_allocated 47478.6669921875 
[2025-03-21 13:11:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 7 loss:0.03295468911528587 norm:0.00014893156185280532 max memory_allocated 47478.6669921875 
[2025-03-21 13:13:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 8 loss:0.032765086740255356 norm:0.00014543466386385262 max memory_allocated 47478.6669921875 
[2025-03-21 13:14:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 9 loss:0.03267741948366165 norm:0.00014220335287973285 max memory_allocated 47478.6669921875 
[2025-03-21 13:15:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 10 loss:0.03256472945213318 norm:0.00013335043331608176 max memory_allocated 47478.6669921875 
[2025-03-21 13:17:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 11 loss:0.03247547522187233 norm:0.00012954331759829074 max memory_allocated 47478.6669921875 
[2025-03-21 13:18:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 12 loss:0.0324171781539917 norm:0.00013011299597565085 max memory_allocated 47478.6669921875 
[2025-03-21 13:19:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 13 loss:0.03234332799911499 norm:0.00012228998821228743 max memory_allocated 47478.6669921875 
[2025-03-21 13:21:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 14 loss:0.03228869289159775 norm:0.00011855935008497909 max memory_allocated 47478.6669921875 
[2025-03-21 13:22:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 15 loss:0.03222934529185295 norm:0.00011550760245881975 max memory_allocated 47478.6669921875 
[2025-03-21 13:23:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 16 loss:0.032175954431295395 norm:0.00011555122182471678 max memory_allocated 47478.6669921875 
[2025-03-21 13:25:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 17 loss:0.032134026288986206 norm:0.00011063346028095111 max memory_allocated 47478.6669921875 
[2025-03-21 13:26:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 18 loss:0.032088566571474075 norm:0.00011210330558242276 max memory_allocated 47478.6669921875 
[2025-03-21 13:27:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 19 loss:0.03203195706009865 norm:0.00010848746751435101 max memory_allocated 47478.6669921875 
[2025-03-21 13:30:58 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [20, 21, 22]
[2025-03-21 13:30:58 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [23, 24, 25] ===
[2025-03-21 13:32:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 0 loss:0.06475166231393814 norm:0.0014958815881982446 max memory_allocated 47478.8544921875 
[2025-03-21 13:33:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 1 loss:0.059483446180820465 norm:0.0004938382189720869 max memory_allocated 47478.8544921875 
[2025-03-21 13:35:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 2 loss:0.056150343269109726 norm:0.00034764717565849423 max memory_allocated 47478.8544921875 
[2025-03-21 13:36:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 3 loss:0.052556052803993225 norm:0.0002772142179310322 max memory_allocated 47478.8544921875 
[2025-03-21 13:37:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 4 loss:0.04999990016222 norm:0.0002530774218030274 max memory_allocated 47478.8544921875 
[2025-03-21 13:39:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 5 loss:0.049293629825115204 norm:0.00023651632363907993 max memory_allocated 47478.8544921875 
[2025-03-21 13:40:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 6 loss:0.049029938876628876 norm:0.00022353373060468584 max memory_allocated 47478.8544921875 
[2025-03-21 13:41:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 7 loss:0.04884415864944458 norm:0.00021005481539759785 max memory_allocated 47478.8544921875 
[2025-03-21 13:43:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 8 loss:0.04869864135980606 norm:0.00020420174405444413 max memory_allocated 47478.8544921875 
[2025-03-21 13:44:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 9 loss:0.04856158792972565 norm:0.00019606968271546066 max memory_allocated 47478.8544921875 
[2025-03-21 13:45:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 10 loss:0.04846048727631569 norm:0.00018843793077394366 max memory_allocated 47478.8544921875 
[2025-03-21 13:47:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 11 loss:0.04836041480302811 norm:0.00018086274212691933 max memory_allocated 47478.8544921875 
[2025-03-21 13:48:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 12 loss:0.04829386994242668 norm:0.00017797062173485756 max memory_allocated 47478.8544921875 
[2025-03-21 13:49:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 13 loss:0.048195429146289825 norm:0.0001775984710548073 max memory_allocated 47478.8544921875 
[2025-03-21 13:51:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 14 loss:0.048109862953424454 norm:0.00017037367797456682 max memory_allocated 47478.8544921875 
[2025-03-21 13:52:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 15 loss:0.048057131469249725 norm:0.00015989103121683002 max memory_allocated 47478.8544921875 
[2025-03-21 13:53:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 16 loss:0.04797269403934479 norm:0.000167661186424084 max memory_allocated 47478.8544921875 
[2025-03-21 13:55:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 17 loss:0.04793424531817436 norm:0.0001660545531194657 max memory_allocated 47478.8544921875 
[2025-03-21 13:56:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 18 loss:0.04789505526423454 norm:0.0001537674106657505 max memory_allocated 47478.8544921875 
[2025-03-21 13:57:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 19 loss:0.047850411385297775 norm:0.00015132850967347622 max memory_allocated 47478.8544921875 
[2025-03-21 14:01:03 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [23, 24, 25]
[2025-03-21 14:01:03 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [26, 27, 28] ===
[2025-03-21 14:01:04 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 14:02:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 0 loss:0.09160902351140976 norm:0.005421792156994343 max memory_allocated 47479.0419921875 
[2025-03-21 14:03:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 1 loss:0.08559908717870712 norm:0.004016796126961708 max memory_allocated 47479.0419921875 
[2025-03-21 14:05:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 2 loss:0.08078104257583618 norm:0.0030500823631882668 max memory_allocated 47479.0419921875 
[2025-03-21 14:06:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 3 loss:0.07592514902353287 norm:0.0023833136074244976 max memory_allocated 47479.0419921875 
[2025-03-21 14:07:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 4 loss:0.07391180098056793 norm:0.002001621527597308 max memory_allocated 47479.0419921875 
[2025-03-21 14:09:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 5 loss:0.07331008464097977 norm:0.0017054068157449365 max memory_allocated 47479.0419921875 
[2025-03-21 14:10:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 6 loss:0.07296619564294815 norm:0.0014620959991589189 max memory_allocated 47479.0419921875 
[2025-03-21 14:11:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 7 loss:0.07273315638303757 norm:0.0014168821508064866 max memory_allocated 47479.0419921875 
[2025-03-21 14:13:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 8 loss:0.07255437970161438 norm:0.0013963131932541728 max memory_allocated 47479.0419921875 
[2025-03-21 14:14:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 9 loss:0.07244842499494553 norm:0.0012868168996647 max memory_allocated 47479.0419921875 
[2025-03-21 14:16:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 10 loss:0.07226093858480453 norm:0.0013489259872585535 max memory_allocated 47479.0419921875 
[2025-03-21 14:17:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 11 loss:0.07207229733467102 norm:0.0012187997344881296 max memory_allocated 47479.0419921875 
[2025-03-21 14:18:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 12 loss:0.07194530963897705 norm:0.0012093780096620321 max memory_allocated 47479.0419921875 
[2025-03-21 14:20:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 13 loss:0.07182633131742477 norm:0.0011865543201565742 max memory_allocated 47479.0419921875 
[2025-03-21 14:21:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 14 loss:0.07179416716098785 norm:0.0010697486577555537 max memory_allocated 47479.0419921875 
[2025-03-21 14:22:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 15 loss:0.07167337834835052 norm:0.0011769006960093975 max memory_allocated 47479.0419921875 
[2025-03-21 14:24:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 16 loss:0.07164809107780457 norm:0.001009484869427979 max memory_allocated 47479.0419921875 
[2025-03-21 14:25:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 17 loss:0.07152354717254639 norm:0.0011409741127863526 max memory_allocated 47479.0419921875 
[2025-03-21 14:26:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 18 loss:0.07148297131061554 norm:0.0010333894751966 max memory_allocated 47479.0419921875 
[2025-03-21 14:28:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 19 loss:0.0714087039232254 norm:0.0010605365969240665 max memory_allocated 47479.0419921875 
[2025-03-21 14:30:59 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [26, 27, 28]
[2025-03-21 14:30:59 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [29] ===
[2025-03-21 14:30:59 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 14:31:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 0 loss:0.08953682333230972 norm:0.008965983055531979 max memory_allocated 47479.0419921875 
[2025-03-21 14:31:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 1 loss:0.08452612161636353 norm:0.0068540130741894245 max memory_allocated 47479.0419921875 
[2025-03-21 14:32:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 2 loss:0.08097273111343384 norm:0.00432923948392272 max memory_allocated 47479.0419921875 
[2025-03-21 14:32:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 3 loss:0.07877720147371292 norm:0.0033814909402281046 max memory_allocated 47479.0419921875 
[2025-03-21 14:33:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 4 loss:0.07817183434963226 norm:0.002882956527173519 max memory_allocated 47479.0419921875 
[2025-03-21 14:33:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 5 loss:0.07793161273002625 norm:0.0024698544293642044 max memory_allocated 47479.0419921875 
[2025-03-21 14:34:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 6 loss:0.07775512337684631 norm:0.0021465737372636795 max memory_allocated 47479.0419921875 
[2025-03-21 14:34:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 7 loss:0.07763345539569855 norm:0.0018326927674934268 max memory_allocated 47479.0419921875 
[2025-03-21 14:35:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 8 loss:0.07758348435163498 norm:0.0017158420523628592 max memory_allocated 47479.0419921875 
[2025-03-21 14:35:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 9 loss:0.0775211751461029 norm:0.0016823281766846776 max memory_allocated 47479.0419921875 
[2025-03-21 14:36:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 10 loss:0.07745470106601715 norm:0.0016337097622454166 max memory_allocated 47479.0419921875 
[2025-03-21 14:36:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 11 loss:0.07744722068309784 norm:0.0015566216316074133 max memory_allocated 47479.0419921875 
[2025-03-21 14:36:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 12 loss:0.07742786407470703 norm:0.0016171461902558804 max memory_allocated 47479.0419921875 
[2025-03-21 14:37:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 13 loss:0.07745464891195297 norm:0.0013961703516542912 max memory_allocated 47479.0419921875 
[2025-03-21 14:37:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 14 loss:0.07738947868347168 norm:0.0015363260172307491 max memory_allocated 47479.0419921875 
[2025-03-21 14:38:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 15 loss:0.07738840579986572 norm:0.0013981882948428392 max memory_allocated 47479.0419921875 
[2025-03-21 14:38:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 16 loss:0.07734169065952301 norm:0.001436782069504261 max memory_allocated 47479.0419921875 
[2025-03-21 14:39:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 17 loss:0.07731248438358307 norm:0.0012881932780146599 max memory_allocated 47479.0419921875 
[2025-03-21 14:39:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 18 loss:0.07727167755365372 norm:0.0012852172367274761 max memory_allocated 47479.0419921875 
[2025-03-21 14:40:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 19 loss:0.07727529853582382 norm:0.0012117280857637525 max memory_allocated 47479.0419921875 
[2025-03-21 14:41:05 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [29]
[2025-03-21 14:41:05 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [30] ===
[2025-03-21 14:41:05 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 14:41:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 0 loss:0.12641140818595886 norm:0.015413500368595123 max memory_allocated 47479.0419921875 
[2025-03-21 14:42:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 1 loss:0.11141493171453476 norm:0.010857783257961273 max memory_allocated 47479.0419921875 
[2025-03-21 14:42:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 2 loss:0.10408935695886612 norm:0.007683292962610722 max memory_allocated 47479.0419921875 
[2025-03-21 14:42:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 3 loss:0.10043696314096451 norm:0.006245848722755909 max memory_allocated 47479.0419921875 
[2025-03-21 14:43:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 4 loss:0.0993831679224968 norm:0.005351095460355282 max memory_allocated 47479.0419921875 
[2025-03-21 14:43:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 5 loss:0.09881517291069031 norm:0.004726231098175049 max memory_allocated 47479.0419921875 
[2025-03-21 14:44:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 6 loss:0.09846799820661545 norm:0.004296480678021908 max memory_allocated 47479.0419921875 
[2025-03-21 14:44:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 7 loss:0.09824217110872269 norm:0.003792349249124527 max memory_allocated 47479.0419921875 
[2025-03-21 14:45:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 8 loss:0.09803172945976257 norm:0.003374359803274274 max memory_allocated 47479.0419921875 
[2025-03-21 14:45:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 9 loss:0.09784697741270065 norm:0.0029994843062013388 max memory_allocated 47479.0419921875 
[2025-03-21 14:46:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 10 loss:0.09776324033737183 norm:0.002839162014424801 max memory_allocated 47479.0419921875 
[2025-03-21 14:46:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 11 loss:0.09779403358697891 norm:0.0028615219052881002 max memory_allocated 47479.0419921875 
[2025-03-21 14:47:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 12 loss:0.09775508940219879 norm:0.0029143700376152992 max memory_allocated 47479.0419921875 
[2025-03-21 14:47:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 13 loss:0.09772276133298874 norm:0.0027329353615641594 max memory_allocated 47479.0419921875 
[2025-03-21 14:47:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 14 loss:0.09766136109828949 norm:0.0025494927540421486 max memory_allocated 47479.0419921875 
[2025-03-21 14:48:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 15 loss:0.09741001576185226 norm:0.002197801135480404 max memory_allocated 47479.0419921875 
[2025-03-21 14:48:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 16 loss:0.09741304814815521 norm:0.0023355623707175255 max memory_allocated 47479.0419921875 
[2025-03-21 14:49:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 17 loss:0.09741975367069244 norm:0.0023188753984868526 max memory_allocated 47479.0419921875 
[2025-03-21 14:49:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 18 loss:0.0974699929356575 norm:0.002424881560727954 max memory_allocated 47479.0419921875 
[2025-03-21 14:50:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 19 loss:0.09741728007793427 norm:0.0022567128762602806 max memory_allocated 47479.0419921875 
[2025-03-21 14:51:12 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [30]
[2025-03-21 14:51:12 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [31] ===
[2025-03-21 14:51:12 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-21 14:51:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 0 loss:0.19507800042629242 norm:0.01888452097773552 max memory_allocated 47479.0419921875 
[2025-03-21 14:52:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 1 loss:0.18098264932632446 norm:0.014280209317803383 max memory_allocated 47479.0419921875 
[2025-03-21 14:52:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 2 loss:0.17216485738754272 norm:0.010442710481584072 max memory_allocated 47479.0419921875 
[2025-03-21 14:53:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 3 loss:0.16795934736728668 norm:0.00867067463696003 max memory_allocated 47479.0419921875 
[2025-03-21 14:53:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 4 loss:0.1659410297870636 norm:0.0072265747003257275 max memory_allocated 47479.0419921875 
[2025-03-21 14:53:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 5 loss:0.1650385856628418 norm:0.006427631713449955 max memory_allocated 47479.0419921875 
[2025-03-21 14:54:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 6 loss:0.1645023077726364 norm:0.0058077918365597725 max memory_allocated 47479.0419921875 
[2025-03-21 14:54:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 7 loss:0.16418272256851196 norm:0.005452016834169626 max memory_allocated 47479.0419921875 
[2025-03-21 14:55:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 8 loss:0.16381767392158508 norm:0.005028317682445049 max memory_allocated 47479.0419921875 
[2025-03-21 14:55:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 9 loss:0.16366225481033325 norm:0.004992886446416378 max memory_allocated 47479.0419921875 
[2025-03-21 14:56:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 10 loss:0.1636050045490265 norm:0.004991776309907436 max memory_allocated 47479.0419921875 
[2025-03-21 14:56:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 11 loss:0.16349777579307556 norm:0.005062926560640335 max memory_allocated 47479.0419921875 
[2025-03-21 14:57:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 12 loss:0.16335122287273407 norm:0.00489408103749156 max memory_allocated 47479.0419921875 
[2025-03-21 14:57:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 13 loss:0.16341261565685272 norm:0.004819395020604134 max memory_allocated 47479.0419921875 
[2025-03-21 14:58:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 14 loss:0.16318966448307037 norm:0.0046798246912658215 max memory_allocated 47479.0419921875 
[2025-03-21 14:58:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 15 loss:0.1631976068019867 norm:0.004867170006036758 max memory_allocated 47479.0419921875 
[2025-03-21 14:58:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 16 loss:0.16312652826309204 norm:0.004734665621072054 max memory_allocated 47479.0419921875 
[2025-03-21 14:59:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 17 loss:0.16287773847579956 norm:0.004506009165197611 max memory_allocated 47479.0419921875 
[2025-03-21 14:59:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 18 loss:0.16290202736854553 norm:0.004652456380426884 max memory_allocated 47479.0419921875 
[2025-03-21 15:00:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 19 loss:0.16280248761177063 norm:0.004635276738554239 max memory_allocated 47479.0419921875 
[2025-03-21 15:01:16 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [31]
[2025-03-21 15:01:16 root] (main_calib_config3_attn.py 379): INFO 19308.072204351425
[2025-03-21 15:01:26 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-21 15:02:12 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.559484481811523
[2025-03-21 15:02:12 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-21 15:03:24 root] (main_calib_config3_attn.py 161): INFO c4 : 7.079771518707275
[2025-03-21 16:06:19 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.559484481811523, 'c4': 7.079771518707275, 'results': {'arc_easy': {'acc': 0.6994949494949495, 'acc_stderr': 0.009407763090599316, 'acc_norm': 0.5303030303030303, 'acc_norm_stderr': 0.010240923608726533}, 'hellaswag': {'acc': 0.5605457080262896, 'acc_stderr': 0.004953063404791445, 'acc_norm': 0.7228639713204541, 'acc_norm_stderr': 0.0044666950236778285}, 'winogrande': {'acc': 0.6614048934490924, 'acc_stderr': 0.013300169865842414}, 'piqa': {'acc': 0.7823721436343852, 'acc_stderr': 0.009627407474840883, 'acc_norm': 0.7687704026115343, 'acc_norm_stderr': 0.009837063180625334}, 'arc_challenge': {'acc': 0.3967576791808874, 'acc_stderr': 0.014296513020180632, 'acc_norm': 0.39761092150170646, 'acc_norm_stderr': 0.014301752223279538}, 'boolq': {'acc': 0.7134556574923547, 'acc_stderr': 0.00790809634810261}}, 'versions': {'arc_easy': 0, 'hellaswag': 0, 'winogrande': 0, 'piqa': 0, 'arc_challenge': 0, 'boolq': 1}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-21 16:06:19 root] (main_calib_config3_attn.py 175): INFO 39.68,69.95,71.35,56.05,78.24,66.14
