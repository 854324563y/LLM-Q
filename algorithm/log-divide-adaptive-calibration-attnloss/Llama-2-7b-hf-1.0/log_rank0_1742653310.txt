[2025-03-22 14:21:50 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/Llama-2-7b-hf-1.0', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_1.0.pkl', blocks_pkl='./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-22 14:21:58 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-22 14:21:58 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-22 14:21:58 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-22 14:21:58 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_1.0.pkl
[2025-03-22 14:21:58 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 9), (9, 11), (11, 14), (14, 17), (17, 20), (20, 23), (23, 26), (26, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-22 14:21:58 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12, 13], [14, 15, 16], [17, 18, 19], [20, 21, 22], [23, 24, 25], [26, 27, 28], [29], [30], [31]]
[2025-03-22 14:22:01 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-22 14:22:01 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:22:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.0007456542225554585 norm:0.003442796878516674 max memory_allocated 34633.880859375 
[2025-03-22 14:23:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.00026974131469614804 norm:0.0006787611055187881 max memory_allocated 34633.880859375 
[2025-03-22 14:23:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.00024739440414123237 norm:0.0008091313065961003 max memory_allocated 34633.880859375 
[2025-03-22 14:23:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.00024379516253247857 norm:0.0009829784976318479 max memory_allocated 34633.880859375 
[2025-03-22 14:24:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.00023794930893927813 norm:0.0009103274205699563 max memory_allocated 34633.880859375 
[2025-03-22 14:24:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.00023233579122461379 norm:0.0008905748836696148 max memory_allocated 34633.880859375 
[2025-03-22 14:25:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.0002299068437423557 norm:0.000879126600921154 max memory_allocated 34633.880859375 
[2025-03-22 14:25:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.00022295815870165825 norm:0.0007837138255126774 max memory_allocated 34633.880859375 
[2025-03-22 14:26:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.00022328621707856655 norm:0.0007908873958513141 max memory_allocated 34633.880859375 
[2025-03-22 14:26:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.00021876963728573173 norm:0.0007410502294078469 max memory_allocated 34633.880859375 
[2025-03-22 14:27:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.00021612641285173595 norm:0.0007284718449227512 max memory_allocated 34633.880859375 
[2025-03-22 14:27:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.00021370951435528696 norm:0.000701455632224679 max memory_allocated 34633.880859375 
[2025-03-22 14:28:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.0002146250772057101 norm:0.0007173307240009308 max memory_allocated 34633.880859375 
[2025-03-22 14:28:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.00021204049699008465 norm:0.0006614737212657928 max memory_allocated 34633.880859375 
[2025-03-22 14:29:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.00021241730428300798 norm:0.0006603955407626927 max memory_allocated 34633.880859375 
[2025-03-22 14:29:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.00021539110457524657 norm:0.0006771912449039519 max memory_allocated 34633.880859375 
[2025-03-22 14:30:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.0002115341048920527 norm:0.0006414821837097406 max memory_allocated 34633.880859375 
[2025-03-22 14:30:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.00021672183356713504 norm:0.0006818007677793503 max memory_allocated 34633.880859375 
[2025-03-22 14:31:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.00020822370424866676 norm:0.0005545347812585533 max memory_allocated 34633.880859375 
[2025-03-22 14:31:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.00020299172319937497 norm:0.0005234833806753159 max memory_allocated 34633.880859375 
[2025-03-22 14:32:19 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-22 14:32:19 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-22 14:32:19 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:32:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.0027640878688544035 norm:0.004099708981812 max memory_allocated 35100.7724609375 
[2025-03-22 14:33:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.00127169000916183 norm:0.001854646485298872 max memory_allocated 35100.7724609375 
[2025-03-22 14:33:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.0011742530623450875 norm:0.0017671744571998715 max memory_allocated 35100.7724609375 
[2025-03-22 14:34:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.0010857468005269766 norm:0.0015477839624509215 max memory_allocated 35100.7724609375 
[2025-03-22 14:34:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.0010481467470526695 norm:0.0015502605820074677 max memory_allocated 35100.7724609375 
[2025-03-22 14:35:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.0010108011774718761 norm:0.001570274936966598 max memory_allocated 35100.7724609375 
[2025-03-22 14:35:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.0009839492850005627 norm:0.001507606590166688 max memory_allocated 35100.7724609375 
[2025-03-22 14:36:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.0009390247287228703 norm:0.0016758214915171266 max memory_allocated 35100.7724609375 
[2025-03-22 14:36:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.0009166563395410776 norm:0.0015071919187903404 max memory_allocated 35100.7724609375 
[2025-03-22 14:37:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.000876133213751018 norm:0.0014243883779272437 max memory_allocated 35100.7724609375 
[2025-03-22 14:37:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.000920940306968987 norm:0.001631508581340313 max memory_allocated 35100.7724609375 
[2025-03-22 14:38:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.0008274727151729167 norm:0.001314847031608224 max memory_allocated 35100.7724609375 
[2025-03-22 14:38:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.0008751460118219256 norm:0.0013268962502479553 max memory_allocated 35100.7724609375 
[2025-03-22 14:39:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.0013714354718104005 norm:0.00239107059314847 max memory_allocated 35100.7724609375 
[2025-03-22 14:39:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.0008727395907044411 norm:0.001362288254313171 max memory_allocated 35100.7724609375 
[2025-03-22 14:40:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.0008444502600468695 norm:0.001220580656081438 max memory_allocated 35100.7724609375 
[2025-03-22 14:40:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.0008414678159169853 norm:0.001162181724794209 max memory_allocated 35100.7724609375 
[2025-03-22 14:41:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.0008410318987444043 norm:0.0011512769851833582 max memory_allocated 35100.7724609375 
[2025-03-22 14:41:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.0008408526773564517 norm:0.0011642896570265293 max memory_allocated 35100.7724609375 
[2025-03-22 14:42:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.001040664385072887 norm:0.001513174967840314 max memory_allocated 35100.7724609375 
[2025-03-22 14:42:39 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-22 14:42:39 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
[2025-03-22 14:42:39 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:43:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.0021314271725714207 norm:0.0024799073580652475 max memory_allocated 35100.8349609375 
[2025-03-22 14:43:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.001819733763113618 norm:0.0010829187231138349 max memory_allocated 35100.8349609375 
[2025-03-22 14:44:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.001770071336068213 norm:0.0013259222032502294 max memory_allocated 35100.8349609375 
[2025-03-22 14:44:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.001705945935100317 norm:0.0012510510860010982 max memory_allocated 35100.8349609375 
[2025-03-22 14:45:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.0016047611134126782 norm:0.0011356731411069632 max memory_allocated 35100.8349609375 
[2025-03-22 14:45:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.0014853805769234896 norm:0.001014728331938386 max memory_allocated 35100.8349609375 
[2025-03-22 14:46:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.0014012762112542987 norm:0.0009313247282989323 max memory_allocated 35100.8349609375 
[2025-03-22 14:46:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.0013727574842050672 norm:0.0008964224252849817 max memory_allocated 35100.8349609375 
[2025-03-22 14:47:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.0013685785233974457 norm:0.0008619574946351349 max memory_allocated 35100.8349609375 
[2025-03-22 14:47:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.0013663239078596234 norm:0.00082387775182724 max memory_allocated 35100.8349609375 
[2025-03-22 14:47:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.0013658202951774001 norm:0.0007918177871033549 max memory_allocated 35100.8349609375 
[2025-03-22 14:48:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.001362612470984459 norm:0.0007572260219603777 max memory_allocated 35100.8349609375 
[2025-03-22 14:48:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.001360913272947073 norm:0.00071388587821275 max memory_allocated 35100.8349609375 
[2025-03-22 14:49:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.0013591923052445054 norm:0.0006770617910660803 max memory_allocated 35100.8349609375 
[2025-03-22 14:49:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.0013561260420829058 norm:0.0006251099985092878 max memory_allocated 35100.8349609375 
[2025-03-22 14:50:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.0013546508271247149 norm:0.0005883743287995458 max memory_allocated 35100.8349609375 
[2025-03-22 14:50:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.0013528179842978716 norm:0.0005523792933672667 max memory_allocated 35100.8349609375 
[2025-03-22 14:51:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.0013524469686672091 norm:0.0005149409989826381 max memory_allocated 35100.8349609375 
[2025-03-22 14:51:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.0013496688334271312 norm:0.00047723003081046045 max memory_allocated 35100.8349609375 
[2025-03-22 14:52:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.001347866840660572 norm:0.00043472304241731763 max memory_allocated 35100.8349609375 
[2025-03-22 14:52:57 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2]
[2025-03-22 14:52:57 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4, 5] ===
[2025-03-22 14:54:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 0 loss:0.005608881823718548 norm:0.00010192918125540018 max memory_allocated 47477.6044921875 
[2025-03-22 14:55:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 1 loss:0.005247163120657206 norm:7.31759937480092e-05 max memory_allocated 47477.6044921875 
[2025-03-22 14:57:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 2 loss:0.004973491188138723 norm:6.095850039855577e-05 max memory_allocated 47477.6044921875 
[2025-03-22 14:58:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 3 loss:0.004652000032365322 norm:5.360662180464715e-05 max memory_allocated 47477.6044921875 
[2025-03-22 15:00:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 4 loss:0.004268201533704996 norm:5.1924715080531314e-05 max memory_allocated 47477.6044921875 
[2025-03-22 15:01:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 5 loss:0.00394976744428277 norm:5.438271182356402e-05 max memory_allocated 47477.6044921875 
[2025-03-22 15:03:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 6 loss:0.0037592295557260513 norm:5.869235974387266e-05 max memory_allocated 47477.6044921875 
[2025-03-22 15:04:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 7 loss:0.0036764435935765505 norm:7.09981395630166e-05 max memory_allocated 47477.6044921875 
[2025-03-22 15:05:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 8 loss:0.0036498485133051872 norm:7.734453538432717e-05 max memory_allocated 47477.6044921875 
[2025-03-22 15:07:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 9 loss:0.0036423171404749155 norm:8.466096187476069e-05 max memory_allocated 47477.6044921875 
[2025-03-22 15:08:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 10 loss:0.0036411003675311804 norm:8.758980402490124e-05 max memory_allocated 47477.6044921875 
[2025-03-22 15:10:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 11 loss:0.0036395422648638487 norm:8.663119660923257e-05 max memory_allocated 47477.6044921875 
[2025-03-22 15:11:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 12 loss:0.0036310222931206226 norm:8.149784116540104e-05 max memory_allocated 47477.6044921875 
[2025-03-22 15:13:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 13 loss:0.003628639504313469 norm:8.227297803387046e-05 max memory_allocated 47477.6044921875 
[2025-03-22 15:14:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 14 loss:0.0036315254401415586 norm:9.03298714547418e-05 max memory_allocated 47477.6044921875 
[2025-03-22 15:16:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 15 loss:0.003628437640145421 norm:8.082658314378932e-05 max memory_allocated 47477.6044921875 
[2025-03-22 15:17:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 16 loss:0.0036242606583982706 norm:8.455327770207077e-05 max memory_allocated 47477.6044921875 
[2025-03-22 15:18:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 17 loss:0.0036219495814293623 norm:7.901018398115411e-05 max memory_allocated 47477.6044921875 
[2025-03-22 15:20:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 18 loss:0.0036220194306224585 norm:9.119683818425983e-05 max memory_allocated 47477.6044921875 
[2025-03-22 15:21:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 19 loss:0.0036200634203851223 norm:9.026808402268216e-05 max memory_allocated 47477.6044921875 
[2025-03-22 15:23:36 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [3, 4, 5]
[2025-03-22 15:23:36 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [6, 7, 8] ===
[2025-03-22 15:25:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 0 loss:0.007127614226192236 norm:0.00012537377187982202 max memory_allocated 47477.7919921875 
[2025-03-22 15:26:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 1 loss:0.006615156773477793 norm:9.737100481288508e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:28:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 2 loss:0.006253420375287533 norm:8.135004463838413e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:29:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 3 loss:0.005852147005498409 norm:7.219521648949012e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:30:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 4 loss:0.005439949221909046 norm:6.100467362557538e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:32:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 5 loss:0.005135081708431244 norm:6.60863152006641e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:33:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 6 loss:0.004950660280883312 norm:5.994723687763326e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:35:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 7 loss:0.004861216992139816 norm:6.201964424690232e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:36:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 8 loss:0.004815802443772554 norm:5.58401552552823e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:38:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 9 loss:0.004801001865416765 norm:6.24401363893412e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:39:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 10 loss:0.004780780524015427 norm:5.074821092421189e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:40:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 11 loss:0.004768603015691042 norm:4.982041355106048e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:42:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 12 loss:0.0047632488422095776 norm:4.7029163397382945e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:43:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 13 loss:0.004753729794174433 norm:4.592441109707579e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:45:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 14 loss:0.004754827357828617 norm:4.78310976177454e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:46:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 15 loss:0.004750601015985012 norm:4.728579369839281e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:48:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 16 loss:0.0047476449981331825 norm:4.7689587518107146e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:49:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 17 loss:0.004748017061501741 norm:4.935228207614273e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:51:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 18 loss:0.004744630306959152 norm:4.627331145456992e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:52:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 19 loss:0.004741569049656391 norm:4.694441668107174e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:54:16 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [6, 7, 8]
[2025-03-22 15:54:16 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [9, 10] ===
[2025-03-22 15:55:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 0 loss:0.006266559474170208 norm:6.636977195739746e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:56:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 1 loss:0.0059145791456103325 norm:4.3991825805278495e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:57:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 2 loss:0.0056765093468129635 norm:3.107347947661765e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:58:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 3 loss:0.005402803421020508 norm:2.6940604584524408e-05 max memory_allocated 47477.7919921875 
[2025-03-22 15:59:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 4 loss:0.00513184629380703 norm:2.627028698043432e-05 max memory_allocated 47477.7919921875 
[2025-03-22 16:00:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 5 loss:0.004925915040075779 norm:2.482134004822001e-05 max memory_allocated 47477.7919921875 
[2025-03-22 16:01:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 6 loss:0.004808992613106966 norm:2.3146190869738348e-05 max memory_allocated 47477.7919921875 
[2025-03-22 16:02:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 7 loss:0.00475959200412035 norm:2.3471897293347865e-05 max memory_allocated 47477.7919921875 
[2025-03-22 16:02:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 8 loss:0.004747319500893354 norm:2.3606386093888432e-05 max memory_allocated 47477.7919921875 
[2025-03-22 16:03:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 9 loss:0.004740061704069376 norm:2.346753899473697e-05 max memory_allocated 47477.7919921875 
[2025-03-22 16:04:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 10 loss:0.004734741523861885 norm:2.3123473511077464e-05 max memory_allocated 47477.7919921875 
[2025-03-22 16:05:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 11 loss:0.004730977583676577 norm:2.3129716282710433e-05 max memory_allocated 47477.7919921875 
[2025-03-22 16:06:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 12 loss:0.004725131206214428 norm:2.33946066146018e-05 max memory_allocated 47477.7919921875 
[2025-03-22 16:07:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 13 loss:0.004722896963357925 norm:2.2885036742081866e-05 max memory_allocated 47477.7919921875 
[2025-03-22 16:08:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 14 loss:0.004719406832009554 norm:2.2901185730006546e-05 max memory_allocated 47477.7919921875 
[2025-03-22 16:09:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 15 loss:0.004715706687420607 norm:2.1688425476895645e-05 max memory_allocated 47477.7919921875 
[2025-03-22 16:10:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 16 loss:0.004716572817414999 norm:2.304863119206857e-05 max memory_allocated 47477.7919921875 
[2025-03-22 16:11:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 17 loss:0.004712903406471014 norm:2.2387988792615943e-05 max memory_allocated 47477.7919921875 
[2025-03-22 16:12:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 18 loss:0.00471280375495553 norm:2.2570880901184864e-05 max memory_allocated 47477.7919921875 
[2025-03-22 16:13:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 19 loss:0.004710087552666664 norm:2.1450192434713244e-05 max memory_allocated 47477.7919921875 
[2025-03-22 16:14:48 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [9, 10]
[2025-03-22 16:14:49 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [11, 12, 13] ===
[2025-03-22 16:16:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 0 loss:0.0074006337672472 norm:7.765056216157973e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:17:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 1 loss:0.00694133248180151 norm:4.360457751317881e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:19:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 2 loss:0.006654415279626846 norm:3.378468318260275e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:20:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 3 loss:0.006369166076183319 norm:2.8535931051010266e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:22:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 4 loss:0.0060804844833910465 norm:2.7175887225894257e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:23:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 5 loss:0.005828102119266987 norm:2.5015950086526573e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:24:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 6 loss:0.005653674714267254 norm:2.522013892303221e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:26:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 7 loss:0.005545994266867638 norm:2.4370559913222678e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:27:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 8 loss:0.005486107897013426 norm:2.4442259018542245e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:29:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 9 loss:0.005454585887491703 norm:2.358335768803954e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:30:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 10 loss:0.005439070984721184 norm:2.3036187485558912e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:32:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 11 loss:0.005429619923233986 norm:2.3506077923229896e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:33:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 12 loss:0.005420281086117029 norm:2.239804780401755e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:35:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 13 loss:0.005416455212980509 norm:2.3611220967723057e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:36:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 14 loss:0.005410848185420036 norm:2.262559974042233e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:37:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 15 loss:0.00540927005931735 norm:2.2095431631896645e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:39:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 16 loss:0.005404825322329998 norm:2.2056239686207846e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:40:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 17 loss:0.0054038213565945625 norm:2.2585958504350856e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:42:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 18 loss:0.005402877926826477 norm:2.2616444766754285e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:43:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 19 loss:0.005399486981332302 norm:2.3099826648831367e-05 max memory_allocated 47478.1044921875 
[2025-03-22 16:45:34 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [11, 12, 13]
[2025-03-22 16:45:34 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [14, 15, 16] ===
[2025-03-22 16:47:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 0 loss:0.007659568451344967 norm:5.870869063073769e-05 max memory_allocated 47478.2919921875 
[2025-03-22 16:48:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 1 loss:0.007222466636449099 norm:3.860095239360817e-05 max memory_allocated 47478.2919921875 
[2025-03-22 16:49:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 2 loss:0.006919534876942635 norm:2.6449113647686318e-05 max memory_allocated 47478.2919921875 
[2025-03-22 16:51:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 3 loss:0.006633215118199587 norm:2.1423025827971287e-05 max memory_allocated 47478.2919921875 
[2025-03-22 16:52:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 4 loss:0.006343391723930836 norm:1.95456941582961e-05 max memory_allocated 47478.2919921875 
[2025-03-22 16:54:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 5 loss:0.006083184387534857 norm:1.911218714667484e-05 max memory_allocated 47478.2919921875 
[2025-03-22 16:55:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 6 loss:0.005906717386096716 norm:1.8370819816482253e-05 max memory_allocated 47478.2919921875 
[2025-03-22 16:57:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 7 loss:0.005801481194794178 norm:1.840477852965705e-05 max memory_allocated 47478.2919921875 
[2025-03-22 16:58:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 8 loss:0.005747658666223288 norm:1.7724611097946763e-05 max memory_allocated 47478.2919921875 
[2025-03-22 17:00:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 9 loss:0.005713903810828924 norm:1.7878453945741057e-05 max memory_allocated 47478.2919921875 
[2025-03-22 17:01:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 10 loss:0.005696628242731094 norm:1.8029737475444563e-05 max memory_allocated 47478.2919921875 
[2025-03-22 17:02:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 11 loss:0.005687268450856209 norm:1.7731686966726556e-05 max memory_allocated 47478.2919921875 
[2025-03-22 17:04:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 12 loss:0.005682019982486963 norm:1.7861559172160923e-05 max memory_allocated 47478.2919921875 
[2025-03-22 17:05:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 13 loss:0.005677094683051109 norm:1.782078470569104e-05 max memory_allocated 47478.2919921875 
[2025-03-22 17:07:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 14 loss:0.005672319792211056 norm:1.804304883989971e-05 max memory_allocated 47478.2919921875 
[2025-03-22 17:08:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 15 loss:0.005667736753821373 norm:1.7494074199930765e-05 max memory_allocated 47478.2919921875 
[2025-03-22 17:10:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 16 loss:0.005668099969625473 norm:1.8049795471597463e-05 max memory_allocated 47478.2919921875 
[2025-03-22 17:11:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 17 loss:0.005664006341248751 norm:1.787048313417472e-05 max memory_allocated 47478.2919921875 
[2025-03-22 17:12:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 18 loss:0.005664519965648651 norm:1.7797636246541515e-05 max memory_allocated 47478.2919921875 
[2025-03-22 17:14:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 19 loss:0.0056623127311468124 norm:1.7619218851905316e-05 max memory_allocated 47478.2919921875 
[2025-03-22 17:16:15 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [14, 15, 16]
[2025-03-22 17:16:15 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [17, 18, 19] ===
[2025-03-22 17:17:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 0 loss:0.009174641221761703 norm:4.600939428200945e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:19:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 1 loss:0.00882269348949194 norm:3.2434876629849896e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:20:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 2 loss:0.008521074429154396 norm:2.5420993551961146e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:22:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 3 loss:0.008150957524776459 norm:2.1120808014529757e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:23:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 4 loss:0.007710748817771673 norm:1.956095547939185e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:24:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 5 loss:0.007365160621702671 norm:1.8669612472876906e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:26:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 6 loss:0.0071954550221562386 norm:1.788343615771737e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:27:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 7 loss:0.007129942532628775 norm:1.7284401110373437e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:29:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 8 loss:0.007102573290467262 norm:1.6966998373391107e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:30:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 9 loss:0.00709160091355443 norm:1.6868907550815493e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:32:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 10 loss:0.007084236945956945 norm:1.672996768320445e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:33:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 11 loss:0.007079789415001869 norm:1.6606769349891692e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:35:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 12 loss:0.007074242457747459 norm:1.6609421436442062e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:36:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 13 loss:0.007072019390761852 norm:1.613628410268575e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:37:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 14 loss:0.0070695290341973305 norm:1.6104286260087974e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:39:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 15 loss:0.0070666540414094925 norm:1.6016787412809208e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:40:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 16 loss:0.007065108045935631 norm:1.5666588296880946e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:42:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 17 loss:0.007063767872750759 norm:1.592489206814207e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:43:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 18 loss:0.007061230484396219 norm:1.581465949129779e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:45:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 19 loss:0.007059822324663401 norm:1.5799458196852356e-05 max memory_allocated 47478.4794921875 
[2025-03-22 17:46:55 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [17, 18, 19]
[2025-03-22 17:46:55 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [20, 21, 22] ===
[2025-03-22 17:48:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 0 loss:0.012704810127615929 norm:4.381533290143125e-05 max memory_allocated 47478.6669921875 
[2025-03-22 17:49:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 1 loss:0.012348483316600323 norm:3.198215563315898e-05 max memory_allocated 47478.6669921875 
[2025-03-22 17:51:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 2 loss:0.011964302510023117 norm:2.7360547392163426e-05 max memory_allocated 47478.6669921875 
[2025-03-22 17:52:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 3 loss:0.011371203698217869 norm:2.3367781977867708e-05 max memory_allocated 47478.6669921875 
[2025-03-22 17:54:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 4 loss:0.010690592229366302 norm:2.3748169041937217e-05 max memory_allocated 47478.6669921875 
[2025-03-22 17:55:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 5 loss:0.010352995246648788 norm:2.2073660147725604e-05 max memory_allocated 47478.6669921875 
[2025-03-22 17:57:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 6 loss:0.010274693369865417 norm:2.2535470634466037e-05 max memory_allocated 47478.6669921875 
[2025-03-22 17:58:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 7 loss:0.01025583315640688 norm:2.2083908334025182e-05 max memory_allocated 47478.6669921875 
[2025-03-22 17:59:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 8 loss:0.010244958102703094 norm:2.1270830984576605e-05 max memory_allocated 47478.6669921875 
[2025-03-22 18:01:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 9 loss:0.010241534560918808 norm:2.199260052293539e-05 max memory_allocated 47478.6669921875 
[2025-03-22 18:02:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 10 loss:0.010236123576760292 norm:2.1446528990054503e-05 max memory_allocated 47478.6669921875 
[2025-03-22 18:04:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 11 loss:0.010232893750071526 norm:2.1284693502821028e-05 max memory_allocated 47478.6669921875 
[2025-03-22 18:05:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 12 loss:0.010229649022221565 norm:2.112487709382549e-05 max memory_allocated 47478.6669921875 
[2025-03-22 18:07:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 13 loss:0.01022441778331995 norm:2.0389996279845946e-05 max memory_allocated 47478.6669921875 
[2025-03-22 18:08:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 14 loss:0.010219898074865341 norm:2.0852379748248495e-05 max memory_allocated 47478.6669921875 
[2025-03-22 18:10:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 15 loss:0.010216380469501019 norm:2.0503259293036535e-05 max memory_allocated 47478.6669921875 
[2025-03-22 18:11:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 16 loss:0.010215427726507187 norm:2.0868645151495002e-05 max memory_allocated 47478.6669921875 
[2025-03-22 18:12:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 17 loss:0.010211430490016937 norm:2.043409585894551e-05 max memory_allocated 47478.6669921875 
[2025-03-22 18:14:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 18 loss:0.010207273066043854 norm:2.0525611034827307e-05 max memory_allocated 47478.6669921875 
[2025-03-22 18:15:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 19 loss:0.010204959660768509 norm:2.0865763872279786e-05 max memory_allocated 47478.6669921875 
[2025-03-22 18:17:34 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [20, 21, 22]
[2025-03-22 18:17:34 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [23, 24, 25] ===
[2025-03-22 18:19:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 0 loss:0.018902430310845375 norm:6.165401282487437e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:20:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 1 loss:0.01838293857872486 norm:5.1212449761806056e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:21:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 2 loss:0.017699074000120163 norm:4.316962440498173e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:23:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 3 loss:0.016550429165363312 norm:4.010665725218132e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:24:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 4 loss:0.015782387927174568 norm:4.004550646641292e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:26:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 5 loss:0.015672367066144943 norm:3.7616358895320445e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:27:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 6 loss:0.015658218413591385 norm:3.56398886651732e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:29:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 7 loss:0.01564701646566391 norm:3.500274760881439e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:30:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 8 loss:0.015637030825018883 norm:3.320573887322098e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:32:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 9 loss:0.015630273148417473 norm:3.3141572203021497e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:33:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 10 loss:0.015622743405401707 norm:3.3857002563308924e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:34:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 11 loss:0.01561744511127472 norm:3.337824819027446e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:36:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 12 loss:0.015620039775967598 norm:3.331976040499285e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:37:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 13 loss:0.015611216425895691 norm:3.2547621231060475e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:39:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 14 loss:0.015605824999511242 norm:3.1828076316742226e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:40:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 15 loss:0.015605214051902294 norm:3.355039734742604e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:42:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 16 loss:0.015600555576384068 norm:3.1267911253962666e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:43:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 17 loss:0.015594900585711002 norm:3.1203941034618765e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:44:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 18 loss:0.01559496857225895 norm:2.9802691642544232e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:46:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 19 loss:0.015589749440550804 norm:3.207572808605619e-05 max memory_allocated 47478.8544921875 
[2025-03-22 18:48:14 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [23, 24, 25]
[2025-03-22 18:48:14 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [26, 27, 28] ===
[2025-03-22 18:48:15 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 18:49:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 0 loss:0.029079463332891464 norm:0.001313850050792098 max memory_allocated 47479.0419921875 
[2025-03-22 18:51:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 1 loss:0.028066646307706833 norm:0.0009938093135133386 max memory_allocated 47479.0419921875 
[2025-03-22 18:52:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 2 loss:0.026627203449606895 norm:0.0008976663229987025 max memory_allocated 47479.0419921875 
[2025-03-22 18:54:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 3 loss:0.024930071085691452 norm:0.0006854170351289213 max memory_allocated 47479.0419921875 
[2025-03-22 18:55:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 4 loss:0.02449151873588562 norm:0.0005699497414752841 max memory_allocated 47479.0419921875 
[2025-03-22 18:56:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 5 loss:0.024463606998324394 norm:0.0005961962742730975 max memory_allocated 47479.0419921875 
[2025-03-22 18:58:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 6 loss:0.024446960538625717 norm:0.0005496758385561407 max memory_allocated 47479.0419921875 
[2025-03-22 18:59:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 7 loss:0.02442329004406929 norm:0.00045899127144366503 max memory_allocated 47479.0419921875 
[2025-03-22 19:01:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 8 loss:0.02439720742404461 norm:0.00042581016896292567 max memory_allocated 47479.0419921875 
[2025-03-22 19:02:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 9 loss:0.024464165791869164 norm:0.0005272444686852396 max memory_allocated 47479.0419921875 
[2025-03-22 19:04:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 10 loss:0.024526117369532585 norm:0.0006693424074910581 max memory_allocated 47479.0419921875 
[2025-03-22 19:05:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 11 loss:0.024361586198210716 norm:0.00037152619916014373 max memory_allocated 47479.0419921875 
[2025-03-22 19:07:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 12 loss:0.024335838854312897 norm:0.00031100038904696703 max memory_allocated 47479.0419921875 
[2025-03-22 19:08:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 13 loss:0.024319574236869812 norm:0.0002909478498622775 max memory_allocated 47479.0419921875 
[2025-03-22 19:09:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 14 loss:0.024310536682605743 norm:0.0002756055910140276 max memory_allocated 47479.0419921875 
[2025-03-22 19:11:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 15 loss:0.0243077389895916 norm:0.0002645376662258059 max memory_allocated 47479.0419921875 
[2025-03-22 19:12:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 16 loss:0.02430155873298645 norm:0.0002603541361168027 max memory_allocated 47479.0419921875 
[2025-03-22 19:14:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 17 loss:0.024297650903463364 norm:0.0002548798220232129 max memory_allocated 47479.0419921875 
[2025-03-22 19:15:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 18 loss:0.024296266958117485 norm:0.0002586499322205782 max memory_allocated 47479.0419921875 
[2025-03-22 19:17:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 19 loss:0.024292591959238052 norm:0.00026017261552624404 max memory_allocated 47479.0419921875 
[2025-03-22 19:19:04 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [26, 27, 28]
[2025-03-22 19:19:04 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [29] ===
[2025-03-22 19:19:04 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:19:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 0 loss:0.028554115444421768 norm:0.0013782463502138853 max memory_allocated 47479.0419921875 
[2025-03-22 19:20:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 1 loss:0.0275298859924078 norm:0.00095779774710536 max memory_allocated 47479.0419921875 
[2025-03-22 19:20:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 2 loss:0.026650356128811836 norm:0.0008126168977469206 max memory_allocated 47479.0419921875 
[2025-03-22 19:21:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 3 loss:0.026048852130770683 norm:0.0006664573447778821 max memory_allocated 47479.0419921875 
[2025-03-22 19:21:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 4 loss:0.025962192565202713 norm:0.0005954308435320854 max memory_allocated 47479.0419921875 
[2025-03-22 19:21:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 5 loss:0.025952007621526718 norm:0.0005359422648325562 max memory_allocated 47479.0419921875 
[2025-03-22 19:22:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 6 loss:0.02593807503581047 norm:0.0004920890787616372 max memory_allocated 47479.0419921875 
[2025-03-22 19:22:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 7 loss:0.025925427675247192 norm:0.00042468603351153433 max memory_allocated 47479.0419921875 
[2025-03-22 19:23:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 8 loss:0.025925353169441223 norm:0.0003786023589782417 max memory_allocated 47479.0419921875 
[2025-03-22 19:23:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 9 loss:0.02591736800968647 norm:0.0003607738472055644 max memory_allocated 47479.0419921875 
[2025-03-22 19:24:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 10 loss:0.02592761628329754 norm:0.0003638769849203527 max memory_allocated 47479.0419921875 
[2025-03-22 19:24:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 11 loss:0.025927437469363213 norm:0.00030715318280272186 max memory_allocated 47479.0419921875 
[2025-03-22 19:25:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 12 loss:0.025924284011125565 norm:0.00031677563674747944 max memory_allocated 47479.0419921875 
[2025-03-22 19:25:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 13 loss:0.02593258023262024 norm:0.0003364990698173642 max memory_allocated 47479.0419921875 
[2025-03-22 19:26:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 14 loss:0.02592751942574978 norm:0.0003452388336881995 max memory_allocated 47479.0419921875 
[2025-03-22 19:26:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 15 loss:0.025901515036821365 norm:0.00028517318423837423 max memory_allocated 47479.0419921875 
[2025-03-22 19:27:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 16 loss:0.02588777430355549 norm:0.0002469198952894658 max memory_allocated 47479.0419921875 
[2025-03-22 19:27:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 17 loss:0.025881649926304817 norm:0.000251442426815629 max memory_allocated 47479.0419921875 
[2025-03-22 19:28:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 18 loss:0.025879809632897377 norm:0.00023067116853781044 max memory_allocated 47479.0419921875 
[2025-03-22 19:28:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 19 loss:0.02589358203113079 norm:0.00026198712293989956 max memory_allocated 47479.0419921875 
[2025-03-22 19:29:22 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [29]
[2025-03-22 19:29:22 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [30] ===
[2025-03-22 19:29:22 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:29:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 0 loss:0.039046138525009155 norm:0.002799693029373884 max memory_allocated 47479.0419921875 
[2025-03-22 19:30:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 1 loss:0.034974969923496246 norm:0.0020601486321538687 max memory_allocated 47479.0419921875 
[2025-03-22 19:30:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 2 loss:0.03293071687221527 norm:0.0015088387299329042 max memory_allocated 47479.0419921875 
[2025-03-22 19:31:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 3 loss:0.032254189252853394 norm:0.001421445980668068 max memory_allocated 47479.0419921875 
[2025-03-22 19:31:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 4 loss:0.03200680390000343 norm:0.0012015055399388075 max memory_allocated 47479.0419921875 
[2025-03-22 19:32:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 5 loss:0.03197319433093071 norm:0.0011296418961137533 max memory_allocated 47479.0419921875 
[2025-03-22 19:32:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 6 loss:0.03197566047310829 norm:0.0009701842209324241 max memory_allocated 47479.0419921875 
[2025-03-22 19:33:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 7 loss:0.03186619281768799 norm:0.0008319000480696559 max memory_allocated 47479.0419921875 
[2025-03-22 19:33:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 8 loss:0.031828057020902634 norm:0.000749992614146322 max memory_allocated 47479.0419921875 
[2025-03-22 19:34:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 9 loss:0.031801413744688034 norm:0.000711559783667326 max memory_allocated 47479.0419921875 
[2025-03-22 19:34:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 10 loss:0.0318300798535347 norm:0.0007449708064086735 max memory_allocated 47479.0419921875 
[2025-03-22 19:35:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 11 loss:0.031846266239881516 norm:0.0007637684466317296 max memory_allocated 47479.0419921875 
[2025-03-22 19:35:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 12 loss:0.03183961659669876 norm:0.0007477178587578237 max memory_allocated 47479.0419921875 
[2025-03-22 19:36:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 13 loss:0.03178403154015541 norm:0.0005516285309568048 max memory_allocated 47479.0419921875 
[2025-03-22 19:36:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 14 loss:0.03173426538705826 norm:0.0005163552123121917 max memory_allocated 47479.0419921875 
[2025-03-22 19:37:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 15 loss:0.03171496093273163 norm:0.0004945507971569896 max memory_allocated 47479.0419921875 
[2025-03-22 19:37:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 16 loss:0.03171294927597046 norm:0.0004921957734040916 max memory_allocated 47479.0419921875 
[2025-03-22 19:38:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 17 loss:0.031718309968709946 norm:0.0005156679544597864 max memory_allocated 47479.0419921875 
[2025-03-22 19:38:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 18 loss:0.03171824663877487 norm:0.00048528105253353715 max memory_allocated 47479.0419921875 
[2025-03-22 19:39:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 19 loss:0.03172138333320618 norm:0.000468477257527411 max memory_allocated 47479.0419921875 
[2025-03-22 19:39:43 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [30]
[2025-03-22 19:39:43 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [31] ===
[2025-03-22 19:39:43 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:40:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 0 loss:0.06035882234573364 norm:0.006291293539106846 max memory_allocated 47479.0419921875 
[2025-03-22 19:40:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 1 loss:0.056924134492874146 norm:0.0043480959720909595 max memory_allocated 47479.0419921875 
[2025-03-22 19:41:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 2 loss:0.05457933247089386 norm:0.003527121152728796 max memory_allocated 47479.0419921875 
[2025-03-22 19:41:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 3 loss:0.05371609702706337 norm:0.0031154968310147524 max memory_allocated 47479.0419921875 
[2025-03-22 19:42:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 4 loss:0.05342046171426773 norm:0.002686354098841548 max memory_allocated 47479.0419921875 
[2025-03-22 19:42:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 5 loss:0.05332794412970543 norm:0.002543205162510276 max memory_allocated 47479.0419921875 
[2025-03-22 19:43:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 6 loss:0.053196001797914505 norm:0.0022816690616309643 max memory_allocated 47479.0419921875 
[2025-03-22 19:43:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 7 loss:0.053168997168540955 norm:0.002173532033339143 max memory_allocated 47479.0419921875 
[2025-03-22 19:44:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 8 loss:0.05327825993299484 norm:0.002465912140905857 max memory_allocated 47479.0419921875 
[2025-03-22 19:44:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 9 loss:0.05310964584350586 norm:0.002127767773345113 max memory_allocated 47479.0419921875 
[2025-03-22 19:45:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 10 loss:0.053063251078128815 norm:0.0021489495411515236 max memory_allocated 47479.0419921875 
[2025-03-22 19:45:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 11 loss:0.05301651731133461 norm:0.0020293574780225754 max memory_allocated 47479.0419921875 
[2025-03-22 19:46:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 12 loss:0.05302192270755768 norm:0.0020833287853747606 max memory_allocated 47479.0419921875 
[2025-03-22 19:46:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 13 loss:0.052969470620155334 norm:0.0019818972796201706 max memory_allocated 47479.0419921875 
[2025-03-22 19:47:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 14 loss:0.05291232466697693 norm:0.0018810802139341831 max memory_allocated 47479.0419921875 
[2025-03-22 19:47:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 15 loss:0.05291834473609924 norm:0.0018157378071919084 max memory_allocated 47479.0419921875 
[2025-03-22 19:47:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 16 loss:0.05286683887243271 norm:0.0018196807941421866 max memory_allocated 47479.0419921875 
[2025-03-22 19:48:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 17 loss:0.05276431888341904 norm:0.0016289821360260248 max memory_allocated 47479.0419921875 
[2025-03-22 19:48:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 18 loss:0.05278320610523224 norm:0.0017103466670960188 max memory_allocated 47479.0419921875 
[2025-03-22 19:49:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 19 loss:0.05277411639690399 norm:0.0016118743224069476 max memory_allocated 47479.0419921875 
[2025-03-22 19:50:09 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [31]
[2025-03-22 19:50:09 root] (main_calib_config3_attn.py 379): INFO 19690.717374563217
[2025-03-22 19:50:15 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-22 19:51:06 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.499436378479004
[2025-03-22 19:51:06 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-22 19:52:26 root] (main_calib_config3_attn.py 161): INFO c4 : 7.005956172943115
[2025-03-22 20:52:04 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.499436378479004, 'c4': 7.005956172943115, 'results': {'arc_challenge': {'acc': 0.4087030716723549, 'acc_stderr': 0.014365750345427006, 'acc_norm': 0.40187713310580203, 'acc_norm_stderr': 0.014327268614578274}, 'piqa': {'acc': 0.7856365614798694, 'acc_stderr': 0.009574842136050964, 'acc_norm': 0.7736670293797606, 'acc_norm_stderr': 0.009763294246879418}, 'winogrande': {'acc': 0.6685082872928176, 'acc_stderr': 0.013230397198964655}, 'boolq': {'acc': 0.7192660550458716, 'acc_stderr': 0.00785931664284952}, 'arc_easy': {'acc': 0.6944444444444444, 'acc_stderr': 0.009452181213593468, 'acc_norm': 0.5298821548821548, 'acc_norm_stderr': 0.010241444322886433}, 'hellaswag': {'acc': 0.5658235411272655, 'acc_stderr': 0.004946353590937028, 'acc_norm': 0.7268472415853415, 'acc_norm_stderr': 0.004446680081493753}}, 'versions': {'arc_challenge': 0, 'piqa': 0, 'winogrande': 0, 'boolq': 1, 'arc_easy': 0, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 20:52:04 root] (main_calib_config3_attn.py 175): INFO 40.87,69.44,71.93,56.58,78.56,66.85
