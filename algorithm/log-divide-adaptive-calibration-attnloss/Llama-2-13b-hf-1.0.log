nohup: ignoring input
/opt/conda/envs/abq-llm/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/opt/conda/envs/abq-llm/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/opt/conda/envs/abq-llm/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
['main_calib_config3_attn.py', '--model', '/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', '--epochs', '20', '--output_dir', './log-divide-adaptive-calibration-attnloss/Llama-2-13b-hf-1.0', '--eval_ppl', '--wbits', '4', '--abits', '4', '--let', '--lwc', '--tasks', 'piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', '--compensation_calibration', '--quant_map', 'log-divide-adaptive/Llama-2-13b-hf/quant_map_Llama-2-13b-hf_1.0.pkl', '--blocks_pkl', './log-divide/Llama-2-13b-hf-w4a4/Llama-2-13b-hf_blocks.pkl']
[2025-03-23 02:01:29 root](main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/Llama-2-13b-hf-1.0', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/Llama-2-13b-hf/quant_map_Llama-2-13b-hf_1.0.pkl', blocks_pkl='./log-divide/Llama-2-13b-hf-w4a4/Llama-2-13b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  6.78it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  7.47it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.77it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.60it/s]
vocab size:  32000
[2025-03-23 02:01:31 root](main_calib_config3_attn.py 350): INFO === start quantization ===
/workspace/volume/yangzhe/ABQ-LLM/algorithm/main_calib_config3_attn.py:355: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  dataloader = torch.load(cache_dataloader)
[2025-03-23 02:01:31 root](main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
/workspace/volume/yangzhe/ABQ-LLM/algorithm/main_calib_config3_attn.py:369: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  act_scales = torch.load(args.act_scales)
/workspace/volume/yangzhe/ABQ-LLM/algorithm/main_calib_config3_attn.py:370: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  act_shifts = torch.load(args.act_shifts)
[2025-03-23 02:01:31 root](abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-23 02:01:31 root](abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/Llama-2-13b-hf/quant_map_Llama-2-13b-hf_1.0.pkl
[2025-03-23 02:01:31 root](abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/Llama-2-13b-hf-w4a4/Llama-2-13b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 9), (9, 12), (12, 15), (15, 18), (18, 21), (21, 24), (24, 27), (27, 30), (30, 33), (33, 36), (36, 37), (37, 38), (38, 39), (39, 40)]
[2025-03-23 02:01:31 root](abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17], [18, 19, 20], [21, 22, 23], [24, 25, 26], [27, 28, 29], [30, 31, 32], [33, 34, 35], [36], [37], [38], [39]]
[2025-03-23 02:01:33 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 0 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 0 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 0 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 0 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 0 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 0 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 02:01:34 root](abq_llm_calib_config3_attn.py 309): INFO use compensation vector
/workspace/volume/yangzhe/ABQ-LLM/algorithm/quantize/abq_llm_calib_config3_attn.py:342: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/workspace/volume/yangzhe/ABQ-LLM/algorithm/utils.py:45: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
/opt/conda/envs/abq-llm/lib/python3.10/site-packages/torch/nn/functional.py:3369: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
[2025-03-23 02:02:21 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.0010575376218184829 norm:0.004043658263981342 max memory_allocated 44358.7939453125 
[2025-03-23 02:03:04 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.0002648137742653489 norm:0.00055480602895841 max memory_allocated 44358.7939453125 
[2025-03-23 02:03:47 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.00022921941126696765 norm:0.0008194604888558388 max memory_allocated 44358.7939453125 
[2025-03-23 02:04:31 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.00021621680934913456 norm:0.0013302480801939964 max memory_allocated 44358.7939453125 
[2025-03-23 02:05:14 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.00020632297673728317 norm:0.0013497192412614822 max memory_allocated 44358.7939453125 
[2025-03-23 02:05:58 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.00019878253806382418 norm:0.0012940901797264814 max memory_allocated 44358.7939453125 
[2025-03-23 02:06:42 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.00018802584963850677 norm:0.0011386909754946828 max memory_allocated 44358.7939453125 
[2025-03-23 02:07:25 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.00019550725119188428 norm:0.0012025529285892844 max memory_allocated 44358.7939453125 
[2025-03-23 02:08:09 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0001857266470324248 norm:0.0010867886012420058 max memory_allocated 44358.7939453125 
[2025-03-23 02:08:53 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.00017709135136101395 norm:0.0009620860801078379 max memory_allocated 44358.7939453125 
[2025-03-23 02:09:37 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.00017845979891717434 norm:0.000933528586756438 max memory_allocated 44358.7939453125 
[2025-03-23 02:10:20 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0001676796964602545 norm:0.0008330001146532595 max memory_allocated 44358.7939453125 
[2025-03-23 02:11:04 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.00017246084462385625 norm:0.0008292822749353945 max memory_allocated 44358.7939453125 
[2025-03-23 02:11:48 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.00016666790179442614 norm:0.0007681440911255777 max memory_allocated 44358.7939453125 
[2025-03-23 02:12:31 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.00016172743926290423 norm:0.0006819840637035668 max memory_allocated 44358.7939453125 
[2025-03-23 02:13:15 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.00016147899441421032 norm:0.000634732365142554 max memory_allocated 44358.7939453125 
[2025-03-23 02:13:59 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.00015517948486376554 norm:0.0006016865954734385 max memory_allocated 44358.7939453125 
[2025-03-23 02:14:43 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.00015319345402531326 norm:0.0005589532083831728 max memory_allocated 44358.7939453125 
[2025-03-23 02:15:26 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.00015439765411429107 norm:0.0005349004059098661 max memory_allocated 44358.7939453125 
[2025-03-23 02:16:10 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.00015455298125743866 norm:0.00050703453598544 max memory_allocated 44358.7939453125 
[2025-03-23 02:17:10 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-23 02:17:10 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 1 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 1 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 1 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 1 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 1 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 1 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 02:17:11 root](abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:17:57 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.0010465166997164488 norm:0.0024424174334853888 max memory_allocated 44358.7939453125 
[2025-03-23 02:18:40 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.0006846641190350056 norm:0.0007754231919534504 max memory_allocated 44358.7939453125 
[2025-03-23 02:19:24 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.0006784777506254613 norm:0.001317791873589158 max memory_allocated 44358.7939453125 
[2025-03-23 02:20:08 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.0006684331456199288 norm:0.0012514741392806172 max memory_allocated 44358.7939453125 
[2025-03-23 02:20:51 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.0006577010499313474 norm:0.0011732412967830896 max memory_allocated 44358.7939453125 
[2025-03-23 02:21:35 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.0006469505606219172 norm:0.001089396420866251 max memory_allocated 44358.7939453125 
[2025-03-23 02:22:19 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.0006385810556821525 norm:0.0010306843323633075 max memory_allocated 44358.7939453125 
[2025-03-23 02:23:02 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.0006274365005083382 norm:0.0009627318358980119 max memory_allocated 44358.7939453125 
[2025-03-23 02:23:46 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.0006175637827254832 norm:0.0009063609759323299 max memory_allocated 44358.7939453125 
[2025-03-23 02:24:30 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.0006088565569370985 norm:0.0008584195747971535 max memory_allocated 44358.7939453125 
[2025-03-23 02:25:14 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.0006006296607665718 norm:0.0008292121929116547 max memory_allocated 44358.7939453125 
[2025-03-23 02:25:57 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.0005902001284994185 norm:0.0007766959024593234 max memory_allocated 44358.7939453125 
[2025-03-23 02:26:41 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.0005804337561130524 norm:0.0007383671472780406 max memory_allocated 44358.7939453125 
[2025-03-23 02:27:25 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.0005738838808611035 norm:0.0007129036239348352 max memory_allocated 44358.7939453125 
[2025-03-23 02:28:08 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.0005616752314381301 norm:0.000652928079944104 max memory_allocated 44358.7939453125 
[2025-03-23 02:28:52 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.0005521345301531255 norm:0.0006214709719642997 max memory_allocated 44358.7939453125 
[2025-03-23 02:29:36 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.0005428596632555127 norm:0.0005910914624109864 max memory_allocated 44358.7939453125 
[2025-03-23 02:30:20 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.0005339788622222841 norm:0.0005532066570594907 max memory_allocated 44358.7939453125 
[2025-03-23 02:31:03 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.0005230639362707734 norm:0.0005164037575013936 max memory_allocated 44358.7939453125 
[2025-03-23 02:31:47 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.0005161772714927793 norm:0.00048723703366704285 max memory_allocated 44358.7939453125 
[2025-03-23 02:32:43 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-23 02:32:43 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 2 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 2 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 2 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 2 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 2 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 2 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 02:32:43 root](abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:33:30 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.0016886687371879816 norm:0.002058317419141531 max memory_allocated 44358.7939453125 
[2025-03-23 02:34:14 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.0013759273570030928 norm:0.0007257199613377452 max memory_allocated 44358.7939453125 
[2025-03-23 02:34:57 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.0013557138154283166 norm:0.001126338029280305 max memory_allocated 44358.7939453125 
[2025-03-23 02:35:41 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.0013340163277462125 norm:0.001278855255804956 max memory_allocated 44358.7939453125 
[2025-03-23 02:36:25 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.0013040085323154926 norm:0.0012215602910146117 max memory_allocated 44358.7939453125 
[2025-03-23 02:37:08 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.0012633568840101361 norm:0.0011436383938416839 max memory_allocated 44358.7939453125 
[2025-03-23 02:37:52 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.0012145148357376456 norm:0.00107970938552171 max memory_allocated 44358.7939453125 
[2025-03-23 02:38:36 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.0011548562906682491 norm:0.0010286252945661545 max memory_allocated 44358.7939453125 
[2025-03-23 02:39:20 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.0010952884331345558 norm:0.0009618093608878553 max memory_allocated 44358.7939453125 
[2025-03-23 02:40:03 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.0010492042638361454 norm:0.0009009463828988373 max memory_allocated 44358.7939453125 
[2025-03-23 02:40:47 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.0010235903318971395 norm:0.000875515048392117 max memory_allocated 44358.7939453125 
[2025-03-23 02:41:31 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.0010084109380841255 norm:0.0008021201938390732 max memory_allocated 44358.7939453125 
[2025-03-23 02:42:14 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.00100642628967762 norm:0.0007703242008574307 max memory_allocated 44358.7939453125 
[2025-03-23 02:42:58 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.0010029859840869904 norm:0.0007131922175176442 max memory_allocated 44358.7939453125 
[2025-03-23 02:43:42 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.0010015808511525393 norm:0.000680129393003881 max memory_allocated 44358.7939453125 
[2025-03-23 02:44:26 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.0009973194682970643 norm:0.0006169878179207444 max memory_allocated 44358.7939453125 
[2025-03-23 02:45:09 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.0009940318996086717 norm:0.0005652482504956424 max memory_allocated 44358.7939453125 
[2025-03-23 02:45:53 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.0009898002026602626 norm:0.0005037655355408788 max memory_allocated 44358.7939453125 
[2025-03-23 02:46:37 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.000989772961474955 norm:0.0004719213175121695 max memory_allocated 44358.7939453125 
[2025-03-23 02:47:21 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.0009865295141935349 norm:0.00042884930735453963 max memory_allocated 44358.7939453125 
[2025-03-23 02:48:19 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2]
[2025-03-23 02:48:19 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4, 5] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 3 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 3 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 3 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 3 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 3 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 3 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 4 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 4 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 4 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 4 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 4 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 4 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 5 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 5 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 5 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 5 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 5 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 5 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 02:50:38 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 0 loss:0.004363672807812691 norm:0.0001990408345591277 max memory_allocated 62760.0654296875 
[2025-03-23 02:52:47 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 1 loss:0.004096413962543011 norm:0.00011801690561696887 max memory_allocated 62760.0654296875 
[2025-03-23 02:54:57 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 2 loss:0.003972490783780813 norm:0.00011174354585818946 max memory_allocated 62760.0654296875 
[2025-03-23 02:57:07 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 3 loss:0.003842664649710059 norm:0.00011166561307618394 max memory_allocated 62760.0654296875 
[2025-03-23 02:59:17 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 4 loss:0.0036920567508786917 norm:0.00020065564604010433 max memory_allocated 62760.0654296875 
[2025-03-23 03:01:26 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 5 loss:0.0035034650936722755 norm:0.00019577318744268268 max memory_allocated 62760.0654296875 
[2025-03-23 03:03:36 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 6 loss:0.003248354885727167 norm:0.00010096158075612038 max memory_allocated 62760.0654296875 
[2025-03-23 03:05:46 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 7 loss:0.0030845701694488525 norm:0.00012510699161794037 max memory_allocated 62760.0654296875 
[2025-03-23 03:07:56 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 8 loss:0.0029785980004817247 norm:0.00015248864656314254 max memory_allocated 62760.0654296875 
[2025-03-23 03:10:06 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 9 loss:0.002887253649532795 norm:0.00012834425433538854 max memory_allocated 62760.0654296875 
[2025-03-23 03:12:16 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 10 loss:0.002840327564626932 norm:0.00012719824735540897 max memory_allocated 62760.0654296875 
[2025-03-23 03:14:25 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 11 loss:0.002807868644595146 norm:0.00011930032633244991 max memory_allocated 62760.0654296875 
[2025-03-23 03:16:35 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 12 loss:0.002785617485642433 norm:0.00011227771756239235 max memory_allocated 62760.0654296875 
[2025-03-23 03:18:45 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 13 loss:0.0027803690172731876 norm:0.00010304240277037024 max memory_allocated 62760.0654296875 
[2025-03-23 03:20:55 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 14 loss:0.0027826563455164433 norm:0.00011600610741879791 max memory_allocated 62760.0654296875 
[2025-03-23 03:23:05 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 15 loss:0.0027734925970435143 norm:9.48765518842265e-05 max memory_allocated 62760.0654296875 
[2025-03-23 03:25:14 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 16 loss:0.002774301916360855 norm:8.300218178192154e-05 max memory_allocated 62760.0654296875 
[2025-03-23 03:27:24 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 17 loss:0.002773081883788109 norm:9.47004446061328e-05 max memory_allocated 62760.0654296875 
[2025-03-23 03:29:34 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 18 loss:0.0027767722494900227 norm:0.00010077810293296352 max memory_allocated 62760.0654296875 
[2025-03-23 03:31:44 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 19 loss:0.0027739261277019978 norm:9.342977864434943e-05 max memory_allocated 62760.0654296875 
[2025-03-23 03:34:40 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [3, 4, 5]
[2025-03-23 03:34:40 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [6, 7, 8] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 6 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 6 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 6 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 6 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 6 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 6 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 7 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 7 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 7 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 7 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 7 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 7 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 8 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 8 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 8 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 8 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 8 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 8 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 03:36:59 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 0 loss:0.005899805575609207 norm:0.00010047961404779926 max memory_allocated 62760.2998046875 
[2025-03-23 03:39:09 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 1 loss:0.005521573591977358 norm:6.663668318651617e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:41:18 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 2 loss:0.0053021786734461784 norm:5.064816650701687e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:43:28 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 3 loss:0.005092773586511612 norm:4.5865501306252554e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:45:38 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 4 loss:0.004839141853153706 norm:4.222612551529892e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:47:48 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 5 loss:0.00456374604254961 norm:4.24278769060038e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:49:58 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 6 loss:0.004319390282034874 norm:4.242729119141586e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:52:08 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 7 loss:0.004146449733525515 norm:4.0864924812922254e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:54:17 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 8 loss:0.004036521073430777 norm:4.168076702626422e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:56:27 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 9 loss:0.003971975762397051 norm:4.2415376810822636e-05 max memory_allocated 62760.2998046875 
[2025-03-23 03:58:37 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 10 loss:0.003935576882213354 norm:4.311537486501038e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:00:47 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 11 loss:0.003916017711162567 norm:3.888824721798301e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:02:57 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 12 loss:0.003903638571500778 norm:3.95175811718218e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:05:07 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 13 loss:0.0038965572603046894 norm:3.90103341487702e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:07:17 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 14 loss:0.003895214293152094 norm:3.6641220503952354e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:09:26 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 15 loss:0.0038903753738850355 norm:3.809778354479931e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:11:36 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 16 loss:0.0038900936488062143 norm:3.798681063926779e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:13:46 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 17 loss:0.0038878985214978456 norm:3.77519718313124e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:15:56 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 18 loss:0.003881253534927964 norm:3.659052890725434e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:18:06 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 19 loss:0.0038765964563935995 norm:3.3877327950904146e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:20:56 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [6, 7, 8]
[2025-03-23 04:20:56 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [9, 10, 11] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 9 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 9 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 9 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 9 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 9 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 9 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 10 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 10 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 10 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 10 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 10 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 10 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 11 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 11 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 11 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 11 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 11 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 11 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 04:23:14 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 0 loss:0.007622820790857077 norm:0.00013040498015470803 max memory_allocated 62760.2998046875 
[2025-03-23 04:25:23 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 1 loss:0.007011725567281246 norm:8.760283526498824e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:27:33 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 2 loss:0.006644555367529392 norm:6.428843335015699e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:29:43 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 3 loss:0.006326914764940739 norm:5.461544787976891e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:31:53 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 4 loss:0.006007832009345293 norm:5.0950126023963094e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:34:03 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 5 loss:0.005692383274435997 norm:4.874367004958913e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:36:13 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 6 loss:0.0054388451389968395 norm:4.5060394768370315e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:38:22 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 7 loss:0.005268943030387163 norm:4.079182690475136e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:40:32 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 8 loss:0.00516847288236022 norm:3.7272311601554975e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:42:42 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 9 loss:0.0051186298951506615 norm:3.876288246829063e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:44:52 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 10 loss:0.005083003547042608 norm:3.6325403925729915e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:47:02 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 11 loss:0.005065491423010826 norm:3.598294279072434e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:49:12 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 12 loss:0.005061195231974125 norm:3.606658719945699e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:51:21 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 13 loss:0.0050572617910802364 norm:3.6680336052086204e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:53:31 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 14 loss:0.0050556231290102005 norm:3.974094215664081e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:55:41 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 15 loss:0.005051829386502504 norm:3.766014924622141e-05 max memory_allocated 62760.2998046875 
[2025-03-23 04:57:51 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 16 loss:0.005056392401456833 norm:3.928816659026779e-05 max memory_allocated 62760.2998046875 
[2025-03-23 05:00:01 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 17 loss:0.005048351362347603 norm:3.764493885682896e-05 max memory_allocated 62760.2998046875 
[2025-03-23 05:02:11 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 18 loss:0.005043442361056805 norm:3.528215893311426e-05 max memory_allocated 62760.2998046875 
[2025-03-23 05:04:21 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10, 11]) iter 19 loss:0.005040272139012814 norm:3.930788443540223e-05 max memory_allocated 62760.2998046875 
[2025-03-23 05:07:12 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [9, 10, 11]
[2025-03-23 05:07:12 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [12, 13, 14] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 12 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 12 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 12 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 12 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 12 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 12 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 13 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 13 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 13 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 13 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 13 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 13 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 14 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 14 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 14 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 14 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 14 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 14 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 05:09:31 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 0 loss:0.007887573912739754 norm:9.190601849695668e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:11:40 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 1 loss:0.007341998629271984 norm:5.811127630295232e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:13:50 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 2 loss:0.007047790102660656 norm:4.1630100895417854e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:16:00 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 3 loss:0.006800592411309481 norm:3.232764356653206e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:18:10 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 4 loss:0.006537477020174265 norm:2.7985075575998053e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:20:20 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 5 loss:0.006273300386965275 norm:2.5184966943925247e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:22:29 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 6 loss:0.006042972672730684 norm:2.4081648007268086e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:24:39 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 7 loss:0.005874721799045801 norm:2.277096064062789e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:26:49 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 8 loss:0.005764978937804699 norm:2.225252683274448e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:28:59 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 9 loss:0.00569860590621829 norm:2.2359650756698102e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:31:09 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 10 loss:0.0056578489020466805 norm:2.262210000480991e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:33:19 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 11 loss:0.0056349728256464005 norm:2.2274522052612156e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:35:29 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 12 loss:0.005624220706522465 norm:2.240632602479309e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:37:39 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 13 loss:0.005618035793304443 norm:2.2087009710958228e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:39:49 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 14 loss:0.005615667439997196 norm:2.2265669031185098e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:41:59 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 15 loss:0.0056113749742507935 norm:2.2317915863823146e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:44:08 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 16 loss:0.005609325133264065 norm:2.2245938453124836e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:46:18 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 17 loss:0.005606497637927532 norm:2.2564068785868585e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:48:28 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 18 loss:0.005607409868389368 norm:2.30888654186856e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:50:38 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [12, 13, 14]) iter 19 loss:0.005605411250144243 norm:2.2801999875809997e-05 max memory_allocated 62761.7685546875 
[2025-03-23 05:53:30 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [12, 13, 14]
[2025-03-23 05:53:31 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [15, 16, 17] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 15 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 15 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 15 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 15 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 15 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 15 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 16 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 16 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 16 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 16 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 16 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 16 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 17 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 17 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 17 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 17 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 17 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 17 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 05:55:49 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 0 loss:0.008111763745546341 norm:6.6858105128631e-05 max memory_allocated 62763.0029296875 
[2025-03-23 05:57:58 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 1 loss:0.007704088930040598 norm:4.292586163501255e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:00:08 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 2 loss:0.007458468433469534 norm:3.198783451807685e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:02:18 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 3 loss:0.007236616685986519 norm:2.7428570319898427e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:04:28 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 4 loss:0.006992696318775415 norm:2.484718061168678e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:06:38 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 5 loss:0.0067389728501439095 norm:2.293196666869335e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:08:47 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 6 loss:0.0065114726312458515 norm:2.3021199012873694e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:10:57 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 7 loss:0.006345234811306 norm:2.287426104885526e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:13:07 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 8 loss:0.006235033273696899 norm:2.2121059373603202e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:15:17 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 9 loss:0.006169563625007868 norm:2.1741561795352027e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:17:27 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 10 loss:0.006127361673861742 norm:2.1393625502241775e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:19:37 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 11 loss:0.006102368701249361 norm:2.1627631213050336e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:21:47 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 12 loss:0.006088689435273409 norm:2.1896772523177788e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:23:57 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 13 loss:0.006077220197767019 norm:2.118147494911682e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:26:06 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 14 loss:0.0060728476382792 norm:2.128400774381589e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:28:16 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 15 loss:0.006066952832043171 norm:2.1149626263650134e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:30:26 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 16 loss:0.006064298562705517 norm:2.1219761038082652e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:32:36 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 17 loss:0.0060621448792517185 norm:2.1378975361585617e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:34:46 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 18 loss:0.006060222629457712 norm:2.1131909306859598e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:36:56 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [15, 16, 17]) iter 19 loss:0.0060571348294615746 norm:2.1062478481326252e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:40:12 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [15, 16, 17]
[2025-03-23 06:40:12 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [18, 19, 20] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 18 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 18 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 18 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 18 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 18 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 18 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 19 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 19 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 19 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 19 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 19 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 19 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 20 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 20 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 20 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 20 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 20 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 20 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 06:42:30 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 0 loss:0.009339121170341969 norm:5.44772447028663e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:44:40 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 1 loss:0.008983943611383438 norm:3.6881716368952766e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:46:49 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 2 loss:0.008751542307436466 norm:2.8050462788087316e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:48:59 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 3 loss:0.00850633718073368 norm:2.3282167603611015e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:51:09 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 4 loss:0.008211581036448479 norm:2.0028490325785242e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:53:19 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 5 loss:0.007878834381699562 norm:1.9020446416107006e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:55:29 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 6 loss:0.007593801710754633 norm:2.0574610971380025e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:57:39 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 7 loss:0.007410737685859203 norm:1.9811013771686703e-05 max memory_allocated 62763.0029296875 
[2025-03-23 06:59:49 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 8 loss:0.007315861061215401 norm:1.954448271135334e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:01:59 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 9 loss:0.007271561771631241 norm:1.9059780242969282e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:04:09 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 10 loss:0.0072489045560359955 norm:1.8708164134295657e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:06:19 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 11 loss:0.007238961756229401 norm:1.9004377463716082e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:08:29 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 12 loss:0.007235806901007891 norm:1.8725833797361702e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:10:39 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 13 loss:0.00723084295168519 norm:1.836824776546564e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:12:49 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 14 loss:0.007227099500596523 norm:1.8765869754133746e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:14:59 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 15 loss:0.0072247060015797615 norm:1.868647632363718e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:17:09 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 16 loss:0.0072218445129692554 norm:1.847992643888574e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:19:19 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 17 loss:0.007220400497317314 norm:1.872951543191448e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:21:29 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 18 loss:0.007216668222099543 norm:1.849868567660451e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:23:39 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [18, 19, 20]) iter 19 loss:0.007215749006718397 norm:1.904824239318259e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:26:39 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [18, 19, 20]
[2025-03-23 07:26:39 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [21, 22, 23] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 21 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 21 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 21 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 21 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 21 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 21 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 22 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 22 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 22 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 22 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 22 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 22 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 23 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 23 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 23 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 23 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 23 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 23 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 07:28:58 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 0 loss:0.011972280219197273 norm:6.419698183890432e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:31:07 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 1 loss:0.011650467291474342 norm:5.3772706451127306e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:33:17 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 2 loss:0.011400047689676285 norm:4.5530909119406715e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:35:27 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 3 loss:0.011074068024754524 norm:3.573370122467168e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:37:37 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 4 loss:0.0106191486120224 norm:3.6585264751920477e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:39:47 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 5 loss:0.010163298808038235 norm:4.774024273501709e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:41:57 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 6 loss:0.009855994023382664 norm:3.436322367633693e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:44:07 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 7 loss:0.009718944318592548 norm:3.151000419165939e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:46:17 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 8 loss:0.009668596088886261 norm:2.9574344807770103e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:48:27 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 9 loss:0.009646289050579071 norm:2.7647694878396578e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:50:37 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 10 loss:0.009638408198952675 norm:3.919822847819887e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:52:47 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 11 loss:0.009623452089726925 norm:2.7924123060074635e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:54:57 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 12 loss:0.009615194983780384 norm:2.836353814927861e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:57:07 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 13 loss:0.009617832489311695 norm:4.4607368181459606e-05 max memory_allocated 62763.0029296875 
[2025-03-23 07:59:18 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 14 loss:0.009608323685824871 norm:2.707287421799265e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:01:28 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 15 loss:0.009606043808162212 norm:2.3394892195938155e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:03:38 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 16 loss:0.009603378362953663 norm:2.3910477466415614e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:05:48 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 17 loss:0.009601525962352753 norm:2.26422052946873e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:07:58 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 18 loss:0.009599083103239536 norm:2.4255417883978225e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:10:08 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [21, 22, 23]) iter 19 loss:0.009596390649676323 norm:2.410769411653746e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:12:59 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [21, 22, 23]
[2025-03-23 08:12:59 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [24, 25, 26] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 24 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 24 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 24 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 24 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 24 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 24 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 25 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 25 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 25 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 25 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 25 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 25 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 26 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 26 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 26 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 26 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 26 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 26 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 08:15:17 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 0 loss:0.015858057886362076 norm:3.3778931538108736e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:17:26 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 1 loss:0.0155702019110322 norm:2.5118180928984657e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:19:37 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 2 loss:0.015271913260221481 norm:2.128564301528968e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:21:47 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 3 loss:0.014797746203839779 norm:2.0698580556199886e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:23:57 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 4 loss:0.014085564762353897 norm:2.0295412468840368e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:26:07 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 5 loss:0.01351876650005579 norm:2.00402682821732e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:28:17 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 6 loss:0.013315603137016296 norm:2.0375253370730206e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:30:27 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 7 loss:0.013265782035887241 norm:2.0121768102399074e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:32:36 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 8 loss:0.013253039680421352 norm:2.023457818722818e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:34:47 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 9 loss:0.013245442882180214 norm:2.025721551035531e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:36:56 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 10 loss:0.013238932006061077 norm:1.9776653061853722e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:39:06 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 11 loss:0.01323496364057064 norm:1.988168696698267e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:41:16 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 12 loss:0.01323165837675333 norm:2.0898009097436443e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:43:26 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 13 loss:0.013228817842900753 norm:2.089553527184762e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:45:36 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 14 loss:0.013227973133325577 norm:2.065861554001458e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:47:46 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 15 loss:0.013225598260760307 norm:2.0725887225125916e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:49:56 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 16 loss:0.013222454115748405 norm:2.117136500601191e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:52:06 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 17 loss:0.013219330459833145 norm:2.18550212593982e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:54:16 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 18 loss:0.013216669671237469 norm:2.1075136828585528e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:56:26 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [24, 25, 26]) iter 19 loss:0.013215357437729836 norm:2.0287168808863498e-05 max memory_allocated 62763.0029296875 
[2025-03-23 08:59:19 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [24, 25, 26]
[2025-03-23 08:59:20 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [27, 28, 29] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 27 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 27 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 27 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 27 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 27 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 27 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 28 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 28 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 28 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 28 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 28 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 28 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 29 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 29 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 29 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 29 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 29 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 29 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 09:01:38 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 0 loss:0.021401816979050636 norm:4.117473872611299e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:03:47 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 1 loss:0.021055961027741432 norm:3.0228191462811083e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:05:57 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 2 loss:0.020631063729524612 norm:2.7792812034022063e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:08:07 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 3 loss:0.019828904420137405 norm:2.6753930796985514e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:10:17 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 4 loss:0.018809476867318153 norm:2.4958311769296415e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:12:27 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 5 loss:0.018377477303147316 norm:2.5243989512091503e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:14:37 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 6 loss:0.01830778270959854 norm:2.366245462326333e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:16:47 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 7 loss:0.01829524338245392 norm:2.312539618287701e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:18:57 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 8 loss:0.018286772072315216 norm:2.3768057872075588e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:21:07 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 9 loss:0.018281469121575356 norm:2.382479033258278e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:23:17 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 10 loss:0.01827823743224144 norm:2.351961848034989e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:25:27 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 11 loss:0.018275028094649315 norm:2.3882503228378482e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:27:37 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 12 loss:0.018271436914801598 norm:2.3507796868216246e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:29:47 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 13 loss:0.01826801337301731 norm:2.284782203787472e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:31:57 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 14 loss:0.018263256177306175 norm:2.309685805812478e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:34:07 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 15 loss:0.01825915463268757 norm:2.3101918486645445e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:36:17 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 16 loss:0.018258338794112206 norm:2.301033600815572e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:38:27 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 17 loss:0.018254093825817108 norm:2.2533346054842696e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:40:37 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 18 loss:0.018252043053507805 norm:2.2096353859524243e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:42:47 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27, 28, 29]) iter 19 loss:0.018250782042741776 norm:2.250728357466869e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:45:41 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [27, 28, 29]
[2025-03-23 09:45:41 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [30, 31, 32] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 30 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 30 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 30 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 30 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 30 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 30 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 31 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 31 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 31 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 31 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 31 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 31 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 32 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 32 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 32 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 32 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 32 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 32 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 09:48:00 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 0 loss:0.029241790995001793 norm:5.8142992202192545e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:50:10 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 1 loss:0.028687477111816406 norm:4.6473636757582426e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:52:19 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 2 loss:0.027939509600400925 norm:3.6661986086983234e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:54:29 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 3 loss:0.026579130440950394 norm:2.9921615350758657e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:56:39 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 4 loss:0.025500303134322166 norm:2.8052387278876267e-05 max memory_allocated 62763.0029296875 
[2025-03-23 09:58:49 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 5 loss:0.02531140111386776 norm:2.9103506676619872e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:00:59 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 6 loss:0.025287771597504616 norm:2.8365211619529873e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:03:09 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 7 loss:0.025276528671383858 norm:2.9038097636657767e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:05:19 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 8 loss:0.02526862360537052 norm:2.924381260527298e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:07:29 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 9 loss:0.02526051364839077 norm:2.8284595828154124e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:09:39 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 10 loss:0.025251243263483047 norm:2.7414953365223482e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:11:49 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 11 loss:0.02524768002331257 norm:2.8913418645970523e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:13:59 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 12 loss:0.025241639465093613 norm:2.8364715035422705e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:16:09 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 13 loss:0.025236759334802628 norm:2.7920041247853078e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:18:19 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 14 loss:0.025232089683413506 norm:2.6548303139861673e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:20:29 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 15 loss:0.025228550657629967 norm:2.7534015316632576e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:22:39 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 16 loss:0.02522421069443226 norm:2.720197880989872e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:24:49 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 17 loss:0.025219950824975967 norm:2.7207053790334612e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:26:59 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 18 loss:0.02521633915603161 norm:2.6212605007458478e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:29:08 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30, 31, 32]) iter 19 loss:0.02521343156695366 norm:2.632363248267211e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:31:58 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [30, 31, 32]
[2025-03-23 10:31:58 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [33, 34, 35] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 33 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 33 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 33 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 33 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 33 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 33 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 34 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 34 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 34 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 34 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 34 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 34 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 35 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 35 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 35 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 35 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 35 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 35 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 10:34:16 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 0 loss:0.04005405679345131 norm:6.0058697272324935e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:36:26 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 1 loss:0.03918211907148361 norm:3.9592570828972384e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:38:36 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 2 loss:0.037814900279045105 norm:3.0946754122851416e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:40:46 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 3 loss:0.03586454316973686 norm:2.96526250167517e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:42:55 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 4 loss:0.03509056940674782 norm:2.8287890017963946e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:45:05 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 5 loss:0.03503744676709175 norm:2.7715679607354105e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:47:15 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 6 loss:0.035022519528865814 norm:2.7495661925058812e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:49:25 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 7 loss:0.03501022234559059 norm:2.7581507310969755e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:51:35 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 8 loss:0.03499582037329674 norm:2.7444555598776788e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:53:45 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 9 loss:0.034985557198524475 norm:2.7639249310595915e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:55:55 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 10 loss:0.03497433662414551 norm:2.747738290054258e-05 max memory_allocated 62763.0029296875 
[2025-03-23 10:58:05 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 11 loss:0.03496662154793739 norm:2.6994917789124884e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:00:15 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 12 loss:0.0349600613117218 norm:2.6947194783133455e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:02:25 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 13 loss:0.03495705872774124 norm:2.7275176762486808e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:04:35 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 14 loss:0.034951694309711456 norm:2.765845238172915e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:06:45 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 15 loss:0.03494768589735031 norm:2.7116040655528195e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:08:55 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 16 loss:0.034943751990795135 norm:2.6638630515662953e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:11:05 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 17 loss:0.03493611142039299 norm:2.695196235436015e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:13:15 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 18 loss:0.03492739051580429 norm:2.675461655599065e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:15:24 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [33, 34, 35]) iter 19 loss:0.03492093086242676 norm:2.6532130505074747e-05 max memory_allocated 62763.0029296875 
[2025-03-23 11:18:16 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [33, 34, 35]
[2025-03-23 11:18:16 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [36] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 36 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 36 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 36 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 36 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 36 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 36 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 11:18:17 root](abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 11:19:03 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 0 loss:0.03864593058824539 norm:0.0011485142167657614 max memory_allocated 62763.0029296875 
[2025-03-23 11:19:46 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 1 loss:0.0381881520152092 norm:0.0008471137844026089 max memory_allocated 62763.0029296875 
[2025-03-23 11:20:29 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 2 loss:0.03754296153783798 norm:0.0007294559036381543 max memory_allocated 62763.0029296875 
[2025-03-23 11:21:13 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 3 loss:0.03681172803044319 norm:0.000625927874352783 max memory_allocated 62763.0029296875 
[2025-03-23 11:21:57 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 4 loss:0.03663284331560135 norm:0.0005443347617983818 max memory_allocated 62763.0029296875 
[2025-03-23 11:22:40 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 5 loss:0.0366174578666687 norm:0.0004832665144931525 max memory_allocated 62763.0029296875 
[2025-03-23 11:23:24 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 6 loss:0.036602724343538284 norm:0.0004309411742724478 max memory_allocated 62763.0029296875 
[2025-03-23 11:24:08 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 7 loss:0.036597780883312225 norm:0.0003662168746814132 max memory_allocated 62763.0029296875 
[2025-03-23 11:24:52 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 8 loss:0.03660321235656738 norm:0.0003539373283274472 max memory_allocated 62763.0029296875 
[2025-03-23 11:25:35 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 9 loss:0.0366356261074543 norm:0.0003904696786776185 max memory_allocated 62763.0029296875 
[2025-03-23 11:26:19 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 10 loss:0.03662463277578354 norm:0.0003677599597722292 max memory_allocated 62763.0029296875 
[2025-03-23 11:27:03 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 11 loss:0.03658086434006691 norm:0.00022879285097587854 max memory_allocated 62763.0029296875 
[2025-03-23 11:27:47 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 12 loss:0.03655174747109413 norm:0.00022025691578164697 max memory_allocated 62763.0029296875 
[2025-03-23 11:28:31 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 13 loss:0.03654871881008148 norm:0.00020686950301751494 max memory_allocated 62763.0029296875 
[2025-03-23 11:29:14 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 14 loss:0.03655267879366875 norm:0.00020651494560297579 max memory_allocated 62763.0029296875 
[2025-03-23 11:29:58 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 15 loss:0.03656957298517227 norm:0.0002638906007632613 max memory_allocated 62763.0029296875 
[2025-03-23 11:30:42 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 16 loss:0.036649346351623535 norm:0.00042442072299309075 max memory_allocated 62763.0029296875 
[2025-03-23 11:31:26 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 17 loss:0.036817558109760284 norm:0.0006730688619427383 max memory_allocated 62763.0029296875 
[2025-03-23 11:32:09 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 18 loss:0.03720647096633911 norm:0.0012769214808940887 max memory_allocated 62763.0029296875 
[2025-03-23 11:32:53 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [36]) iter 19 loss:0.03757975250482559 norm:0.0016399106243625283 max memory_allocated 62763.0029296875 
[2025-03-23 11:33:51 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [36]
[2025-03-23 11:33:51 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [37] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 37 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 37 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 37 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 37 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 37 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 37 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 11:33:51 root](abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 11:34:38 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 0 loss:0.0440426729619503 norm:0.0013503897935152054 max memory_allocated 62763.0029296875 
[2025-03-23 11:35:21 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 1 loss:0.043309640139341354 norm:0.0009542417246848345 max memory_allocated 62763.0029296875 
[2025-03-23 11:36:05 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 2 loss:0.042402561753988266 norm:0.0007861279300414026 max memory_allocated 62763.0029296875 
[2025-03-23 11:36:49 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 3 loss:0.04163847863674164 norm:0.0006682550301775336 max memory_allocated 62763.0029296875 
[2025-03-23 11:37:32 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 4 loss:0.04148440435528755 norm:0.0006033406243659556 max memory_allocated 62763.0029296875 
[2025-03-23 11:38:16 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 5 loss:0.04147535189986229 norm:0.0005603345925919712 max memory_allocated 62763.0029296875 
[2025-03-23 11:39:00 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 6 loss:0.04145099222660065 norm:0.0004422749625518918 max memory_allocated 62763.0029296875 
[2025-03-23 11:39:44 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 7 loss:0.041439708322286606 norm:0.0004315344849601388 max memory_allocated 62763.0029296875 
[2025-03-23 11:40:27 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 8 loss:0.04142531007528305 norm:0.00037115070153959095 max memory_allocated 62763.0029296875 
[2025-03-23 11:41:11 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 9 loss:0.04141279309988022 norm:0.0003513178089633584 max memory_allocated 62763.0029296875 
[2025-03-23 11:41:55 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 10 loss:0.041413746774196625 norm:0.00036640511825680733 max memory_allocated 62763.0029296875 
[2025-03-23 11:42:39 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 11 loss:0.041414521634578705 norm:0.00038128718733787537 max memory_allocated 62763.0029296875 
[2025-03-23 11:43:23 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 12 loss:0.04139072075486183 norm:0.00030199115280993283 max memory_allocated 62763.0029296875 
[2025-03-23 11:44:06 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 13 loss:0.04139989987015724 norm:0.00028881931211799383 max memory_allocated 62763.0029296875 
[2025-03-23 11:44:50 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 14 loss:0.041447024792432785 norm:0.0004369701782707125 max memory_allocated 62763.0029296875 
[2025-03-23 11:45:34 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 15 loss:0.04154520854353905 norm:0.0006543378112837672 max memory_allocated 62763.0029296875 
[2025-03-23 11:46:18 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 16 loss:0.04143223911523819 norm:0.00041744153713807464 max memory_allocated 62763.0029296875 
[2025-03-23 11:47:02 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 17 loss:0.04134948551654816 norm:0.0002375917974859476 max memory_allocated 62763.0029296875 
[2025-03-23 11:47:45 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 18 loss:0.041324056684970856 norm:0.00023043507826514542 max memory_allocated 62763.0029296875 
[2025-03-23 11:48:29 root](abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [37]) iter 19 loss:0.04131610319018364 norm:0.00022157093917485327 max memory_allocated 62763.0029296875 
[2025-03-23 11:49:26 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 15, block: [37]
[2025-03-23 11:49:26 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 16 with layers [38] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 38 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 38 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 38 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 38 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 38 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 38 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 11:49:26 root](abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 11:50:13 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 0 loss:0.05293463170528412 norm:0.001820066012442112 max memory_allocated 62763.0029296875 
[2025-03-23 11:50:56 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 1 loss:0.05062023550271988 norm:0.0012987158261239529 max memory_allocated 62763.0029296875 
[2025-03-23 11:51:40 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 2 loss:0.0492112822830677 norm:0.0010037970496341586 max memory_allocated 62763.0029296875 
[2025-03-23 11:52:24 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 3 loss:0.048394575715065 norm:0.0008463386911898851 max memory_allocated 62763.0029296875 
[2025-03-23 11:53:07 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 4 loss:0.04822801426053047 norm:0.0008017969084903598 max memory_allocated 62763.0029296875 
[2025-03-23 11:53:51 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 5 loss:0.04818522557616234 norm:0.0007019786862656474 max memory_allocated 62763.0029296875 
[2025-03-23 11:54:35 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 6 loss:0.04815538227558136 norm:0.0006492005777545273 max memory_allocated 62763.0029296875 
[2025-03-23 11:55:19 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 7 loss:0.04814664646983147 norm:0.0006272693281061947 max memory_allocated 62763.0029296875 
[2025-03-23 11:56:03 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 8 loss:0.048143379390239716 norm:0.0005822674138471484 max memory_allocated 62763.0029296875 
[2025-03-23 11:56:46 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 9 loss:0.04812271520495415 norm:0.0006171524291858077 max memory_allocated 62763.0029296875 
[2025-03-23 11:57:30 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 10 loss:0.048100292682647705 norm:0.0005294795264489949 max memory_allocated 62763.0029296875 
[2025-03-23 11:58:14 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 11 loss:0.04808472841978073 norm:0.00048571161460131407 max memory_allocated 62763.0029296875 
[2025-03-23 11:58:58 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 12 loss:0.04807385429739952 norm:0.0004831971600651741 max memory_allocated 62763.0029296875 
[2025-03-23 11:59:41 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 13 loss:0.048073187470436096 norm:0.00046022635069675744 max memory_allocated 62763.0029296875 
[2025-03-23 12:00:25 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 14 loss:0.048068199306726456 norm:0.00046881387243047357 max memory_allocated 62763.0029296875 
[2025-03-23 12:01:09 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 15 loss:0.048092667013406754 norm:0.0006049309158697724 max memory_allocated 62763.0029296875 
[2025-03-23 12:01:53 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 16 loss:0.048117924481630325 norm:0.0006143643404357135 max memory_allocated 62763.0029296875 
[2025-03-23 12:02:37 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 17 loss:0.048090606927871704 norm:0.0005562449805438519 max memory_allocated 62763.0029296875 
[2025-03-23 12:03:20 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 18 loss:0.048114579170942307 norm:0.0006038111750967801 max memory_allocated 62763.0029296875 
[2025-03-23 12:04:04 root](abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [38]) iter 19 loss:0.04806026816368103 norm:0.00046181719517335296 max memory_allocated 62763.0029296875 
[2025-03-23 12:05:00 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 16, block: [38]
[2025-03-23 12:05:00 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 17 with layers [39] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 39 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 39 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 39 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 39 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 39 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 39 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 12:05:01 root](abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 12:05:47 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 0 loss:0.06593044102191925 norm:0.003981234971433878 max memory_allocated 62763.0029296875 
[2025-03-23 12:06:31 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 1 loss:0.0641043558716774 norm:0.0032376290764659643 max memory_allocated 62763.0029296875 
[2025-03-23 12:07:14 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 2 loss:0.0628117099404335 norm:0.0028021596372127533 max memory_allocated 62763.0029296875 
[2025-03-23 12:07:58 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 3 loss:0.06203272566199303 norm:0.0026629066560417414 max memory_allocated 62763.0029296875 
[2025-03-23 12:08:42 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 4 loss:0.06177731603384018 norm:0.0024941761512309313 max memory_allocated 62763.0029296875 
[2025-03-23 12:09:26 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 5 loss:0.061677511781454086 norm:0.002430395223200321 max memory_allocated 62763.0029296875 
[2025-03-23 12:10:09 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 6 loss:0.061651092022657394 norm:0.002332936739549041 max memory_allocated 62763.0029296875 
[2025-03-23 12:10:53 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 7 loss:0.061639945954084396 norm:0.0023096667136996984 max memory_allocated 62763.0029296875 
[2025-03-23 12:11:37 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 8 loss:0.06161302328109741 norm:0.002258470980450511 max memory_allocated 62763.0029296875 
[2025-03-23 12:12:21 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 9 loss:0.06166933476924896 norm:0.0023297499865293503 max memory_allocated 62763.0029296875 
[2025-03-23 12:13:04 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 10 loss:0.06188354641199112 norm:0.0028225460555404425 max memory_allocated 62763.0029296875 
[2025-03-23 12:13:48 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 11 loss:0.06180077791213989 norm:0.00257278885692358 max memory_allocated 62763.0029296875 
[2025-03-23 12:14:32 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 12 loss:0.06151823326945305 norm:0.0021477846894413233 max memory_allocated 62763.0029296875 
[2025-03-23 12:15:16 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 13 loss:0.061458516865968704 norm:0.002092628739774227 max memory_allocated 62763.0029296875 
[2025-03-23 12:16:00 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 14 loss:0.06145011633634567 norm:0.0020624175667762756 max memory_allocated 62763.0029296875 
[2025-03-23 12:16:43 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 15 loss:0.061429522931575775 norm:0.0020328708924353123 max memory_allocated 62763.0029296875 
[2025-03-23 12:17:27 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 16 loss:0.06141708791255951 norm:0.0019585806876420975 max memory_allocated 62763.0029296875 
[2025-03-23 12:18:11 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 17 loss:0.06141170486807823 norm:0.0019532230217009783 max memory_allocated 62763.0029296875 
[2025-03-23 12:18:55 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 18 loss:0.061525195837020874 norm:0.002099362900480628 max memory_allocated 62763.0029296875 
[2025-03-23 12:19:39 root](abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [39]) iter 19 loss:0.06180638447403908 norm:0.002668826375156641 max memory_allocated 62763.0029296875 
[2025-03-23 12:20:39 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 17, block: [39]
[2025-03-23 12:20:40 root](main_calib_config3_attn.py 379): INFO 37148.451874017715
/workspace/volume/yangzhe/ABQ-LLM/algorithm/main_calib_config3_attn.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  testloader = torch.load(cache_testloader)
[2025-03-23 12:20:57 root](main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
  0%|          | 0/166 [00:00<?, ?it/s]  1%|          | 1/166 [00:00<01:23,  1.99it/s]  1%|          | 2/166 [00:01<01:22,  1.98it/s]  2%|▏         | 3/166 [00:01<01:22,  1.98it/s]  2%|▏         | 4/166 [00:02<01:22,  1.97it/s]  3%|▎         | 5/166 [00:02<01:21,  1.97it/s]  4%|▎         | 6/166 [00:03<01:21,  1.97it/s]  4%|▍         | 7/166 [00:03<01:20,  1.97it/s]  5%|▍         | 8/166 [00:04<01:20,  1.97it/s]  5%|▌         | 9/166 [00:04<01:19,  1.97it/s]  6%|▌         | 10/166 [00:05<01:19,  1.97it/s]  7%|▋         | 11/166 [00:05<01:18,  1.97it/s]  7%|▋         | 12/166 [00:06<01:18,  1.97it/s]  8%|▊         | 13/166 [00:06<01:17,  1.97it/s]  8%|▊         | 14/166 [00:07<01:17,  1.97it/s]  9%|▉         | 15/166 [00:07<01:16,  1.97it/s] 10%|▉         | 16/166 [00:08<01:16,  1.97it/s] 10%|█         | 17/166 [00:08<01:15,  1.97it/s] 11%|█         | 18/166 [00:09<01:15,  1.97it/s] 11%|█▏        | 19/166 [00:09<01:14,  1.96it/s] 12%|█▏        | 20/166 [00:10<01:14,  1.96it/s] 13%|█▎        | 21/166 [00:10<01:13,  1.96it/s] 13%|█▎        | 22/166 [00:11<01:13,  1.96it/s] 14%|█▍        | 23/166 [00:11<01:12,  1.96it/s] 14%|█▍        | 24/166 [00:12<01:12,  1.96it/s] 15%|█▌        | 25/166 [00:12<01:11,  1.96it/s] 16%|█▌        | 26/166 [00:13<01:11,  1.96it/s] 16%|█▋        | 27/166 [00:13<01:10,  1.96it/s] 17%|█▋        | 28/166 [00:14<01:10,  1.96it/s] 17%|█▋        | 29/166 [00:14<01:09,  1.96it/s] 18%|█▊        | 30/166 [00:15<01:09,  1.96it/s] 19%|█▊        | 31/166 [00:15<01:08,  1.96it/s] 19%|█▉        | 32/166 [00:16<01:08,  1.96it/s] 20%|█▉        | 33/166 [00:16<01:07,  1.96it/s] 20%|██        | 34/166 [00:17<01:07,  1.96it/s] 21%|██        | 35/166 [00:17<01:06,  1.96it/s] 22%|██▏       | 36/166 [00:18<01:06,  1.96it/s] 22%|██▏       | 37/166 [00:18<01:05,  1.96it/s] 23%|██▎       | 38/166 [00:19<01:05,  1.96it/s] 23%|██▎       | 39/166 [00:19<01:04,  1.96it/s] 24%|██▍       | 40/166 [00:20<01:04,  1.96it/s] 25%|██▍       | 41/166 [00:20<01:03,  1.96it/s] 25%|██▌       | 42/166 [00:21<01:03,  1.96it/s] 26%|██▌       | 43/166 [00:21<01:02,  1.95it/s] 27%|██▋       | 44/166 [00:22<01:02,  1.96it/s] 27%|██▋       | 45/166 [00:22<01:01,  1.95it/s] 28%|██▊       | 46/166 [00:23<01:01,  1.96it/s] 28%|██▊       | 47/166 [00:23<01:00,  1.95it/s] 29%|██▉       | 48/166 [00:24<01:00,  1.95it/s] 30%|██▉       | 49/166 [00:24<00:59,  1.95it/s] 30%|███       | 50/166 [00:25<00:59,  1.95it/s] 31%|███       | 51/166 [00:26<00:58,  1.95it/s] 31%|███▏      | 52/166 [00:26<00:58,  1.95it/s] 32%|███▏      | 53/166 [00:27<00:57,  1.95it/s] 33%|███▎      | 54/166 [00:27<00:57,  1.95it/s] 33%|███▎      | 55/166 [00:28<00:56,  1.95it/s] 34%|███▎      | 56/166 [00:28<00:56,  1.95it/s] 34%|███▍      | 57/166 [00:29<00:55,  1.95it/s] 35%|███▍      | 58/166 [00:29<00:55,  1.95it/s] 36%|███▌      | 59/166 [00:30<00:54,  1.95it/s] 36%|███▌      | 60/166 [00:30<00:54,  1.95it/s] 37%|███▋      | 61/166 [00:31<00:53,  1.95it/s] 37%|███▋      | 62/166 [00:31<00:53,  1.95it/s] 38%|███▊      | 63/166 [00:32<00:52,  1.95it/s] 39%|███▊      | 64/166 [00:32<00:52,  1.95it/s] 39%|███▉      | 65/166 [00:33<00:51,  1.95it/s] 40%|███▉      | 66/166 [00:33<00:51,  1.95it/s] 40%|████      | 67/166 [00:34<00:50,  1.95it/s] 41%|████      | 68/166 [00:34<00:50,  1.95it/s] 42%|████▏     | 69/166 [00:35<00:49,  1.95it/s] 42%|████▏     | 70/166 [00:35<00:49,  1.95it/s] 43%|████▎     | 71/166 [00:36<00:48,  1.95it/s] 43%|████▎     | 72/166 [00:36<00:48,  1.95it/s] 44%|████▍     | 73/166 [00:37<00:47,  1.95it/s] 45%|████▍     | 74/166 [00:37<00:47,  1.95it/s] 45%|████▌     | 75/166 [00:38<00:46,  1.95it/s] 46%|████▌     | 76/166 [00:38<00:46,  1.95it/s] 46%|████▋     | 77/166 [00:39<00:45,  1.94it/s] 47%|████▋     | 78/166 [00:39<00:45,  1.94it/s] 48%|████▊     | 79/166 [00:40<00:44,  1.94it/s] 48%|████▊     | 80/166 [00:40<00:44,  1.94it/s] 49%|████▉     | 81/166 [00:41<00:43,  1.94it/s] 49%|████▉     | 82/166 [00:41<00:43,  1.94it/s] 50%|█████     | 83/166 [00:42<00:42,  1.94it/s] 51%|█████     | 84/166 [00:42<00:42,  1.94it/s] 51%|█████     | 85/166 [00:43<00:41,  1.94it/s] 52%|█████▏    | 86/166 [00:43<00:41,  1.94it/s] 52%|█████▏    | 87/166 [00:44<00:40,  1.94it/s] 53%|█████▎    | 88/166 [00:45<00:40,  1.94it/s] 54%|█████▎    | 89/166 [00:45<00:39,  1.94it/s] 54%|█████▍    | 90/166 [00:46<00:39,  1.94it/s] 55%|█████▍    | 91/166 [00:46<00:38,  1.94it/s] 55%|█████▌    | 92/166 [00:47<00:38,  1.94it/s] 56%|█████▌    | 93/166 [00:47<00:37,  1.94it/s] 57%|█████▋    | 94/166 [00:48<00:37,  1.94it/s] 57%|█████▋    | 95/166 [00:48<00:36,  1.94it/s] 58%|█████▊    | 96/166 [00:49<00:36,  1.94it/s] 58%|█████▊    | 97/166 [00:49<00:35,  1.94it/s] 59%|█████▉    | 98/166 [00:50<00:35,  1.94it/s] 60%|█████▉    | 99/166 [00:50<00:34,  1.94it/s] 60%|██████    | 100/166 [00:51<00:34,  1.94it/s] 61%|██████    | 101/166 [00:51<00:33,  1.94it/s] 61%|██████▏   | 102/166 [00:52<00:33,  1.94it/s] 62%|██████▏   | 103/166 [00:52<00:32,  1.94it/s] 63%|██████▎   | 104/166 [00:53<00:31,  1.94it/s] 63%|██████▎   | 105/166 [00:53<00:31,  1.94it/s] 64%|██████▍   | 106/166 [00:54<00:30,  1.94it/s] 64%|██████▍   | 107/166 [00:54<00:30,  1.94it/s] 65%|██████▌   | 108/166 [00:55<00:29,  1.94it/s] 66%|██████▌   | 109/166 [00:55<00:29,  1.94it/s] 66%|██████▋   | 110/166 [00:56<00:28,  1.94it/s] 67%|██████▋   | 111/166 [00:56<00:28,  1.94it/s] 67%|██████▋   | 112/166 [00:57<00:27,  1.94it/s] 68%|██████▊   | 113/166 [00:57<00:27,  1.94it/s] 69%|██████▊   | 114/166 [00:58<00:26,  1.94it/s] 69%|██████▉   | 115/166 [00:58<00:26,  1.94it/s] 70%|██████▉   | 116/166 [00:59<00:25,  1.94it/s] 70%|███████   | 117/166 [00:59<00:25,  1.94it/s] 71%|███████   | 118/166 [01:00<00:24,  1.94it/s] 72%|███████▏  | 119/166 [01:01<00:24,  1.94it/s] 72%|███████▏  | 120/166 [01:01<00:23,  1.94it/s] 73%|███████▎  | 121/166 [01:02<00:23,  1.93it/s] 73%|███████▎  | 122/166 [01:02<00:22,  1.94it/s] 74%|███████▍  | 123/166 [01:03<00:22,  1.94it/s] 75%|███████▍  | 124/166 [01:03<00:21,  1.94it/s] 75%|███████▌  | 125/166 [01:04<00:21,  1.94it/s] 76%|███████▌  | 126/166 [01:04<00:20,  1.94it/s] 77%|███████▋  | 127/166 [01:05<00:20,  1.94it/s] 77%|███████▋  | 128/166 [01:05<00:19,  1.94it/s] 78%|███████▊  | 129/166 [01:06<00:19,  1.94it/s] 78%|███████▊  | 130/166 [01:06<00:18,  1.93it/s] 79%|███████▉  | 131/166 [01:07<00:18,  1.93it/s] 80%|███████▉  | 132/166 [01:07<00:17,  1.94it/s] 80%|████████  | 133/166 [01:08<00:17,  1.93it/s] 81%|████████  | 134/166 [01:08<00:16,  1.94it/s] 81%|████████▏ | 135/166 [01:09<00:16,  1.93it/s] 82%|████████▏ | 136/166 [01:09<00:15,  1.93it/s] 83%|████████▎ | 137/166 [01:10<00:15,  1.93it/s] 83%|████████▎ | 138/166 [01:10<00:14,  1.93it/s] 84%|████████▎ | 139/166 [01:11<00:13,  1.93it/s] 84%|████████▍ | 140/166 [01:11<00:13,  1.93it/s] 85%|████████▍ | 141/166 [01:12<00:12,  1.93it/s] 86%|████████▌ | 142/166 [01:12<00:12,  1.93it/s] 86%|████████▌ | 143/166 [01:13<00:11,  1.93it/s] 87%|████████▋ | 144/166 [01:13<00:11,  1.93it/s] 87%|████████▋ | 145/166 [01:14<00:10,  1.93it/s] 88%|████████▊ | 146/166 [01:14<00:10,  1.93it/s] 89%|████████▊ | 147/166 [01:15<00:09,  1.93it/s] 89%|████████▉ | 148/166 [01:16<00:09,  1.93it/s] 90%|████████▉ | 149/166 [01:16<00:08,  1.93it/s] 90%|█████████ | 150/166 [01:17<00:08,  1.93it/s] 91%|█████████ | 151/166 [01:17<00:07,  1.93it/s] 92%|█████████▏| 152/166 [01:18<00:07,  1.93it/s] 92%|█████████▏| 153/166 [01:18<00:06,  1.93it/s] 93%|█████████▎| 154/166 [01:19<00:06,  1.93it/s] 93%|█████████▎| 155/166 [01:19<00:05,  1.93it/s] 94%|█████████▍| 156/166 [01:20<00:05,  1.93it/s] 95%|█████████▍| 157/166 [01:20<00:04,  1.93it/s] 95%|█████████▌| 158/166 [01:21<00:04,  1.93it/s] 96%|█████████▌| 159/166 [01:21<00:03,  1.93it/s] 96%|█████████▋| 160/166 [01:22<00:03,  1.93it/s] 97%|█████████▋| 161/166 [01:22<00:02,  1.93it/s] 98%|█████████▊| 162/166 [01:23<00:02,  1.93it/s] 98%|█████████▊| 163/166 [01:23<00:01,  1.93it/s] 99%|█████████▉| 164/166 [01:24<00:01,  1.93it/s] 99%|█████████▉| 165/166 [01:24<00:00,  1.93it/s]100%|██████████| 166/166 [01:25<00:00,  1.93it/s]100%|██████████| 166/166 [01:25<00:00,  1.95it/s]
[2025-03-23 12:22:23 root](main_calib_config3_attn.py 161): INFO wikitext2 : 4.905276298522949
[2025-03-23 12:22:23 root](main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
  0%|          | 0/256 [00:00<?, ?it/s]  0%|          | 1/256 [00:00<02:11,  1.95it/s]  1%|          | 2/256 [00:01<02:11,  1.93it/s]  1%|          | 3/256 [00:01<02:10,  1.93it/s]  2%|▏         | 4/256 [00:02<02:10,  1.93it/s]  2%|▏         | 5/256 [00:02<02:10,  1.93it/s]  2%|▏         | 6/256 [00:03<02:09,  1.93it/s]  3%|▎         | 7/256 [00:03<02:09,  1.93it/s]  3%|▎         | 8/256 [00:04<02:08,  1.93it/s]  4%|▎         | 9/256 [00:04<02:08,  1.93it/s]  4%|▍         | 10/256 [00:05<02:07,  1.93it/s]  4%|▍         | 11/256 [00:05<02:07,  1.93it/s]  5%|▍         | 12/256 [00:06<02:06,  1.93it/s]  5%|▌         | 13/256 [00:06<02:06,  1.93it/s]  5%|▌         | 14/256 [00:07<02:05,  1.93it/s]  6%|▌         | 15/256 [00:07<02:05,  1.93it/s]  6%|▋         | 16/256 [00:08<02:04,  1.93it/s]  7%|▋         | 17/256 [00:08<02:04,  1.93it/s]  7%|▋         | 18/256 [00:09<02:03,  1.93it/s]  7%|▋         | 19/256 [00:09<02:03,  1.93it/s]  8%|▊         | 20/256 [00:10<02:02,  1.93it/s]  8%|▊         | 21/256 [00:10<02:01,  1.93it/s]  9%|▊         | 22/256 [00:11<02:01,  1.93it/s]  9%|▉         | 23/256 [00:11<02:00,  1.93it/s]  9%|▉         | 24/256 [00:12<02:00,  1.93it/s] 10%|▉         | 25/256 [00:12<01:59,  1.93it/s] 10%|█         | 26/256 [00:13<01:59,  1.93it/s] 11%|█         | 27/256 [00:14<01:58,  1.93it/s] 11%|█         | 28/256 [00:14<01:58,  1.93it/s] 11%|█▏        | 29/256 [00:15<01:57,  1.92it/s] 12%|█▏        | 30/256 [00:15<01:57,  1.93it/s] 12%|█▏        | 31/256 [00:16<01:56,  1.92it/s] 12%|█▎        | 32/256 [00:16<01:56,  1.93it/s] 13%|█▎        | 33/256 [00:17<01:55,  1.93it/s] 13%|█▎        | 34/256 [00:17<01:55,  1.93it/s] 14%|█▎        | 35/256 [00:18<01:54,  1.93it/s] 14%|█▍        | 36/256 [00:18<01:54,  1.93it/s] 14%|█▍        | 37/256 [00:19<01:53,  1.93it/s] 15%|█▍        | 38/256 [00:19<01:53,  1.93it/s] 15%|█▌        | 39/256 [00:20<01:52,  1.93it/s] 16%|█▌        | 40/256 [00:20<01:52,  1.92it/s] 16%|█▌        | 41/256 [00:21<01:51,  1.92it/s] 16%|█▋        | 42/256 [00:21<01:51,  1.93it/s] 17%|█▋        | 43/256 [00:22<01:50,  1.92it/s] 17%|█▋        | 44/256 [00:22<01:50,  1.92it/s] 18%|█▊        | 45/256 [00:23<01:49,  1.92it/s] 18%|█▊        | 46/256 [00:23<01:49,  1.93it/s] 18%|█▊        | 47/256 [00:24<01:48,  1.92it/s] 19%|█▉        | 48/256 [00:24<01:48,  1.92it/s] 19%|█▉        | 49/256 [00:25<01:47,  1.92it/s] 20%|█▉        | 50/256 [00:25<01:47,  1.92it/s] 20%|█▉        | 51/256 [00:26<01:46,  1.92it/s] 20%|██        | 52/256 [00:26<01:46,  1.92it/s] 21%|██        | 53/256 [00:27<01:45,  1.92it/s] 21%|██        | 54/256 [00:28<01:44,  1.92it/s] 21%|██▏       | 55/256 [00:28<01:44,  1.93it/s] 22%|██▏       | 56/256 [00:29<01:43,  1.92it/s] 22%|██▏       | 57/256 [00:29<01:43,  1.92it/s] 23%|██▎       | 58/256 [00:30<01:42,  1.92it/s] 23%|██▎       | 59/256 [00:30<01:42,  1.92it/s] 23%|██▎       | 60/256 [00:31<01:41,  1.92it/s] 24%|██▍       | 61/256 [00:31<01:41,  1.92it/s] 24%|██▍       | 62/256 [00:32<01:40,  1.92it/s] 25%|██▍       | 63/256 [00:32<01:40,  1.92it/s] 25%|██▌       | 64/256 [00:33<01:39,  1.92it/s] 25%|██▌       | 65/256 [00:33<01:39,  1.92it/s] 26%|██▌       | 66/256 [00:34<01:38,  1.92it/s] 26%|██▌       | 67/256 [00:34<01:38,  1.92it/s] 27%|██▋       | 68/256 [00:35<01:37,  1.92it/s] 27%|██▋       | 69/256 [00:35<01:37,  1.92it/s] 27%|██▋       | 70/256 [00:36<01:36,  1.92it/s] 28%|██▊       | 71/256 [00:36<01:36,  1.92it/s] 28%|██▊       | 72/256 [00:37<01:35,  1.92it/s] 29%|██▊       | 73/256 [00:37<01:35,  1.92it/s] 29%|██▉       | 74/256 [00:38<01:34,  1.92it/s] 29%|██▉       | 75/256 [00:38<01:34,  1.92it/s] 30%|██▉       | 76/256 [00:39<01:33,  1.92it/s] 30%|███       | 77/256 [00:40<01:33,  1.92it/s] 30%|███       | 78/256 [00:40<01:32,  1.92it/s] 31%|███       | 79/256 [00:41<01:32,  1.92it/s] 31%|███▏      | 80/256 [00:41<01:31,  1.92it/s] 32%|███▏      | 81/256 [00:42<01:31,  1.92it/s] 32%|███▏      | 82/256 [00:42<01:30,  1.92it/s] 32%|███▏      | 83/256 [00:43<01:30,  1.92it/s] 33%|███▎      | 84/256 [00:43<01:29,  1.92it/s] 33%|███▎      | 85/256 [00:44<01:29,  1.92it/s] 34%|███▎      | 86/256 [00:44<01:28,  1.92it/s] 34%|███▍      | 87/256 [00:45<01:28,  1.92it/s] 34%|███▍      | 88/256 [00:45<01:27,  1.92it/s] 35%|███▍      | 89/256 [00:46<01:26,  1.92it/s] 35%|███▌      | 90/256 [00:46<01:26,  1.92it/s] 36%|███▌      | 91/256 [00:47<01:25,  1.92it/s] 36%|███▌      | 92/256 [00:47<01:25,  1.92it/s] 36%|███▋      | 93/256 [00:48<01:24,  1.92it/s] 37%|███▋      | 94/256 [00:48<01:24,  1.92it/s] 37%|███▋      | 95/256 [00:49<01:23,  1.92it/s] 38%|███▊      | 96/256 [00:49<01:23,  1.92it/s] 38%|███▊      | 97/256 [00:50<01:22,  1.92it/s] 38%|███▊      | 98/256 [00:50<01:22,  1.92it/s] 39%|███▊      | 99/256 [00:51<01:21,  1.92it/s] 39%|███▉      | 100/256 [00:51<01:21,  1.92it/s] 39%|███▉      | 101/256 [00:52<01:20,  1.92it/s] 40%|███▉      | 102/256 [00:53<01:20,  1.92it/s] 40%|████      | 103/256 [00:53<01:19,  1.92it/s] 41%|████      | 104/256 [00:54<01:19,  1.92it/s] 41%|████      | 105/256 [00:54<01:18,  1.92it/s] 41%|████▏     | 106/256 [00:55<01:18,  1.92it/s] 42%|████▏     | 107/256 [00:55<01:17,  1.92it/s] 42%|████▏     | 108/256 [00:56<01:17,  1.92it/s] 43%|████▎     | 109/256 [00:56<01:16,  1.92it/s] 43%|████▎     | 110/256 [00:57<01:16,  1.92it/s] 43%|████▎     | 111/256 [00:57<01:15,  1.92it/s] 44%|████▍     | 112/256 [00:58<01:15,  1.92it/s] 44%|████▍     | 113/256 [00:58<01:14,  1.92it/s] 45%|████▍     | 114/256 [00:59<01:13,  1.92it/s] 45%|████▍     | 115/256 [00:59<01:13,  1.92it/s] 45%|████▌     | 116/256 [01:00<01:12,  1.92it/s] 46%|████▌     | 117/256 [01:00<01:12,  1.92it/s] 46%|████▌     | 118/256 [01:01<01:11,  1.92it/s] 46%|████▋     | 119/256 [01:01<01:11,  1.92it/s] 47%|████▋     | 120/256 [01:02<01:10,  1.92it/s] 47%|████▋     | 121/256 [01:02<01:10,  1.92it/s] 48%|████▊     | 122/256 [01:03<01:09,  1.92it/s] 48%|████▊     | 123/256 [01:03<01:09,  1.92it/s] 48%|████▊     | 124/256 [01:04<01:08,  1.92it/s] 49%|████▉     | 125/256 [01:05<01:08,  1.92it/s] 49%|████▉     | 126/256 [01:05<01:07,  1.92it/s] 50%|████▉     | 127/256 [01:06<01:07,  1.92it/s] 50%|█████     | 128/256 [01:06<01:06,  1.92it/s] 50%|█████     | 129/256 [01:07<01:06,  1.92it/s] 51%|█████     | 130/256 [01:07<01:05,  1.92it/s] 51%|█████     | 131/256 [01:08<01:05,  1.92it/s] 52%|█████▏    | 132/256 [01:08<01:04,  1.92it/s] 52%|█████▏    | 133/256 [01:09<01:04,  1.92it/s] 52%|█████▏    | 134/256 [01:09<01:03,  1.92it/s] 53%|█████▎    | 135/256 [01:10<01:03,  1.92it/s] 53%|█████▎    | 136/256 [01:10<01:02,  1.92it/s] 54%|█████▎    | 137/256 [01:11<01:02,  1.92it/s] 54%|█████▍    | 138/256 [01:11<01:01,  1.92it/s] 54%|█████▍    | 139/256 [01:12<01:00,  1.92it/s] 55%|█████▍    | 140/256 [01:12<01:00,  1.92it/s] 55%|█████▌    | 141/256 [01:13<00:59,  1.92it/s] 55%|█████▌    | 142/256 [01:13<00:59,  1.92it/s] 56%|█████▌    | 143/256 [01:14<00:58,  1.92it/s] 56%|█████▋    | 144/256 [01:14<00:58,  1.92it/s] 57%|█████▋    | 145/256 [01:15<00:57,  1.92it/s] 57%|█████▋    | 146/256 [01:15<00:57,  1.92it/s] 57%|█████▋    | 147/256 [01:16<00:56,  1.92it/s] 58%|█████▊    | 148/256 [01:16<00:56,  1.92it/s] 58%|█████▊    | 149/256 [01:17<00:55,  1.92it/s] 59%|█████▊    | 150/256 [01:18<00:55,  1.92it/s] 59%|█████▉    | 151/256 [01:18<00:54,  1.92it/s] 59%|█████▉    | 152/256 [01:19<00:54,  1.92it/s] 60%|█████▉    | 153/256 [01:19<00:53,  1.92it/s] 60%|██████    | 154/256 [01:20<00:53,  1.92it/s] 61%|██████    | 155/256 [01:20<00:52,  1.92it/s] 61%|██████    | 156/256 [01:21<00:52,  1.92it/s] 61%|██████▏   | 157/256 [01:21<00:51,  1.92it/s] 62%|██████▏   | 158/256 [01:22<00:50,  1.92it/s] 62%|██████▏   | 159/256 [01:22<00:50,  1.92it/s] 62%|██████▎   | 160/256 [01:23<00:49,  1.92it/s] 63%|██████▎   | 161/256 [01:23<00:49,  1.92it/s] 63%|██████▎   | 162/256 [01:24<00:48,  1.92it/s] 64%|██████▎   | 163/256 [01:24<00:48,  1.92it/s] 64%|██████▍   | 164/256 [01:25<00:47,  1.92it/s] 64%|██████▍   | 165/256 [01:25<00:47,  1.92it/s] 65%|██████▍   | 166/256 [01:26<00:46,  1.92it/s] 65%|██████▌   | 167/256 [01:26<00:46,  1.92it/s] 66%|██████▌   | 168/256 [01:27<00:45,  1.92it/s] 66%|██████▌   | 169/256 [01:27<00:45,  1.92it/s] 66%|██████▋   | 170/256 [01:28<00:44,  1.92it/s] 67%|██████▋   | 171/256 [01:28<00:44,  1.92it/s] 67%|██████▋   | 172/256 [01:29<00:43,  1.92it/s] 68%|██████▊   | 173/256 [01:29<00:43,  1.92it/s] 68%|██████▊   | 174/256 [01:30<00:42,  1.92it/s] 68%|██████▊   | 175/256 [01:31<00:42,  1.92it/s] 69%|██████▉   | 176/256 [01:31<00:41,  1.92it/s] 69%|██████▉   | 177/256 [01:32<00:41,  1.92it/s] 70%|██████▉   | 178/256 [01:32<00:40,  1.92it/s] 70%|██████▉   | 179/256 [01:33<00:40,  1.92it/s] 70%|███████   | 180/256 [01:33<00:39,  1.92it/s] 71%|███████   | 181/256 [01:34<00:39,  1.92it/s] 71%|███████   | 182/256 [01:34<00:38,  1.92it/s] 71%|███████▏  | 183/256 [01:35<00:38,  1.92it/s] 72%|███████▏  | 184/256 [01:35<00:37,  1.92it/s] 72%|███████▏  | 185/256 [01:36<00:37,  1.92it/s] 73%|███████▎  | 186/256 [01:36<00:36,  1.92it/s] 73%|███████▎  | 187/256 [01:37<00:35,  1.92it/s] 73%|███████▎  | 188/256 [01:37<00:35,  1.92it/s] 74%|███████▍  | 189/256 [01:38<00:34,  1.92it/s] 74%|███████▍  | 190/256 [01:38<00:34,  1.92it/s] 75%|███████▍  | 191/256 [01:39<00:33,  1.92it/s] 75%|███████▌  | 192/256 [01:39<00:33,  1.92it/s] 75%|███████▌  | 193/256 [01:40<00:32,  1.92it/s] 76%|███████▌  | 194/256 [01:40<00:32,  1.92it/s] 76%|███████▌  | 195/256 [01:41<00:31,  1.92it/s] 77%|███████▋  | 196/256 [01:41<00:31,  1.92it/s] 77%|███████▋  | 197/256 [01:42<00:30,  1.92it/s] 77%|███████▋  | 198/256 [01:43<00:30,  1.92it/s] 78%|███████▊  | 199/256 [01:43<00:29,  1.92it/s] 78%|███████▊  | 200/256 [01:44<00:29,  1.92it/s] 79%|███████▊  | 201/256 [01:44<00:28,  1.92it/s] 79%|███████▉  | 202/256 [01:45<00:28,  1.92it/s] 79%|███████▉  | 203/256 [01:45<00:27,  1.92it/s] 80%|███████▉  | 204/256 [01:46<00:27,  1.92it/s] 80%|████████  | 205/256 [01:46<00:26,  1.92it/s] 80%|████████  | 206/256 [01:47<00:26,  1.92it/s] 81%|████████  | 207/256 [01:47<00:25,  1.92it/s] 81%|████████▏ | 208/256 [01:48<00:24,  1.92it/s] 82%|████████▏ | 209/256 [01:48<00:24,  1.92it/s] 82%|████████▏ | 210/256 [01:49<00:23,  1.92it/s] 82%|████████▏ | 211/256 [01:49<00:23,  1.92it/s] 83%|████████▎ | 212/256 [01:50<00:22,  1.92it/s] 83%|████████▎ | 213/256 [01:50<00:22,  1.92it/s] 84%|████████▎ | 214/256 [01:51<00:21,  1.92it/s] 84%|████████▍ | 215/256 [01:51<00:21,  1.92it/s] 84%|████████▍ | 216/256 [01:52<00:20,  1.92it/s] 85%|████████▍ | 217/256 [01:52<00:20,  1.92it/s] 85%|████████▌ | 218/256 [01:53<00:19,  1.92it/s] 86%|████████▌ | 219/256 [01:53<00:19,  1.92it/s] 86%|████████▌ | 220/256 [01:54<00:18,  1.92it/s] 86%|████████▋ | 221/256 [01:54<00:18,  1.92it/s] 87%|████████▋ | 222/256 [01:55<00:17,  1.92it/s] 87%|████████▋ | 223/256 [01:56<00:17,  1.92it/s] 88%|████████▊ | 224/256 [01:56<00:16,  1.92it/s] 88%|████████▊ | 225/256 [01:57<00:16,  1.92it/s] 88%|████████▊ | 226/256 [01:57<00:15,  1.92it/s] 89%|████████▊ | 227/256 [01:58<00:15,  1.92it/s] 89%|████████▉ | 228/256 [01:58<00:14,  1.92it/s] 89%|████████▉ | 229/256 [01:59<00:14,  1.92it/s] 90%|████████▉ | 230/256 [01:59<00:13,  1.92it/s] 90%|█████████ | 231/256 [02:00<00:13,  1.92it/s] 91%|█████████ | 232/256 [02:00<00:12,  1.92it/s] 91%|█████████ | 233/256 [02:01<00:11,  1.92it/s] 91%|█████████▏| 234/256 [02:01<00:11,  1.92it/s] 92%|█████████▏| 235/256 [02:02<00:10,  1.92it/s] 92%|█████████▏| 236/256 [02:02<00:10,  1.92it/s] 93%|█████████▎| 237/256 [02:03<00:09,  1.92it/s] 93%|█████████▎| 238/256 [02:03<00:09,  1.92it/s] 93%|█████████▎| 239/256 [02:04<00:08,  1.92it/s] 94%|█████████▍| 240/256 [02:04<00:08,  1.92it/s] 94%|█████████▍| 241/256 [02:05<00:07,  1.92it/s] 95%|█████████▍| 242/256 [02:05<00:07,  1.92it/s] 95%|█████████▍| 243/256 [02:06<00:06,  1.92it/s] 95%|█████████▌| 244/256 [02:06<00:06,  1.92it/s] 96%|█████████▌| 245/256 [02:07<00:05,  1.92it/s] 96%|█████████▌| 246/256 [02:07<00:05,  1.92it/s] 96%|█████████▋| 247/256 [02:08<00:04,  1.92it/s] 97%|█████████▋| 248/256 [02:09<00:04,  1.92it/s] 97%|█████████▋| 249/256 [02:09<00:03,  1.92it/s] 98%|█████████▊| 250/256 [02:10<00:03,  1.92it/s] 98%|█████████▊| 251/256 [02:10<00:02,  1.92it/s] 98%|█████████▊| 252/256 [02:11<00:02,  1.92it/s] 99%|█████████▉| 253/256 [02:11<00:01,  1.92it/s] 99%|█████████▉| 254/256 [02:12<00:01,  1.92it/s]100%|█████████▉| 255/256 [02:12<00:00,  1.92it/s]100%|██████████| 256/256 [02:13<00:00,  1.92it/s]100%|██████████| 256/256 [02:13<00:00,  1.92it/s]
[2025-03-23 12:24:36 root](main_calib_config3_attn.py 161): INFO c4 : 6.4932637214660645
Selected Tasks: ['arc_easy', 'arc_challenge', 'piqa', 'winogrande', 'boolq', 'hellaswag']
Traceback (most recent call last):
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/main_calib_config3_attn.py", line 407, in <module>
    main()
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/main_calib_config3_attn.py", line 402, in main
    evaluate(lm, args,logger)
  File "/opt/conda/envs/abq-llm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/main_calib_config3_attn.py", line 165, in evaluate
    t_results = evaluator.simple_evaluate(
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/lm_eval/utils.py", line 160, in _wrapper
    return fn(*args, **kwargs)
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/lm_eval/evaluator.py", line 114, in simple_evaluate
    raise e
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/lm_eval/evaluator.py", line 88, in simple_evaluate
    task_dict = lm_eval.tasks.get_task_dict(task_names)
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/lm_eval/tasks/__init__.py", line 342, in get_task_dict
    task_name_dict = {
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/lm_eval/tasks/__init__.py", line 343, in <dictcomp>
    task_name: get_task(task_name)()
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/lm_eval/base.py", line 412, in __init__
    self.download(data_dir, cache_dir, download_mode)
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/lm_eval/base.py", line 441, in download
    self.dataset = datasets.load_dataset(
  File "/opt/conda/envs/abq-llm/lib/python3.10/site-packages/datasets/load.py", line 2129, in load_dataset
    builder_instance = load_dataset_builder(
  File "/opt/conda/envs/abq-llm/lib/python3.10/site-packages/datasets/load.py", line 1815, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/opt/conda/envs/abq-llm/lib/python3.10/site-packages/datasets/load.py", line 1512, in dataset_module_factory
    raise e1 from None
  File "/opt/conda/envs/abq-llm/lib/python3.10/site-packages/datasets/load.py", line 1468, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'ai2_arc' on the Hub (SSLError)
