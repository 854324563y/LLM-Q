nohup: ignoring input
/opt/conda/envs/abq-llm/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/opt/conda/envs/abq-llm/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/opt/conda/envs/abq-llm/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
['main_calib_config3_attn.py', '--model', '/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', '--epochs', '20', '--output_dir', './log-divide-adaptive-calibration-attnloss/Llama-2-7b-hf-0.8', '--eval_ppl', '--wbits', '4', '--abits', '4', '--let', '--lwc', '--tasks', 'piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', '--compensation_calibration', '--quant_map', 'log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.8.pkl', '--blocks_pkl', './log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl']
[2025-03-23 01:50:18 root](main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/Llama-2-7b-hf-0.8', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.8.pkl', blocks_pkl='./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/opt/conda/envs/abq-llm/lib/python3.10/site-packages/transformers/modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location=map_location)
Loading checkpoint shards:  50%|█████     | 1/2 [00:54<00:54, 54.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:20<00:00, 37.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:20<00:00, 40.25s/it]
vocab size:  32000
[2025-03-23 01:51:40 root](main_calib_config3_attn.py 350): INFO === start quantization ===
/workspace/volume/yangzhe/ABQ-LLM/algorithm/main_calib_config3_attn.py:355: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  dataloader = torch.load(cache_dataloader)
[2025-03-23 01:51:40 root](main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
/workspace/volume/yangzhe/ABQ-LLM/algorithm/main_calib_config3_attn.py:369: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  act_scales = torch.load(args.act_scales)
/workspace/volume/yangzhe/ABQ-LLM/algorithm/main_calib_config3_attn.py:370: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  act_shifts = torch.load(args.act_shifts)
[2025-03-23 01:51:40 root](abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-23 01:51:40 root](abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.8.pkl
[2025-03-23 01:51:40 root](abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 9), (9, 11), (11, 14), (14, 17), (17, 20), (20, 23), (23, 26), (26, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-23 01:51:40 root](abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12, 13], [14, 15, 16], [17, 18, 19], [20, 21, 22], [23, 24, 25], [26, 27, 28], [29], [30], [31]]
[2025-03-23 01:51:43 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
quant_map[i]:  {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 0 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 0 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 0 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
layer 0 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 0 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 0 module mlp.up_proj scheme w4a8 wbit 4 abits 8
[2025-03-23 01:51:43 root](abq_llm_calib_config3_attn.py 309): INFO use compensation vector
/workspace/volume/yangzhe/ABQ-LLM/algorithm/quantize/abq_llm_calib_config3_attn.py:342: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/workspace/volume/yangzhe/ABQ-LLM/algorithm/utils.py:45: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
/opt/conda/envs/abq-llm/lib/python3.10/site-packages/torch/nn/functional.py:3369: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
[2025-03-23 01:52:16 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.001646880293264985 norm:0.005481779109686613 max memory_allocated 34633.880859375 
[2025-03-23 01:52:44 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.0006905249902047217 norm:0.002468099817633629 max memory_allocated 34633.880859375 
[2025-03-23 01:53:13 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.0005904923309572041 norm:0.0019022306660190225 max memory_allocated 34633.880859375 
[2025-03-23 01:53:42 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0005571761284954846 norm:0.0016812117537483573 max memory_allocated 34633.880859375 
[2025-03-23 01:54:11 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.0005477633676491678 norm:0.0015103479381650686 max memory_allocated 34633.880859375 
[2025-03-23 01:54:40 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0005378435016609728 norm:0.0014722503256052732 max memory_allocated 34633.880859375 
[2025-03-23 01:55:09 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.0005239768070168793 norm:0.0013091089203953743 max memory_allocated 34633.880859375 
[2025-03-23 01:55:38 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0005143635207787156 norm:0.0012548048980534077 max memory_allocated 34633.880859375 
[2025-03-23 01:56:07 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0005060707917436957 norm:0.001154877245426178 max memory_allocated 34633.880859375 
[2025-03-23 01:56:36 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.0004965780535712838 norm:0.00104379968252033 max memory_allocated 34633.880859375 
[2025-03-23 01:57:05 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.0004902085056528449 norm:0.0010303840972483158 max memory_allocated 34633.880859375 
[2025-03-23 01:57:34 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0004848612588830292 norm:0.0009522775653749704 max memory_allocated 34633.880859375 
[2025-03-23 01:58:02 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.00047463871305808425 norm:0.0008624987676739693 max memory_allocated 34633.880859375 
[2025-03-23 01:58:31 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.0004697894037235528 norm:0.0008078549290075898 max memory_allocated 34633.880859375 
[2025-03-23 01:59:00 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0004595099890138954 norm:0.0007512163138017058 max memory_allocated 34633.880859375 
[2025-03-23 01:59:29 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.0004485922690946609 norm:0.0006756079383194447 max memory_allocated 34633.880859375 
[2025-03-23 01:59:58 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.00044460606295615435 norm:0.0006519460584968328 max memory_allocated 34633.880859375 
[2025-03-23 02:00:27 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0004472354776225984 norm:0.000646424014121294 max memory_allocated 34633.880859375 
[2025-03-23 02:00:56 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.00044829558464698493 norm:0.0005959707777947187 max memory_allocated 34633.880859375 
[2025-03-23 02:01:25 root](abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0004407578962855041 norm:0.00057517911773175 max memory_allocated 34633.880859375 
[2025-03-23 02:02:05 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-23 02:02:05 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
quant_map[i]:  {'self_attn.q_proj': 1, 'self_attn.k_proj': 2, 'self_attn.v_proj': 2, 'self_attn.o_proj': 1, 'mlp.up_proj': 2, 'mlp.down_proj': 1}
layer 1 module self_attn.k_proj scheme w8a8 wbit 8 abits 8
layer 1 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 1 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
layer 1 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
layer 1 module mlp.down_proj scheme w4a8 wbit 4 abits 8
layer 1 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 02:02:05 root](abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:02:37 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.019997332245111465 norm:0.013045408762991428 max memory_allocated 35100.7724609375 
[2025-03-23 02:03:06 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.012379325926303864 norm:0.010368408635258675 max memory_allocated 35100.7724609375 
[2025-03-23 02:03:34 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.009234786964952946 norm:0.011665391735732555 max memory_allocated 35100.7724609375 
[2025-03-23 02:04:03 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.007907364517450333 norm:0.010822172276675701 max memory_allocated 35100.7724609375 
[2025-03-23 02:04:32 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.007161897141486406 norm:0.007719370536506176 max memory_allocated 35100.7724609375 
[2025-03-23 02:05:01 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.006935914512723684 norm:0.006168747320771217 max memory_allocated 35100.7724609375 
[2025-03-23 02:05:30 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.006927717477083206 norm:0.0055550383403897285 max memory_allocated 35100.7724609375 
[2025-03-23 02:05:59 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.006697274744510651 norm:0.004931269679218531 max memory_allocated 35100.7724609375 
[2025-03-23 02:06:28 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.006804545409977436 norm:0.00488212238997221 max memory_allocated 35100.7724609375 
[2025-03-23 02:06:57 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.006703343708068132 norm:0.004925396293401718 max memory_allocated 35100.7724609375 
[2025-03-23 02:07:25 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.006539315916597843 norm:0.0046605924144387245 max memory_allocated 35100.7724609375 
[2025-03-23 02:07:54 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.0064530097879469395 norm:0.004397484939545393 max memory_allocated 35100.7724609375 
[2025-03-23 02:08:23 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.006339059676975012 norm:0.004203554708510637 max memory_allocated 35100.7724609375 
[2025-03-23 02:08:52 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.006701335776597261 norm:0.00442138547077775 max memory_allocated 35100.7724609375 
[2025-03-23 02:09:21 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.007097452878952026 norm:0.0055260746739804745 max memory_allocated 35100.7724609375 
[2025-03-23 02:09:50 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.008305588737130165 norm:0.00813346542418003 max memory_allocated 35100.7724609375 
[2025-03-23 02:10:18 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.006352658849209547 norm:0.005980010610073805 max memory_allocated 35100.7724609375 
[2025-03-23 02:10:47 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.006314116530120373 norm:0.005523199215531349 max memory_allocated 35100.7724609375 
[2025-03-23 02:11:16 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.006368696689605713 norm:0.005370819941163063 max memory_allocated 35100.7724609375 
[2025-03-23 02:11:45 root](abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.00644927890971303 norm:0.005516005214303732 max memory_allocated 35100.7724609375 
[2025-03-23 02:12:21 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-23 02:12:21 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 1, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 2 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 2 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 2 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 2 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
layer 2 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 2 module mlp.up_proj scheme w4a8 wbit 4 abits 8
[2025-03-23 02:12:21 root](abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:12:53 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.01123273279517889 norm:0.005269140936434269 max memory_allocated 35100.8349609375 
[2025-03-23 02:13:21 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.008469857275485992 norm:0.0040510003454983234 max memory_allocated 35100.8349609375 
[2025-03-23 02:13:50 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.007627128157764673 norm:0.003068511374294758 max memory_allocated 35100.8349609375 
[2025-03-23 02:14:19 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.007073301821947098 norm:0.0025745308957993984 max memory_allocated 35100.8349609375 
[2025-03-23 02:14:48 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.006694361101835966 norm:0.002234020736068487 max memory_allocated 35100.8349609375 
[2025-03-23 02:15:16 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.006384551525115967 norm:0.0019445179495960474 max memory_allocated 35100.8349609375 
[2025-03-23 02:15:45 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.006214573513716459 norm:0.0016869562678039074 max memory_allocated 35100.8349609375 
[2025-03-23 02:16:14 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.006145543418824673 norm:0.0014378309715539217 max memory_allocated 35100.8349609375 
[2025-03-23 02:16:43 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.006109223701059818 norm:0.00120948301628232 max memory_allocated 35100.8349609375 
[2025-03-23 02:17:12 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.006073488388210535 norm:0.000998809584416449 max memory_allocated 35100.8349609375 
[2025-03-23 02:17:41 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.00604974664747715 norm:0.0008104215376079082 max memory_allocated 35100.8349609375 
[2025-03-23 02:18:10 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.006052233278751373 norm:0.0007336597773246467 max memory_allocated 35100.8349609375 
[2025-03-23 02:18:39 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.00610320782288909 norm:0.0007722800364717841 max memory_allocated 35100.8349609375 
[2025-03-23 02:19:08 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.006021687760949135 norm:0.0006238053902052343 max memory_allocated 35100.8349609375 
[2025-03-23 02:19:37 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.006028586998581886 norm:0.0006228568963706493 max memory_allocated 35100.8349609375 
[2025-03-23 02:20:06 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.006060851737856865 norm:0.0006803203141316772 max memory_allocated 35100.8349609375 
[2025-03-23 02:20:35 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.006195015273988247 norm:0.0008959532715380192 max memory_allocated 35100.8349609375 
[2025-03-23 02:21:04 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.006249629892408848 norm:0.0008750483975745738 max memory_allocated 35100.8349609375 
[2025-03-23 02:21:33 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.006044068839401007 norm:0.000547770643606782 max memory_allocated 35100.8349609375 
[2025-03-23 02:22:02 root](abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.005964010488241911 norm:0.0003437469422351569 max memory_allocated 35100.8349609375 
[2025-03-23 02:22:38 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2]
[2025-03-23 02:22:38 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4, 5] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 1, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 3 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 3 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 3 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 3 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
layer 3 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 3 module mlp.up_proj scheme w4a8 wbit 4 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 4 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 4 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 4 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 4 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 4 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 4 module mlp.up_proj scheme w4a8 wbit 4 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 5 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 5 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 5 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 5 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 5 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 5 module mlp.up_proj scheme w4a8 wbit 4 abits 8
[2025-03-23 02:24:13 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 0 loss:0.01885981857776642 norm:0.0002917726233135909 max memory_allocated 47477.6044921875 
[2025-03-23 02:25:38 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 1 loss:0.016628548502922058 norm:0.00020847997802775353 max memory_allocated 47477.6044921875 
[2025-03-23 02:27:04 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 2 loss:0.015289390459656715 norm:0.00016815509297885 max memory_allocated 47477.6044921875 
[2025-03-23 02:28:30 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 3 loss:0.013929877430200577 norm:0.00014683970948681235 max memory_allocated 47477.6044921875 
[2025-03-23 02:29:56 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 4 loss:0.012794043868780136 norm:0.00014803583326283842 max memory_allocated 47477.6044921875 
[2025-03-23 02:31:22 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 5 loss:0.012071375735104084 norm:0.00013827322982251644 max memory_allocated 47477.6044921875 
[2025-03-23 02:32:48 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 6 loss:0.011645051650702953 norm:0.00014248202205635607 max memory_allocated 47477.6044921875 
[2025-03-23 02:34:14 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 7 loss:0.011433541774749756 norm:0.00015471891674678773 max memory_allocated 47477.6044921875 
[2025-03-23 02:35:40 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 8 loss:0.01132405549287796 norm:0.00014406454283744097 max memory_allocated 47477.6044921875 
[2025-03-23 02:37:06 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 9 loss:0.011286155320703983 norm:0.00016314238018821925 max memory_allocated 47477.6044921875 
[2025-03-23 02:38:33 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 10 loss:0.011253099888563156 norm:0.00017299712635576725 max memory_allocated 47477.6044921875 
[2025-03-23 02:39:59 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 11 loss:0.011237331666052341 norm:0.00018108272342942655 max memory_allocated 47477.6044921875 
[2025-03-23 02:41:25 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 12 loss:0.011217370629310608 norm:0.0001599045644979924 max memory_allocated 47477.6044921875 
[2025-03-23 02:42:51 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 13 loss:0.011194356717169285 norm:0.00016228470485657454 max memory_allocated 47477.6044921875 
[2025-03-23 02:44:17 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 14 loss:0.01117772702127695 norm:0.00021667443797923625 max memory_allocated 47477.6044921875 
[2025-03-23 02:45:43 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 15 loss:0.01117704901844263 norm:0.0002353210875298828 max memory_allocated 47477.6044921875 
[2025-03-23 02:47:09 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 16 loss:0.011165972799062729 norm:0.0002356524346396327 max memory_allocated 47477.6044921875 
[2025-03-23 02:48:35 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 17 loss:0.011127489618957043 norm:0.00021704749087803066 max memory_allocated 47477.6044921875 
[2025-03-23 02:50:01 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 18 loss:0.011121832765638828 norm:0.00021577472216449678 max memory_allocated 47477.6044921875 
[2025-03-23 02:51:28 root](abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 19 loss:0.01111689954996109 norm:0.00022091779101174325 max memory_allocated 47477.6044921875 
[2025-03-23 02:53:23 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [3, 4, 5]
[2025-03-23 02:53:23 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [6, 7, 8] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 1, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 6 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 6 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 6 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 6 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
layer 6 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 6 module mlp.up_proj scheme w4a8 wbit 4 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 7 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 7 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 7 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 7 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 7 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 7 module mlp.up_proj scheme w4a8 wbit 4 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 8 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 8 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 8 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 8 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 8 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 8 module mlp.up_proj scheme w4a8 wbit 4 abits 8
[2025-03-23 02:54:56 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 0 loss:0.026061182841658592 norm:0.000509183737449348 max memory_allocated 47477.7919921875 
[2025-03-23 02:56:22 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 1 loss:0.02241801656782627 norm:0.00028523860964924097 max memory_allocated 47477.7919921875 
[2025-03-23 02:57:47 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 2 loss:0.020408257842063904 norm:0.000232747828704305 max memory_allocated 47477.7919921875 
[2025-03-23 02:59:13 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 3 loss:0.018554210662841797 norm:0.00019626604625955224 max memory_allocated 47477.7919921875 
[2025-03-23 03:00:39 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 4 loss:0.017137881368398666 norm:0.00017614771786611527 max memory_allocated 47477.7919921875 
[2025-03-23 03:02:05 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 5 loss:0.016265785321593285 norm:0.00016987218987196684 max memory_allocated 47477.7919921875 
[2025-03-23 03:03:31 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 6 loss:0.015771014615893364 norm:0.00017207908967975527 max memory_allocated 47477.7919921875 
[2025-03-23 03:04:57 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 7 loss:0.015488101169466972 norm:0.0001691985671641305 max memory_allocated 47477.7919921875 
[2025-03-23 03:06:23 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 8 loss:0.015294177457690239 norm:0.00016498945478815585 max memory_allocated 47477.7919921875 
[2025-03-23 03:07:49 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 9 loss:0.015176916494965553 norm:0.00016272450739052147 max memory_allocated 47477.7919921875 
[2025-03-23 03:09:15 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 10 loss:0.015116948634386063 norm:0.00016297775437124074 max memory_allocated 47477.7919921875 
[2025-03-23 03:10:41 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 11 loss:0.015065494924783707 norm:0.00015562075714115053 max memory_allocated 47477.7919921875 
[2025-03-23 03:12:07 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 12 loss:0.015008890070021152 norm:0.00015094754053279757 max memory_allocated 47477.7919921875 
[2025-03-23 03:13:33 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 13 loss:0.014948591589927673 norm:0.00014651792298536748 max memory_allocated 47477.7919921875 
[2025-03-23 03:14:59 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 14 loss:0.014906964264810085 norm:0.000145758138387464 max memory_allocated 47477.7919921875 
[2025-03-23 03:16:24 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 15 loss:0.01488126814365387 norm:0.0001407986128469929 max memory_allocated 47477.7919921875 
[2025-03-23 03:17:50 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 16 loss:0.014861887320876122 norm:0.00014992785872891545 max memory_allocated 47477.7919921875 
[2025-03-23 03:19:16 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 17 loss:0.014825358986854553 norm:0.0001407674135407433 max memory_allocated 47477.7919921875 
[2025-03-23 03:20:42 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 18 loss:0.014804764650762081 norm:0.00014513946371152997 max memory_allocated 47477.7919921875 
[2025-03-23 03:22:08 root](abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 19 loss:0.014774714596569538 norm:0.00013302153092809021 max memory_allocated 47477.7919921875 
[2025-03-23 03:23:56 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [6, 7, 8]
[2025-03-23 03:23:56 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [9, 10] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 9 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 9 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 9 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 9 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 9 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 9 module mlp.up_proj scheme w4a8 wbit 4 abits 8
quant_map[i]:  {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 10 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 10 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 10 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
layer 10 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 10 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 10 module mlp.up_proj scheme w4a8 wbit 4 abits 8
[2025-03-23 03:24:58 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 0 loss:0.022444643080234528 norm:0.0004058841150254011 max memory_allocated 47477.7919921875 
[2025-03-23 03:25:55 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 1 loss:0.019942231476306915 norm:0.0002687395899556577 max memory_allocated 47477.7919921875 
[2025-03-23 03:26:52 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 2 loss:0.018532050773501396 norm:0.00020157943072263151 max memory_allocated 47477.7919921875 
[2025-03-23 03:27:49 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 3 loss:0.017280885949730873 norm:0.00017239426961168647 max memory_allocated 47477.7919921875 
[2025-03-23 03:28:47 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 4 loss:0.016325535252690315 norm:0.00015975556743796915 max memory_allocated 47477.7919921875 
[2025-03-23 03:29:44 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 5 loss:0.01572468876838684 norm:0.00014659689622931182 max memory_allocated 47477.7919921875 
[2025-03-23 03:30:41 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 6 loss:0.015358214266598225 norm:0.0001280000724364072 max memory_allocated 47477.7919921875 
[2025-03-23 03:31:39 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 7 loss:0.015196803957223892 norm:0.0001282125012949109 max memory_allocated 47477.7919921875 
[2025-03-23 03:32:36 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 8 loss:0.015081081539392471 norm:0.00011903754784725606 max memory_allocated 47477.7919921875 
[2025-03-23 03:33:34 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 9 loss:0.014987118542194366 norm:0.00011461014946689829 max memory_allocated 47477.7919921875 
[2025-03-23 03:34:31 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 10 loss:0.014910100027918816 norm:0.00011260262544965371 max memory_allocated 47477.7919921875 
[2025-03-23 03:35:28 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 11 loss:0.014863040298223495 norm:0.00010964273678837344 max memory_allocated 47477.7919921875 
[2025-03-23 03:36:26 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 12 loss:0.014816605485975742 norm:0.00010106467379955575 max memory_allocated 47477.7919921875 
[2025-03-23 03:37:23 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 13 loss:0.014790009707212448 norm:0.00010344982001697645 max memory_allocated 47477.7919921875 
[2025-03-23 03:38:21 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 14 loss:0.014782898128032684 norm:0.00010799665324157104 max memory_allocated 47477.7919921875 
[2025-03-23 03:39:18 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 15 loss:0.014750834554433823 norm:0.00010979895159834996 max memory_allocated 47477.7919921875 
[2025-03-23 03:40:16 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 16 loss:0.014746613800525665 norm:0.00011836535850306973 max memory_allocated 47477.7919921875 
[2025-03-23 03:41:13 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 17 loss:0.01470878068357706 norm:0.00010693319200072438 max memory_allocated 47477.7919921875 
[2025-03-23 03:42:10 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 18 loss:0.014688483439385891 norm:0.0001082190647139214 max memory_allocated 47477.7919921875 
[2025-03-23 03:43:08 root](abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 19 loss:0.014710033312439919 norm:0.00012629789125639945 max memory_allocated 47477.7919921875 
[2025-03-23 03:44:24 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [9, 10]
[2025-03-23 03:44:24 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [11, 12, 13] ===
quant_map[i]:  {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 11 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 11 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 11 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
layer 11 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 11 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 11 module mlp.up_proj scheme w4a8 wbit 4 abits 8
quant_map[i]:  {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 12 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 12 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 12 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
layer 12 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 12 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 12 module mlp.up_proj scheme w4a8 wbit 4 abits 8
quant_map[i]:  {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 13 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 13 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 13 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
layer 13 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 13 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 13 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 03:45:58 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 0 loss:0.026873424649238586 norm:0.0005212620599195361 max memory_allocated 47478.1044921875 
[2025-03-23 03:47:23 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 1 loss:0.022991321980953217 norm:0.00031136596226133406 max memory_allocated 47478.1044921875 
[2025-03-23 03:48:49 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 2 loss:0.021124614402651787 norm:0.00024786623544059694 max memory_allocated 47478.1044921875 
[2025-03-23 03:50:15 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 3 loss:0.019726021215319633 norm:0.00021303108951542526 max memory_allocated 47478.1044921875 
[2025-03-23 03:51:40 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 4 loss:0.018607858568429947 norm:0.0001922729570651427 max memory_allocated 47478.1044921875 
[2025-03-23 03:53:06 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 5 loss:0.017741188406944275 norm:0.00017232002574019134 max memory_allocated 47478.1044921875 
[2025-03-23 03:54:32 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 6 loss:0.017181169241666794 norm:0.0001703258603811264 max memory_allocated 47478.1044921875 
[2025-03-23 03:55:58 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 7 loss:0.016766460612416267 norm:0.00016029702965170145 max memory_allocated 47478.1044921875 
[2025-03-23 03:57:24 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 8 loss:0.016487261280417442 norm:0.00015059643192216754 max memory_allocated 47478.1044921875 
[2025-03-23 03:58:50 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 9 loss:0.0162678062915802 norm:0.000147780665429309 max memory_allocated 47478.1044921875 
[2025-03-23 04:00:16 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 10 loss:0.016114449128508568 norm:0.0001361351751256734 max memory_allocated 47478.1044921875 
[2025-03-23 04:01:42 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 11 loss:0.015998033806681633 norm:0.0001315850531682372 max memory_allocated 47478.1044921875 
[2025-03-23 04:03:08 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 12 loss:0.015919161960482597 norm:0.00013214215869084 max memory_allocated 47478.1044921875 
[2025-03-23 04:04:33 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 13 loss:0.01585511863231659 norm:0.00013953291636426002 max memory_allocated 47478.1044921875 
[2025-03-23 04:05:59 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 14 loss:0.015790844336152077 norm:0.0001304071774939075 max memory_allocated 47478.1044921875 
[2025-03-23 04:07:25 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 15 loss:0.015719275921583176 norm:0.00012226610851939768 max memory_allocated 47478.1044921875 
[2025-03-23 04:08:51 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 16 loss:0.015690520405769348 norm:0.00012352188059594482 max memory_allocated 47478.1044921875 
[2025-03-23 04:10:17 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 17 loss:0.015658898279070854 norm:0.00012517659342847764 max memory_allocated 47478.1044921875 
[2025-03-23 04:11:43 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 18 loss:0.015624171122908592 norm:0.00012228393461555243 max memory_allocated 47478.1044921875 
[2025-03-23 04:13:09 root](abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 19 loss:0.015606331638991833 norm:0.00011689977691275999 max memory_allocated 47478.1044921875 
[2025-03-23 04:15:00 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [11, 12, 13]
[2025-03-23 04:15:00 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [14, 15, 16] ===
quant_map[i]:  {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 14 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 14 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 14 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
layer 14 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 14 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 14 module mlp.up_proj scheme w4a8 wbit 4 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 15 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 15 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 15 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 15 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 15 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 15 module mlp.up_proj scheme w4a8 wbit 4 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 16 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 16 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 16 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 16 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 16 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 16 module mlp.up_proj scheme w4a8 wbit 4 abits 8
[2025-03-23 04:16:34 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 0 loss:0.02670440264046192 norm:0.0005021447432227433 max memory_allocated 47479.1669921875 
[2025-03-23 04:17:59 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 1 loss:0.02344154380261898 norm:0.0003032833628822118 max memory_allocated 47479.1669921875 
[2025-03-23 04:19:25 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 2 loss:0.02166362851858139 norm:0.00022125532268546522 max memory_allocated 47479.1669921875 
[2025-03-23 04:20:51 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 3 loss:0.02023172751069069 norm:0.00018038127745967358 max memory_allocated 47479.1669921875 
[2025-03-23 04:22:17 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 4 loss:0.019083108752965927 norm:0.00016148461145348847 max memory_allocated 47479.1669921875 
[2025-03-23 04:23:43 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 5 loss:0.018257979303598404 norm:0.0001492848969064653 max memory_allocated 47479.1669921875 
[2025-03-23 04:25:09 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 6 loss:0.01771174743771553 norm:0.00014313828432932496 max memory_allocated 47479.1669921875 
[2025-03-23 04:26:35 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 7 loss:0.01734812930226326 norm:0.00013103059609420598 max memory_allocated 47479.1669921875 
[2025-03-23 04:28:01 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 8 loss:0.017118312418460846 norm:0.00012535594578366727 max memory_allocated 47479.1669921875 
[2025-03-23 04:29:27 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 9 loss:0.016957957297563553 norm:0.00012185205559944734 max memory_allocated 47479.1669921875 
[2025-03-23 04:30:53 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 10 loss:0.016822559759020805 norm:0.00011533524957485497 max memory_allocated 47479.1669921875 
[2025-03-23 04:32:19 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 11 loss:0.01672329567372799 norm:0.00010986155393766239 max memory_allocated 47479.1669921875 
[2025-03-23 04:33:45 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 12 loss:0.0166497603058815 norm:0.0001057530171237886 max memory_allocated 47479.1669921875 
[2025-03-23 04:35:11 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 13 loss:0.01657293736934662 norm:0.00010437400487717241 max memory_allocated 47479.1669921875 
[2025-03-23 04:36:37 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 14 loss:0.016500767320394516 norm:0.00010024158837040886 max memory_allocated 47479.1669921875 
[2025-03-23 04:38:03 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 15 loss:0.016442961990833282 norm:0.00010177034710068256 max memory_allocated 47479.1669921875 
[2025-03-23 04:39:29 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 16 loss:0.016398487612605095 norm:9.99763433355838e-05 max memory_allocated 47479.1669921875 
[2025-03-23 04:40:55 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 17 loss:0.016348915174603462 norm:9.207909897668287e-05 max memory_allocated 47479.1669921875 
[2025-03-23 04:42:21 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 18 loss:0.016303295269608498 norm:9.188698459183797e-05 max memory_allocated 47479.1669921875 
[2025-03-23 04:43:46 root](abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 19 loss:0.01627851463854313 norm:9.257750934921205e-05 max memory_allocated 47479.1669921875 
[2025-03-23 04:45:34 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [14, 15, 16]
[2025-03-23 04:45:34 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [17, 18, 19] ===
quant_map[i]:  {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 17 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 17 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 17 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
layer 17 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 17 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 17 module mlp.up_proj scheme w4a8 wbit 4 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 18 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 18 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 18 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 18 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 18 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 18 module mlp.up_proj scheme w4a8 wbit 4 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 19 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 19 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 19 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 19 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 19 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 19 module mlp.up_proj scheme w4a8 wbit 4 abits 8
[2025-03-23 04:47:08 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 0 loss:0.027875514701008797 norm:0.0003418750420678407 max memory_allocated 47479.4794921875 
[2025-03-23 04:48:33 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 1 loss:0.02573114074766636 norm:0.00021641362400259823 max memory_allocated 47479.4794921875 
[2025-03-23 04:49:59 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 2 loss:0.02432650700211525 norm:0.00016644703282509 max memory_allocated 47479.4794921875 
[2025-03-23 04:51:25 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 3 loss:0.022992068901658058 norm:0.00014191065565682948 max memory_allocated 47479.4794921875 
[2025-03-23 04:52:51 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 4 loss:0.021682370454072952 norm:0.0001304793549934402 max memory_allocated 47479.4794921875 
[2025-03-23 04:54:17 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 5 loss:0.020780472084879875 norm:0.0001229258195962757 max memory_allocated 47479.4794921875 
[2025-03-23 04:55:43 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 6 loss:0.020365701988339424 norm:0.00011880499368999153 max memory_allocated 47479.4794921875 
[2025-03-23 04:57:09 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 7 loss:0.02011924982070923 norm:0.00010591487807687372 max memory_allocated 47479.4794921875 
[2025-03-23 04:58:35 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 8 loss:0.019962668418884277 norm:0.00010097495396621525 max memory_allocated 47479.4794921875 
[2025-03-23 05:00:01 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 9 loss:0.019852828234434128 norm:9.243206295650452e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:01:27 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 10 loss:0.019777068868279457 norm:8.82241438375786e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:02:53 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 11 loss:0.019701382145285606 norm:8.478846575599164e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:04:19 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 12 loss:0.01964462921023369 norm:8.362310472875834e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:05:45 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 13 loss:0.019607307389378548 norm:8.558542322134599e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:07:12 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 14 loss:0.019549120217561722 norm:7.992239989107475e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:08:38 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 15 loss:0.019519072026014328 norm:8.241604518843815e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:10:04 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 16 loss:0.01949683204293251 norm:7.888674008427188e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:11:30 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 17 loss:0.01944219507277012 norm:7.480243948521093e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:12:56 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 18 loss:0.01942623034119606 norm:7.72071216488257e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:14:22 root](abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 19 loss:0.019386686384677887 norm:7.32584303477779e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:16:09 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [17, 18, 19]
[2025-03-23 05:16:09 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [20, 21, 22] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 20 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 20 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 20 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 20 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 20 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 20 module mlp.up_proj scheme w4a8 wbit 4 abits 8
quant_map[i]:  {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 1, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 21 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 21 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
layer 21 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
layer 21 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
layer 21 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 21 module mlp.up_proj scheme w8a8 wbit 8 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 22 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 22 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 22 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 22 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 22 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 22 module mlp.up_proj scheme w4a8 wbit 4 abits 8
[2025-03-23 05:17:43 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 0 loss:0.037762682884931564 norm:0.0007251927745528519 max memory_allocated 47479.4794921875 
[2025-03-23 05:19:08 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 1 loss:0.03469263017177582 norm:0.00035956379724666476 max memory_allocated 47479.4794921875 
[2025-03-23 05:20:34 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 2 loss:0.03281141817569733 norm:0.0002458635426592082 max memory_allocated 47479.4794921875 
[2025-03-23 05:22:00 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 3 loss:0.0309298075735569 norm:0.0001928800338646397 max memory_allocated 47479.4794921875 
[2025-03-23 05:23:26 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 4 loss:0.029233241453766823 norm:0.00017628514615353197 max memory_allocated 47479.4794921875 
[2025-03-23 05:24:52 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 5 loss:0.028298087418079376 norm:0.00016336936096195132 max memory_allocated 47479.4794921875 
[2025-03-23 05:26:18 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 6 loss:0.027980118989944458 norm:0.00016189657617360353 max memory_allocated 47479.4794921875 
[2025-03-23 05:27:44 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 7 loss:0.027805142104625702 norm:0.00014195014955475926 max memory_allocated 47479.4794921875 
[2025-03-23 05:29:10 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 8 loss:0.02766634337604046 norm:0.00014145382738206536 max memory_allocated 47479.4794921875 
[2025-03-23 05:30:36 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 9 loss:0.027549192309379578 norm:0.00013389057130552828 max memory_allocated 47479.4794921875 
[2025-03-23 05:32:02 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 10 loss:0.02746998891234398 norm:0.0001331526436842978 max memory_allocated 47479.4794921875 
[2025-03-23 05:33:28 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 11 loss:0.027374841272830963 norm:0.0001252203219337389 max memory_allocated 47479.4794921875 
[2025-03-23 05:34:54 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 12 loss:0.027296941727399826 norm:0.00012482755118981004 max memory_allocated 47479.4794921875 
[2025-03-23 05:36:20 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 13 loss:0.027233034372329712 norm:0.00011463902774266899 max memory_allocated 47479.4794921875 
[2025-03-23 05:37:46 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 14 loss:0.02718203514814377 norm:0.00011365484533598647 max memory_allocated 47479.4794921875 
[2025-03-23 05:39:12 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 15 loss:0.02710839919745922 norm:0.00011144389281980693 max memory_allocated 47479.4794921875 
[2025-03-23 05:40:38 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 16 loss:0.02704940363764763 norm:0.00010839701280929148 max memory_allocated 47479.4794921875 
[2025-03-23 05:42:04 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 17 loss:0.026990141719579697 norm:0.00010561817180132493 max memory_allocated 47479.4794921875 
[2025-03-23 05:43:30 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 18 loss:0.02694406360387802 norm:0.00010153894254472107 max memory_allocated 47479.4794921875 
[2025-03-23 05:44:57 root](abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 19 loss:0.026917608454823494 norm:9.840990242082626e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:46:44 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [20, 21, 22]
[2025-03-23 05:46:45 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [23, 24, 25] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 1, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 23 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 23 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 23 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 23 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
layer 23 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 23 module mlp.up_proj scheme w4a8 wbit 4 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 1, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 24 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 24 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
layer 24 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 24 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
layer 24 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 24 module mlp.up_proj scheme w4a8 wbit 4 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 25 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 25 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 25 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 25 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 25 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 25 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 05:48:18 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 0 loss:0.05535558611154556 norm:0.001562854740768671 max memory_allocated 47479.4794921875 
[2025-03-23 05:49:43 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 1 loss:0.050439540296792984 norm:0.000497407338116318 max memory_allocated 47479.4794921875 
[2025-03-23 05:51:09 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 2 loss:0.04751797765493393 norm:0.00034831176162697375 max memory_allocated 47479.4794921875 
[2025-03-23 05:52:35 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 3 loss:0.04453587904572487 norm:0.0002787630073726177 max memory_allocated 47479.4794921875 
[2025-03-23 05:54:01 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 4 loss:0.042379625141620636 norm:0.0002515286032576114 max memory_allocated 47479.4794921875 
[2025-03-23 05:55:28 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 5 loss:0.041761789470911026 norm:0.0002411598397884518 max memory_allocated 47479.4794921875 
[2025-03-23 05:56:54 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 6 loss:0.04150193929672241 norm:0.0002266931114718318 max memory_allocated 47479.4794921875 
[2025-03-23 05:58:20 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 7 loss:0.04130806773900986 norm:0.0002093568618874997 max memory_allocated 47479.4794921875 
[2025-03-23 05:59:46 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 8 loss:0.04118061438202858 norm:0.00020430440781638026 max memory_allocated 47479.4794921875 
[2025-03-23 06:01:12 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 9 loss:0.04102194309234619 norm:0.00018926852499134839 max memory_allocated 47479.4794921875 
[2025-03-23 06:02:38 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 10 loss:0.040954917669296265 norm:0.00019812236132565886 max memory_allocated 47479.4794921875 
[2025-03-23 06:04:04 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 11 loss:0.04081632196903229 norm:0.00017694183043204248 max memory_allocated 47479.4794921875 
[2025-03-23 06:05:30 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 12 loss:0.0407286062836647 norm:0.00017151236534118652 max memory_allocated 47479.4794921875 
[2025-03-23 06:06:56 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 13 loss:0.040630895644426346 norm:0.00017342880892101675 max memory_allocated 47479.4794921875 
[2025-03-23 06:08:22 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 14 loss:0.04054480046033859 norm:0.00016555702313780785 max memory_allocated 47479.4794921875 
[2025-03-23 06:09:48 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 15 loss:0.04045158252120018 norm:0.00015949604858178645 max memory_allocated 47479.4794921875 
[2025-03-23 06:11:14 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 16 loss:0.040397025644779205 norm:0.0001558760995976627 max memory_allocated 47479.4794921875 
[2025-03-23 06:12:41 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 17 loss:0.040364183485507965 norm:0.00014910625759512186 max memory_allocated 47479.4794921875 
[2025-03-23 06:14:07 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 18 loss:0.04032950475811958 norm:0.00016006140504032373 max memory_allocated 47479.4794921875 
[2025-03-23 06:15:33 root](abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 19 loss:0.04028774052858353 norm:0.00015391134365927428 max memory_allocated 47479.4794921875 
[2025-03-23 06:17:22 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [23, 24, 25]
[2025-03-23 06:17:22 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [26, 27, 28] ===
quant_map[i]:  {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 26 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 26 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 26 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
layer 26 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 26 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 26 module mlp.up_proj scheme w4a8 wbit 4 abits 8
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 1, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 27 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 27 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 27 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 27 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
layer 27 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 27 module mlp.up_proj scheme w4a8 wbit 4 abits 8
quant_map[i]:  {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 2, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 28 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 28 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 28 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
layer 28 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 28 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 28 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 06:17:23 root](abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 06:18:55 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 0 loss:0.07682256400585175 norm:0.004158841911703348 max memory_allocated 47479.4794921875 
[2025-03-23 06:20:21 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 1 loss:0.07167704403400421 norm:0.003051223000511527 max memory_allocated 47479.4794921875 
[2025-03-23 06:21:47 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 2 loss:0.06781598180532455 norm:0.0024185264483094215 max memory_allocated 47479.4794921875 
[2025-03-23 06:23:13 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 3 loss:0.06386371701955795 norm:0.0019485754892230034 max memory_allocated 47479.4794921875 
[2025-03-23 06:24:39 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 4 loss:0.06219135969877243 norm:0.0016543288948014379 max memory_allocated 47479.4794921875 
[2025-03-23 06:26:05 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 5 loss:0.0616462305188179 norm:0.0014050842728465796 max memory_allocated 47479.4794921875 
[2025-03-23 06:27:32 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 6 loss:0.06132534146308899 norm:0.0012778909876942635 max memory_allocated 47479.4794921875 
[2025-03-23 06:28:58 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 7 loss:0.06114134192466736 norm:0.0011829897994175553 max memory_allocated 47479.4794921875 
[2025-03-23 06:30:24 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 8 loss:0.06094159930944443 norm:0.0012533057015389204 max memory_allocated 47479.4794921875 
[2025-03-23 06:31:50 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 9 loss:0.06077516824007034 norm:0.0009420958813279867 max memory_allocated 47479.4794921875 
[2025-03-23 06:33:16 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 10 loss:0.06053396314382553 norm:0.001077683991752565 max memory_allocated 47479.4794921875 
[2025-03-23 06:34:43 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 11 loss:0.06044231727719307 norm:0.0009579053148627281 max memory_allocated 47479.4794921875 
[2025-03-23 06:36:09 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 12 loss:0.06026832386851311 norm:0.0010440826881676912 max memory_allocated 47479.4794921875 
[2025-03-23 06:37:35 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 13 loss:0.06020139157772064 norm:0.0009282925166189671 max memory_allocated 47479.4794921875 
[2025-03-23 06:39:01 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 14 loss:0.060104381293058395 norm:0.0009421625290997326 max memory_allocated 47479.4794921875 
[2025-03-23 06:40:28 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 15 loss:0.05996683984994888 norm:0.0009037971612997353 max memory_allocated 47479.4794921875 
[2025-03-23 06:41:54 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 16 loss:0.059851258993148804 norm:0.0008767676190473139 max memory_allocated 47479.4794921875 
[2025-03-23 06:43:20 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 17 loss:0.059800948947668076 norm:0.0008642892935313284 max memory_allocated 47479.4794921875 
[2025-03-23 06:44:46 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 18 loss:0.05975016579031944 norm:0.0008367772679775953 max memory_allocated 47479.4794921875 
[2025-03-23 06:46:13 root](abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 19 loss:0.05971073359251022 norm:0.0008148596389219165 max memory_allocated 47479.4794921875 
[2025-03-23 06:48:01 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [26, 27, 28]
[2025-03-23 06:48:01 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [29] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 2, 'self_attn.o_proj': 1, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 29 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 29 module self_attn.v_proj scheme w8a8 wbit 8 abits 8
layer 29 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 29 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
layer 29 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 29 module mlp.up_proj scheme w4a8 wbit 4 abits 8
[2025-03-23 06:48:02 root](abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 06:48:33 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 0 loss:0.07443132251501083 norm:0.004617026541382074 max memory_allocated 47479.4794921875 
[2025-03-23 06:49:01 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 1 loss:0.07093024998903275 norm:0.0036793285980820656 max memory_allocated 47479.4794921875 
[2025-03-23 06:49:30 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 2 loss:0.0681774914264679 norm:0.002735479734838009 max memory_allocated 47479.4794921875 
[2025-03-23 06:49:59 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 3 loss:0.06636321544647217 norm:0.002212500898167491 max memory_allocated 47479.4794921875 
[2025-03-23 06:50:27 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 4 loss:0.06595766544342041 norm:0.0019332633819431067 max memory_allocated 47479.4794921875 
[2025-03-23 06:50:56 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 5 loss:0.06579062342643738 norm:0.001663627801463008 max memory_allocated 47479.4794921875 
[2025-03-23 06:51:25 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 6 loss:0.06563335657119751 norm:0.001409997814334929 max memory_allocated 47479.4794921875 
[2025-03-23 06:51:54 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 7 loss:0.06553281098604202 norm:0.0012315066996961832 max memory_allocated 47479.4794921875 
[2025-03-23 06:52:23 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 8 loss:0.0654667466878891 norm:0.001146547612734139 max memory_allocated 47479.4794921875 
[2025-03-23 06:52:52 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 9 loss:0.06539580225944519 norm:0.0011168060591444373 max memory_allocated 47479.4794921875 
[2025-03-23 06:53:21 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 10 loss:0.06542110443115234 norm:0.0010817149886861444 max memory_allocated 47479.4794921875 
[2025-03-23 06:53:49 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 11 loss:0.06534865498542786 norm:0.0010681002167984843 max memory_allocated 47479.4794921875 
[2025-03-23 06:54:18 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 12 loss:0.06533791124820709 norm:0.0008846708224155009 max memory_allocated 47479.4794921875 
[2025-03-23 06:54:47 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 13 loss:0.06527459621429443 norm:0.001043774769641459 max memory_allocated 47479.4794921875 
[2025-03-23 06:55:16 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 14 loss:0.06529540568590164 norm:0.0008231651736423373 max memory_allocated 47479.4794921875 
[2025-03-23 06:55:45 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 15 loss:0.0651744082570076 norm:0.0009212063159793615 max memory_allocated 47479.4794921875 
[2025-03-23 06:56:14 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 16 loss:0.06520268321037292 norm:0.00082716642646119 max memory_allocated 47479.4794921875 
[2025-03-23 06:56:43 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 17 loss:0.06513465195894241 norm:0.0008743939106352627 max memory_allocated 47479.4794921875 
[2025-03-23 06:57:12 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 18 loss:0.06511692702770233 norm:0.0008111090864986181 max memory_allocated 47479.4794921875 
[2025-03-23 06:57:41 root](abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 19 loss:0.06509348750114441 norm:0.0008247519144788384 max memory_allocated 47479.4794921875 
[2025-03-23 06:58:19 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [29]
[2025-03-23 06:58:19 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [30] ===
quant_map[i]:  {'self_attn.q_proj': 2, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 2, 'mlp.up_proj': 1, 'mlp.down_proj': 2}
layer 30 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 30 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
layer 30 module self_attn.q_proj scheme w8a8 wbit 8 abits 8
layer 30 module self_attn.o_proj scheme w8a8 wbit 8 abits 8
layer 30 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 30 module mlp.up_proj scheme w4a8 wbit 4 abits 8
[2025-03-23 06:58:20 root](abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 06:58:51 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 0 loss:0.11208675801753998 norm:0.015015057288110256 max memory_allocated 47479.4794921875 
[2025-03-23 06:59:20 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 1 loss:0.09733442217111588 norm:0.010187406092882156 max memory_allocated 47479.4794921875 
[2025-03-23 06:59:49 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 2 loss:0.09038436412811279 norm:0.007028961554169655 max memory_allocated 47479.4794921875 
[2025-03-23 07:00:17 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 3 loss:0.08717374503612518 norm:0.005821993108838797 max memory_allocated 47479.4794921875 
[2025-03-23 07:00:46 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 4 loss:0.08629346638917923 norm:0.004978764336556196 max memory_allocated 47479.4794921875 
[2025-03-23 07:01:15 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 5 loss:0.08577921986579895 norm:0.004424295853823423 max memory_allocated 47479.4794921875 
[2025-03-23 07:01:44 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 6 loss:0.08544366806745529 norm:0.003995959181338549 max memory_allocated 47479.4794921875 
[2025-03-23 07:02:13 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 7 loss:0.08525686711072922 norm:0.003538526827469468 max memory_allocated 47479.4794921875 
[2025-03-23 07:02:42 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 8 loss:0.08501389622688293 norm:0.003049625549465418 max memory_allocated 47479.4794921875 
[2025-03-23 07:03:11 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 9 loss:0.08486916124820709 norm:0.0028065978549420834 max memory_allocated 47479.4794921875 
[2025-03-23 07:03:40 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 10 loss:0.08477611094713211 norm:0.0026378510519862175 max memory_allocated 47479.4794921875 
[2025-03-23 07:04:09 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 11 loss:0.08476456999778748 norm:0.0026320579927414656 max memory_allocated 47479.4794921875 
[2025-03-23 07:04:38 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 12 loss:0.08475631475448608 norm:0.0027227357495576143 max memory_allocated 47479.4794921875 
[2025-03-23 07:05:07 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 13 loss:0.08484190702438354 norm:0.002629521768540144 max memory_allocated 47479.4794921875 
[2025-03-23 07:05:36 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 14 loss:0.08459111303091049 norm:0.0024400583934038877 max memory_allocated 47479.4794921875 
[2025-03-23 07:06:04 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 15 loss:0.08454012870788574 norm:0.0021908266935497522 max memory_allocated 47479.4794921875 
[2025-03-23 07:06:33 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 16 loss:0.08450954407453537 norm:0.0021574816200882196 max memory_allocated 47479.4794921875 
[2025-03-23 07:07:02 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 17 loss:0.08451128005981445 norm:0.0021044323220849037 max memory_allocated 47479.4794921875 
[2025-03-23 07:07:31 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 18 loss:0.08451858907938004 norm:0.0022281473502516747 max memory_allocated 47479.4794921875 
[2025-03-23 07:08:00 root](abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 19 loss:0.08449653536081314 norm:0.002197868889197707 max memory_allocated 47479.4794921875 
[2025-03-23 07:08:36 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [30]
[2025-03-23 07:08:36 root](abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [31] ===
quant_map[i]:  {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 1, 'mlp.up_proj': 2, 'mlp.down_proj': 2}
layer 31 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
layer 31 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
layer 31 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
layer 31 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
layer 31 module mlp.down_proj scheme w8a8 wbit 8 abits 8
layer 31 module mlp.up_proj scheme w8a8 wbit 8 abits 8
[2025-03-23 07:08:37 root](abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 07:09:08 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 0 loss:0.15229052305221558 norm:0.015072263777256012 max memory_allocated 47479.4794921875 
[2025-03-23 07:09:37 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 1 loss:0.14489176869392395 norm:0.01137389987707138 max memory_allocated 47479.4794921875 
[2025-03-23 07:10:05 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 2 loss:0.13988561928272247 norm:0.008572826161980629 max memory_allocated 47479.4794921875 
[2025-03-23 07:10:34 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 3 loss:0.13688795268535614 norm:0.006829538848251104 max memory_allocated 47479.4794921875 
[2025-03-23 07:11:03 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 4 loss:0.13560424745082855 norm:0.00590863823890686 max memory_allocated 47479.4794921875 
[2025-03-23 07:11:32 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 5 loss:0.13491956889629364 norm:0.00522914994508028 max memory_allocated 47479.4794921875 
[2025-03-23 07:12:01 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 6 loss:0.13436323404312134 norm:0.004670052323490381 max memory_allocated 47479.4794921875 
[2025-03-23 07:12:30 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 7 loss:0.1341005265712738 norm:0.0042808642610907555 max memory_allocated 47479.4794921875 
[2025-03-23 07:12:59 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 8 loss:0.1338154673576355 norm:0.004020031541585922 max memory_allocated 47479.4794921875 
[2025-03-23 07:13:28 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 9 loss:0.13367611169815063 norm:0.0039600105956196785 max memory_allocated 47479.4794921875 
[2025-03-23 07:13:57 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 10 loss:0.1337234377861023 norm:0.004174534231424332 max memory_allocated 47479.4794921875 
[2025-03-23 07:14:26 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 11 loss:0.13344819843769073 norm:0.003870847402140498 max memory_allocated 47479.4794921875 
[2025-03-23 07:14:55 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 12 loss:0.1333610862493515 norm:0.0038313153199851513 max memory_allocated 47479.4794921875 
[2025-03-23 07:15:23 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 13 loss:0.13324587047100067 norm:0.0036832797341048717 max memory_allocated 47479.4794921875 
[2025-03-23 07:15:52 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 14 loss:0.13317491114139557 norm:0.003647885285317898 max memory_allocated 47479.4794921875 
[2025-03-23 07:16:21 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 15 loss:0.13301856815814972 norm:0.0034875720739364624 max memory_allocated 47479.4794921875 
[2025-03-23 07:16:50 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 16 loss:0.13304279744625092 norm:0.0036929382476955652 max memory_allocated 47479.4794921875 
[2025-03-23 07:17:19 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 17 loss:0.1330907940864563 norm:0.0036672414280474186 max memory_allocated 47479.4794921875 
[2025-03-23 07:17:48 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 18 loss:0.13309940695762634 norm:0.003915281966328621 max memory_allocated 47479.4794921875 
[2025-03-23 07:18:17 root](abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 19 loss:0.1328762024641037 norm:0.0034644140396267176 max memory_allocated 47479.4794921875 
[2025-03-23 07:18:53 root](abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [31]
[2025-03-23 07:18:53 root](main_calib_config3_attn.py 379): INFO 19633.766493558884
/workspace/volume/yangzhe/ABQ-LLM/algorithm/main_calib_config3_attn.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  testloader = torch.load(cache_testloader)
[2025-03-23 07:18:59 root](main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
  0%|          | 0/166 [00:00<?, ?it/s]  1%|          | 1/166 [00:00<00:49,  3.34it/s]  1%|          | 2/166 [00:00<00:49,  3.32it/s]  2%|▏         | 3/166 [00:00<00:49,  3.31it/s]  2%|▏         | 4/166 [00:01<00:48,  3.31it/s]  3%|▎         | 5/166 [00:01<00:48,  3.31it/s]  4%|▎         | 6/166 [00:01<00:48,  3.31it/s]  4%|▍         | 7/166 [00:02<00:48,  3.31it/s]  5%|▍         | 8/166 [00:02<00:47,  3.31it/s]  5%|▌         | 9/166 [00:02<00:47,  3.31it/s]  6%|▌         | 10/166 [00:03<00:47,  3.31it/s]  7%|▋         | 11/166 [00:03<00:46,  3.31it/s]  7%|▋         | 12/166 [00:03<00:46,  3.31it/s]  8%|▊         | 13/166 [00:03<00:46,  3.31it/s]  8%|▊         | 14/166 [00:04<00:45,  3.31it/s]  9%|▉         | 15/166 [00:04<00:45,  3.31it/s] 10%|▉         | 16/166 [00:04<00:45,  3.31it/s] 10%|█         | 17/166 [00:05<00:45,  3.31it/s] 11%|█         | 18/166 [00:05<00:44,  3.31it/s] 11%|█▏        | 19/166 [00:05<00:44,  3.31it/s] 12%|█▏        | 20/166 [00:06<00:44,  3.31it/s] 13%|█▎        | 21/166 [00:06<00:43,  3.30it/s] 13%|█▎        | 22/166 [00:06<00:43,  3.30it/s] 14%|█▍        | 23/166 [00:06<00:43,  3.31it/s] 14%|█▍        | 24/166 [00:07<00:42,  3.30it/s] 15%|█▌        | 25/166 [00:07<00:42,  3.30it/s] 16%|█▌        | 26/166 [00:07<00:42,  3.30it/s] 16%|█▋        | 27/166 [00:08<00:42,  3.30it/s] 17%|█▋        | 28/166 [00:08<00:41,  3.30it/s] 17%|█▋        | 29/166 [00:08<00:41,  3.30it/s] 18%|█▊        | 30/166 [00:09<00:41,  3.30it/s] 19%|█▊        | 31/166 [00:09<00:40,  3.30it/s] 19%|█▉        | 32/166 [00:09<00:40,  3.30it/s] 20%|█▉        | 33/166 [00:09<00:40,  3.30it/s] 20%|██        | 34/166 [00:10<00:40,  3.30it/s] 21%|██        | 35/166 [00:10<00:39,  3.30it/s] 22%|██▏       | 36/166 [00:10<00:39,  3.30it/s] 22%|██▏       | 37/166 [00:11<00:39,  3.30it/s] 23%|██▎       | 38/166 [00:11<00:38,  3.30it/s] 23%|██▎       | 39/166 [00:11<00:38,  3.30it/s] 24%|██▍       | 40/166 [00:12<00:38,  3.30it/s] 25%|██▍       | 41/166 [00:12<00:37,  3.30it/s] 25%|██▌       | 42/166 [00:12<00:37,  3.30it/s] 26%|██▌       | 43/166 [00:13<00:37,  3.30it/s] 27%|██▋       | 44/166 [00:13<00:36,  3.30it/s] 27%|██▋       | 45/166 [00:13<00:36,  3.30it/s] 28%|██▊       | 46/166 [00:13<00:36,  3.30it/s] 28%|██▊       | 47/166 [00:14<00:36,  3.30it/s] 29%|██▉       | 48/166 [00:14<00:35,  3.30it/s] 30%|██▉       | 49/166 [00:14<00:35,  3.30it/s] 30%|███       | 50/166 [00:15<00:35,  3.30it/s] 31%|███       | 51/166 [00:15<00:34,  3.30it/s] 31%|███▏      | 52/166 [00:15<00:34,  3.30it/s] 32%|███▏      | 53/166 [00:16<00:34,  3.30it/s] 33%|███▎      | 54/166 [00:16<00:33,  3.30it/s] 33%|███▎      | 55/166 [00:16<00:33,  3.30it/s] 34%|███▎      | 56/166 [00:16<00:33,  3.30it/s] 34%|███▍      | 57/166 [00:17<00:33,  3.30it/s] 35%|███▍      | 58/166 [00:17<00:32,  3.30it/s] 36%|███▌      | 59/166 [00:17<00:32,  3.30it/s] 36%|███▌      | 60/166 [00:18<00:32,  3.30it/s] 37%|███▋      | 61/166 [00:18<00:31,  3.30it/s] 37%|███▋      | 62/166 [00:18<00:31,  3.30it/s] 38%|███▊      | 63/166 [00:19<00:31,  3.30it/s] 39%|███▊      | 64/166 [00:19<00:30,  3.30it/s] 39%|███▉      | 65/166 [00:19<00:30,  3.30it/s] 40%|███▉      | 66/166 [00:19<00:30,  3.30it/s] 40%|████      | 67/166 [00:20<00:30,  3.30it/s] 41%|████      | 68/166 [00:20<00:29,  3.30it/s] 42%|████▏     | 69/166 [00:20<00:29,  3.30it/s] 42%|████▏     | 70/166 [00:21<00:29,  3.30it/s] 43%|████▎     | 71/166 [00:21<00:28,  3.30it/s] 43%|████▎     | 72/166 [00:21<00:28,  3.30it/s] 44%|████▍     | 73/166 [00:22<00:28,  3.30it/s] 45%|████▍     | 74/166 [00:22<00:27,  3.30it/s] 45%|████▌     | 75/166 [00:22<00:27,  3.30it/s] 46%|████▌     | 76/166 [00:23<00:27,  3.30it/s] 46%|████▋     | 77/166 [00:23<00:26,  3.30it/s] 47%|████▋     | 78/166 [00:23<00:26,  3.29it/s] 48%|████▊     | 79/166 [00:23<00:26,  3.29it/s] 48%|████▊     | 80/166 [00:24<00:26,  3.29it/s] 49%|████▉     | 81/166 [00:24<00:25,  3.30it/s] 49%|████▉     | 82/166 [00:24<00:25,  3.30it/s] 50%|█████     | 83/166 [00:25<00:25,  3.29it/s] 51%|█████     | 84/166 [00:25<00:24,  3.29it/s] 51%|█████     | 85/166 [00:25<00:24,  3.29it/s] 52%|█████▏    | 86/166 [00:26<00:24,  3.30it/s] 52%|█████▏    | 87/166 [00:26<00:23,  3.29it/s] 53%|█████▎    | 88/166 [00:26<00:23,  3.29it/s] 54%|█████▎    | 89/166 [00:26<00:23,  3.29it/s] 54%|█████▍    | 90/166 [00:27<00:23,  3.29it/s] 55%|█████▍    | 91/166 [00:27<00:22,  3.29it/s] 55%|█████▌    | 92/166 [00:27<00:22,  3.29it/s] 56%|█████▌    | 93/166 [00:28<00:22,  3.29it/s] 57%|█████▋    | 94/166 [00:28<00:21,  3.28it/s] 57%|█████▋    | 95/166 [00:28<00:21,  3.29it/s] 58%|█████▊    | 96/166 [00:29<00:21,  3.29it/s] 58%|█████▊    | 97/166 [00:29<00:20,  3.29it/s] 59%|█████▉    | 98/166 [00:29<00:20,  3.29it/s] 60%|█████▉    | 99/166 [00:29<00:20,  3.29it/s] 60%|██████    | 100/166 [00:30<00:20,  3.29it/s] 61%|██████    | 101/166 [00:30<00:19,  3.29it/s] 61%|██████▏   | 102/166 [00:30<00:19,  3.29it/s] 62%|██████▏   | 103/166 [00:31<00:19,  3.29it/s] 63%|██████▎   | 104/166 [00:31<00:18,  3.29it/s] 63%|██████▎   | 105/166 [00:31<00:18,  3.29it/s] 64%|██████▍   | 106/166 [00:32<00:18,  3.29it/s] 64%|██████▍   | 107/166 [00:32<00:17,  3.29it/s] 65%|██████▌   | 108/166 [00:32<00:17,  3.28it/s] 66%|██████▌   | 109/166 [00:33<00:17,  3.29it/s] 66%|██████▋   | 110/166 [00:33<00:17,  3.29it/s] 67%|██████▋   | 111/166 [00:33<00:16,  3.29it/s] 67%|██████▋   | 112/166 [00:33<00:16,  3.29it/s] 68%|██████▊   | 113/166 [00:34<00:16,  3.29it/s] 69%|██████▊   | 114/166 [00:34<00:15,  3.29it/s] 69%|██████▉   | 115/166 [00:34<00:15,  3.28it/s] 70%|██████▉   | 116/166 [00:35<00:15,  3.28it/s] 70%|███████   | 117/166 [00:35<00:14,  3.28it/s] 71%|███████   | 118/166 [00:35<00:14,  3.29it/s] 72%|███████▏  | 119/166 [00:36<00:14,  3.29it/s] 72%|███████▏  | 120/166 [00:36<00:13,  3.29it/s] 73%|███████▎  | 121/166 [00:36<00:13,  3.28it/s] 73%|███████▎  | 122/166 [00:36<00:13,  3.28it/s] 74%|███████▍  | 123/166 [00:37<00:13,  3.28it/s] 75%|███████▍  | 124/166 [00:37<00:12,  3.29it/s] 75%|███████▌  | 125/166 [00:37<00:12,  3.29it/s] 76%|███████▌  | 126/166 [00:38<00:12,  3.29it/s] 77%|███████▋  | 127/166 [00:38<00:11,  3.29it/s] 77%|███████▋  | 128/166 [00:38<00:11,  3.29it/s] 78%|███████▊  | 129/166 [00:39<00:11,  3.28it/s] 78%|███████▊  | 130/166 [00:39<00:10,  3.28it/s] 79%|███████▉  | 131/166 [00:39<00:10,  3.28it/s] 80%|███████▉  | 132/166 [00:40<00:10,  3.28it/s] 80%|████████  | 133/166 [00:40<00:10,  3.28it/s] 81%|████████  | 134/166 [00:40<00:09,  3.28it/s] 81%|████████▏ | 135/166 [00:40<00:09,  3.28it/s] 82%|████████▏ | 136/166 [00:41<00:09,  3.28it/s] 83%|████████▎ | 137/166 [00:41<00:08,  3.28it/s] 83%|████████▎ | 138/166 [00:41<00:08,  3.29it/s] 84%|████████▎ | 139/166 [00:42<00:08,  3.28it/s] 84%|████████▍ | 140/166 [00:42<00:07,  3.28it/s] 85%|████████▍ | 141/166 [00:42<00:07,  3.28it/s] 86%|████████▌ | 142/166 [00:43<00:07,  3.28it/s] 86%|████████▌ | 143/166 [00:43<00:07,  3.28it/s] 87%|████████▋ | 144/166 [00:43<00:06,  3.28it/s] 87%|████████▋ | 145/166 [00:44<00:06,  3.28it/s] 88%|████████▊ | 146/166 [00:44<00:06,  3.28it/s] 89%|████████▊ | 147/166 [00:44<00:05,  3.28it/s] 89%|████████▉ | 148/166 [00:44<00:05,  3.28it/s] 90%|████████▉ | 149/166 [00:45<00:05,  3.28it/s] 90%|█████████ | 150/166 [00:45<00:04,  3.28it/s] 91%|█████████ | 151/166 [00:45<00:04,  3.28it/s] 92%|█████████▏| 152/166 [00:46<00:04,  3.28it/s] 92%|█████████▏| 153/166 [00:46<00:03,  3.28it/s] 93%|█████████▎| 154/166 [00:46<00:03,  3.28it/s] 93%|█████████▎| 155/166 [00:47<00:03,  3.28it/s] 94%|█████████▍| 156/166 [00:47<00:03,  3.28it/s] 95%|█████████▍| 157/166 [00:47<00:02,  3.28it/s] 95%|█████████▌| 158/166 [00:47<00:02,  3.28it/s] 96%|█████████▌| 159/166 [00:48<00:02,  3.28it/s] 96%|█████████▋| 160/166 [00:48<00:01,  3.28it/s] 97%|█████████▋| 161/166 [00:48<00:01,  3.28it/s] 98%|█████████▊| 162/166 [00:49<00:01,  3.28it/s] 98%|█████████▊| 163/166 [00:49<00:00,  3.28it/s] 99%|█████████▉| 164/166 [00:49<00:00,  3.28it/s] 99%|█████████▉| 165/166 [00:50<00:00,  3.28it/s]100%|██████████| 166/166 [00:50<00:00,  3.28it/s]100%|██████████| 166/166 [00:50<00:00,  3.29it/s]
[2025-03-23 07:19:49 root](main_calib_config3_attn.py 161): INFO wikitext2 : 5.541950225830078
[2025-03-23 07:19:50 root](main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
  0%|          | 0/256 [00:00<?, ?it/s]  0%|          | 1/256 [00:00<01:16,  3.31it/s]  1%|          | 2/256 [00:00<01:17,  3.29it/s]  1%|          | 3/256 [00:00<01:16,  3.29it/s]  2%|▏         | 4/256 [00:01<01:16,  3.28it/s]  2%|▏         | 5/256 [00:01<01:16,  3.28it/s]  2%|▏         | 6/256 [00:01<01:16,  3.28it/s]  3%|▎         | 7/256 [00:02<01:15,  3.28it/s]  3%|▎         | 8/256 [00:02<01:15,  3.28it/s]  4%|▎         | 9/256 [00:02<01:15,  3.28it/s]  4%|▍         | 10/256 [00:03<01:15,  3.28it/s]  4%|▍         | 11/256 [00:03<01:14,  3.28it/s]  5%|▍         | 12/256 [00:03<01:14,  3.28it/s]  5%|▌         | 13/256 [00:03<01:14,  3.28it/s]  5%|▌         | 14/256 [00:04<01:13,  3.28it/s]  6%|▌         | 15/256 [00:04<01:13,  3.27it/s]  6%|▋         | 16/256 [00:04<01:13,  3.28it/s]  7%|▋         | 17/256 [00:05<01:12,  3.28it/s]  7%|▋         | 18/256 [00:05<01:12,  3.28it/s]  7%|▋         | 19/256 [00:05<01:12,  3.27it/s]  8%|▊         | 20/256 [00:06<01:12,  3.27it/s]  8%|▊         | 21/256 [00:06<01:11,  3.28it/s]  9%|▊         | 22/256 [00:06<01:11,  3.28it/s]  9%|▉         | 23/256 [00:07<01:11,  3.28it/s]  9%|▉         | 24/256 [00:07<01:10,  3.28it/s] 10%|▉         | 25/256 [00:07<01:10,  3.28it/s] 10%|█         | 26/256 [00:07<01:10,  3.28it/s] 11%|█         | 27/256 [00:08<01:09,  3.28it/s] 11%|█         | 28/256 [00:08<01:09,  3.27it/s] 11%|█▏        | 29/256 [00:08<01:09,  3.28it/s] 12%|█▏        | 30/256 [00:09<01:08,  3.28it/s] 12%|█▏        | 31/256 [00:09<01:08,  3.27it/s] 12%|█▎        | 32/256 [00:09<01:08,  3.27it/s] 13%|█▎        | 33/256 [00:10<01:08,  3.28it/s] 13%|█▎        | 34/256 [00:10<01:07,  3.27it/s] 14%|█▎        | 35/256 [00:10<01:07,  3.27it/s] 14%|█▍        | 36/256 [00:10<01:07,  3.27it/s] 14%|█▍        | 37/256 [00:11<01:06,  3.28it/s] 15%|█▍        | 38/256 [00:11<01:06,  3.27it/s] 15%|█▌        | 39/256 [00:11<01:06,  3.27it/s] 16%|█▌        | 40/256 [00:12<01:06,  3.27it/s] 16%|█▌        | 41/256 [00:12<01:05,  3.27it/s] 16%|█▋        | 42/256 [00:12<01:05,  3.27it/s] 17%|█▋        | 43/256 [00:13<01:05,  3.27it/s] 17%|█▋        | 44/256 [00:13<01:04,  3.27it/s] 18%|█▊        | 45/256 [00:13<01:04,  3.27it/s] 18%|█▊        | 46/256 [00:14<01:04,  3.27it/s] 18%|█▊        | 47/256 [00:14<01:03,  3.27it/s] 19%|█▉        | 48/256 [00:14<01:03,  3.27it/s] 19%|█▉        | 49/256 [00:14<01:03,  3.27it/s] 20%|█▉        | 50/256 [00:15<01:02,  3.27it/s] 20%|█▉        | 51/256 [00:15<01:02,  3.27it/s] 20%|██        | 52/256 [00:15<01:02,  3.27it/s] 21%|██        | 53/256 [00:16<01:02,  3.27it/s] 21%|██        | 54/256 [00:16<01:01,  3.27it/s] 21%|██▏       | 55/256 [00:16<01:01,  3.27it/s] 22%|██▏       | 56/256 [00:17<01:01,  3.27it/s] 22%|██▏       | 57/256 [00:17<01:00,  3.27it/s] 23%|██▎       | 58/256 [00:17<01:00,  3.27it/s] 23%|██▎       | 59/256 [00:18<01:00,  3.27it/s] 23%|██▎       | 60/256 [00:18<00:59,  3.27it/s] 24%|██▍       | 61/256 [00:18<00:59,  3.27it/s] 24%|██▍       | 62/256 [00:18<00:59,  3.27it/s] 25%|██▍       | 63/256 [00:19<00:58,  3.27it/s] 25%|██▌       | 64/256 [00:19<00:58,  3.27it/s] 25%|██▌       | 65/256 [00:19<00:58,  3.27it/s] 26%|██▌       | 66/256 [00:20<00:58,  3.27it/s] 26%|██▌       | 67/256 [00:20<00:57,  3.27it/s] 27%|██▋       | 68/256 [00:20<00:57,  3.27it/s] 27%|██▋       | 69/256 [00:21<00:57,  3.27it/s] 27%|██▋       | 70/256 [00:21<00:56,  3.27it/s] 28%|██▊       | 71/256 [00:21<00:56,  3.27it/s] 28%|██▊       | 72/256 [00:21<00:56,  3.27it/s] 29%|██▊       | 73/256 [00:22<00:56,  3.27it/s] 29%|██▉       | 74/256 [00:22<00:55,  3.27it/s] 29%|██▉       | 75/256 [00:22<00:55,  3.27it/s] 30%|██▉       | 76/256 [00:23<00:55,  3.27it/s] 30%|███       | 77/256 [00:23<00:54,  3.27it/s] 30%|███       | 78/256 [00:23<00:54,  3.27it/s] 31%|███       | 79/256 [00:24<00:54,  3.27it/s] 31%|███▏      | 80/256 [00:24<00:53,  3.27it/s] 32%|███▏      | 81/256 [00:24<00:53,  3.27it/s] 32%|███▏      | 82/256 [00:25<00:53,  3.27it/s] 32%|███▏      | 83/256 [00:25<00:52,  3.27it/s] 33%|███▎      | 84/256 [00:25<00:52,  3.27it/s] 33%|███▎      | 85/256 [00:25<00:52,  3.27it/s] 34%|███▎      | 86/256 [00:26<00:52,  3.27it/s] 34%|███▍      | 87/256 [00:26<00:51,  3.27it/s] 34%|███▍      | 88/256 [00:26<00:51,  3.27it/s] 35%|███▍      | 89/256 [00:27<00:51,  3.27it/s] 35%|███▌      | 90/256 [00:27<00:50,  3.27it/s] 36%|███▌      | 91/256 [00:27<00:50,  3.27it/s] 36%|███▌      | 92/256 [00:28<00:50,  3.27it/s] 36%|███▋      | 93/256 [00:28<00:49,  3.27it/s] 37%|███▋      | 94/256 [00:28<00:49,  3.27it/s] 37%|███▋      | 95/256 [00:29<00:49,  3.27it/s] 38%|███▊      | 96/256 [00:29<00:48,  3.27it/s] 38%|███▊      | 97/256 [00:29<00:48,  3.27it/s] 38%|███▊      | 98/256 [00:29<00:48,  3.27it/s] 39%|███▊      | 99/256 [00:30<00:48,  3.27it/s] 39%|███▉      | 100/256 [00:30<00:47,  3.27it/s] 39%|███▉      | 101/256 [00:30<00:47,  3.27it/s] 40%|███▉      | 102/256 [00:31<00:47,  3.27it/s] 40%|████      | 103/256 [00:31<00:46,  3.27it/s] 41%|████      | 104/256 [00:31<00:46,  3.27it/s] 41%|████      | 105/256 [00:32<00:46,  3.26it/s] 41%|████▏     | 106/256 [00:32<00:45,  3.26it/s] 42%|████▏     | 107/256 [00:32<00:45,  3.27it/s] 42%|████▏     | 108/256 [00:33<00:45,  3.27it/s] 43%|████▎     | 109/256 [00:33<00:45,  3.26it/s] 43%|████▎     | 110/256 [00:33<00:44,  3.26it/s] 43%|████▎     | 111/256 [00:33<00:44,  3.27it/s] 44%|████▍     | 112/256 [00:34<00:44,  3.27it/s] 44%|████▍     | 113/256 [00:34<00:43,  3.27it/s] 45%|████▍     | 114/256 [00:34<00:43,  3.27it/s] 45%|████▍     | 115/256 [00:35<00:43,  3.27it/s] 45%|████▌     | 116/256 [00:35<00:42,  3.27it/s] 46%|████▌     | 117/256 [00:35<00:42,  3.26it/s] 46%|████▌     | 118/256 [00:36<00:42,  3.27it/s] 46%|████▋     | 119/256 [00:36<00:41,  3.27it/s] 47%|████▋     | 120/256 [00:36<00:41,  3.27it/s] 47%|████▋     | 121/256 [00:36<00:41,  3.27it/s] 48%|████▊     | 122/256 [00:37<00:41,  3.27it/s] 48%|████▊     | 123/256 [00:37<00:40,  3.26it/s] 48%|████▊     | 124/256 [00:37<00:40,  3.26it/s] 49%|████▉     | 125/256 [00:38<00:40,  3.26it/s] 49%|████▉     | 126/256 [00:38<00:39,  3.26it/s] 50%|████▉     | 127/256 [00:38<00:39,  3.26it/s] 50%|█████     | 128/256 [00:39<00:39,  3.26it/s] 50%|█████     | 129/256 [00:39<00:38,  3.26it/s] 51%|█████     | 130/256 [00:39<00:38,  3.27it/s] 51%|█████     | 131/256 [00:40<00:38,  3.27it/s] 52%|█████▏    | 132/256 [00:40<00:37,  3.27it/s] 52%|█████▏    | 133/256 [00:40<00:37,  3.26it/s] 52%|█████▏    | 134/256 [00:40<00:37,  3.27it/s] 53%|█████▎    | 135/256 [00:41<00:37,  3.27it/s] 53%|█████▎    | 136/256 [00:41<00:36,  3.26it/s] 54%|█████▎    | 137/256 [00:41<00:36,  3.26it/s] 54%|█████▍    | 138/256 [00:42<00:36,  3.26it/s] 54%|█████▍    | 139/256 [00:42<00:35,  3.27it/s] 55%|█████▍    | 140/256 [00:42<00:35,  3.27it/s] 55%|█████▌    | 141/256 [00:43<00:35,  3.26it/s] 55%|█████▌    | 142/256 [00:43<00:34,  3.26it/s] 56%|█████▌    | 143/256 [00:43<00:34,  3.27it/s] 56%|█████▋    | 144/256 [00:44<00:34,  3.26it/s] 57%|█████▋    | 145/256 [00:44<00:34,  3.26it/s] 57%|█████▋    | 146/256 [00:44<00:33,  3.26it/s] 57%|█████▋    | 147/256 [00:44<00:33,  3.26it/s] 58%|█████▊    | 148/256 [00:45<00:33,  3.26it/s] 58%|█████▊    | 149/256 [00:45<00:32,  3.27it/s] 59%|█████▊    | 150/256 [00:45<00:32,  3.26it/s] 59%|█████▉    | 151/256 [00:46<00:32,  3.26it/s] 59%|█████▉    | 152/256 [00:46<00:31,  3.27it/s] 60%|█████▉    | 153/256 [00:46<00:31,  3.26it/s] 60%|██████    | 154/256 [00:47<00:31,  3.26it/s] 61%|██████    | 155/256 [00:47<00:30,  3.26it/s] 61%|██████    | 156/256 [00:47<00:30,  3.26it/s] 61%|██████▏   | 157/256 [00:48<00:30,  3.26it/s] 62%|██████▏   | 158/256 [00:48<00:30,  3.26it/s] 62%|██████▏   | 159/256 [00:48<00:29,  3.26it/s] 62%|██████▎   | 160/256 [00:48<00:29,  3.26it/s] 63%|██████▎   | 161/256 [00:49<00:29,  3.26it/s] 63%|██████▎   | 162/256 [00:49<00:28,  3.26it/s] 64%|██████▎   | 163/256 [00:49<00:28,  3.26it/s] 64%|██████▍   | 164/256 [00:50<00:28,  3.26it/s] 64%|██████▍   | 165/256 [00:50<00:27,  3.26it/s] 65%|██████▍   | 166/256 [00:50<00:27,  3.26it/s] 65%|██████▌   | 167/256 [00:51<00:27,  3.26it/s] 66%|██████▌   | 168/256 [00:51<00:27,  3.26it/s] 66%|██████▌   | 169/256 [00:51<00:26,  3.26it/s] 66%|██████▋   | 170/256 [00:52<00:26,  3.26it/s] 67%|██████▋   | 171/256 [00:52<00:26,  3.26it/s] 67%|██████▋   | 172/256 [00:52<00:25,  3.26it/s] 68%|██████▊   | 173/256 [00:52<00:25,  3.26it/s] 68%|██████▊   | 174/256 [00:53<00:25,  3.26it/s] 68%|██████▊   | 175/256 [00:53<00:24,  3.26it/s] 69%|██████▉   | 176/256 [00:53<00:24,  3.26it/s] 69%|██████▉   | 177/256 [00:54<00:24,  3.26it/s] 70%|██████▉   | 178/256 [00:54<00:23,  3.26it/s] 70%|██████▉   | 179/256 [00:54<00:23,  3.26it/s] 70%|███████   | 180/256 [00:55<00:23,  3.26it/s] 71%|███████   | 181/256 [00:55<00:23,  3.26it/s] 71%|███████   | 182/256 [00:55<00:22,  3.26it/s] 71%|███████▏  | 183/256 [00:55<00:22,  3.26it/s] 72%|███████▏  | 184/256 [00:56<00:22,  3.26it/s] 72%|███████▏  | 185/256 [00:56<00:21,  3.26it/s] 73%|███████▎  | 186/256 [00:56<00:21,  3.26it/s] 73%|███████▎  | 187/256 [00:57<00:21,  3.26it/s] 73%|███████▎  | 188/256 [00:57<00:20,  3.26it/s] 74%|███████▍  | 189/256 [00:57<00:20,  3.25it/s] 74%|███████▍  | 190/256 [00:58<00:20,  3.25it/s] 75%|███████▍  | 191/256 [00:58<00:19,  3.26it/s] 75%|███████▌  | 192/256 [00:58<00:19,  3.25it/s] 75%|███████▌  | 193/256 [00:59<00:19,  3.25it/s] 76%|███████▌  | 194/256 [00:59<00:19,  3.25it/s] 76%|███████▌  | 195/256 [00:59<00:18,  3.26it/s] 77%|███████▋  | 196/256 [00:59<00:18,  3.25it/s] 77%|███████▋  | 197/256 [01:00<00:18,  3.26it/s] 77%|███████▋  | 198/256 [01:00<00:17,  3.25it/s] 78%|███████▊  | 199/256 [01:00<00:17,  3.25it/s] 78%|███████▊  | 200/256 [01:01<00:17,  3.25it/s] 79%|███████▊  | 201/256 [01:01<00:16,  3.25it/s] 79%|███████▉  | 202/256 [01:01<00:16,  3.25it/s] 79%|███████▉  | 203/256 [01:02<00:16,  3.25it/s] 80%|███████▉  | 204/256 [01:02<00:15,  3.26it/s] 80%|████████  | 205/256 [01:02<00:15,  3.25it/s] 80%|████████  | 206/256 [01:03<00:15,  3.25it/s] 81%|████████  | 207/256 [01:03<00:15,  3.26it/s] 81%|████████▏ | 208/256 [01:03<00:14,  3.25it/s] 82%|████████▏ | 209/256 [01:03<00:14,  3.25it/s] 82%|████████▏ | 210/256 [01:04<00:14,  3.25it/s] 82%|████████▏ | 211/256 [01:04<00:13,  3.26it/s] 83%|████████▎ | 212/256 [01:04<00:13,  3.25it/s] 83%|████████▎ | 213/256 [01:05<00:13,  3.25it/s] 84%|████████▎ | 214/256 [01:05<00:12,  3.25it/s] 84%|████████▍ | 215/256 [01:05<00:12,  3.26it/s] 84%|████████▍ | 216/256 [01:06<00:12,  3.26it/s] 85%|████████▍ | 217/256 [01:06<00:11,  3.26it/s] 85%|████████▌ | 218/256 [01:06<00:11,  3.26it/s] 86%|████████▌ | 219/256 [01:07<00:11,  3.26it/s] 86%|████████▌ | 220/256 [01:07<00:11,  3.26it/s] 86%|████████▋ | 221/256 [01:07<00:10,  3.26it/s] 87%|████████▋ | 222/256 [01:07<00:10,  3.26it/s] 87%|████████▋ | 223/256 [01:08<00:10,  3.26it/s] 88%|████████▊ | 224/256 [01:08<00:09,  3.26it/s] 88%|████████▊ | 225/256 [01:08<00:09,  3.26it/s] 88%|████████▊ | 226/256 [01:09<00:09,  3.26it/s] 89%|████████▊ | 227/256 [01:09<00:08,  3.26it/s] 89%|████████▉ | 228/256 [01:09<00:08,  3.26it/s] 89%|████████▉ | 229/256 [01:10<00:08,  3.26it/s] 90%|████████▉ | 230/256 [01:10<00:07,  3.26it/s] 90%|█████████ | 231/256 [01:10<00:07,  3.25it/s] 91%|█████████ | 232/256 [01:11<00:07,  3.26it/s] 91%|█████████ | 233/256 [01:11<00:07,  3.25it/s] 91%|█████████▏| 234/256 [01:11<00:06,  3.26it/s] 92%|█████████▏| 235/256 [01:11<00:06,  3.25it/s] 92%|█████████▏| 236/256 [01:12<00:06,  3.26it/s] 93%|█████████▎| 237/256 [01:12<00:05,  3.26it/s] 93%|█████████▎| 238/256 [01:12<00:05,  3.25it/s] 93%|█████████▎| 239/256 [01:13<00:05,  3.26it/s] 94%|█████████▍| 240/256 [01:13<00:04,  3.25it/s] 94%|█████████▍| 241/256 [01:13<00:04,  3.25it/s] 95%|█████████▍| 242/256 [01:14<00:04,  3.25it/s] 95%|█████████▍| 243/256 [01:14<00:04,  3.25it/s] 95%|█████████▌| 244/256 [01:14<00:03,  3.25it/s] 96%|█████████▌| 245/256 [01:15<00:03,  3.25it/s] 96%|█████████▌| 246/256 [01:15<00:03,  3.25it/s] 96%|█████████▋| 247/256 [01:15<00:02,  3.25it/s] 97%|█████████▋| 248/256 [01:15<00:02,  3.25it/s] 97%|█████████▋| 249/256 [01:16<00:02,  3.25it/s] 98%|█████████▊| 250/256 [01:16<00:01,  3.25it/s] 98%|█████████▊| 251/256 [01:16<00:01,  3.25it/s] 98%|█████████▊| 252/256 [01:17<00:01,  3.25it/s] 99%|█████████▉| 253/256 [01:17<00:00,  3.25it/s] 99%|█████████▉| 254/256 [01:17<00:00,  3.25it/s]100%|█████████▉| 255/256 [01:18<00:00,  3.25it/s]100%|██████████| 256/256 [01:18<00:00,  3.25it/s]100%|██████████| 256/256 [01:18<00:00,  3.26it/s]
[2025-03-23 07:21:08 root](main_calib_config3_attn.py 161): INFO c4 : 7.060171604156494
Selected Tasks: ['arc_easy', 'hellaswag', 'arc_challenge', 'piqa', 'winogrande', 'boolq']
Traceback (most recent call last):
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/main_calib_config3_attn.py", line 407, in <module>
    main()
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/main_calib_config3_attn.py", line 402, in main
    evaluate(lm, args,logger)
  File "/opt/conda/envs/abq-llm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/main_calib_config3_attn.py", line 165, in evaluate
    t_results = evaluator.simple_evaluate(
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/lm_eval/utils.py", line 160, in _wrapper
    return fn(*args, **kwargs)
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/lm_eval/evaluator.py", line 114, in simple_evaluate
    raise e
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/lm_eval/evaluator.py", line 88, in simple_evaluate
    task_dict = lm_eval.tasks.get_task_dict(task_names)
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/lm_eval/tasks/__init__.py", line 342, in get_task_dict
    task_name_dict = {
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/lm_eval/tasks/__init__.py", line 343, in <dictcomp>
    task_name: get_task(task_name)()
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/lm_eval/base.py", line 412, in __init__
    self.download(data_dir, cache_dir, download_mode)
  File "/workspace/volume/yangzhe/ABQ-LLM/algorithm/lm_eval/base.py", line 441, in download
    self.dataset = datasets.load_dataset(
  File "/opt/conda/envs/abq-llm/lib/python3.10/site-packages/datasets/load.py", line 2129, in load_dataset
    builder_instance = load_dataset_builder(
  File "/opt/conda/envs/abq-llm/lib/python3.10/site-packages/datasets/load.py", line 1815, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/opt/conda/envs/abq-llm/lib/python3.10/site-packages/datasets/load.py", line 1512, in dataset_module_factory
    raise e1 from None
  File "/opt/conda/envs/abq-llm/lib/python3.10/site-packages/datasets/load.py", line 1468, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'ai2_arc' on the Hub (SSLError)
