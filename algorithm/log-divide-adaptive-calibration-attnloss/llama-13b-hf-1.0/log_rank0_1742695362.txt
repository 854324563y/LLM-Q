[2025-03-23 02:02:42 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/llama-13b-hf-1.0', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/llama-13b-hf/quant_map_llama-13b-hf_1.0.pkl', blocks_pkl='./log-divide/llama-13b-hf-w4a4/llama-13b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-23 02:05:28 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-23 02:05:28 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-23 02:05:29 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-23 02:05:29 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/llama-13b-hf/quant_map_llama-13b-hf_1.0.pkl
[2025-03-23 02:05:29 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/llama-13b-hf-w4a4/llama-13b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 7), (7, 10), (10, 13), (13, 16), (16, 19), (19, 22), (22, 25), (25, 28), (28, 30), (30, 32), (32, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40)]
[2025-03-23 02:05:29 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4, 5], [6], [7, 8, 9], [10, 11, 12], [13, 14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26, 27], [28, 29], [30, 31], [32, 33], [34], [35], [36], [37], [38], [39]]
[2025-03-23 02:05:31 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-23 02:05:32 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:06:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.0006451682420447469 norm:0.0018364902352914214 max memory_allocated 44355.7939453125 
[2025-03-23 02:07:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.0003058940637856722 norm:0.0006364067085087299 max memory_allocated 44355.7939453125 
[2025-03-23 02:07:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.00025682448176667094 norm:0.0008779949857853353 max memory_allocated 44355.7939453125 
[2025-03-23 02:08:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.00021987776563037187 norm:0.000669974775519222 max memory_allocated 44355.7939453125 
[2025-03-23 02:09:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.00021721699158661067 norm:0.0008244879427365959 max memory_allocated 44355.7939453125 
[2025-03-23 02:09:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0001945634139701724 norm:0.0006148486863821745 max memory_allocated 44355.7939453125 
[2025-03-23 02:10:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.00017698743613436818 norm:0.0005033243796788156 max memory_allocated 44355.7939453125 
[2025-03-23 02:11:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.00017182721057906747 norm:0.0004739217401947826 max memory_allocated 44355.7939453125 
[2025-03-23 02:12:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.00017026832210831344 norm:0.00047844674554653466 max memory_allocated 44355.7939453125 
[2025-03-23 02:12:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.00016776284610386938 norm:0.0004421192279551178 max memory_allocated 44355.7939453125 
[2025-03-23 02:13:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.00016573653556406498 norm:0.0004367549263406545 max memory_allocated 44355.7939453125 
[2025-03-23 02:14:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.00016120477812364697 norm:0.0003970122488681227 max memory_allocated 44355.7939453125 
[2025-03-23 02:14:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.0001595925714354962 norm:0.00037228845758363605 max memory_allocated 44355.7939453125 
[2025-03-23 02:15:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.00015837298997212201 norm:0.0003589749976526946 max memory_allocated 44355.7939453125 
[2025-03-23 02:16:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.00015395057562272996 norm:0.0003163360815960914 max memory_allocated 44355.7939453125 
[2025-03-23 02:17:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.00015151200932450593 norm:0.00029501476092264056 max memory_allocated 44355.7939453125 
[2025-03-23 02:17:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.0001513881579739973 norm:0.0002950890630017966 max memory_allocated 44355.7939453125 
[2025-03-23 02:18:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.00015308428555727005 norm:0.00029123303829692304 max memory_allocated 44355.7939453125 
[2025-03-23 02:19:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.000151338754221797 norm:0.00027084717294201255 max memory_allocated 44355.7939453125 
[2025-03-23 02:20:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0001492443261668086 norm:0.00024439161643385887 max memory_allocated 44355.7939453125 
[2025-03-23 02:20:59 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-23 02:20:59 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-23 02:21:00 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:21:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.0010125464759767056 norm:0.00213162275031209 max memory_allocated 44355.7939453125 
[2025-03-23 02:22:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.0006970510585233569 norm:0.0006740947137586772 max memory_allocated 44355.7939453125 
[2025-03-23 02:23:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.0006806285819038749 norm:0.0007815742865204811 max memory_allocated 44355.7939453125 
[2025-03-23 02:23:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.0006722481921315193 norm:0.0008131269132718444 max memory_allocated 44355.7939453125 
[2025-03-23 02:24:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.0006614099256694317 norm:0.0007648871978744864 max memory_allocated 44355.7939453125 
[2025-03-23 02:25:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.0006520480383187532 norm:0.00073242123471573 max memory_allocated 44355.7939453125 
[2025-03-23 02:26:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.0006443118909373879 norm:0.0007186652510426939 max memory_allocated 44355.7939453125 
[2025-03-23 02:26:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.0006342091364786029 norm:0.0006787842721678317 max memory_allocated 44355.7939453125 
[2025-03-23 02:27:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.0006234407774172723 norm:0.0006525011267513037 max memory_allocated 44355.7939453125 
[2025-03-23 02:28:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.000609563838224858 norm:0.0006312580080702901 max memory_allocated 44355.7939453125 
[2025-03-23 02:28:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.000594818324316293 norm:0.0006041520973667502 max memory_allocated 44355.7939453125 
[2025-03-23 02:29:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.0005751526914536953 norm:0.0005635248962789774 max memory_allocated 44355.7939453125 
[2025-03-23 02:30:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.0005626687780022621 norm:0.0005566043546423316 max memory_allocated 44355.7939453125 
[2025-03-23 02:31:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.0005511131603270769 norm:0.0005612578243017197 max memory_allocated 44355.7939453125 
[2025-03-23 02:31:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.0005286377272568643 norm:0.0005061731208115816 max memory_allocated 44355.7939453125 
[2025-03-23 02:32:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.0005118766566738486 norm:0.0004527149721980095 max memory_allocated 44355.7939453125 
[2025-03-23 02:33:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.0004960412625223398 norm:0.00040783212170936167 max memory_allocated 44355.7939453125 
[2025-03-23 02:34:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.0004932331503368914 norm:0.00040541458292864263 max memory_allocated 44355.7939453125 
[2025-03-23 02:34:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.00048401241656392813 norm:0.0003531299589667469 max memory_allocated 44355.7939453125 
[2025-03-23 02:35:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.0004795461136382073 norm:0.00033205249928869307 max memory_allocated 44355.7939453125 
[2025-03-23 02:36:25 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-23 02:36:25 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
[2025-03-23 02:36:26 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:37:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.0016760468715801835 norm:0.0015665258979424834 max memory_allocated 44355.7939453125 
[2025-03-23 02:37:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.0014245244674384594 norm:0.0011203300673514605 max memory_allocated 44355.7939453125 
[2025-03-23 02:38:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.0013802002649754286 norm:0.0009381785057485104 max memory_allocated 44355.7939453125 
[2025-03-23 02:39:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.0014069202588871121 norm:0.001042861957103014 max memory_allocated 44355.7939453125 
[2025-03-23 02:40:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.0013237341772764921 norm:0.000869325827807188 max memory_allocated 44355.7939453125 
[2025-03-23 02:40:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.001242541242390871 norm:0.0007123940158635378 max memory_allocated 44355.7939453125 
[2025-03-23 02:41:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.001188098220154643 norm:0.0006782167474739254 max memory_allocated 44355.7939453125 
[2025-03-23 02:42:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.001124182716012001 norm:0.0006407214095816016 max memory_allocated 44355.7939453125 
[2025-03-23 02:42:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.0010511501459404826 norm:0.0005516651435755193 max memory_allocated 44355.7939453125 
[2025-03-23 02:43:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.0010078537743538618 norm:0.0004987451247870922 max memory_allocated 44355.7939453125 
[2025-03-23 02:44:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.000986026250757277 norm:0.0004581501707434654 max memory_allocated 44355.7939453125 
[2025-03-23 02:45:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.000975741189904511 norm:0.00041414963197894394 max memory_allocated 44355.7939453125 
[2025-03-23 02:45:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.0010072712320834398 norm:0.0005159752909094095 max memory_allocated 44355.7939453125 
[2025-03-23 02:46:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.0009949211962521076 norm:0.00042580452281981707 max memory_allocated 44355.7939453125 
[2025-03-23 02:47:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.0009829092305153608 norm:0.0003638192138168961 max memory_allocated 44355.7939453125 
[2025-03-23 02:48:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.0009973186533898115 norm:0.00040253353654406965 max memory_allocated 44355.7939453125 
[2025-03-23 02:48:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.0010578890796750784 norm:0.0005606822087429464 max memory_allocated 44355.7939453125 
[2025-03-23 02:49:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.001024831784889102 norm:0.0004643484717234969 max memory_allocated 44355.7939453125 
[2025-03-23 02:50:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.0010436761658638716 norm:0.0004927649279125035 max memory_allocated 44355.7939453125 
[2025-03-23 02:50:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.0010087824193760753 norm:0.0003636253532022238 max memory_allocated 44355.7939453125 
[2025-03-23 02:51:54 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2]
[2025-03-23 02:51:54 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4, 5] ===
[2025-03-23 02:54:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 0 loss:0.0047714137472212315 norm:0.00015356979565694928 max memory_allocated 62749.0654296875 
[2025-03-23 02:56:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 1 loss:0.0045257871970534325 norm:8.1818230682984e-05 max memory_allocated 62749.0654296875 
[2025-03-23 02:58:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 2 loss:0.004378721117973328 norm:7.269266643561423e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:00:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 3 loss:0.0042036003433167934 norm:7.073229789966717e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:02:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 4 loss:0.0039803520776331425 norm:6.563289207406342e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:04:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 5 loss:0.003718552878126502 norm:6.035115075064823e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:07:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 6 loss:0.0034818467684090137 norm:5.698738095816225e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:09:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 7 loss:0.003309335093945265 norm:6.010475772200152e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:11:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 8 loss:0.0032018530182540417 norm:5.3848685638513416e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:13:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 9 loss:0.003137244377285242 norm:5.09758829139173e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:15:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 10 loss:0.0031032338738441467 norm:4.9608319386607036e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:17:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 11 loss:0.0030852053314447403 norm:4.4783471821574494e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:19:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 12 loss:0.0030771750025451183 norm:4.547743083094247e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:22:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 13 loss:0.00307606253772974 norm:4.218925460008904e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:24:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 14 loss:0.0030709081329405308 norm:3.834105154965073e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:26:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 15 loss:0.0030716941691935062 norm:3.561407356755808e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:28:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 16 loss:0.003069796599447727 norm:3.346349694766104e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:30:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 17 loss:0.003068565623834729 norm:3.464294786681421e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:32:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 18 loss:0.0030667742248624563 norm:3.405611641937867e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:34:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 19 loss:0.003068688791245222 norm:3.383704461157322e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:37:58 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [3, 4, 5]
[2025-03-23 03:37:58 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [6] ===
[2025-03-23 03:38:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 0 loss:0.003950593993067741 norm:0.00021649124391842633 max memory_allocated 62749.0654296875 
[2025-03-23 03:39:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 1 loss:0.0034506837837398052 norm:8.006643474800512e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:40:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 2 loss:0.0032831684220582247 norm:5.036319635109976e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:40:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 3 loss:0.0031527148094028234 norm:4.501234070630744e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:41:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 4 loss:0.003012827830389142 norm:4.732238448923454e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:42:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 5 loss:0.0028771436773240566 norm:4.895679376204498e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:43:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 6 loss:0.002777233486995101 norm:3.8687729102093726e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:43:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 7 loss:0.0027316864579916 norm:3.5923101677326486e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:44:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 8 loss:0.002705957042053342 norm:3.3537933632032946e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:45:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 9 loss:0.0026991204358637333 norm:3.459541767369956e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:45:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 10 loss:0.002710929373279214 norm:5.286626037559472e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:46:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 11 loss:0.0026999348774552345 norm:3.705483322846703e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:47:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 12 loss:0.0027000075206160545 norm:3.436673796386458e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:48:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 13 loss:0.0026976652443408966 norm:3.630927676567808e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:48:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 14 loss:0.0026950309984385967 norm:3.2946132705546916e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:49:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 15 loss:0.0026964673306792974 norm:3.952212864533067e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:50:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 16 loss:0.0026914277113974094 norm:3.541460682754405e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:50:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 17 loss:0.002697477350011468 norm:3.9103444578358904e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:51:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 18 loss:0.002691691741347313 norm:2.6284620616934262e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:52:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 19 loss:0.0026884390972554684 norm:2.7423542633187026e-05 max memory_allocated 62749.0654296875 
[2025-03-23 03:53:19 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [6]
[2025-03-23 03:53:19 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [7, 8, 9] ===
[2025-03-23 03:55:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 0 loss:0.007026865612715483 norm:0.00014685047790408134 max memory_allocated 62752.3779296875 
[2025-03-23 03:57:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 1 loss:0.00657392293214798 norm:9.542776388116181e-05 max memory_allocated 62752.3779296875 
[2025-03-23 03:59:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 2 loss:0.006308381911367178 norm:6.432004738599062e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:02:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 3 loss:0.006035150960087776 norm:5.5423559388145804e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:04:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 4 loss:0.005720975808799267 norm:5.832838360220194e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:06:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 5 loss:0.005387690383940935 norm:5.642512405756861e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:08:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 6 loss:0.005126023665070534 norm:5.682573828380555e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:10:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 7 loss:0.0049438769929111 norm:5.151672303327359e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:12:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 8 loss:0.0048499577678740025 norm:6.139334436738864e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:14:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 9 loss:0.004811333492398262 norm:6.682193634333089e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:17:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 10 loss:0.004775861743837595 norm:5.531482747755945e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:19:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 11 loss:0.0047620320692658424 norm:5.529348709387705e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:21:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 12 loss:0.0047547114081680775 norm:5.483850691234693e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:23:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 13 loss:0.0047501916997134686 norm:5.633514956571162e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:25:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 14 loss:0.004748940467834473 norm:5.8173602155875415e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:27:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 15 loss:0.0047479416243731976 norm:5.519511614693329e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:29:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 16 loss:0.004743306431919336 norm:5.5176769819809124e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:32:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 17 loss:0.0047404030337929726 norm:5.611121014226228e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:34:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 18 loss:0.004734271205961704 norm:5.379867798183113e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:36:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 19 loss:0.004730967339128256 norm:5.4788626584922895e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:39:09 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [7, 8, 9]
[2025-03-23 04:39:09 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [10, 11, 12] ===
[2025-03-23 04:41:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 0 loss:0.007955809123814106 norm:7.060087955323979e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:43:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 1 loss:0.007593287620693445 norm:4.194429857307114e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:45:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 2 loss:0.007351107429713011 norm:3.0869887268636376e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:47:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 3 loss:0.007068273611366749 norm:2.7438914912636392e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:50:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 4 loss:0.006745702121406794 norm:2.6628375053405762e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:52:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 5 loss:0.006423249840736389 norm:2.5426241336390376e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:54:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 6 loss:0.006175111513584852 norm:2.4447568648611195e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:56:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 7 loss:0.0060111526399850845 norm:2.396278978267219e-05 max memory_allocated 62752.3779296875 
[2025-03-23 04:58:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 8 loss:0.005922116804867983 norm:2.4019693228183314e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:00:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 9 loss:0.0058812350034713745 norm:2.3789993065292947e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:02:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 10 loss:0.00585850328207016 norm:2.3553384380647913e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:05:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 11 loss:0.0058461567386984825 norm:2.3065964342094958e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:07:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 12 loss:0.00584192480891943 norm:2.3040609448798932e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:09:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 13 loss:0.005836907774209976 norm:2.260609471704811e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:11:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 14 loss:0.005835067480802536 norm:2.2577782146981917e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:13:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 15 loss:0.005833849776536226 norm:2.27748641918879e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:15:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 16 loss:0.00583017710596323 norm:2.263667556690052e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:17:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 17 loss:0.005826869979500771 norm:2.275563201692421e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:20:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 18 loss:0.0058259908109903336 norm:2.252691047033295e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:22:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 19 loss:0.005824144463986158 norm:2.232950646430254e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:25:05 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [10, 11, 12]
[2025-03-23 05:25:05 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [13, 14, 15] ===
[2025-03-23 05:27:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 0 loss:0.009838808327913284 norm:7.159155211411417e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:29:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 1 loss:0.009415452368557453 norm:4.450637788977474e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:31:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 2 loss:0.009090553969144821 norm:3.126360024907626e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:33:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 3 loss:0.008710303343832493 norm:2.556123217800632e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:35:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 4 loss:0.008265813812613487 norm:2.4350752937607467e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:38:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 5 loss:0.0078691141679883 norm:2.3903492547105998e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:40:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 6 loss:0.007613745983690023 norm:2.320739622518886e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:42:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 7 loss:0.007482751272618771 norm:2.284264519403223e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:44:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 8 loss:0.007421043701469898 norm:2.2536798496730626e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:46:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 9 loss:0.007390349637717009 norm:2.231691178167239e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:48:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 10 loss:0.0073760151863098145 norm:2.2002703190082684e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:50:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 11 loss:0.00736709451302886 norm:2.1976598873152398e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:53:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 12 loss:0.007360988762229681 norm:2.1933392417849973e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:55:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 13 loss:0.007358008995652199 norm:2.202366158599034e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:57:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 14 loss:0.007351529784500599 norm:2.172692256863229e-05 max memory_allocated 62752.3779296875 
[2025-03-23 05:59:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 15 loss:0.007349184714257717 norm:2.1545032723224722e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:01:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 16 loss:0.007346380036324263 norm:2.1323734472389333e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:03:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 17 loss:0.007343494798988104 norm:2.1327256035874598e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:06:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 18 loss:0.007342333905398846 norm:2.0958415916538797e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:08:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 19 loss:0.007340149953961372 norm:2.073083487630356e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:11:03 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [13, 14, 15]
[2025-03-23 06:11:03 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [16, 17, 18] ===
[2025-03-23 06:13:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 0 loss:0.013159970752894878 norm:9.622902143746614e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:15:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 1 loss:0.012599770911037922 norm:5.6259123084601015e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:17:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 2 loss:0.012106278911232948 norm:3.878408460877836e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:19:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 3 loss:0.011467838659882545 norm:3.1352974474430084e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:21:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 4 loss:0.010765548795461655 norm:2.809973193507176e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:24:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 5 loss:0.010293902829289436 norm:2.7217447495786473e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:26:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 6 loss:0.010106763802468777 norm:2.610114825074561e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:28:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 7 loss:0.010049005970358849 norm:2.5027729861903936e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:30:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 8 loss:0.010028152726590633 norm:2.5286251911893487e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:32:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 9 loss:0.010015600360929966 norm:2.4148357624653727e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:34:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 10 loss:0.010009516030550003 norm:2.4441997084068134e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:36:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 11 loss:0.009999259375035763 norm:2.3754973881295882e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:39:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 12 loss:0.009995326399803162 norm:2.361687074881047e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:41:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 13 loss:0.009988018311560154 norm:2.3221731680678204e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:43:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 14 loss:0.009983562864363194 norm:2.309773481101729e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:45:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 15 loss:0.009978555142879486 norm:2.2495471057482064e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:47:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 16 loss:0.00997437909245491 norm:2.2230688045965508e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:49:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 17 loss:0.00997079536318779 norm:2.2129299395601265e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:52:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 18 loss:0.00996813178062439 norm:2.2394600819097832e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:54:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 19 loss:0.009965728968381882 norm:2.1910385839873925e-05 max memory_allocated 62752.3779296875 
[2025-03-23 06:57:04 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [16, 17, 18]
[2025-03-23 06:57:04 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [19, 20, 21] ===
[2025-03-23 06:59:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 0 loss:0.018540605902671814 norm:8.999665442388505e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:01:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 1 loss:0.017830712720751762 norm:5.859645898453891e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:03:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 2 loss:0.016983382403850555 norm:3.865337203023955e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:05:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 3 loss:0.01579795591533184 norm:2.9926304705440998e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:07:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 4 loss:0.014883779920637608 norm:2.872886398108676e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:10:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 5 loss:0.014598616398870945 norm:2.7975551347481087e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:12:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 6 loss:0.01453571766614914 norm:2.722793578868732e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:14:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 7 loss:0.01451596524566412 norm:2.647709334269166e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:16:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 8 loss:0.014504105783998966 norm:2.6459347282070667e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:18:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 9 loss:0.014491814188659191 norm:2.5573564926162362e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:20:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 10 loss:0.01448810938745737 norm:2.55949762504315e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:22:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 11 loss:0.014480250887572765 norm:2.594338366179727e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:25:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 12 loss:0.014473596587777138 norm:2.572242374299094e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:27:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 13 loss:0.014468254521489143 norm:2.5413919502170756e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:29:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 14 loss:0.014462217688560486 norm:2.551359102653805e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:31:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 15 loss:0.014456863515079021 norm:2.5065834051929414e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:33:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 16 loss:0.014451059512794018 norm:2.5571243895683438e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:35:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 17 loss:0.014446104876697063 norm:2.5092027499340475e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:38:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 18 loss:0.014442256651818752 norm:2.5107710825977847e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:40:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 19 loss:0.014438839629292488 norm:2.477571615600027e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:43:14 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [19, 20, 21]
[2025-03-23 07:43:14 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [22, 23, 24] ===
[2025-03-23 07:45:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 0 loss:0.02730366587638855 norm:0.00010548065620241687 max memory_allocated 62752.3779296875 
[2025-03-23 07:47:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 1 loss:0.026382800191640854 norm:7.656975503778085e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:49:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 2 loss:0.024997083470225334 norm:5.667924779118039e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:51:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 3 loss:0.023075969889760017 norm:4.747546336147934e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:54:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 4 loss:0.022222386673092842 norm:4.517174238571897e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:56:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 5 loss:0.02210775576531887 norm:4.2335821490269154e-05 max memory_allocated 62752.3779296875 
[2025-03-23 07:58:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 6 loss:0.022080939263105392 norm:4.17738119722344e-05 max memory_allocated 62752.3779296875 
[2025-03-23 08:00:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 7 loss:0.022060610353946686 norm:4.138420626986772e-05 max memory_allocated 62752.3779296875 
[2025-03-23 08:02:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 8 loss:0.0220489539206028 norm:4.2516374378465116e-05 max memory_allocated 62752.3779296875 
[2025-03-23 08:04:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 9 loss:0.022033395245671272 norm:4.150572931393981e-05 max memory_allocated 62752.3779296875 
[2025-03-23 08:06:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 10 loss:0.02202272042632103 norm:4.244837327860296e-05 max memory_allocated 62752.3779296875 
[2025-03-23 08:09:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 11 loss:0.022013938054442406 norm:4.24943063990213e-05 max memory_allocated 62752.3779296875 
[2025-03-23 08:11:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 12 loss:0.02200229838490486 norm:4.135024573770352e-05 max memory_allocated 62752.3779296875 
[2025-03-23 08:13:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 13 loss:0.021991929039359093 norm:4.071127841598354e-05 max memory_allocated 62752.3779296875 
[2025-03-23 08:15:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 14 loss:0.021982135251164436 norm:4.074006574228406e-05 max memory_allocated 62752.3779296875 
[2025-03-23 08:17:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 15 loss:0.02197643369436264 norm:3.975193976657465e-05 max memory_allocated 62752.3779296875 
[2025-03-23 08:19:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 16 loss:0.021968107670545578 norm:3.98662086809054e-05 max memory_allocated 62752.3779296875 
[2025-03-23 08:22:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 17 loss:0.021960442885756493 norm:3.9447091694455594e-05 max memory_allocated 62752.3779296875 
[2025-03-23 08:24:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 18 loss:0.021953923627734184 norm:3.9193168049678206e-05 max memory_allocated 62752.3779296875 
[2025-03-23 08:26:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 19 loss:0.021954800933599472 norm:3.8738591683795676e-05 max memory_allocated 62752.3779296875 
[2025-03-23 08:29:09 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [22, 23, 24]
[2025-03-23 08:29:09 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [25, 26, 27] ===
[2025-03-23 08:31:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 0 loss:0.03880705684423447 norm:0.00010296596155967563 max memory_allocated 62752.7841796875 
[2025-03-23 08:33:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 1 loss:0.03761342167854309 norm:7.270993228303269e-05 max memory_allocated 62752.7841796875 
[2025-03-23 08:35:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 2 loss:0.035424888134002686 norm:5.3251533245202154e-05 max memory_allocated 62752.7841796875 
[2025-03-23 08:37:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 3 loss:0.03286677598953247 norm:4.480839561438188e-05 max memory_allocated 62752.7841796875 
[2025-03-23 08:40:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 4 loss:0.03234342485666275 norm:4.413790884427726e-05 max memory_allocated 62752.7841796875 
[2025-03-23 08:42:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 5 loss:0.03229069709777832 norm:4.303944297134876e-05 max memory_allocated 62752.7841796875 
[2025-03-23 08:44:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 6 loss:0.032269395887851715 norm:4.326536145526916e-05 max memory_allocated 62752.7841796875 
[2025-03-23 08:46:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 7 loss:0.032251693308353424 norm:4.2998108256142586e-05 max memory_allocated 62752.7841796875 
[2025-03-23 08:48:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 8 loss:0.032234564423561096 norm:4.2866966396104544e-05 max memory_allocated 62752.7841796875 
[2025-03-23 08:50:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 9 loss:0.032219454646110535 norm:4.185998477623798e-05 max memory_allocated 62752.7841796875 
[2025-03-23 08:52:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 10 loss:0.03220897167921066 norm:4.1815495933406055e-05 max memory_allocated 62752.7841796875 
[2025-03-23 08:55:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 11 loss:0.03219884634017944 norm:4.065991015522741e-05 max memory_allocated 62752.7841796875 
[2025-03-23 08:57:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 12 loss:0.03218543156981468 norm:4.063529195263982e-05 max memory_allocated 62752.7841796875 
[2025-03-23 08:59:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 13 loss:0.03217640891671181 norm:4.0285318391397595e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:01:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 14 loss:0.03216862678527832 norm:4.062878360855393e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:03:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 15 loss:0.032157961279153824 norm:4.0415176044916734e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:05:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 16 loss:0.03215290978550911 norm:4.074230673722923e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:07:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 17 loss:0.032143957912921906 norm:4.0908918890636414e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:10:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 18 loss:0.03213556110858917 norm:3.981668851338327e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:12:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 19 loss:0.03212923929095268 norm:3.9809874579077587e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:15:02 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [25, 26, 27]
[2025-03-23 09:15:02 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [28, 29] ===
[2025-03-23 09:16:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 0 loss:0.04359486326575279 norm:0.00013951444998383522 max memory_allocated 62752.7841796875 
[2025-03-23 09:17:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 1 loss:0.04261578619480133 norm:9.56208969000727e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:19:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 2 loss:0.040671199560165405 norm:7.819609891157597e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:20:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 3 loss:0.03882623463869095 norm:7.036760507617146e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:22:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 4 loss:0.03861571475863457 norm:7.357991125900298e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:23:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 5 loss:0.03858893737196922 norm:6.963650230318308e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:25:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 6 loss:0.038579851388931274 norm:7.079848001012579e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:26:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 7 loss:0.038567688316106796 norm:7.033196743577719e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:27:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 8 loss:0.038558363914489746 norm:6.591006967937574e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:29:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 9 loss:0.03854919224977493 norm:6.791069608880207e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:30:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 10 loss:0.0385432243347168 norm:6.702900282107294e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:32:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 11 loss:0.03853375464677811 norm:6.354304059641436e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:33:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 12 loss:0.0385267473757267 norm:6.605637463508174e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:35:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 13 loss:0.03852073848247528 norm:5.839646473759785e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:36:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 14 loss:0.0385165698826313 norm:5.5456657719332725e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:38:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 15 loss:0.03851008042693138 norm:5.0126618589274585e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:39:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 16 loss:0.03850423917174339 norm:5.6333938118768856e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:40:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 17 loss:0.03850090503692627 norm:5.928698374191299e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:42:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 18 loss:0.03849789500236511 norm:6.109732203185558e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:43:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 19 loss:0.038492731750011444 norm:5.9624133427860215e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:45:42 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [28, 29]
[2025-03-23 09:45:42 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [30, 31] ===
[2025-03-23 09:47:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 0 loss:0.05389584228396416 norm:9.122904157266021e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:48:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 1 loss:0.052651356905698776 norm:6.255455082282424e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:50:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 2 loss:0.05014707148075104 norm:4.9516467697685584e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:51:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 3 loss:0.04834833741188049 norm:4.719743083114736e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:52:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 4 loss:0.04822640120983124 norm:4.559644003165886e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:54:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 5 loss:0.04820078983902931 norm:4.388455272419378e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:55:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 6 loss:0.04818199202418327 norm:4.319631989346817e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:57:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 7 loss:0.04816935956478119 norm:4.342140528024174e-05 max memory_allocated 62752.7841796875 
[2025-03-23 09:58:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 8 loss:0.048162076622247696 norm:4.341680323705077e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:00:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 9 loss:0.04814969748258591 norm:4.3594391172518954e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:01:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 10 loss:0.04813814163208008 norm:4.3562817154452205e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:02:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 11 loss:0.04812544956803322 norm:4.240461566951126e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:04:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 12 loss:0.04811908304691315 norm:4.16160182794556e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:05:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 13 loss:0.04811165854334831 norm:4.207794336252846e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:07:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 14 loss:0.04810292273759842 norm:3.9867405575932935e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:08:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 15 loss:0.04809639975428581 norm:4.082864325027913e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:10:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 16 loss:0.048095304518938065 norm:4.102326784050092e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:11:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 17 loss:0.04808878153562546 norm:4.123331746086478e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:13:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 18 loss:0.04808764159679413 norm:3.994302460341714e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:14:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 19 loss:0.048081111162900925 norm:4.024672307423316e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:16:18 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [30, 31]
[2025-03-23 10:16:18 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [32, 33] ===
[2025-03-23 10:17:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 0 loss:0.06598536670207977 norm:0.00017640097939874977 max memory_allocated 62752.7841796875 
[2025-03-23 10:19:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 1 loss:0.06438834965229034 norm:0.00020845347899012268 max memory_allocated 62752.7841796875 
[2025-03-23 10:20:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 2 loss:0.06126411631703377 norm:0.00013320140715222806 max memory_allocated 62752.7841796875 
[2025-03-23 10:22:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 3 loss:0.05958695337176323 norm:6.44130414002575e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:23:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 4 loss:0.0595160573720932 norm:6.0201036831131205e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:24:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 5 loss:0.05949404090642929 norm:5.4779782658442855e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:26:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 6 loss:0.05948096886277199 norm:5.3916322940494865e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:27:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 7 loss:0.05946122482419014 norm:5.19573331985157e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:29:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 8 loss:0.059449922293424606 norm:5.2409788622753695e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:30:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 9 loss:0.05944161117076874 norm:5.104775482323021e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:32:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 10 loss:0.059429727494716644 norm:5.149248681846075e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:33:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 11 loss:0.05941532552242279 norm:5.2710405725520104e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:35:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 12 loss:0.05940694734454155 norm:5.192992102820426e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:36:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 13 loss:0.05939985811710358 norm:5.1193310355301946e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:37:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 14 loss:0.05939371883869171 norm:5.170282020117156e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:39:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 15 loss:0.0593847893178463 norm:5.1190185331506655e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:40:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 16 loss:0.059377677738666534 norm:5.095515007269569e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:42:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 17 loss:0.059370920062065125 norm:5.156378756510094e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:43:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 18 loss:0.0593680776655674 norm:5.180662265047431e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:45:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 19 loss:0.05936364457011223 norm:4.900905696558766e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:46:57 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [32, 33]
[2025-03-23 10:46:57 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [34] ===
[2025-03-23 10:47:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 0 loss:0.06750605255365372 norm:7.874662696849555e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:48:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 1 loss:0.06655765324831009 norm:5.527593384613283e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:49:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 2 loss:0.06478115916252136 norm:3.910317172994837e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:49:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 3 loss:0.06401344388723373 norm:3.4400905860820785e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:50:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 4 loss:0.06397898495197296 norm:3.38803838531021e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:51:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 5 loss:0.06396257132291794 norm:3.41543200192973e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:52:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 6 loss:0.0639498233795166 norm:3.224238025723025e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:52:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 7 loss:0.06393736600875854 norm:3.078128429478966e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:53:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 8 loss:0.06392794847488403 norm:3.077173823839985e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:54:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 9 loss:0.0639185756444931 norm:3.136925806757063e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:54:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 10 loss:0.06391256302595139 norm:3.166326496284455e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:55:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 11 loss:0.06390577554702759 norm:3.202004882041365e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:56:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 12 loss:0.06389884650707245 norm:2.971439971588552e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:57:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 13 loss:0.06389439851045609 norm:2.9221700970083475e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:57:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 14 loss:0.06389889121055603 norm:2.9865559554309584e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:58:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 15 loss:0.06389490514993668 norm:3.197196565452032e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:59:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 16 loss:0.0638873502612114 norm:2.9384129447862506e-05 max memory_allocated 62752.7841796875 
[2025-03-23 10:59:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 17 loss:0.06388834118843079 norm:2.9568687750725076e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:00:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 18 loss:0.06388034671545029 norm:3.051984094781801e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:01:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 19 loss:0.06388179212808609 norm:2.984164166264236e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:02:18 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 15, block: [34]
[2025-03-23 11:02:18 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 16 with layers [35] ===
[2025-03-23 11:03:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 0 loss:0.0751977413892746 norm:0.00010143368126591668 max memory_allocated 62752.7841796875 
[2025-03-23 11:03:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 1 loss:0.07407733052968979 norm:7.512116280850023e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:04:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 2 loss:0.07209407538175583 norm:5.925611549173482e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:05:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 3 loss:0.0713786780834198 norm:5.487372982315719e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:05:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 4 loss:0.07134758681058884 norm:5.5370546760968864e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:06:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 5 loss:0.0713300034403801 norm:4.904920933768153e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:07:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 6 loss:0.07131186127662659 norm:4.8192101530730724e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:08:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 7 loss:0.07130567729473114 norm:4.748363789985888e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:08:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 8 loss:0.07129892706871033 norm:5.028007581131533e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:09:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 9 loss:0.07128389179706573 norm:4.225006705382839e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:10:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 10 loss:0.0712781697511673 norm:4.304526373744011e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:10:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 11 loss:0.07127094268798828 norm:4.3129679397679865e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:11:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 12 loss:0.07126153260469437 norm:4.277436528354883e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:12:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 13 loss:0.07125845551490784 norm:4.174682544544339e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:13:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 14 loss:0.07124845683574677 norm:4.058479316881858e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:13:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 15 loss:0.07123969495296478 norm:4.310769872972742e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:14:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 16 loss:0.07123666256666183 norm:4.496929614106193e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:15:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 17 loss:0.071229487657547 norm:3.90486566175241e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:16:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 18 loss:0.07122532278299332 norm:3.9213584386743605e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:16:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 19 loss:0.07122015208005905 norm:3.8963178667472675e-05 max memory_allocated 62752.7841796875 
[2025-03-23 11:17:43 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 16, block: [35]
[2025-03-23 11:17:43 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 17 with layers [36] ===
[2025-03-23 11:17:43 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 11:18:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 0 loss:0.0844697654247284 norm:0.0020585418678820133 max memory_allocated 62752.7841796875 
[2025-03-23 11:19:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 1 loss:0.08299180865287781 norm:0.001485415967181325 max memory_allocated 62752.7841796875 
[2025-03-23 11:19:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 2 loss:0.08073289692401886 norm:0.0012336443178355694 max memory_allocated 62752.7841796875 
[2025-03-23 11:20:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 3 loss:0.0800224095582962 norm:0.0010290489299222827 max memory_allocated 62752.7841796875 
[2025-03-23 11:21:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 4 loss:0.07996861636638641 norm:0.0008730076951906085 max memory_allocated 62752.7841796875 
[2025-03-23 11:22:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 5 loss:0.07993912696838379 norm:0.0007616422371938825 max memory_allocated 62752.7841796875 
[2025-03-23 11:22:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 6 loss:0.07995828986167908 norm:0.00082958530401811 max memory_allocated 62752.7841796875 
[2025-03-23 11:23:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 7 loss:0.08005152642726898 norm:0.0009797553066164255 max memory_allocated 62752.7841796875 
[2025-03-23 11:24:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 8 loss:0.08000829815864563 norm:0.0009406530298292637 max memory_allocated 62752.7841796875 
[2025-03-23 11:25:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 9 loss:0.07987459748983383 norm:0.0005910815089009702 max memory_allocated 62752.7841796875 
[2025-03-23 11:25:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 10 loss:0.07981554418802261 norm:0.0005096846725791693 max memory_allocated 62752.7841796875 
[2025-03-23 11:26:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 11 loss:0.07978346198797226 norm:0.0004639637190848589 max memory_allocated 62752.7841796875 
[2025-03-23 11:27:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 12 loss:0.07976343482732773 norm:0.0004405642976053059 max memory_allocated 62752.7841796875 
[2025-03-23 11:27:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 13 loss:0.07975781708955765 norm:0.00041011424036696553 max memory_allocated 62752.7841796875 
[2025-03-23 11:28:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 14 loss:0.07974997162818909 norm:0.0003931631799787283 max memory_allocated 62752.7841796875 
[2025-03-23 11:29:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 15 loss:0.07976739853620529 norm:0.00040544618968851864 max memory_allocated 62752.7841796875 
[2025-03-23 11:30:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 16 loss:0.0797848179936409 norm:0.0004329494549892843 max memory_allocated 62752.7841796875 
[2025-03-23 11:30:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 17 loss:0.07993026077747345 norm:0.000710225896909833 max memory_allocated 62752.7841796875 
[2025-03-23 11:31:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 18 loss:0.08089584112167358 norm:0.00205858307890594 max memory_allocated 62752.7841796875 
[2025-03-23 11:32:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 19 loss:0.08001916110515594 norm:0.0009445120813325047 max memory_allocated 62752.7841796875 
[2025-03-23 11:33:09 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 17, block: [36]
[2025-03-23 11:33:09 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 18 with layers [37] ===
[2025-03-23 11:33:10 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 11:33:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 0 loss:0.09552935510873795 norm:0.002863408997654915 max memory_allocated 62752.7841796875 
[2025-03-23 11:34:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 1 loss:0.09361803531646729 norm:0.002031962852925062 max memory_allocated 62752.7841796875 
[2025-03-23 11:35:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 2 loss:0.09105528146028519 norm:0.0016786621417850256 max memory_allocated 62752.7841796875 
[2025-03-23 11:36:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 3 loss:0.09035495668649673 norm:0.00149258051533252 max memory_allocated 62752.7841796875 
[2025-03-23 11:36:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 4 loss:0.09029380977153778 norm:0.00131591921672225 max memory_allocated 62752.7841796875 
[2025-03-23 11:37:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 5 loss:0.0902567133307457 norm:0.0011592477094382048 max memory_allocated 62752.7841796875 
[2025-03-23 11:38:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 6 loss:0.09022358059883118 norm:0.0010471331188455224 max memory_allocated 62752.7841796875 
[2025-03-23 11:38:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 7 loss:0.09020723402500153 norm:0.0010157930664718151 max memory_allocated 62752.7841796875 
[2025-03-23 11:39:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 8 loss:0.0902700200676918 norm:0.0011666412465274334 max memory_allocated 62752.7841796875 
[2025-03-23 11:40:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 9 loss:0.09013472497463226 norm:0.0008752779103815556 max memory_allocated 62752.7841796875 
[2025-03-23 11:41:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 10 loss:0.09005765616893768 norm:0.0007411711267195642 max memory_allocated 62752.7841796875 
[2025-03-23 11:41:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 11 loss:0.09003178030252457 norm:0.0007003672071732581 max memory_allocated 62752.7841796875 
[2025-03-23 11:42:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 12 loss:0.09002169221639633 norm:0.0006698983488604426 max memory_allocated 62752.7841796875 
[2025-03-23 11:43:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 13 loss:0.09000498056411743 norm:0.0006247472483664751 max memory_allocated 62752.7841796875 
[2025-03-23 11:44:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 14 loss:0.08998516201972961 norm:0.0005869814194738865 max memory_allocated 62752.7841796875 
[2025-03-23 11:44:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 15 loss:0.08999672532081604 norm:0.0005937076639384031 max memory_allocated 62752.7841796875 
[2025-03-23 11:45:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 16 loss:0.08998239785432816 norm:0.0005763277295045555 max memory_allocated 62752.7841796875 
[2025-03-23 11:46:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 17 loss:0.09002428501844406 norm:0.0006105521460995078 max memory_allocated 62752.7841796875 
[2025-03-23 11:46:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 18 loss:0.0900358036160469 norm:0.0005262806080281734 max memory_allocated 62752.7841796875 
[2025-03-23 11:47:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 19 loss:0.09000183641910553 norm:0.0005408944562077522 max memory_allocated 62752.7841796875 
[2025-03-23 11:48:34 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 18, block: [37]
[2025-03-23 11:48:34 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 19 with layers [38] ===
[2025-03-23 11:48:35 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 11:49:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 0 loss:0.1149919182062149 norm:0.0043389745987951756 max memory_allocated 62752.7841796875 
[2025-03-23 11:50:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 1 loss:0.11159850656986237 norm:0.0028539292979985476 max memory_allocated 62752.7841796875 
[2025-03-23 11:50:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 2 loss:0.10861562192440033 norm:0.002298328559845686 max memory_allocated 62752.7841796875 
[2025-03-23 11:51:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 3 loss:0.10773718357086182 norm:0.0019479504553601146 max memory_allocated 62752.7841796875 
[2025-03-23 11:52:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 4 loss:0.10762899369001389 norm:0.0018027962651103735 max memory_allocated 62752.7841796875 
[2025-03-23 11:52:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 5 loss:0.10754329711198807 norm:0.0016162848332896829 max memory_allocated 62752.7841796875 
[2025-03-23 11:53:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 6 loss:0.10746803134679794 norm:0.0013854987919330597 max memory_allocated 62752.7841796875 
[2025-03-23 11:54:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 7 loss:0.10741693526506424 norm:0.0012290405575186014 max memory_allocated 62752.7841796875 
[2025-03-23 11:55:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 8 loss:0.10738087445497513 norm:0.0012137071462348104 max memory_allocated 62752.7841796875 
[2025-03-23 11:55:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 9 loss:0.10746178030967712 norm:0.0012663875240832567 max memory_allocated 62752.7841796875 
[2025-03-23 11:56:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 10 loss:0.10745712369680405 norm:0.001241153571754694 max memory_allocated 62752.7841796875 
[2025-03-23 11:57:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 11 loss:0.10747746378183365 norm:0.0011451467871665955 max memory_allocated 62752.7841796875 
[2025-03-23 11:58:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 12 loss:0.10751796513795853 norm:0.001282860990613699 max memory_allocated 62752.7841796875 
[2025-03-23 11:58:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 13 loss:0.10743934661149979 norm:0.0010739532299339771 max memory_allocated 62752.7841796875 
[2025-03-23 11:59:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 14 loss:0.10738390684127808 norm:0.0010314213577657938 max memory_allocated 62752.7841796875 
[2025-03-23 12:00:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 15 loss:0.10741136968135834 norm:0.0010233053471893072 max memory_allocated 62752.7841796875 
[2025-03-23 12:00:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 16 loss:0.10740487277507782 norm:0.0010757104028016329 max memory_allocated 62752.7841796875 
[2025-03-23 12:01:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 17 loss:0.10740199685096741 norm:0.0010101982625201344 max memory_allocated 62752.7841796875 
[2025-03-23 12:02:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 18 loss:0.10744801163673401 norm:0.0011198504362255335 max memory_allocated 62752.7841796875 
[2025-03-23 12:03:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 19 loss:0.10734415799379349 norm:0.0009465294424444437 max memory_allocated 62752.7841796875 
[2025-03-23 12:03:58 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 19, block: [38]
[2025-03-23 12:03:58 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 20 with layers [39] ===
[2025-03-23 12:03:59 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 12:04:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 0 loss:0.1712038218975067 norm:0.008878522552549839 max memory_allocated 62752.7841796875 
[2025-03-23 12:05:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 1 loss:0.16640934348106384 norm:0.00705430842936039 max memory_allocated 62752.7841796875 
[2025-03-23 12:06:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 2 loss:0.16254065930843353 norm:0.005738305393606424 max memory_allocated 62752.7841796875 
[2025-03-23 12:06:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 3 loss:0.16105538606643677 norm:0.005171672906726599 max memory_allocated 62752.7841796875 
[2025-03-23 12:07:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 4 loss:0.1605415642261505 norm:0.004903058521449566 max memory_allocated 62752.7841796875 
[2025-03-23 12:08:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 5 loss:0.1599850356578827 norm:0.004201218485832214 max memory_allocated 62752.7841796875 
[2025-03-23 12:09:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 6 loss:0.15974605083465576 norm:0.0041430131532251835 max memory_allocated 62752.7841796875 
[2025-03-23 12:09:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 7 loss:0.15943409502506256 norm:0.003874341957271099 max memory_allocated 62752.7841796875 
[2025-03-23 12:10:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 8 loss:0.15919999778270721 norm:0.0039731599390506744 max memory_allocated 62752.7841796875 
[2025-03-23 12:11:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 9 loss:0.1590263396501541 norm:0.0038242219015955925 max memory_allocated 62752.7841796875 
[2025-03-23 12:11:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 10 loss:0.15895597636699677 norm:0.003611926222220063 max memory_allocated 62752.7841796875 
[2025-03-23 12:12:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 11 loss:0.15885870158672333 norm:0.0035723508335649967 max memory_allocated 62752.7841796875 
[2025-03-23 12:13:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 12 loss:0.15903908014297485 norm:0.004083735402673483 max memory_allocated 62752.7841796875 
[2025-03-23 12:14:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 13 loss:0.16025927662849426 norm:0.006362150423228741 max memory_allocated 62752.7841796875 
[2025-03-23 12:14:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 14 loss:0.15999028086662292 norm:0.0046614729799330235 max memory_allocated 62752.7841796875 
[2025-03-23 12:15:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 15 loss:0.15842477977275848 norm:0.0025364444591104984 max memory_allocated 62752.7841796875 
[2025-03-23 12:16:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 16 loss:0.15825286507606506 norm:0.002656955737620592 max memory_allocated 62752.7841796875 
[2025-03-23 12:17:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 17 loss:0.1582382768392563 norm:0.002731784014031291 max memory_allocated 62752.7841796875 
[2025-03-23 12:17:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 18 loss:0.15830090641975403 norm:0.002997544128447771 max memory_allocated 62752.7841796875 
[2025-03-23 12:18:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 19 loss:0.15828871726989746 norm:0.002872613025829196 max memory_allocated 62752.7841796875 
[2025-03-23 12:19:23 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 20, block: [39]
[2025-03-23 12:19:23 root] (main_calib_config3_attn.py 379): INFO 36834.874009132385
[2025-03-23 12:19:34 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-23 12:20:58 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.108382701873779
[2025-03-23 12:20:58 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-23 12:23:10 root] (main_calib_config3_attn.py 161): INFO c4 : 6.632966041564941
[2025-03-23 12:23:20 datasets.load] (load.py 1272): WARNING Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/piqa/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011 (last modified on Tue Feb 18 03:13:08 2025) since it couldn't be found locally at piqa., or remotely on the Hugging Face Hub.
[2025-03-23 13:15:51 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.108382701873779, 'c4': 6.632966041564941, 'results': {'piqa': {'acc': 0.7899891186071817, 'acc_stderr': 0.009503353305818573, 'acc_norm': 0.7889009793253536, 'acc_norm_stderr': 0.00952137737873414}, 'boolq': {'acc': 0.6853211009174311, 'acc_stderr': 0.008122194827042028}, 'arc_challenge': {'acc': 0.4334470989761092, 'acc_stderr': 0.014481376224558896, 'acc_norm': 0.4445392491467577, 'acc_norm_stderr': 0.014521226405627079}, 'winogrande': {'acc': 0.7048145224940805, 'acc_stderr': 0.012819410741754774}, 'arc_easy': {'acc': 0.7432659932659933, 'acc_stderr': 0.008963590834042407, 'acc_norm': 0.5942760942760943, 'acc_norm_stderr': 0.01007575554012888}, 'hellaswag': {'acc': 0.5891256721768572, 'acc_stderr': 0.004909870006388837, 'acc_norm': 0.7609042023501295, 'acc_norm_stderr': 0.004256596457810718}}, 'versions': {'piqa': 0, 'boolq': 1, 'arc_challenge': 0, 'winogrande': 0, 'arc_easy': 0, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-23 13:15:51 root] (main_calib_config3_attn.py 175): INFO 43.34,74.33,68.53,58.91,79.00,70.48
