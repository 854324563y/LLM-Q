[2025-03-22 14:14:48 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/llama-7b-hf-0.85', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.85.pkl', blocks_pkl='./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-22 14:15:05 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-22 14:15:05 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-22 14:15:06 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-22 14:15:06 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.85.pkl
[2025-03-22 14:15:06 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 5), (5, 8), (8, 11), (11, 14), (14, 16), (16, 19), (19, 22), (22, 25), (25, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-22 14:15:06 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2, 3, 4], [5, 6, 7], [8, 9, 10], [11, 12, 13], [14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26], [27], [28], [29], [30], [31]]
[2025-03-22 14:15:08 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-22 14:15:08 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:15:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.0008254675194621086 norm:0.001997064333409071 max memory_allocated 34630.880859375 
[2025-03-22 14:16:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.0004639995750039816 norm:0.0005118025583215058 max memory_allocated 34630.880859375 
[2025-03-22 14:16:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.0004401081532705575 norm:0.0009487208444625139 max memory_allocated 34630.880859375 
[2025-03-22 14:17:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.00045045441947877407 norm:0.0010298506822437048 max memory_allocated 34630.880859375 
[2025-03-22 14:17:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.0004464888188522309 norm:0.0010405853390693665 max memory_allocated 34630.880859375 
[2025-03-22 14:18:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.00043531524715945125 norm:0.0009048773208633065 max memory_allocated 34630.880859375 
[2025-03-22 14:18:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.00042746029794216156 norm:0.0008443691185675561 max memory_allocated 34630.880859375 
[2025-03-22 14:19:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.00041868496919050813 norm:0.0007638663519173861 max memory_allocated 34630.880859375 
[2025-03-22 14:19:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0004078693164046854 norm:0.0006867435877211392 max memory_allocated 34630.880859375 
[2025-03-22 14:19:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.00039567629573866725 norm:0.0006288630538620055 max memory_allocated 34630.880859375 
[2025-03-22 14:20:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.0003961219044867903 norm:0.0006091616814956069 max memory_allocated 34630.880859375 
[2025-03-22 14:20:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0003909928200300783 norm:0.0005653216503560543 max memory_allocated 34630.880859375 
[2025-03-22 14:21:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.000390970817534253 norm:0.0005334458546712995 max memory_allocated 34630.880859375 
[2025-03-22 14:21:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.0003881151496898383 norm:0.0004990075249224901 max memory_allocated 34630.880859375 
[2025-03-22 14:22:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0003809858753811568 norm:0.00043501678737811744 max memory_allocated 34630.880859375 
[2025-03-22 14:22:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.000374651572201401 norm:0.0004047841648571193 max memory_allocated 34630.880859375 
[2025-03-22 14:23:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.00037208409048616886 norm:0.000377257470972836 max memory_allocated 34630.880859375 
[2025-03-22 14:23:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.00036440620897337794 norm:0.0003387534525245428 max memory_allocated 34630.880859375 
[2025-03-22 14:24:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.0003656573826447129 norm:0.00032792898127809167 max memory_allocated 34630.880859375 
[2025-03-22 14:24:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0003628080594353378 norm:0.00030941839213483036 max memory_allocated 34630.880859375 
[2025-03-22 14:25:26 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-22 14:25:26 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-22 14:25:27 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:25:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.00845391396433115 norm:0.014471180737018585 max memory_allocated 35097.7724609375 
[2025-03-22 14:26:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.0040710922330617905 norm:0.008900845423340797 max memory_allocated 35097.7724609375 
[2025-03-22 14:26:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.0032012006267905235 norm:0.005249516572803259 max memory_allocated 35097.7724609375 
[2025-03-22 14:27:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.0029262141324579716 norm:0.00411144969984889 max memory_allocated 35097.7724609375 
[2025-03-22 14:27:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.0027523550670593977 norm:0.003629811806604266 max memory_allocated 35097.7724609375 
[2025-03-22 14:28:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.002658919198438525 norm:0.0033434834331274033 max memory_allocated 35097.7724609375 
[2025-03-22 14:28:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.002577231964096427 norm:0.003090236335992813 max memory_allocated 35097.7724609375 
[2025-03-22 14:29:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.002502468414604664 norm:0.0027887425385415554 max memory_allocated 35097.7724609375 
[2025-03-22 14:29:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.0024258214980363846 norm:0.002549735363572836 max memory_allocated 35097.7724609375 
[2025-03-22 14:30:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.002374017145484686 norm:0.002320529194548726 max memory_allocated 35097.7724609375 
[2025-03-22 14:30:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.0023188339546322823 norm:0.0021279100328683853 max memory_allocated 35097.7724609375 
[2025-03-22 14:31:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.0022572434972971678 norm:0.0019250340992584825 max memory_allocated 35097.7724609375 
[2025-03-22 14:31:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.00227652071043849 norm:0.0017432535532861948 max memory_allocated 35097.7724609375 
[2025-03-22 14:32:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.0022486348170787096 norm:0.0014885369455441833 max memory_allocated 35097.7724609375 
[2025-03-22 14:32:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.0022081653587520123 norm:0.0012797616655007005 max memory_allocated 35097.7724609375 
[2025-03-22 14:33:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.0022093032021075487 norm:0.001132499543018639 max memory_allocated 35097.7724609375 
[2025-03-22 14:33:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.00214857654646039 norm:0.0010224488796666265 max memory_allocated 35097.7724609375 
[2025-03-22 14:34:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.002126151928678155 norm:0.0008485298021696508 max memory_allocated 35097.7724609375 
[2025-03-22 14:34:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.002110469853505492 norm:0.0007691584760323167 max memory_allocated 35097.7724609375 
[2025-03-22 14:35:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.0021434093359857798 norm:0.000762424897402525 max memory_allocated 35097.7724609375 
[2025-03-22 14:35:45 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-22 14:35:45 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2, 3, 4] ===
[2025-03-22 14:35:46 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:37:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 0 loss:0.019929805770516396 norm:0.013611024245619774 max memory_allocated 47468.5419921875 
[2025-03-22 14:38:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 1 loss:0.013742377050220966 norm:0.006500803399831057 max memory_allocated 47468.5419921875 
[2025-03-22 14:40:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 2 loss:0.010455355979502201 norm:0.002892894670367241 max memory_allocated 47468.5419921875 
[2025-03-22 14:41:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 3 loss:0.009464298374950886 norm:0.0032987981103360653 max memory_allocated 47468.5419921875 
[2025-03-22 14:43:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 4 loss:0.008882923051714897 norm:0.0035456535406410694 max memory_allocated 47468.5419921875 
[2025-03-22 14:44:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 5 loss:0.008396481163799763 norm:0.0033211398404091597 max memory_allocated 47468.5419921875 
[2025-03-22 14:45:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 6 loss:0.008078117854893208 norm:0.0030284100212156773 max memory_allocated 47468.5419921875 
[2025-03-22 14:47:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 7 loss:0.007898183539509773 norm:0.0027513669338077307 max memory_allocated 47468.5419921875 
[2025-03-22 14:48:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 8 loss:0.00778777664527297 norm:0.0025807716883718967 max memory_allocated 47468.5419921875 
[2025-03-22 14:50:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 9 loss:0.007704675663262606 norm:0.002470924286171794 max memory_allocated 47468.5419921875 
[2025-03-22 14:51:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 10 loss:0.007633526343852282 norm:0.002240377012640238 max memory_allocated 47468.5419921875 
[2025-03-22 14:53:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 11 loss:0.007583324331790209 norm:0.002205334138125181 max memory_allocated 47468.5419921875 
[2025-03-22 14:54:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 12 loss:0.007539850659668446 norm:0.0020241839811205864 max memory_allocated 47468.5419921875 
[2025-03-22 14:56:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 13 loss:0.007575828116387129 norm:0.0018421884160488844 max memory_allocated 47468.5419921875 
[2025-03-22 14:57:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 14 loss:0.007493697106838226 norm:0.0016366456402465701 max memory_allocated 47468.5419921875 
[2025-03-22 14:58:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 15 loss:0.007433437742292881 norm:0.0015470220241695642 max memory_allocated 47468.5419921875 
[2025-03-22 15:00:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 16 loss:0.0074647823348641396 norm:0.0013960859505459666 max memory_allocated 47468.5419921875 
[2025-03-22 15:01:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 17 loss:0.007696609012782574 norm:0.0014962784480303526 max memory_allocated 47468.5419921875 
[2025-03-22 15:03:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 18 loss:0.007541365921497345 norm:0.0012782212579622865 max memory_allocated 47468.5419921875 
[2025-03-22 15:04:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 19 loss:0.00737414788454771 norm:0.0010731058428063989 max memory_allocated 47468.5419921875 
[2025-03-22 15:06:32 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2, 3, 4]
[2025-03-22 15:06:32 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [5, 6, 7] ===
[2025-03-22 15:08:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 0 loss:0.016519326716661453 norm:0.00043267334694974124 max memory_allocated 47468.7294921875 
[2025-03-22 15:09:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 1 loss:0.014419090002775192 norm:0.00025569507852196693 max memory_allocated 47468.7294921875 
[2025-03-22 15:10:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 2 loss:0.013386068865656853 norm:0.00019598033395595849 max memory_allocated 47468.7294921875 
[2025-03-22 15:12:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 3 loss:0.012301290407776833 norm:0.0001508635759819299 max memory_allocated 47468.7294921875 
[2025-03-22 15:13:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 4 loss:0.011335272341966629 norm:0.00013929198030382395 max memory_allocated 47468.7294921875 
[2025-03-22 15:15:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 5 loss:0.010621289722621441 norm:0.0001342468458460644 max memory_allocated 47468.7294921875 
[2025-03-22 15:16:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 6 loss:0.010173767805099487 norm:0.00013699618284590542 max memory_allocated 47468.7294921875 
[2025-03-22 15:18:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 7 loss:0.009992733597755432 norm:0.00012436567340046167 max memory_allocated 47468.7294921875 
[2025-03-22 15:19:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 8 loss:0.009895832277834415 norm:0.00013466508244164288 max memory_allocated 47468.7294921875 
[2025-03-22 15:21:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 9 loss:0.009856689721345901 norm:0.00014242077304515988 max memory_allocated 47468.7294921875 
[2025-03-22 15:22:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 10 loss:0.009812567383050919 norm:0.00013903065701015294 max memory_allocated 47468.7294921875 
[2025-03-22 15:23:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 11 loss:0.009763036854565144 norm:0.00014252294204197824 max memory_allocated 47468.7294921875 
[2025-03-22 15:25:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 12 loss:0.009729250334203243 norm:0.00013385254715103656 max memory_allocated 47468.7294921875 
[2025-03-22 15:26:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 13 loss:0.009735239669680595 norm:0.00013818018487654626 max memory_allocated 47468.7294921875 
[2025-03-22 15:28:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 14 loss:0.009707030840218067 norm:0.0001325768098467961 max memory_allocated 47468.7294921875 
[2025-03-22 15:29:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 15 loss:0.009687647223472595 norm:0.0001307450293097645 max memory_allocated 47468.7294921875 
[2025-03-22 15:31:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 16 loss:0.009686536155641079 norm:0.00013927674444857985 max memory_allocated 47468.7294921875 
[2025-03-22 15:32:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 17 loss:0.009695501998066902 norm:0.00014677447325084358 max memory_allocated 47468.7294921875 
[2025-03-22 15:33:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 18 loss:0.009662235155701637 norm:0.000139736570417881 max memory_allocated 47468.7294921875 
[2025-03-22 15:35:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 19 loss:0.009651729837059975 norm:0.0001320917799603194 max memory_allocated 47468.7294921875 
[2025-03-22 15:37:15 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [5, 6, 7]
[2025-03-22 15:37:15 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [8, 9, 10] ===
[2025-03-22 15:38:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 0 loss:0.018421929329633713 norm:0.000343725667335093 max memory_allocated 47468.9169921875 
[2025-03-22 15:40:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 1 loss:0.016944142058491707 norm:0.00020434707403182983 max memory_allocated 47468.9169921875 
[2025-03-22 15:41:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 2 loss:0.015849512070417404 norm:0.00013161179958842695 max memory_allocated 47468.9169921875 
[2025-03-22 15:43:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 3 loss:0.014646893367171288 norm:0.0001029659979394637 max memory_allocated 47468.9169921875 
[2025-03-22 15:44:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 4 loss:0.013645540922880173 norm:9.276071068597957e-05 max memory_allocated 47468.9169921875 
[2025-03-22 15:45:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 5 loss:0.013001395389437675 norm:8.496108057443053e-05 max memory_allocated 47468.9169921875 
[2025-03-22 15:47:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 6 loss:0.012691267766058445 norm:8.224174962379038e-05 max memory_allocated 47468.9169921875 
[2025-03-22 15:48:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 7 loss:0.012509382329881191 norm:7.76090455474332e-05 max memory_allocated 47468.9169921875 
[2025-03-22 15:50:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 8 loss:0.012433219701051712 norm:7.579149678349495e-05 max memory_allocated 47468.9169921875 
[2025-03-22 15:51:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 9 loss:0.012382267974317074 norm:7.5156545790378e-05 max memory_allocated 47468.9169921875 
[2025-03-22 15:53:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 10 loss:0.01235487125813961 norm:7.420071779051796e-05 max memory_allocated 47468.9169921875 
[2025-03-22 15:54:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 11 loss:0.01232120767235756 norm:7.158207154134288e-05 max memory_allocated 47468.9169921875 
[2025-03-22 15:56:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 12 loss:0.012297285720705986 norm:6.961401959415525e-05 max memory_allocated 47468.9169921875 
[2025-03-22 15:57:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 13 loss:0.012291749007999897 norm:6.9184105086606e-05 max memory_allocated 47468.9169921875 
[2025-03-22 15:58:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 14 loss:0.012281795032322407 norm:6.872659287182614e-05 max memory_allocated 47468.9169921875 
[2025-03-22 16:00:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 15 loss:0.012271705083549023 norm:6.808022590121254e-05 max memory_allocated 47468.9169921875 
[2025-03-22 16:01:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 16 loss:0.0122638875618577 norm:6.713146285619587e-05 max memory_allocated 47468.9169921875 
[2025-03-22 16:03:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 17 loss:0.012258842587471008 norm:6.696633499814197e-05 max memory_allocated 47468.9169921875 
[2025-03-22 16:04:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 18 loss:0.012249295599758625 norm:7.023852231213823e-05 max memory_allocated 47468.9169921875 
[2025-03-22 16:06:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 19 loss:0.012236585840582848 norm:6.6354354203213e-05 max memory_allocated 47468.9169921875 
[2025-03-22 16:07:58 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [8, 9, 10]
[2025-03-22 16:07:58 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [11, 12, 13] ===
[2025-03-22 16:09:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 0 loss:0.021426554769277573 norm:0.00039476825622841716 max memory_allocated 47469.1044921875 
[2025-03-22 16:10:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 1 loss:0.01871657371520996 norm:0.000255475752055645 max memory_allocated 47469.1044921875 
[2025-03-22 16:12:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 2 loss:0.01724870316684246 norm:0.000202685478143394 max memory_allocated 47469.1044921875 
[2025-03-22 16:13:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 3 loss:0.016126137226819992 norm:0.0001714961399557069 max memory_allocated 47469.1044921875 
[2025-03-22 16:15:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 4 loss:0.01522126141935587 norm:0.00016551795124541968 max memory_allocated 47469.1044921875 
[2025-03-22 16:16:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 5 loss:0.014460565522313118 norm:0.00014282831398304552 max memory_allocated 47469.1044921875 
[2025-03-22 16:18:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 6 loss:0.013980573043227196 norm:0.00013456534361466765 max memory_allocated 47469.1044921875 
[2025-03-22 16:19:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 7 loss:0.013723047450184822 norm:0.0001356485008727759 max memory_allocated 47469.1044921875 
[2025-03-22 16:21:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 8 loss:0.013513135723769665 norm:0.00011548956535989419 max memory_allocated 47469.1044921875 
[2025-03-22 16:22:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 9 loss:0.013323363848030567 norm:0.00010872301209019497 max memory_allocated 47469.1044921875 
[2025-03-22 16:23:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 10 loss:0.013223373331129551 norm:0.00010243697033729404 max memory_allocated 47469.1044921875 
[2025-03-22 16:25:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 11 loss:0.013153573498129845 norm:9.603623038856313e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:26:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 12 loss:0.013110642321407795 norm:9.543709165882319e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:28:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 13 loss:0.013063862919807434 norm:9.495037375018e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:29:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 14 loss:0.013021592982113361 norm:9.329857130069286e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:31:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 15 loss:0.0129732396453619 norm:9.117232548305765e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:32:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 16 loss:0.012962769716978073 norm:9.409648919245228e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:33:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 17 loss:0.012954278849065304 norm:9.925299673341215e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:35:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 18 loss:0.012919142842292786 norm:9.659142233431339e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:36:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 19 loss:0.01288631185889244 norm:9.374743240186945e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:38:39 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [11, 12, 13]
[2025-03-22 16:38:40 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [14, 15] ===
[2025-03-22 16:39:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 0 loss:0.019626887515187263 norm:0.00034232030156999826 max memory_allocated 47469.1044921875 
[2025-03-22 16:40:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 1 loss:0.017842553555965424 norm:0.0002045352739514783 max memory_allocated 47469.1044921875 
[2025-03-22 16:41:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 2 loss:0.016793323680758476 norm:0.00015298214566428214 max memory_allocated 47469.1044921875 
[2025-03-22 16:42:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 3 loss:0.015847545117139816 norm:0.0001308433129452169 max memory_allocated 47469.1044921875 
[2025-03-22 16:43:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 4 loss:0.015037424862384796 norm:0.00011634609109023586 max memory_allocated 47469.1044921875 
[2025-03-22 16:44:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 5 loss:0.014516622759401798 norm:0.00010749438661150634 max memory_allocated 47469.1044921875 
[2025-03-22 16:45:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 6 loss:0.01426786370575428 norm:9.534190030535683e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:46:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 7 loss:0.014118580147624016 norm:8.670664828969166e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:47:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 8 loss:0.014010634273290634 norm:8.022523252293468e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:48:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 9 loss:0.013943983241915703 norm:7.858671597205102e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:49:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 10 loss:0.013857973739504814 norm:7.490182906622067e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:50:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 11 loss:0.013798889704048634 norm:6.86923012835905e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:51:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 12 loss:0.013753966428339481 norm:6.119605677668005e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:52:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 13 loss:0.013726817443966866 norm:5.994227103656158e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:53:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 14 loss:0.013720262795686722 norm:6.122052582213655e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:54:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 15 loss:0.01369609497487545 norm:5.74072400922887e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:55:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 16 loss:0.01370149850845337 norm:6.610443961108103e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:55:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 17 loss:0.013664819300174713 norm:5.9008401876781136e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:56:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 18 loss:0.013644944876432419 norm:5.657511064782739e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:57:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 19 loss:0.01362966001033783 norm:5.693341154255904e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:59:05 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [14, 15]
[2025-03-22 16:59:05 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [16, 17, 18] ===
[2025-03-22 17:00:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 0 loss:0.029713541269302368 norm:0.00037739388062618673 max memory_allocated 47469.4169921875 
[2025-03-22 17:02:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 1 loss:0.026813192293047905 norm:0.00025148389977402985 max memory_allocated 47469.4169921875 
[2025-03-22 17:03:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 2 loss:0.024952424690127373 norm:0.00020133054931648076 max memory_allocated 47469.4169921875 
[2025-03-22 17:04:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 3 loss:0.02327914349734783 norm:0.00017551452037878335 max memory_allocated 47469.4169921875 
[2025-03-22 17:06:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 4 loss:0.02194463647902012 norm:0.00016112955927383155 max memory_allocated 47469.4169921875 
[2025-03-22 17:07:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 5 loss:0.021301593631505966 norm:0.0001455508463550359 max memory_allocated 47469.4169921875 
[2025-03-22 17:09:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 6 loss:0.02090275101363659 norm:0.0001296333211939782 max memory_allocated 47469.4169921875 
[2025-03-22 17:10:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 7 loss:0.020710160955786705 norm:0.00012084614718332887 max memory_allocated 47469.4169921875 
[2025-03-22 17:12:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 8 loss:0.020535415038466454 norm:0.00011525871377671137 max memory_allocated 47469.4169921875 
[2025-03-22 17:13:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 9 loss:0.02043174020946026 norm:0.000113152083940804 max memory_allocated 47469.4169921875 
[2025-03-22 17:15:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 10 loss:0.020337367430329323 norm:0.00010991727322107181 max memory_allocated 47469.4169921875 
[2025-03-22 17:16:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 11 loss:0.020231999456882477 norm:0.0001009640545817092 max memory_allocated 47469.4169921875 
[2025-03-22 17:17:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 12 loss:0.0201650932431221 norm:0.00010055683378595859 max memory_allocated 47469.4169921875 
[2025-03-22 17:19:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 13 loss:0.02011841908097267 norm:9.619850607123226e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:20:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 14 loss:0.020066041499376297 norm:9.755442442838103e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:22:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 15 loss:0.020021744072437286 norm:9.702504030428827e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:23:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 16 loss:0.01999918557703495 norm:9.84770740615204e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:25:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 17 loss:0.020001336932182312 norm:0.00010360407759435475 max memory_allocated 47469.4169921875 
[2025-03-22 17:26:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 18 loss:0.019968055188655853 norm:9.758371743373573e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:27:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 19 loss:0.019951894879341125 norm:0.00010200637188972905 max memory_allocated 47469.4169921875 
[2025-03-22 17:29:47 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [16, 17, 18]
[2025-03-22 17:29:47 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [19, 20, 21] ===
[2025-03-22 17:31:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 0 loss:0.04679935425519943 norm:0.0004939166246913373 max memory_allocated 47469.6044921875 
[2025-03-22 17:32:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 1 loss:0.04367504268884659 norm:0.0002964279556181282 max memory_allocated 47469.6044921875 
[2025-03-22 17:34:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 2 loss:0.040590550750494 norm:0.0002525893214624375 max memory_allocated 47469.6044921875 
[2025-03-22 17:35:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 3 loss:0.037211235612630844 norm:0.0002196638088207692 max memory_allocated 47469.6044921875 
[2025-03-22 17:37:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 4 loss:0.03587578982114792 norm:0.00019550882279872894 max memory_allocated 47469.6044921875 
[2025-03-22 17:38:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 5 loss:0.03548336401581764 norm:0.0001790137030184269 max memory_allocated 47469.6044921875 
[2025-03-22 17:39:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 6 loss:0.03521536663174629 norm:0.0001632598286960274 max memory_allocated 47469.6044921875 
[2025-03-22 17:41:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 7 loss:0.035026323050260544 norm:0.00015690713189542294 max memory_allocated 47469.6044921875 
[2025-03-22 17:42:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 8 loss:0.034851524978876114 norm:0.00014228408690541983 max memory_allocated 47469.6044921875 
[2025-03-22 17:44:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 9 loss:0.03473852947354317 norm:0.00014275769353844225 max memory_allocated 47469.6044921875 
[2025-03-22 17:45:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 10 loss:0.03458733856678009 norm:0.0001328243815805763 max memory_allocated 47469.6044921875 
[2025-03-22 17:47:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 11 loss:0.034501854330301285 norm:0.00012489655637182295 max memory_allocated 47469.6044921875 
[2025-03-22 17:48:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 12 loss:0.0344366580247879 norm:0.00012219262134749442 max memory_allocated 47469.6044921875 
[2025-03-22 17:50:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 13 loss:0.0344010591506958 norm:0.0001266102772206068 max memory_allocated 47469.6044921875 
[2025-03-22 17:51:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 14 loss:0.03437444567680359 norm:0.00011976045061601326 max memory_allocated 47469.6044921875 
[2025-03-22 17:52:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 15 loss:0.03432200103998184 norm:0.00012352959311101586 max memory_allocated 47469.6044921875 
[2025-03-22 17:54:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 16 loss:0.034248724579811096 norm:0.00011201378947589546 max memory_allocated 47469.6044921875 
[2025-03-22 17:55:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 17 loss:0.0342051200568676 norm:0.00010885267693083733 max memory_allocated 47469.6044921875 
[2025-03-22 17:57:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 18 loss:0.034193698316812515 norm:0.00010908835974987596 max memory_allocated 47469.6044921875 
[2025-03-22 17:58:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 19 loss:0.0341627337038517 norm:0.00010752404341474175 max memory_allocated 47469.6044921875 
[2025-03-22 18:00:28 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [19, 20, 21]
[2025-03-22 18:00:28 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [22, 23, 24] ===
[2025-03-22 18:02:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 0 loss:0.06846637278795242 norm:0.000496435328386724 max memory_allocated 47469.7919921875 
[2025-03-22 18:03:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 1 loss:0.0644795373082161 norm:0.0003684557741507888 max memory_allocated 47469.7919921875 
[2025-03-22 18:04:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 2 loss:0.05987247824668884 norm:0.0003189432027284056 max memory_allocated 47469.7919921875 
[2025-03-22 18:06:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 3 loss:0.0561530701816082 norm:0.0002867518924176693 max memory_allocated 47469.7919921875 
[2025-03-22 18:07:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 4 loss:0.055278316140174866 norm:0.00025851227110251784 max memory_allocated 47469.7919921875 
[2025-03-22 18:09:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 5 loss:0.054983898997306824 norm:0.0002675195864867419 max memory_allocated 47469.7919921875 
[2025-03-22 18:10:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 6 loss:0.05457388982176781 norm:0.00021988016669638455 max memory_allocated 47469.7919921875 
[2025-03-22 18:12:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 7 loss:0.054374150931835175 norm:0.00021432142239063978 max memory_allocated 47469.7919921875 
[2025-03-22 18:13:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 8 loss:0.05420122295618057 norm:0.00020160760323051363 max memory_allocated 47469.7919921875 
[2025-03-22 18:14:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 9 loss:0.054037097841501236 norm:0.00019394513219594955 max memory_allocated 47469.7919921875 
[2025-03-22 18:16:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 10 loss:0.05389467626810074 norm:0.0001763245090842247 max memory_allocated 47469.7919921875 
[2025-03-22 18:17:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 11 loss:0.05377558618783951 norm:0.0001675900857662782 max memory_allocated 47469.7919921875 
[2025-03-22 18:19:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 12 loss:0.053687386214733124 norm:0.0001618065289221704 max memory_allocated 47469.7919921875 
[2025-03-22 18:20:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 13 loss:0.05362772196531296 norm:0.00016037463501561433 max memory_allocated 47469.7919921875 
[2025-03-22 18:22:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 14 loss:0.05354761704802513 norm:0.00015178979083430022 max memory_allocated 47469.7919921875 
[2025-03-22 18:23:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 15 loss:0.0534818097949028 norm:0.000149088678881526 max memory_allocated 47469.7919921875 
[2025-03-22 18:24:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 16 loss:0.053442854434251785 norm:0.00014932113117538393 max memory_allocated 47469.7919921875 
[2025-03-22 18:26:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 17 loss:0.05342722311615944 norm:0.00014664042100775987 max memory_allocated 47469.7919921875 
[2025-03-22 18:27:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 18 loss:0.05341212823987007 norm:0.00014626856136601418 max memory_allocated 47469.7919921875 
[2025-03-22 18:29:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 19 loss:0.05336230620741844 norm:0.00014831795124337077 max memory_allocated 47469.7919921875 
[2025-03-22 18:31:08 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [22, 23, 24]
[2025-03-22 18:31:08 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [25, 26] ===
[2025-03-22 18:32:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 0 loss:0.07931184768676758 norm:0.00044148482265882194 max memory_allocated 47469.7919921875 
[2025-03-22 18:33:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 1 loss:0.07639163732528687 norm:0.0002976340474560857 max memory_allocated 47469.7919921875 
[2025-03-22 18:34:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 2 loss:0.07211136072874069 norm:0.00023380845959763974 max memory_allocated 47469.7919921875 
[2025-03-22 18:35:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 3 loss:0.06959959864616394 norm:0.00018861623539123684 max memory_allocated 47469.7919921875 
[2025-03-22 18:36:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 4 loss:0.0692295953631401 norm:0.00017785976524464786 max memory_allocated 47469.7919921875 
[2025-03-22 18:36:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 5 loss:0.06898876279592514 norm:0.00016088967095129192 max memory_allocated 47469.7919921875 
[2025-03-22 18:37:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 6 loss:0.06884420663118362 norm:0.000160729352501221 max memory_allocated 47469.7919921875 
[2025-03-22 18:38:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 7 loss:0.06867587566375732 norm:0.00015571351104881614 max memory_allocated 47469.7919921875 
[2025-03-22 18:39:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 8 loss:0.06857460737228394 norm:0.00013319231220521033 max memory_allocated 47469.7919921875 
[2025-03-22 18:40:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 9 loss:0.06848889589309692 norm:0.000130849119159393 max memory_allocated 47469.7919921875 
[2025-03-22 18:41:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 10 loss:0.06843578815460205 norm:0.00012563883501570672 max memory_allocated 47469.7919921875 
[2025-03-22 18:42:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 11 loss:0.06837069243192673 norm:0.00012225095997564495 max memory_allocated 47469.7919921875 
[2025-03-22 18:43:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 12 loss:0.06833502650260925 norm:0.00012521262397058308 max memory_allocated 47469.7919921875 
[2025-03-22 18:44:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 13 loss:0.06828045845031738 norm:0.00012857529509346932 max memory_allocated 47469.7919921875 
[2025-03-22 18:45:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 14 loss:0.06823264807462692 norm:0.0001322402968071401 max memory_allocated 47469.7919921875 
[2025-03-22 18:46:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 15 loss:0.06820415705442429 norm:0.00012620433699339628 max memory_allocated 47469.7919921875 
[2025-03-22 18:47:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 16 loss:0.0681663304567337 norm:0.00013353777467273176 max memory_allocated 47469.7919921875 
[2025-03-22 18:48:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 17 loss:0.06814097613096237 norm:0.00013183962437324226 max memory_allocated 47469.7919921875 
[2025-03-22 18:49:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 18 loss:0.06812145560979843 norm:0.00011925251601496711 max memory_allocated 47469.7919921875 
[2025-03-22 18:50:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 19 loss:0.0681016743183136 norm:0.00011917182564502582 max memory_allocated 47469.7919921875 
[2025-03-22 18:51:35 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [25, 26]
[2025-03-22 18:51:35 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [27] ===
[2025-03-22 18:52:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 0 loss:0.08066021651029587 norm:0.0007747334311716259 max memory_allocated 47469.7919921875 
[2025-03-22 18:52:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 1 loss:0.07850472629070282 norm:0.00042636352009139955 max memory_allocated 47469.7919921875 
[2025-03-22 18:53:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 2 loss:0.07602453231811523 norm:0.00025829445803537965 max memory_allocated 47469.7919921875 
[2025-03-22 18:53:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 3 loss:0.07488824427127838 norm:0.00020920996030326933 max memory_allocated 47469.7919921875 
[2025-03-22 18:54:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 4 loss:0.07472516596317291 norm:0.00019413293921388686 max memory_allocated 47469.7919921875 
[2025-03-22 18:54:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 5 loss:0.07462845742702484 norm:0.00016806395433377475 max memory_allocated 47469.7919921875 
[2025-03-22 18:54:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 6 loss:0.07452288269996643 norm:0.00014363329682964832 max memory_allocated 47469.7919921875 
[2025-03-22 18:55:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 7 loss:0.07443354278802872 norm:0.00012566443183459342 max memory_allocated 47469.7919921875 
[2025-03-22 18:55:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 8 loss:0.07438168674707413 norm:0.00010970018047373742 max memory_allocated 47469.7919921875 
[2025-03-22 18:56:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 9 loss:0.07433191686868668 norm:0.0001034519518725574 max memory_allocated 47469.7919921875 
[2025-03-22 18:56:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 10 loss:0.07429445534944534 norm:0.00010309452773071826 max memory_allocated 47469.7919921875 
[2025-03-22 18:57:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 11 loss:0.07426954805850983 norm:9.74307258729823e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:57:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 12 loss:0.07425717264413834 norm:9.11544484551996e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:58:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 13 loss:0.07423467934131622 norm:8.889727178029716e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:58:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 14 loss:0.07420692592859268 norm:8.860912203090265e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:59:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 15 loss:0.07419361919164658 norm:9.122564370045438e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:59:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 16 loss:0.07418955862522125 norm:9.388808393850923e-05 max memory_allocated 47469.7919921875 
[2025-03-22 19:00:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 17 loss:0.07419450581073761 norm:8.740934572415426e-05 max memory_allocated 47469.7919921875 
[2025-03-22 19:00:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 18 loss:0.07418475300073624 norm:9.181336645269766e-05 max memory_allocated 47469.7919921875 
[2025-03-22 19:01:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 19 loss:0.07417923957109451 norm:9.246962144970894e-05 max memory_allocated 47469.7919921875 
[2025-03-22 19:01:49 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [27]
[2025-03-22 19:01:49 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [28] ===
[2025-03-22 19:01:49 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:02:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 0 loss:0.0956294983625412 norm:0.005169954150915146 max memory_allocated 47469.7919921875 
[2025-03-22 19:02:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 1 loss:0.09223923087120056 norm:0.004218560177832842 max memory_allocated 47469.7919921875 
[2025-03-22 19:03:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 2 loss:0.08888868987560272 norm:0.0031172409653663635 max memory_allocated 47469.7919921875 
[2025-03-22 19:03:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 3 loss:0.08764631301164627 norm:0.0025906311348080635 max memory_allocated 47469.7919921875 
[2025-03-22 19:04:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 4 loss:0.08730018883943558 norm:0.0022497803438454866 max memory_allocated 47469.7919921875 
[2025-03-22 19:04:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 5 loss:0.08706454187631607 norm:0.0019030957482755184 max memory_allocated 47469.7919921875 
[2025-03-22 19:05:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 6 loss:0.0868828147649765 norm:0.0016313702799379826 max memory_allocated 47469.7919921875 
[2025-03-22 19:05:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 7 loss:0.08675441145896912 norm:0.0015069462824612856 max memory_allocated 47469.7919921875 
[2025-03-22 19:06:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 8 loss:0.08666504919528961 norm:0.0015034446259960532 max memory_allocated 47469.7919921875 
[2025-03-22 19:06:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 9 loss:0.0866222009062767 norm:0.001380336470901966 max memory_allocated 47469.7919921875 
[2025-03-22 19:07:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 10 loss:0.08662666380405426 norm:0.0015352503396570683 max memory_allocated 47469.7919921875 
[2025-03-22 19:07:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 11 loss:0.08661248534917831 norm:0.0013960530050098896 max memory_allocated 47469.7919921875 
[2025-03-22 19:08:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 12 loss:0.08655808866024017 norm:0.0009866984328255057 max memory_allocated 47469.7919921875 
[2025-03-22 19:08:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 13 loss:0.08638749271631241 norm:0.0011198553256690502 max memory_allocated 47469.7919921875 
[2025-03-22 19:09:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 14 loss:0.08633651584386826 norm:0.0011423847172409296 max memory_allocated 47469.7919921875 
[2025-03-22 19:09:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 15 loss:0.0863088071346283 norm:0.0011153051164001226 max memory_allocated 47469.7919921875 
[2025-03-22 19:10:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 16 loss:0.08632265776395798 norm:0.0011632894165813923 max memory_allocated 47469.7919921875 
[2025-03-22 19:10:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 17 loss:0.08638478815555573 norm:0.0010918757179751992 max memory_allocated 47469.7919921875 
[2025-03-22 19:11:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 18 loss:0.08644922822713852 norm:0.0013317496050149202 max memory_allocated 47469.7919921875 
[2025-03-22 19:11:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 19 loss:0.08669193089008331 norm:0.0012981132604181767 max memory_allocated 47469.7919921875 
[2025-03-22 19:12:07 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [28]
[2025-03-22 19:12:07 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [29] ===
[2025-03-22 19:12:07 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:12:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 0 loss:0.11033646762371063 norm:0.004682363476604223 max memory_allocated 47469.7919921875 
[2025-03-22 19:13:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 1 loss:0.10694341361522675 norm:0.003722856752574444 max memory_allocated 47469.7919921875 
[2025-03-22 19:13:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 2 loss:0.10353372246026993 norm:0.00295573309995234 max memory_allocated 47469.7919921875 
[2025-03-22 19:14:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 3 loss:0.1024085283279419 norm:0.002526621799916029 max memory_allocated 47469.7919921875 
[2025-03-22 19:14:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 4 loss:0.10209095478057861 norm:0.002206888748332858 max memory_allocated 47469.7919921875 
[2025-03-22 19:15:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 5 loss:0.10183128714561462 norm:0.0019259765977039933 max memory_allocated 47469.7919921875 
[2025-03-22 19:15:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 6 loss:0.10164333879947662 norm:0.0016946225659921765 max memory_allocated 47469.7919921875 
[2025-03-22 19:16:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 7 loss:0.10158129781484604 norm:0.0016126275295391679 max memory_allocated 47469.7919921875 
[2025-03-22 19:16:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 8 loss:0.10156461596488953 norm:0.0014394775498658419 max memory_allocated 47469.7919921875 
[2025-03-22 19:16:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 9 loss:0.10149547457695007 norm:0.0014203892787918448 max memory_allocated 47469.7919921875 
[2025-03-22 19:17:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 10 loss:0.10143820941448212 norm:0.0012288556899875402 max memory_allocated 47469.7919921875 
[2025-03-22 19:17:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 11 loss:0.10129404813051224 norm:0.0013311317889019847 max memory_allocated 47469.7919921875 
[2025-03-22 19:18:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 12 loss:0.10123427957296371 norm:0.0011961081763729453 max memory_allocated 47469.7919921875 
[2025-03-22 19:18:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 13 loss:0.10118752717971802 norm:0.0012497678399085999 max memory_allocated 47469.7919921875 
[2025-03-22 19:19:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 14 loss:0.10126402229070663 norm:0.0010159029625356197 max memory_allocated 47469.7919921875 
[2025-03-22 19:19:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 15 loss:0.10121505707502365 norm:0.0012741576647385955 max memory_allocated 47469.7919921875 
[2025-03-22 19:20:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 16 loss:0.10134221613407135 norm:0.001311115687713027 max memory_allocated 47469.7919921875 
[2025-03-22 19:20:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 17 loss:0.1012439876794815 norm:0.0012113262200728059 max memory_allocated 47469.7919921875 
[2025-03-22 19:21:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 18 loss:0.10106973350048065 norm:0.001082058297470212 max memory_allocated 47469.7919921875 
[2025-03-22 19:21:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 19 loss:0.1010618507862091 norm:0.00102393445558846 max memory_allocated 47469.7919921875 
[2025-03-22 19:22:25 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [29]
[2025-03-22 19:22:25 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [30] ===
[2025-03-22 19:22:26 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:22:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 0 loss:0.13740350306034088 norm:0.008407794870436192 max memory_allocated 47469.7919921875 
[2025-03-22 19:23:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 1 loss:0.13229471445083618 norm:0.005593331530690193 max memory_allocated 47469.7919921875 
[2025-03-22 19:23:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 2 loss:0.12772759795188904 norm:0.004188310820609331 max memory_allocated 47469.7919921875 
[2025-03-22 19:24:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 3 loss:0.12640349566936493 norm:0.004091440234333277 max memory_allocated 47469.7919921875 
[2025-03-22 19:24:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 4 loss:0.12604564428329468 norm:0.004117415752261877 max memory_allocated 47469.7919921875 
[2025-03-22 19:25:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 5 loss:0.12570376694202423 norm:0.003741131629794836 max memory_allocated 47469.7919921875 
[2025-03-22 19:25:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 6 loss:0.1254582405090332 norm:0.003512260038405657 max memory_allocated 47469.7919921875 
[2025-03-22 19:26:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 7 loss:0.125177264213562 norm:0.002960571786388755 max memory_allocated 47469.7919921875 
[2025-03-22 19:26:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 8 loss:0.12500658631324768 norm:0.0030119474977254868 max memory_allocated 47469.7919921875 
[2025-03-22 19:27:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 9 loss:0.12485635280609131 norm:0.0027892575599253178 max memory_allocated 47469.7919921875 
[2025-03-22 19:27:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 10 loss:0.12469933182001114 norm:0.002440067706629634 max memory_allocated 47469.7919921875 
[2025-03-22 19:28:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 11 loss:0.12458781898021698 norm:0.002193125896155834 max memory_allocated 47469.7919921875 
[2025-03-22 19:28:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 12 loss:0.12445197999477386 norm:0.0018214837182313204 max memory_allocated 47469.7919921875 
[2025-03-22 19:29:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 13 loss:0.12442396581172943 norm:0.0018676273757591844 max memory_allocated 47469.7919921875 
[2025-03-22 19:29:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 14 loss:0.12437021732330322 norm:0.0019138831412419677 max memory_allocated 47469.7919921875 
[2025-03-22 19:30:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 15 loss:0.12441188097000122 norm:0.001975391758605838 max memory_allocated 47469.7919921875 
[2025-03-22 19:30:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 16 loss:0.12443915009498596 norm:0.0021974521223455667 max memory_allocated 47469.7919921875 
[2025-03-22 19:31:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 17 loss:0.12440849840641022 norm:0.002081354847177863 max memory_allocated 47469.7919921875 
[2025-03-22 19:31:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 18 loss:0.1242990791797638 norm:0.0019420292228460312 max memory_allocated 47469.7919921875 
[2025-03-22 19:32:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 19 loss:0.1243242472410202 norm:0.0019038739847019315 max memory_allocated 47469.7919921875 
[2025-03-22 19:32:43 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [30]
[2025-03-22 19:32:43 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [31] ===
[2025-03-22 19:32:43 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:33:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 0 loss:0.21876472234725952 norm:0.022890113294124603 max memory_allocated 47469.7919921875 
[2025-03-22 19:33:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 1 loss:0.20643538236618042 norm:0.015842042863368988 max memory_allocated 47469.7919921875 
[2025-03-22 19:34:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 2 loss:0.19885852932929993 norm:0.011347472667694092 max memory_allocated 47469.7919921875 
[2025-03-22 19:34:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 3 loss:0.19533683359622955 norm:0.009209180250763893 max memory_allocated 47469.7919921875 
[2025-03-22 19:35:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 4 loss:0.19363069534301758 norm:0.007844568230211735 max memory_allocated 47469.7919921875 
[2025-03-22 19:35:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 5 loss:0.19280102849006653 norm:0.0071892691776156425 max memory_allocated 47469.7919921875 
[2025-03-22 19:36:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 6 loss:0.1921733021736145 norm:0.006701736245304346 max memory_allocated 47469.7919921875 
[2025-03-22 19:36:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 7 loss:0.19169025123119354 norm:0.006044367328286171 max memory_allocated 47469.7919921875 
[2025-03-22 19:37:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 8 loss:0.19135740399360657 norm:0.005576866678893566 max memory_allocated 47469.7919921875 
[2025-03-22 19:37:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 9 loss:0.19114935398101807 norm:0.005304096732288599 max memory_allocated 47469.7919921875 
[2025-03-22 19:38:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 10 loss:0.1909768134355545 norm:0.005348623730242252 max memory_allocated 47469.7919921875 
[2025-03-22 19:38:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 11 loss:0.19110119342803955 norm:0.005860889796167612 max memory_allocated 47469.7919921875 
[2025-03-22 19:39:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 12 loss:0.19114477932453156 norm:0.005636455025523901 max memory_allocated 47469.7919921875 
[2025-03-22 19:39:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 13 loss:0.19123773276805878 norm:0.0055770836770534515 max memory_allocated 47469.7919921875 
[2025-03-22 19:40:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 14 loss:0.19119463860988617 norm:0.0047158533707261086 max memory_allocated 47469.7919921875 
[2025-03-22 19:40:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 15 loss:0.1910862773656845 norm:0.005323892924934626 max memory_allocated 47469.7919921875 
[2025-03-22 19:40:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 16 loss:0.19115713238716125 norm:0.005103779025375843 max memory_allocated 47469.7919921875 
[2025-03-22 19:41:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 17 loss:0.19103869795799255 norm:0.004671842325478792 max memory_allocated 47469.7919921875 
[2025-03-22 19:41:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 18 loss:0.19081120193004608 norm:0.004800455644726753 max memory_allocated 47469.7919921875 
[2025-03-22 19:42:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 19 loss:0.1906595379114151 norm:0.004776853136718273 max memory_allocated 47469.7919921875 
[2025-03-22 19:43:04 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 15, block: [31]
[2025-03-22 19:43:04 root] (main_calib_config3_attn.py 379): INFO 19678.72342824936
[2025-03-22 19:43:09 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-22 19:44:00 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.725292205810547
[2025-03-22 19:44:00 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-22 19:45:19 root] (main_calib_config3_attn.py 161): INFO c4 : 7.1417975425720215
[2025-03-22 20:46:54 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.725292205810547, 'c4': 7.1417975425720215, 'results': {'piqa': {'acc': 0.7823721436343852, 'acc_stderr': 0.009627407474840878, 'acc_norm': 0.7747551686615887, 'acc_norm_stderr': 0.00974664347103214}, 'arc_challenge': {'acc': 0.3796928327645051, 'acc_stderr': 0.014182119866974872, 'acc_norm': 0.4069965870307167, 'acc_norm_stderr': 0.014356399418009128}, 'hellaswag': {'acc': 0.5599482174865564, 'acc_stderr': 0.004953787146510941, 'acc_norm': 0.7248556064528978, 'acc_norm_stderr': 0.004456743108170734}, 'boolq': {'acc': 0.7296636085626911, 'acc_stderr': 0.007767944951388913}, 'winogrande': {'acc': 0.6748224151539068, 'acc_stderr': 0.01316552547176435}, 'arc_easy': {'acc': 0.6637205387205387, 'acc_stderr': 0.009694178072725206, 'acc_norm': 0.5180976430976431, 'acc_norm_stderr': 0.010253060653479166}}, 'versions': {'piqa': 0, 'arc_challenge': 0, 'hellaswag': 0, 'boolq': 1, 'winogrande': 0, 'arc_easy': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 20:46:54 root] (main_calib_config3_attn.py 175): INFO 37.97,66.37,72.97,55.99,78.24,67.48
