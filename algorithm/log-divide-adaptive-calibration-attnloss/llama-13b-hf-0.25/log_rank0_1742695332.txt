[2025-03-23 02:02:12 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/llama-13b-hf-0.25', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/llama-13b-hf/quant_map_llama-13b-hf_0.25.pkl', blocks_pkl='./log-divide/llama-13b-hf-w4a4/llama-13b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-23 02:05:28 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-23 02:05:28 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-23 02:05:29 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-23 02:05:29 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/llama-13b-hf/quant_map_llama-13b-hf_0.25.pkl
[2025-03-23 02:05:29 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/llama-13b-hf-w4a4/llama-13b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 7), (7, 10), (10, 13), (13, 16), (16, 19), (19, 22), (22, 25), (25, 28), (28, 30), (30, 32), (32, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40)]
[2025-03-23 02:05:29 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4, 5], [6], [7, 8, 9], [10, 11, 12], [13, 14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26, 27], [28, 29], [30, 31], [32, 33], [34], [35], [36], [37], [38], [39]]
[2025-03-23 02:05:31 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-23 02:05:31 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:06:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.01228626724332571 norm:0.012274431996047497 max memory_allocated 44355.7939453125 
[2025-03-23 02:07:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.006841652560979128 norm:0.0064192526042461395 max memory_allocated 44355.7939453125 
[2025-03-23 02:07:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.004316836595535278 norm:0.004162510856986046 max memory_allocated 44355.7939453125 
[2025-03-23 02:08:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0035236093681305647 norm:0.0030657516326755285 max memory_allocated 44355.7939453125 
[2025-03-23 02:09:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.0032379785552620888 norm:0.0025502261705696583 max memory_allocated 44355.7939453125 
[2025-03-23 02:09:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0030680429190397263 norm:0.0021557470317929983 max memory_allocated 44355.7939453125 
[2025-03-23 02:10:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.0029477751813828945 norm:0.0018402517307549715 max memory_allocated 44355.7939453125 
[2025-03-23 02:11:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0028111692517995834 norm:0.001569821615703404 max memory_allocated 44355.7939453125 
[2025-03-23 02:12:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0027361249085515738 norm:0.0014569833874702454 max memory_allocated 44355.7939453125 
[2025-03-23 02:12:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.0026721390895545483 norm:0.0013115069596096873 max memory_allocated 44355.7939453125 
[2025-03-23 02:13:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.00264039752073586 norm:0.0011995892273262143 max memory_allocated 44355.7939453125 
[2025-03-23 02:14:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.002596708480268717 norm:0.0010808455990627408 max memory_allocated 44355.7939453125 
[2025-03-23 02:14:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.002599494531750679 norm:0.0009940129239112139 max memory_allocated 44355.7939453125 
[2025-03-23 02:15:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.0025390111841261387 norm:0.0008669111994095147 max memory_allocated 44355.7939453125 
[2025-03-23 02:16:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0025455616414546967 norm:0.0008150936919264495 max memory_allocated 44355.7939453125 
[2025-03-23 02:17:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.002524977782741189 norm:0.0007868128595873713 max memory_allocated 44355.7939453125 
[2025-03-23 02:17:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.0025062558706849813 norm:0.0007579269586130977 max memory_allocated 44355.7939453125 
[2025-03-23 02:18:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0024940413422882557 norm:0.000730000261683017 max memory_allocated 44355.7939453125 
[2025-03-23 02:19:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.002502705901861191 norm:0.0007105389377102256 max memory_allocated 44355.7939453125 
[2025-03-23 02:19:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0024953430984169245 norm:0.0006625511450693011 max memory_allocated 44355.7939453125 
[2025-03-23 02:21:08 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-23 02:21:08 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-23 02:21:08 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:21:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.022139625623822212 norm:0.01589668169617653 max memory_allocated 44355.7939453125 
[2025-03-23 02:22:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.012355457060039043 norm:0.009958520531654358 max memory_allocated 44355.7939453125 
[2025-03-23 02:23:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.008448625914752483 norm:0.006660887971520424 max memory_allocated 44355.7939453125 
[2025-03-23 02:24:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.007371725980192423 norm:0.004657278768718243 max memory_allocated 44355.7939453125 
[2025-03-23 02:24:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.006936215795576572 norm:0.0038929434958845377 max memory_allocated 44355.7939453125 
[2025-03-23 02:25:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.0066392309963703156 norm:0.003513803705573082 max memory_allocated 44355.7939453125 
[2025-03-23 02:26:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.00641984585672617 norm:0.0031911986880004406 max memory_allocated 44355.7939453125 
[2025-03-23 02:26:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.006244294345378876 norm:0.00293587613850832 max memory_allocated 44355.7939453125 
[2025-03-23 02:27:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.0060921162366867065 norm:0.002696189796552062 max memory_allocated 44355.7939453125 
[2025-03-23 02:28:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.005972902290523052 norm:0.0024550470989197493 max memory_allocated 44355.7939453125 
[2025-03-23 02:29:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.005840667523443699 norm:0.0022187060676515102 max memory_allocated 44355.7939453125 
[2025-03-23 02:29:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.0057340641506016254 norm:0.002026097383350134 max memory_allocated 44355.7939453125 
[2025-03-23 02:30:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.0056426445953547955 norm:0.0018473027739673853 max memory_allocated 44355.7939453125 
[2025-03-23 02:31:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.00555648235604167 norm:0.001682480564340949 max memory_allocated 44355.7939453125 
[2025-03-23 02:31:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.005496652331203222 norm:0.001539930934086442 max memory_allocated 44355.7939453125 
[2025-03-23 02:32:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.005465368274599314 norm:0.0013971845619380474 max memory_allocated 44355.7939453125 
[2025-03-23 02:33:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.005402429960668087 norm:0.0012443345040082932 max memory_allocated 44355.7939453125 
[2025-03-23 02:34:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.005370797589421272 norm:0.0011143068550154567 max memory_allocated 44355.7939453125 
[2025-03-23 02:34:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.005346595775336027 norm:0.001024623867124319 max memory_allocated 44355.7939453125 
[2025-03-23 02:35:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.005326984450221062 norm:0.0010169313754886389 max memory_allocated 44355.7939453125 
[2025-03-23 02:36:29 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-23 02:36:29 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
[2025-03-23 02:36:30 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:37:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.01935388706624508 norm:0.009130174294114113 max memory_allocated 44355.7939453125 
[2025-03-23 02:37:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.01490760874003172 norm:0.006159904878586531 max memory_allocated 44355.7939453125 
[2025-03-23 02:38:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.01204361766576767 norm:0.005101717542856932 max memory_allocated 44355.7939453125 
[2025-03-23 02:39:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.011183010414242744 norm:0.004620993044227362 max memory_allocated 44355.7939453125 
[2025-03-23 02:40:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.010871196165680885 norm:0.004225876182317734 max memory_allocated 44355.7939453125 
[2025-03-23 02:40:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.010453596711158752 norm:0.0038654361851513386 max memory_allocated 44355.7939453125 
[2025-03-23 02:41:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.010040748864412308 norm:0.0036497958935797215 max memory_allocated 44355.7939453125 
[2025-03-23 02:42:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.009947407990694046 norm:0.003557051531970501 max memory_allocated 44355.7939453125 
[2025-03-23 02:43:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.01004802156239748 norm:0.003581894561648369 max memory_allocated 44355.7939453125 
[2025-03-23 02:43:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.009774720296263695 norm:0.0033408422023057938 max memory_allocated 44355.7939453125 
[2025-03-23 02:44:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.009657250717282295 norm:0.003228852292522788 max memory_allocated 44355.7939453125 
[2025-03-23 02:45:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.009701193310320377 norm:0.003297568531706929 max memory_allocated 44355.7939453125 
[2025-03-23 02:45:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.009903039783239365 norm:0.003322928911074996 max memory_allocated 44355.7939453125 
[2025-03-23 02:46:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.009639717638492584 norm:0.00306806736625731 max memory_allocated 44355.7939453125 
[2025-03-23 02:47:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.009742443449795246 norm:0.003253028029575944 max memory_allocated 44355.7939453125 
[2025-03-23 02:48:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.009708055295050144 norm:0.003110777586698532 max memory_allocated 44355.7939453125 
[2025-03-23 02:48:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.009826632216572762 norm:0.0031610080040991306 max memory_allocated 44355.7939453125 
[2025-03-23 02:49:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.00977671891450882 norm:0.0029799023177474737 max memory_allocated 44355.7939453125 
[2025-03-23 02:50:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.009664876386523247 norm:0.002713452558964491 max memory_allocated 44355.7939453125 
[2025-03-23 02:50:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.00958206970244646 norm:0.002617433201521635 max memory_allocated 44355.7939453125 
[2025-03-23 02:52:11 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2]
[2025-03-23 02:52:11 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4, 5] ===
[2025-03-23 02:54:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 0 loss:0.06852513551712036 norm:0.006819234229624271 max memory_allocated 62749.0654296875 
[2025-03-23 02:56:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 1 loss:0.04590895026922226 norm:0.0016651726327836514 max memory_allocated 62749.0654296875 
[2025-03-23 02:58:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 2 loss:0.03470228612422943 norm:0.0007706057513132691 max memory_allocated 62749.0654296875 
[2025-03-23 03:00:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 3 loss:0.02977096103131771 norm:0.00047043836093507707 max memory_allocated 62749.0654296875 
[2025-03-23 03:03:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 4 loss:0.027293123304843903 norm:0.0003621047653723508 max memory_allocated 62749.0654296875 
[2025-03-23 03:05:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 5 loss:0.025712840259075165 norm:0.00034034106647595763 max memory_allocated 62749.0654296875 
[2025-03-23 03:07:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 6 loss:0.024556264281272888 norm:0.0003103995695710182 max memory_allocated 62749.0654296875 
[2025-03-23 03:09:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 7 loss:0.023741204291582108 norm:0.00025836919667199254 max memory_allocated 62749.0654296875 
[2025-03-23 03:11:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 8 loss:0.023200878873467445 norm:0.0002646230277605355 max memory_allocated 62749.0654296875 
[2025-03-23 03:13:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 9 loss:0.02280360832810402 norm:0.000261651846813038 max memory_allocated 62749.0654296875 
[2025-03-23 03:15:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 10 loss:0.022525783628225327 norm:0.00024208494869526476 max memory_allocated 62749.0654296875 
[2025-03-23 03:17:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 11 loss:0.02236415259540081 norm:0.000276245700661093 max memory_allocated 62749.0654296875 
[2025-03-23 03:20:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 12 loss:0.022249706089496613 norm:0.00024902087170630693 max memory_allocated 62749.0654296875 
[2025-03-23 03:22:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 13 loss:0.022134508937597275 norm:0.00025127953267656267 max memory_allocated 62749.0654296875 
[2025-03-23 03:24:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 14 loss:0.022077342495322227 norm:0.0002599497092887759 max memory_allocated 62749.0654296875 
[2025-03-23 03:26:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 15 loss:0.021953336894512177 norm:0.0002131548390025273 max memory_allocated 62749.0654296875 
[2025-03-23 03:28:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 16 loss:0.021870872005820274 norm:0.00021540079615078866 max memory_allocated 62749.0654296875 
[2025-03-23 03:30:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 17 loss:0.021800929680466652 norm:0.00021940015722066164 max memory_allocated 62749.0654296875 
[2025-03-23 03:32:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 18 loss:0.021795613691210747 norm:0.00022937596077099442 max memory_allocated 62749.0654296875 
[2025-03-23 03:35:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 19 loss:0.02183939330279827 norm:0.0002346097317058593 max memory_allocated 62749.0654296875 
[2025-03-23 03:38:47 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [3, 4, 5]
[2025-03-23 03:38:47 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [6] ===
[2025-03-23 03:39:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 0 loss:0.041799094527959824 norm:0.0009304628474637866 max memory_allocated 62749.0654296875 
[2025-03-23 03:40:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 1 loss:0.03164851292967796 norm:0.0003647023404482752 max memory_allocated 62749.0654296875 
[2025-03-23 03:40:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 2 loss:0.02579198032617569 norm:0.0002519695262890309 max memory_allocated 62749.0654296875 
[2025-03-23 03:41:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 3 loss:0.023768534883856773 norm:0.00022277601237874478 max memory_allocated 62749.0654296875 
[2025-03-23 03:42:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 4 loss:0.022818008437752724 norm:0.00024454991216771305 max memory_allocated 62749.0654296875 
[2025-03-23 03:43:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 5 loss:0.022249186411499977 norm:0.00025381584418937564 max memory_allocated 62749.0654296875 
[2025-03-23 03:43:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 6 loss:0.02194938436150551 norm:0.0002774662571027875 max memory_allocated 62749.0654296875 
[2025-03-23 03:44:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 7 loss:0.02155148983001709 norm:0.00018786177679430693 max memory_allocated 62749.0654296875 
[2025-03-23 03:45:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 8 loss:0.02143092267215252 norm:0.0002105514140566811 max memory_allocated 62749.0654296875 
[2025-03-23 03:45:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 9 loss:0.02132573164999485 norm:0.000206893848371692 max memory_allocated 62749.0654296875 
[2025-03-23 03:46:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 10 loss:0.021306416019797325 norm:0.00020620904979296029 max memory_allocated 62749.0654296875 
[2025-03-23 03:47:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 11 loss:0.02120564505457878 norm:0.00019121693912893534 max memory_allocated 62749.0654296875 
[2025-03-23 03:48:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 12 loss:0.02118876948952675 norm:0.00014136251411400735 max memory_allocated 62749.0654296875 
[2025-03-23 03:48:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 13 loss:0.02144893817603588 norm:0.00032070453744381666 max memory_allocated 62749.0654296875 
[2025-03-23 03:49:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 14 loss:0.02116103284060955 norm:0.0001188533497042954 max memory_allocated 62749.0654296875 
[2025-03-23 03:50:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 15 loss:0.021325502544641495 norm:0.0001796371361706406 max memory_allocated 62749.0654296875 
[2025-03-23 03:51:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 16 loss:0.021137073636054993 norm:0.00013981698430143297 max memory_allocated 62749.0654296875 
[2025-03-23 03:51:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 17 loss:0.02121291123330593 norm:0.0003079323214478791 max memory_allocated 62749.0654296875 
[2025-03-23 03:52:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 18 loss:0.021228652447462082 norm:0.00014676286082249135 max memory_allocated 62749.0654296875 
[2025-03-23 03:53:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6]) iter 19 loss:0.021376729011535645 norm:0.000494598934892565 max memory_allocated 62749.0654296875 
[2025-03-23 03:54:09 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [6]
[2025-03-23 03:54:09 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [7, 8, 9] ===
[2025-03-23 03:56:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 0 loss:0.07767418771982193 norm:0.0008826456032693386 max memory_allocated 62749.3779296875 
[2025-03-23 03:58:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 1 loss:0.05963843688368797 norm:0.00044166509178467095 max memory_allocated 62749.3779296875 
[2025-03-23 04:00:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 2 loss:0.04800451546907425 norm:0.00032116638612933457 max memory_allocated 62749.3779296875 
[2025-03-23 04:02:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 3 loss:0.042836688458919525 norm:0.0002760694769676775 max memory_allocated 62749.3779296875 
[2025-03-23 04:05:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 4 loss:0.04011417552828789 norm:0.00025323411682620645 max memory_allocated 62749.3779296875 
[2025-03-23 04:07:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 5 loss:0.03826894983649254 norm:0.0002390829467913136 max memory_allocated 62749.3779296875 
[2025-03-23 04:09:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 6 loss:0.03701658546924591 norm:0.0002355045871809125 max memory_allocated 62749.3779296875 
[2025-03-23 04:11:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 7 loss:0.0361967459321022 norm:0.00022851876565255225 max memory_allocated 62749.3779296875 
[2025-03-23 04:13:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 8 loss:0.035661481320858 norm:0.0002113435766659677 max memory_allocated 62749.3779296875 
[2025-03-23 04:15:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 9 loss:0.03529722988605499 norm:0.00021127346553839743 max memory_allocated 62749.3779296875 
[2025-03-23 04:17:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 10 loss:0.03502363711595535 norm:0.00020906834106426686 max memory_allocated 62749.3779296875 
[2025-03-23 04:19:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 11 loss:0.034843236207962036 norm:0.00021166398073546588 max memory_allocated 62749.3779296875 
[2025-03-23 04:22:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 12 loss:0.03471324220299721 norm:0.00020903813128825277 max memory_allocated 62749.3779296875 
[2025-03-23 04:24:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 13 loss:0.03460325673222542 norm:0.00020728442177642137 max memory_allocated 62749.3779296875 
[2025-03-23 04:26:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 14 loss:0.03455469012260437 norm:0.00022185839770827442 max memory_allocated 62749.3779296875 
[2025-03-23 04:28:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 15 loss:0.03447503596544266 norm:0.00021202355856075883 max memory_allocated 62749.3779296875 
[2025-03-23 04:30:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 16 loss:0.034418463706970215 norm:0.00021696535986848176 max memory_allocated 62749.3779296875 
[2025-03-23 04:32:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 17 loss:0.03434944897890091 norm:0.00020703219342976809 max memory_allocated 62749.3779296875 
[2025-03-23 04:34:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 18 loss:0.03433121740818024 norm:0.00021115582785569131 max memory_allocated 62749.3779296875 
[2025-03-23 04:37:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [7, 8, 9]) iter 19 loss:0.03431728854775429 norm:0.00021556831779889762 max memory_allocated 62749.3779296875 
[2025-03-23 04:40:01 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [7, 8, 9]
[2025-03-23 04:40:01 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [10, 11, 12] ===
[2025-03-23 04:42:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 0 loss:0.08488365262746811 norm:0.0007152329199016094 max memory_allocated 62749.6123046875 
[2025-03-23 04:44:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 1 loss:0.06779328733682632 norm:0.0004175937210675329 max memory_allocated 62749.6123046875 
[2025-03-23 04:46:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 2 loss:0.05536305904388428 norm:0.0002924177679233253 max memory_allocated 62749.6123046875 
[2025-03-23 04:48:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 3 loss:0.0500367134809494 norm:0.00023916229838505387 max memory_allocated 62749.6123046875 
[2025-03-23 04:50:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 4 loss:0.04744219407439232 norm:0.0002089694025926292 max memory_allocated 62749.6123046875 
[2025-03-23 04:52:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 5 loss:0.04573066905140877 norm:0.00019429724488873035 max memory_allocated 62749.6123046875 
[2025-03-23 04:55:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 6 loss:0.04459032788872719 norm:0.00018450334027875215 max memory_allocated 62749.6123046875 
[2025-03-23 04:57:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 7 loss:0.04385627806186676 norm:0.00017797641339711845 max memory_allocated 62749.6123046875 
[2025-03-23 04:59:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 8 loss:0.043396227061748505 norm:0.00017273850971832871 max memory_allocated 62749.6123046875 
[2025-03-23 05:01:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 9 loss:0.04310142248868942 norm:0.00017065569409169257 max memory_allocated 62749.6123046875 
[2025-03-23 05:03:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 10 loss:0.04286736249923706 norm:0.00016710047202650458 max memory_allocated 62749.6123046875 
[2025-03-23 05:05:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 11 loss:0.04269528016448021 norm:0.00016520830104127526 max memory_allocated 62749.6123046875 
[2025-03-23 05:07:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 12 loss:0.042603231966495514 norm:0.00016569349099881947 max memory_allocated 62749.6123046875 
[2025-03-23 05:10:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 13 loss:0.04250092804431915 norm:0.00016174100164789706 max memory_allocated 62749.6123046875 
[2025-03-23 05:12:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 14 loss:0.042431820183992386 norm:0.0001605306169949472 max memory_allocated 62749.6123046875 
[2025-03-23 05:14:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 15 loss:0.042372964322566986 norm:0.00016063849034253508 max memory_allocated 62749.6123046875 
[2025-03-23 05:16:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 16 loss:0.04231330007314682 norm:0.0001589711318956688 max memory_allocated 62749.6123046875 
[2025-03-23 05:18:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 17 loss:0.04225236549973488 norm:0.00015757013170514256 max memory_allocated 62749.6123046875 
[2025-03-23 05:20:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 18 loss:0.04222472012042999 norm:0.0001596182119101286 max memory_allocated 62749.6123046875 
[2025-03-23 05:22:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [10, 11, 12]) iter 19 loss:0.04220674932003021 norm:0.00016009621322155 max memory_allocated 62749.6123046875 
[2025-03-23 05:25:56 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [10, 11, 12]
[2025-03-23 05:25:56 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [13, 14, 15] ===
[2025-03-23 05:28:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 0 loss:0.09657740592956543 norm:0.000648789806291461 max memory_allocated 62749.8466796875 
[2025-03-23 05:30:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 1 loss:0.07946772873401642 norm:0.00038096573553048074 max memory_allocated 62749.8466796875 
[2025-03-23 05:32:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 2 loss:0.0658254325389862 norm:0.00028848910005763173 max memory_allocated 62749.8466796875 
[2025-03-23 05:34:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 3 loss:0.060273006558418274 norm:0.00024501688312739134 max memory_allocated 62749.8466796875 
[2025-03-23 05:36:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 4 loss:0.057147711515426636 norm:0.0002249028766527772 max memory_allocated 62749.8466796875 
[2025-03-23 05:38:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 5 loss:0.05509836599230766 norm:0.00021168698731344193 max memory_allocated 62749.8466796875 
[2025-03-23 05:41:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 6 loss:0.053873658180236816 norm:0.00020083633717149496 max memory_allocated 62749.8466796875 
[2025-03-23 05:43:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 7 loss:0.05310259386897087 norm:0.00019092057482339442 max memory_allocated 62749.8466796875 
[2025-03-23 05:45:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 8 loss:0.052578166127204895 norm:0.00018381699919700623 max memory_allocated 62749.8466796875 
[2025-03-23 05:47:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 9 loss:0.05221252888441086 norm:0.00018399456166662276 max memory_allocated 62749.8466796875 
[2025-03-23 05:49:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 10 loss:0.05198090523481369 norm:0.00018065969925373793 max memory_allocated 62749.8466796875 
[2025-03-23 05:51:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 11 loss:0.05172580108046532 norm:0.00017274533456657082 max memory_allocated 62749.8466796875 
[2025-03-23 05:53:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 12 loss:0.05151938647031784 norm:0.00016957407933659852 max memory_allocated 62749.8466796875 
[2025-03-23 05:56:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 13 loss:0.0513826422393322 norm:0.00016393601254094392 max memory_allocated 62749.8466796875 
[2025-03-23 05:58:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 14 loss:0.05130587890744209 norm:0.00016519674682058394 max memory_allocated 62749.8466796875 
[2025-03-23 06:00:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 15 loss:0.0512232780456543 norm:0.00016484013758599758 max memory_allocated 62749.8466796875 
[2025-03-23 06:02:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 16 loss:0.05114932730793953 norm:0.00016323682211805135 max memory_allocated 62749.8466796875 
[2025-03-23 06:04:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 17 loss:0.051107123494148254 norm:0.00015969318337738514 max memory_allocated 62749.8466796875 
[2025-03-23 06:06:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 18 loss:0.051043566316366196 norm:0.00015899758727755398 max memory_allocated 62749.8466796875 
[2025-03-23 06:08:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [13, 14, 15]) iter 19 loss:0.05098443478345871 norm:0.00016145998961292207 max memory_allocated 62749.8466796875 
[2025-03-23 06:12:08 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [13, 14, 15]
[2025-03-23 06:12:09 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [16, 17, 18] ===
[2025-03-23 06:14:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 0 loss:0.11129418760538101 norm:0.0005644820630550385 max memory_allocated 62750.0810546875 
[2025-03-23 06:16:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 1 loss:0.09350970387458801 norm:0.00033542909659445286 max memory_allocated 62750.0810546875 
[2025-03-23 06:18:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 2 loss:0.07845596969127655 norm:0.0002500198897905648 max memory_allocated 62750.0810546875 
[2025-03-23 06:20:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 3 loss:0.07322943210601807 norm:0.00022442617046181113 max memory_allocated 62750.0810546875 
[2025-03-23 06:22:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 4 loss:0.06990306079387665 norm:0.0002088467444991693 max memory_allocated 62750.0810546875 
[2025-03-23 06:25:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 5 loss:0.06786318123340607 norm:0.0001967108401004225 max memory_allocated 62750.0810546875 
[2025-03-23 06:27:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 6 loss:0.06676514446735382 norm:0.00018704890680965036 max memory_allocated 62750.0810546875 
[2025-03-23 06:29:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 7 loss:0.06615012884140015 norm:0.00017999866395257413 max memory_allocated 62750.0810546875 
[2025-03-23 06:31:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 8 loss:0.06573455035686493 norm:0.00017675360140856355 max memory_allocated 62750.0810546875 
[2025-03-23 06:33:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 9 loss:0.06543440371751785 norm:0.0001703242742223665 max memory_allocated 62750.0810546875 
[2025-03-23 06:35:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 10 loss:0.06515005975961685 norm:0.00016236407100223005 max memory_allocated 62750.0810546875 
[2025-03-23 06:37:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 11 loss:0.06492652744054794 norm:0.00015891683869995177 max memory_allocated 62750.0810546875 
[2025-03-23 06:40:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 12 loss:0.06477626413106918 norm:0.00015645919484086335 max memory_allocated 62750.0810546875 
[2025-03-23 06:42:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 13 loss:0.06462790071964264 norm:0.00015378111856989563 max memory_allocated 62750.0810546875 
[2025-03-23 06:44:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 14 loss:0.06448493897914886 norm:0.0001520243677077815 max memory_allocated 62750.0810546875 
[2025-03-23 06:46:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 15 loss:0.06438111513853073 norm:0.00015025046013761312 max memory_allocated 62750.0810546875 
[2025-03-23 06:48:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 16 loss:0.06428992003202438 norm:0.00014915468636900187 max memory_allocated 62750.0810546875 
[2025-03-23 06:50:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 17 loss:0.06417883187532425 norm:0.00014586707402486354 max memory_allocated 62750.0810546875 
[2025-03-23 06:52:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 18 loss:0.06407823413610458 norm:0.00014530145563185215 max memory_allocated 62750.0810546875 
[2025-03-23 06:55:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [16, 17, 18]) iter 19 loss:0.06401965022087097 norm:0.00014461323735304177 max memory_allocated 62750.0810546875 
[2025-03-23 06:58:29 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [16, 17, 18]
[2025-03-23 06:58:29 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [19, 20, 21] ===
[2025-03-23 07:00:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 0 loss:0.1513335257768631 norm:0.0006997283780947328 max memory_allocated 62750.3154296875 
[2025-03-23 07:02:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 1 loss:0.1271437555551529 norm:0.0004205862060189247 max memory_allocated 62750.3154296875 
[2025-03-23 07:05:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 2 loss:0.106022909283638 norm:0.0002852406178135425 max memory_allocated 62750.3154296875 
[2025-03-23 07:07:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 3 loss:0.09928517788648605 norm:0.0002581986482255161 max memory_allocated 62750.3154296875 
[2025-03-23 07:09:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 4 loss:0.09527549147605896 norm:0.00024136045249179006 max memory_allocated 62750.3154296875 
[2025-03-23 07:11:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 5 loss:0.09346790611743927 norm:0.00022927389363758266 max memory_allocated 62750.3154296875 
[2025-03-23 07:13:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 6 loss:0.09256088733673096 norm:0.00022086234821472317 max memory_allocated 62750.3154296875 
[2025-03-23 07:15:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 7 loss:0.09192728251218796 norm:0.0002090609777951613 max memory_allocated 62750.3154296875 
[2025-03-23 07:17:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 8 loss:0.0914309024810791 norm:0.00020410868455655873 max memory_allocated 62750.3154296875 
[2025-03-23 07:20:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 9 loss:0.09105594456195831 norm:0.0001966628769878298 max memory_allocated 62750.3154296875 
[2025-03-23 07:22:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 10 loss:0.09070749580860138 norm:0.0001930450089275837 max memory_allocated 62750.3154296875 
[2025-03-23 07:24:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 11 loss:0.09038384258747101 norm:0.0001905984536278993 max memory_allocated 62750.3154296875 
[2025-03-23 07:26:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 12 loss:0.09008719027042389 norm:0.0001844721264205873 max memory_allocated 62750.3154296875 
[2025-03-23 07:28:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 13 loss:0.08986414223909378 norm:0.00018214347073808312 max memory_allocated 62750.3154296875 
[2025-03-23 07:30:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 14 loss:0.08967670798301697 norm:0.00018015227396972477 max memory_allocated 62750.3154296875 
[2025-03-23 07:32:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 15 loss:0.08948925882577896 norm:0.0001790858368622139 max memory_allocated 62750.3154296875 
[2025-03-23 07:35:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 16 loss:0.08932710438966751 norm:0.00017508894961792976 max memory_allocated 62750.3154296875 
[2025-03-23 07:37:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 17 loss:0.08918222039937973 norm:0.00017127393221016973 max memory_allocated 62750.3154296875 
[2025-03-23 07:39:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 18 loss:0.08906954526901245 norm:0.00016786425840109587 max memory_allocated 62750.3154296875 
[2025-03-23 07:41:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [19, 20, 21]) iter 19 loss:0.08896160125732422 norm:0.0001658349356148392 max memory_allocated 62750.3154296875 
[2025-03-23 07:44:35 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [19, 20, 21]
[2025-03-23 07:44:35 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [22, 23, 24] ===
[2025-03-23 07:46:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 0 loss:0.21265211701393127 norm:0.0008319379412569106 max memory_allocated 62750.5498046875 
[2025-03-23 07:49:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 1 loss:0.18077650666236877 norm:0.0005149696953594685 max memory_allocated 62750.5498046875 
[2025-03-23 07:51:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 2 loss:0.15196938812732697 norm:0.0003880050790030509 max memory_allocated 62750.5498046875 
[2025-03-23 07:53:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 3 loss:0.14315009117126465 norm:0.00037402487942017615 max memory_allocated 62750.5498046875 
[2025-03-23 07:55:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 4 loss:0.13845129311084747 norm:0.0003618146583903581 max memory_allocated 62750.5498046875 
[2025-03-23 07:57:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 5 loss:0.13659945130348206 norm:0.00034694935311563313 max memory_allocated 62750.5498046875 
[2025-03-23 07:59:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 6 loss:0.13552799820899963 norm:0.0003349807229824364 max memory_allocated 62750.5498046875 
[2025-03-23 08:01:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 7 loss:0.13463008403778076 norm:0.0003251163871027529 max memory_allocated 62750.5498046875 
[2025-03-23 08:03:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 8 loss:0.13395139575004578 norm:0.00032002973603084683 max memory_allocated 62750.5498046875 
[2025-03-23 08:06:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 9 loss:0.13337969779968262 norm:0.0003211211587768048 max memory_allocated 62750.5498046875 
[2025-03-23 08:08:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 10 loss:0.13283315300941467 norm:0.00031052916892804205 max memory_allocated 62750.5498046875 
[2025-03-23 08:10:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 11 loss:0.13236047327518463 norm:0.000298305822070688 max memory_allocated 62750.5498046875 
[2025-03-23 08:12:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 12 loss:0.13193638622760773 norm:0.00029599107801914215 max memory_allocated 62750.5498046875 
[2025-03-23 08:14:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 13 loss:0.13154511153697968 norm:0.00028968247352167964 max memory_allocated 62750.5498046875 
[2025-03-23 08:16:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 14 loss:0.13124892115592957 norm:0.00028539347113110125 max memory_allocated 62750.5498046875 
[2025-03-23 08:18:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 15 loss:0.13096167147159576 norm:0.0002819885849021375 max memory_allocated 62750.5498046875 
[2025-03-23 08:21:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 16 loss:0.13073331117630005 norm:0.0002791481383610517 max memory_allocated 62750.5498046875 
[2025-03-23 08:23:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 17 loss:0.13048331439495087 norm:0.00027212477289140224 max memory_allocated 62750.5498046875 
[2025-03-23 08:25:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 18 loss:0.1302667260169983 norm:0.00026940018869936466 max memory_allocated 62750.5498046875 
[2025-03-23 08:27:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [22, 23, 24]) iter 19 loss:0.13009804487228394 norm:0.0002674776769708842 max memory_allocated 62750.5498046875 
[2025-03-23 08:30:22 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [22, 23, 24]
[2025-03-23 08:30:22 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [25, 26, 27] ===
[2025-03-23 08:32:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 0 loss:0.28553903102874756 norm:0.0008679782040417194 max memory_allocated 62751.7841796875 
[2025-03-23 08:34:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 1 loss:0.2450951188802719 norm:0.0005326814716681838 max memory_allocated 62751.7841796875 
[2025-03-23 08:36:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 2 loss:0.2074688971042633 norm:0.00037907404475845397 max memory_allocated 62751.7841796875 
[2025-03-23 08:39:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 3 loss:0.1965194046497345 norm:0.000353072042344138 max memory_allocated 62751.7841796875 
[2025-03-23 08:41:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 4 loss:0.1917191445827484 norm:0.00033712125150486827 max memory_allocated 62751.7841796875 
[2025-03-23 08:43:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 5 loss:0.18995627760887146 norm:0.0003294568741694093 max memory_allocated 62751.7841796875 
[2025-03-23 08:45:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 6 loss:0.18884170055389404 norm:0.00032288202783092856 max memory_allocated 62751.7841796875 
[2025-03-23 08:47:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 7 loss:0.1878715306520462 norm:0.00031490903347730637 max memory_allocated 62751.7841796875 
[2025-03-23 08:49:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 8 loss:0.18706625699996948 norm:0.0003041755117010325 max memory_allocated 62751.7841796875 
[2025-03-23 08:51:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 9 loss:0.18637099862098694 norm:0.0002954191295430064 max memory_allocated 62751.7841796875 
[2025-03-23 08:53:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 10 loss:0.185727059841156 norm:0.0002895819488912821 max memory_allocated 62751.7841796875 
[2025-03-23 08:56:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 11 loss:0.1851387768983841 norm:0.00028530729468911886 max memory_allocated 62751.7841796875 
[2025-03-23 08:58:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 12 loss:0.18467317521572113 norm:0.00027961365412920713 max memory_allocated 62751.7841796875 
[2025-03-23 09:00:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 13 loss:0.18420052528381348 norm:0.00027570861857384443 max memory_allocated 62751.7841796875 
[2025-03-23 09:02:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 14 loss:0.1838228404521942 norm:0.0002730104315560311 max memory_allocated 62751.7841796875 
[2025-03-23 09:04:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 15 loss:0.18346188962459564 norm:0.00027289451099932194 max memory_allocated 62751.7841796875 
[2025-03-23 09:06:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 16 loss:0.18314175307750702 norm:0.00026823379448615015 max memory_allocated 62751.7841796875 
[2025-03-23 09:08:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 17 loss:0.18285201489925385 norm:0.000266267015831545 max memory_allocated 62751.7841796875 
[2025-03-23 09:11:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 18 loss:0.18256033957004547 norm:0.00026484529371373355 max memory_allocated 62751.7841796875 
[2025-03-23 09:13:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [25, 26, 27]) iter 19 loss:0.1822969764471054 norm:0.0002591955999378115 max memory_allocated 62751.7841796875 
[2025-03-23 09:16:05 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [25, 26, 27]
[2025-03-23 09:16:05 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [28, 29] ===
[2025-03-23 09:17:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 0 loss:0.2917550504207611 norm:0.0008190660737454891 max memory_allocated 62751.7841796875 
[2025-03-23 09:19:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 1 loss:0.2599996030330658 norm:0.00045311273424886167 max memory_allocated 62751.7841796875 
[2025-03-23 09:20:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 2 loss:0.23028215765953064 norm:0.0003273416659794748 max memory_allocated 62751.7841796875 
[2025-03-23 09:21:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 3 loss:0.22215886414051056 norm:0.00030686272657476366 max memory_allocated 62751.7841796875 
[2025-03-23 09:23:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 4 loss:0.21950945258140564 norm:0.0002796312910504639 max memory_allocated 62751.7841796875 
[2025-03-23 09:24:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 5 loss:0.218428373336792 norm:0.0002774590393528342 max memory_allocated 62751.7841796875 
[2025-03-23 09:26:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 6 loss:0.2176855206489563 norm:0.0002801063237711787 max memory_allocated 62751.7841796875 
[2025-03-23 09:27:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 7 loss:0.2171023190021515 norm:0.0002598755818326026 max memory_allocated 62751.7841796875 
[2025-03-23 09:28:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 8 loss:0.21650448441505432 norm:0.00024372180632781237 max memory_allocated 62751.7841796875 
[2025-03-23 09:30:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 9 loss:0.21599188446998596 norm:0.00023868594144005328 max memory_allocated 62751.7841796875 
[2025-03-23 09:31:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 10 loss:0.2156044840812683 norm:0.00023471337044611573 max memory_allocated 62751.7841796875 
[2025-03-23 09:33:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 11 loss:0.21519052982330322 norm:0.00024238284095190465 max memory_allocated 62751.7841796875 
[2025-03-23 09:34:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 12 loss:0.21483838558197021 norm:0.0002551395446062088 max memory_allocated 62751.7841796875 
[2025-03-23 09:36:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 13 loss:0.21448735892772675 norm:0.00023536404478363693 max memory_allocated 62751.7841796875 
[2025-03-23 09:37:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 14 loss:0.2141917496919632 norm:0.00022915046429261565 max memory_allocated 62751.7841796875 
[2025-03-23 09:38:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 15 loss:0.21392065286636353 norm:0.00022872514091432095 max memory_allocated 62751.7841796875 
[2025-03-23 09:40:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 16 loss:0.21368172764778137 norm:0.0002286797680426389 max memory_allocated 62751.7841796875 
[2025-03-23 09:41:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 17 loss:0.21348698437213898 norm:0.00023098720703274012 max memory_allocated 62751.7841796875 
[2025-03-23 09:43:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 18 loss:0.21328571438789368 norm:0.00023580608831252903 max memory_allocated 62751.7841796875 
[2025-03-23 09:44:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28, 29]) iter 19 loss:0.2130851149559021 norm:0.00023341845371760428 max memory_allocated 62751.7841796875 
[2025-03-23 09:46:36 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [28, 29]
[2025-03-23 09:46:37 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [30, 31] ===
[2025-03-23 09:48:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 0 loss:0.34235793352127075 norm:0.0006328177405521274 max memory_allocated 62751.7841796875 
[2025-03-23 09:49:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 1 loss:0.3094322979450226 norm:0.000421343429479748 max memory_allocated 62751.7841796875 
[2025-03-23 09:50:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 2 loss:0.27752619981765747 norm:0.0002828384458553046 max memory_allocated 62751.7841796875 
[2025-03-23 09:52:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 3 loss:0.2686609923839569 norm:0.00026111974148079753 max memory_allocated 62751.7841796875 
[2025-03-23 09:53:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 4 loss:0.2661779522895813 norm:0.00025578789063729346 max memory_allocated 62751.7841796875 
[2025-03-23 09:55:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 5 loss:0.265033483505249 norm:0.0002465681463945657 max memory_allocated 62751.7841796875 
[2025-03-23 09:56:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 6 loss:0.2642802298069 norm:0.0002440213575027883 max memory_allocated 62751.7841796875 
[2025-03-23 09:58:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 7 loss:0.2635515630245209 norm:0.00023908258299343288 max memory_allocated 62751.7841796875 
[2025-03-23 09:59:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 8 loss:0.26291677355766296 norm:0.0002372652234043926 max memory_allocated 62751.7841796875 
[2025-03-23 10:00:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 9 loss:0.26236504316329956 norm:0.00023465372214559466 max memory_allocated 62751.7841796875 
[2025-03-23 10:02:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 10 loss:0.2618864178657532 norm:0.00022756292310077697 max memory_allocated 62751.7841796875 
[2025-03-23 10:03:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 11 loss:0.2614333927631378 norm:0.0002272852580063045 max memory_allocated 62751.7841796875 
[2025-03-23 10:05:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 12 loss:0.261059045791626 norm:0.0002291792188771069 max memory_allocated 62751.7841796875 
[2025-03-23 10:06:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 13 loss:0.26069319248199463 norm:0.00022917719616089016 max memory_allocated 62751.7841796875 
[2025-03-23 10:08:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 14 loss:0.26040175557136536 norm:0.00022200591047294438 max memory_allocated 62751.7841796875 
[2025-03-23 10:09:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 15 loss:0.2601023316383362 norm:0.00021849488257430494 max memory_allocated 62751.7841796875 
[2025-03-23 10:10:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 16 loss:0.2598229646682739 norm:0.0002255730942124501 max memory_allocated 62751.7841796875 
[2025-03-23 10:12:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 17 loss:0.25955262780189514 norm:0.00023112358758226037 max memory_allocated 62751.7841796875 
[2025-03-23 10:13:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 18 loss:0.2593643367290497 norm:0.00024589267559349537 max memory_allocated 62751.7841796875 
[2025-03-23 10:15:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30, 31]) iter 19 loss:0.25918418169021606 norm:0.0002568371710367501 max memory_allocated 62751.7841796875 
[2025-03-23 10:17:10 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [30, 31]
[2025-03-23 10:17:10 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [32, 33] ===
[2025-03-23 10:18:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 0 loss:0.4002455174922943 norm:0.0008541590650565922 max memory_allocated 62751.7841796875 
[2025-03-23 10:20:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 1 loss:0.3644205927848816 norm:0.0006865072064101696 max memory_allocated 62751.7841796875 
[2025-03-23 10:21:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 2 loss:0.3299606442451477 norm:0.0004279148415662348 max memory_allocated 62751.7841796875 
[2025-03-23 10:22:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 3 loss:0.3208032250404358 norm:0.0003125793591607362 max memory_allocated 62751.7841796875 
[2025-03-23 10:24:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 4 loss:0.31842777132987976 norm:0.00028649240266531706 max memory_allocated 62751.7841796875 
[2025-03-23 10:25:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 5 loss:0.3172188699245453 norm:0.0002766057732515037 max memory_allocated 62751.7841796875 
[2025-03-23 10:27:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 6 loss:0.31628865003585815 norm:0.000264021975453943 max memory_allocated 62751.7841796875 
[2025-03-23 10:28:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 7 loss:0.31553736329078674 norm:0.0002608434879221022 max memory_allocated 62751.7841796875 
[2025-03-23 10:30:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 8 loss:0.31490224599838257 norm:0.0002610786759760231 max memory_allocated 62751.7841796875 
[2025-03-23 10:31:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 9 loss:0.31436821818351746 norm:0.00026329903630539775 max memory_allocated 62751.7841796875 
[2025-03-23 10:32:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 10 loss:0.3138445317745209 norm:0.0002588556963019073 max memory_allocated 62751.7841796875 
[2025-03-23 10:34:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 11 loss:0.31340524554252625 norm:0.0002584043249953538 max memory_allocated 62751.7841796875 
[2025-03-23 10:35:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 12 loss:0.31296706199645996 norm:0.000251912948442623 max memory_allocated 62751.7841796875 
[2025-03-23 10:37:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 13 loss:0.3125641644001007 norm:0.0002474344219081104 max memory_allocated 62751.7841796875 
[2025-03-23 10:38:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 14 loss:0.31219467520713806 norm:0.000247057992964983 max memory_allocated 62751.7841796875 
[2025-03-23 10:40:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 15 loss:0.3118871748447418 norm:0.00024310000299010426 max memory_allocated 62751.7841796875 
[2025-03-23 10:41:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 16 loss:0.31163227558135986 norm:0.00024216229212470353 max memory_allocated 62751.7841796875 
[2025-03-23 10:42:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 17 loss:0.31136876344680786 norm:0.00024635886074975133 max memory_allocated 62751.7841796875 
[2025-03-23 10:44:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 18 loss:0.31112560629844666 norm:0.00024365280114579946 max memory_allocated 62751.7841796875 
[2025-03-23 10:45:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [32, 33]) iter 19 loss:0.3109171986579895 norm:0.00024330485030077398 max memory_allocated 62751.7841796875 
[2025-03-23 10:47:40 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [32, 33]
[2025-03-23 10:47:40 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [34] ===
[2025-03-23 10:48:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 0 loss:0.3849852681159973 norm:0.0007304649916477501 max memory_allocated 62751.7841796875 
[2025-03-23 10:49:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 1 loss:0.3638782501220703 norm:0.0004315657133702189 max memory_allocated 62751.7841796875 
[2025-03-23 10:49:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 2 loss:0.3438038229942322 norm:0.00023228046484291553 max memory_allocated 62751.7841796875 
[2025-03-23 10:50:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 3 loss:0.33912837505340576 norm:0.0002076751843560487 max memory_allocated 62751.7841796875 
[2025-03-23 10:51:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 4 loss:0.3381204903125763 norm:0.0001993951154872775 max memory_allocated 62751.7841796875 
[2025-03-23 10:52:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 5 loss:0.3375239372253418 norm:0.00019970335415564477 max memory_allocated 62751.7841796875 
[2025-03-23 10:52:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 6 loss:0.33709168434143066 norm:0.00018999390886165202 max memory_allocated 62751.7841796875 
[2025-03-23 10:53:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 7 loss:0.33668869733810425 norm:0.00018813378119375557 max memory_allocated 62751.7841796875 
[2025-03-23 10:54:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 8 loss:0.3363373279571533 norm:0.0001835968141676858 max memory_allocated 62751.7841796875 
[2025-03-23 10:54:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 9 loss:0.3360295295715332 norm:0.0001799125602701679 max memory_allocated 62751.7841796875 
[2025-03-23 10:55:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 10 loss:0.33575519919395447 norm:0.00018853769870474935 max memory_allocated 62751.7841796875 
[2025-03-23 10:56:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 11 loss:0.33552616834640503 norm:0.00018254316819366068 max memory_allocated 62751.7841796875 
[2025-03-23 10:57:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 12 loss:0.335345983505249 norm:0.000175861336174421 max memory_allocated 62751.7841796875 
[2025-03-23 10:57:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 13 loss:0.3351515233516693 norm:0.00017213223327416927 max memory_allocated 62751.7841796875 
[2025-03-23 10:58:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 14 loss:0.3349606394767761 norm:0.00017045468848664314 max memory_allocated 62751.7841796875 
[2025-03-23 10:59:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 15 loss:0.33482950925827026 norm:0.00016802808386273682 max memory_allocated 62751.7841796875 
[2025-03-23 10:59:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 16 loss:0.334648996591568 norm:0.00016586013953201473 max memory_allocated 62751.7841796875 
[2025-03-23 11:00:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 17 loss:0.33453914523124695 norm:0.00016945452080108225 max memory_allocated 62751.7841796875 
[2025-03-23 11:01:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 18 loss:0.33442994952201843 norm:0.0001670453930273652 max memory_allocated 62751.7841796875 
[2025-03-23 11:02:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [34]) iter 19 loss:0.3342950940132141 norm:0.00016578516806475818 max memory_allocated 62751.7841796875 
[2025-03-23 11:02:59 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 15, block: [34]
[2025-03-23 11:02:59 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 16 with layers [35] ===
[2025-03-23 11:03:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 0 loss:0.421949177980423 norm:0.0008020103559829295 max memory_allocated 62751.7841796875 
[2025-03-23 11:04:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 1 loss:0.3994651734828949 norm:0.0004921077052131295 max memory_allocated 62751.7841796875 
[2025-03-23 11:05:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 2 loss:0.3784983158111572 norm:0.0002830229641404003 max memory_allocated 62751.7841796875 
[2025-03-23 11:05:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 3 loss:0.37387099862098694 norm:0.0002555392275098711 max memory_allocated 62751.7841796875 
[2025-03-23 11:06:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 4 loss:0.3727911710739136 norm:0.0002483401040080935 max memory_allocated 62751.7841796875 
[2025-03-23 11:07:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 5 loss:0.3720756769180298 norm:0.0002373082679696381 max memory_allocated 62751.7841796875 
[2025-03-23 11:08:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 6 loss:0.3715473711490631 norm:0.00022925002849660814 max memory_allocated 62751.7841796875 
[2025-03-23 11:08:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 7 loss:0.37107664346694946 norm:0.00022324203746393323 max memory_allocated 62751.7841796875 
[2025-03-23 11:09:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 8 loss:0.3707101047039032 norm:0.00022563365928363055 max memory_allocated 62751.7841796875 
[2025-03-23 11:10:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 9 loss:0.3703359067440033 norm:0.0002170367370126769 max memory_allocated 62751.7841796875 
[2025-03-23 11:10:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 10 loss:0.36998844146728516 norm:0.00020629653590731323 max memory_allocated 62751.7841796875 
[2025-03-23 11:11:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 11 loss:0.3696725070476532 norm:0.000205293414182961 max memory_allocated 62751.7841796875 
[2025-03-23 11:12:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 12 loss:0.3694024384021759 norm:0.00020370743004605174 max memory_allocated 62751.7841796875 
[2025-03-23 11:13:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 13 loss:0.36917147040367126 norm:0.00020219814905431122 max memory_allocated 62751.7841796875 
[2025-03-23 11:13:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 14 loss:0.36896824836730957 norm:0.00020342062634881586 max memory_allocated 62751.7841796875 
[2025-03-23 11:14:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 15 loss:0.36878740787506104 norm:0.00019390812667552382 max memory_allocated 62751.7841796875 
[2025-03-23 11:15:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 16 loss:0.36860135197639465 norm:0.00018817553063854575 max memory_allocated 62751.7841796875 
[2025-03-23 11:15:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 17 loss:0.36845022439956665 norm:0.00018987187650054693 max memory_allocated 62751.7841796875 
[2025-03-23 11:16:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 18 loss:0.3682868778705597 norm:0.0001923952077049762 max memory_allocated 62751.7841796875 
[2025-03-23 11:17:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [35]) iter 19 loss:0.36813661456108093 norm:0.00019026613153982908 max memory_allocated 62751.7841796875 
[2025-03-23 11:18:19 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 16, block: [35]
[2025-03-23 11:18:19 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 17 with layers [36] ===
[2025-03-23 11:18:19 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 11:19:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 0 loss:0.4678785800933838 norm:0.013869783841073513 max memory_allocated 62751.7841796875 
[2025-03-23 11:19:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 1 loss:0.44167524576187134 norm:0.010556323453783989 max memory_allocated 62751.7841796875 
[2025-03-23 11:20:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 2 loss:0.4175977110862732 norm:0.007788673508912325 max memory_allocated 62751.7841796875 
[2025-03-23 11:21:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 3 loss:0.41265901923179626 norm:0.006512871943414211 max memory_allocated 62751.7841796875 
[2025-03-23 11:21:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 4 loss:0.41121339797973633 norm:0.00534098083153367 max memory_allocated 62751.7841796875 
[2025-03-23 11:22:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 5 loss:0.4101930260658264 norm:0.004405046813189983 max memory_allocated 62751.7841796875 
[2025-03-23 11:23:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 6 loss:0.409505695104599 norm:0.003976583946496248 max memory_allocated 62751.7841796875 
[2025-03-23 11:24:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 7 loss:0.4089798033237457 norm:0.003838147036731243 max memory_allocated 62751.7841796875 
[2025-03-23 11:24:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 8 loss:0.40853428840637207 norm:0.003619767725467682 max memory_allocated 62751.7841796875 
[2025-03-23 11:25:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 9 loss:0.40817755460739136 norm:0.003547005821019411 max memory_allocated 62751.7841796875 
[2025-03-23 11:26:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 10 loss:0.40779030323028564 norm:0.0035145985893905163 max memory_allocated 62751.7841796875 
[2025-03-23 11:27:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 11 loss:0.4074627757072449 norm:0.003368380479514599 max memory_allocated 62751.7841796875 
[2025-03-23 11:27:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 12 loss:0.4070644676685333 norm:0.003199224127456546 max memory_allocated 62751.7841796875 
[2025-03-23 11:28:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 13 loss:0.4067789912223816 norm:0.0030855699442327023 max memory_allocated 62751.7841796875 
[2025-03-23 11:29:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 14 loss:0.40650007128715515 norm:0.002906010253354907 max memory_allocated 62751.7841796875 
[2025-03-23 11:29:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 15 loss:0.40624427795410156 norm:0.002895865822210908 max memory_allocated 62751.7841796875 
[2025-03-23 11:30:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 16 loss:0.40601474046707153 norm:0.002664471510797739 max memory_allocated 62751.7841796875 
[2025-03-23 11:31:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 17 loss:0.40592193603515625 norm:0.0028121676295995712 max memory_allocated 62751.7841796875 
[2025-03-23 11:32:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 18 loss:0.4058709740638733 norm:0.002846800023689866 max memory_allocated 62751.7841796875 
[2025-03-23 11:32:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [36]) iter 19 loss:0.4057522416114807 norm:0.0029964677523821592 max memory_allocated 62751.7841796875 
[2025-03-23 11:33:44 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 17, block: [36]
[2025-03-23 11:33:44 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 18 with layers [37] ===
[2025-03-23 11:33:44 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 11:34:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 0 loss:0.5394139289855957 norm:0.020465116947889328 max memory_allocated 62751.7841796875 
[2025-03-23 11:35:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 1 loss:0.5028716325759888 norm:0.014120375737547874 max memory_allocated 62751.7841796875 
[2025-03-23 11:35:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 2 loss:0.4715259373188019 norm:0.009910326451063156 max memory_allocated 62751.7841796875 
[2025-03-23 11:36:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 3 loss:0.4650430679321289 norm:0.008217241615056992 max memory_allocated 62751.7841796875 
[2025-03-23 11:37:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 4 loss:0.4631502628326416 norm:0.006932355463504791 max memory_allocated 62751.7841796875 
[2025-03-23 11:38:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 5 loss:0.4617759585380554 norm:0.005856345873326063 max memory_allocated 62751.7841796875 
[2025-03-23 11:38:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 6 loss:0.4608322083950043 norm:0.005024597980082035 max memory_allocated 62751.7841796875 
[2025-03-23 11:39:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 7 loss:0.4601181745529175 norm:0.004627895541489124 max memory_allocated 62751.7841796875 
[2025-03-23 11:40:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 8 loss:0.4596107602119446 norm:0.004738808609545231 max memory_allocated 62751.7841796875 
[2025-03-23 11:40:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 9 loss:0.4591068923473358 norm:0.004635277204215527 max memory_allocated 62751.7841796875 
[2025-03-23 11:41:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 10 loss:0.4586363732814789 norm:0.004500979091972113 max memory_allocated 62751.7841796875 
[2025-03-23 11:42:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 11 loss:0.45827341079711914 norm:0.0042280349880456924 max memory_allocated 62751.7841796875 
[2025-03-23 11:43:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 12 loss:0.4578453004360199 norm:0.004156921524554491 max memory_allocated 62751.7841796875 
[2025-03-23 11:43:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 13 loss:0.45742183923721313 norm:0.003830916481092572 max memory_allocated 62751.7841796875 
[2025-03-23 11:44:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 14 loss:0.4570891261100769 norm:0.003719605738297105 max memory_allocated 62751.7841796875 
[2025-03-23 11:45:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 15 loss:0.45680204033851624 norm:0.003517726669088006 max memory_allocated 62751.7841796875 
[2025-03-23 11:46:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 16 loss:0.4567091464996338 norm:0.003687493735924363 max memory_allocated 62751.7841796875 
[2025-03-23 11:46:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 17 loss:0.45643556118011475 norm:0.00358861917629838 max memory_allocated 62751.7841796875 
[2025-03-23 11:47:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 18 loss:0.4562632739543915 norm:0.0034579418133944273 max memory_allocated 62751.7841796875 
[2025-03-23 11:48:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [37]) iter 19 loss:0.45608073472976685 norm:0.0034591485746204853 max memory_allocated 62751.7841796875 
[2025-03-23 11:49:08 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 18, block: [37]
[2025-03-23 11:49:09 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 19 with layers [38] ===
[2025-03-23 11:49:09 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 11:49:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 0 loss:0.6498072743415833 norm:0.03300183266401291 max memory_allocated 62751.7841796875 
[2025-03-23 11:50:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 1 loss:0.6030210256576538 norm:0.022405507043004036 max memory_allocated 62751.7841796875 
[2025-03-23 11:51:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 2 loss:0.5680110454559326 norm:0.015496763400733471 max memory_allocated 62751.7841796875 
[2025-03-23 11:52:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 3 loss:0.5591899156570435 norm:0.013148537836968899 max memory_allocated 62751.7841796875 
[2025-03-23 11:52:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 4 loss:0.5564202070236206 norm:0.011692226864397526 max memory_allocated 62751.7841796875 
[2025-03-23 11:53:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 5 loss:0.5543342232704163 norm:0.010114713571965694 max memory_allocated 62751.7841796875 
[2025-03-23 11:54:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 6 loss:0.5523703098297119 norm:0.008836058899760246 max memory_allocated 62751.7841796875 
[2025-03-23 11:54:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 7 loss:0.5510196685791016 norm:0.007675035856664181 max memory_allocated 62751.7841796875 
[2025-03-23 11:55:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 8 loss:0.5500074625015259 norm:0.007066372781991959 max memory_allocated 62751.7841796875 
[2025-03-23 11:56:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 9 loss:0.5494115352630615 norm:0.006998894270509481 max memory_allocated 62751.7841796875 
[2025-03-23 11:57:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 10 loss:0.5488971471786499 norm:0.007105954457074404 max memory_allocated 62751.7841796875 
[2025-03-23 11:57:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 11 loss:0.5483783483505249 norm:0.006660448852926493 max memory_allocated 62751.7841796875 
[2025-03-23 11:58:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 12 loss:0.5478454828262329 norm:0.006377001293003559 max memory_allocated 62751.7841796875 
[2025-03-23 11:59:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 13 loss:0.5471940040588379 norm:0.005944185424596071 max memory_allocated 62751.7841796875 
[2025-03-23 11:59:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 14 loss:0.5466938018798828 norm:0.005716362502425909 max memory_allocated 62751.7841796875 
[2025-03-23 12:00:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 15 loss:0.5462414026260376 norm:0.005132791120558977 max memory_allocated 62751.7841796875 
[2025-03-23 12:01:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 16 loss:0.5459193587303162 norm:0.004750075750052929 max memory_allocated 62751.7841796875 
[2025-03-23 12:02:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 17 loss:0.5459408760070801 norm:0.005408032331615686 max memory_allocated 62751.7841796875 
[2025-03-23 12:02:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 18 loss:0.5459500551223755 norm:0.005881726276129484 max memory_allocated 62751.7841796875 
[2025-03-23 12:03:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [38]) iter 19 loss:0.5458093881607056 norm:0.0055923620238900185 max memory_allocated 62751.7841796875 
[2025-03-23 12:04:33 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 19, block: [38]
[2025-03-23 12:04:33 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 20 with layers [39] ===
[2025-03-23 12:04:33 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 12:05:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 0 loss:1.039482831954956 norm:0.09702524542808533 max memory_allocated 62751.7841796875 
[2025-03-23 12:06:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 1 loss:0.9455295205116272 norm:0.06574632972478867 max memory_allocated 62751.7841796875 
[2025-03-23 12:06:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 2 loss:0.8701110482215881 norm:0.03685571253299713 max memory_allocated 62751.7841796875 
[2025-03-23 12:07:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 3 loss:0.8521875143051147 norm:0.03424770385026932 max memory_allocated 62751.7841796875 
[2025-03-23 12:08:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 4 loss:0.8446464538574219 norm:0.03071148321032524 max memory_allocated 62751.7841796875 
[2025-03-23 12:08:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 5 loss:0.8388233184814453 norm:0.028670860454440117 max memory_allocated 62751.7841796875 
[2025-03-23 12:09:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 6 loss:0.8348421454429626 norm:0.026865165680646896 max memory_allocated 62751.7841796875 
[2025-03-23 12:10:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 7 loss:0.8317094445228577 norm:0.02648783288896084 max memory_allocated 62751.7841796875 
[2025-03-23 12:11:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 8 loss:0.8290292024612427 norm:0.024996556341648102 max memory_allocated 62751.7841796875 
[2025-03-23 12:11:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 9 loss:0.8275535702705383 norm:0.02417653426527977 max memory_allocated 62751.7841796875 
[2025-03-23 12:12:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 10 loss:0.8267691135406494 norm:0.022239811718463898 max memory_allocated 62751.7841796875 
[2025-03-23 12:13:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 11 loss:0.8244920969009399 norm:0.022984446957707405 max memory_allocated 62751.7841796875 
[2025-03-23 12:13:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 12 loss:0.8237394094467163 norm:0.022269707173109055 max memory_allocated 62751.7841796875 
[2025-03-23 12:14:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 13 loss:0.8227578997612 norm:0.022437572479248047 max memory_allocated 62751.7841796875 
[2025-03-23 12:15:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 14 loss:0.8218836784362793 norm:0.022086648270487785 max memory_allocated 62751.7841796875 
[2025-03-23 12:16:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 15 loss:0.820320725440979 norm:0.021066419780254364 max memory_allocated 62751.7841796875 
[2025-03-23 12:16:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 16 loss:0.8192450404167175 norm:0.020220492035150528 max memory_allocated 62751.7841796875 
[2025-03-23 12:17:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 17 loss:0.8184475302696228 norm:0.019883964210748672 max memory_allocated 62751.7841796875 
[2025-03-23 12:18:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 18 loss:0.8177412152290344 norm:0.019317859783768654 max memory_allocated 62751.7841796875 
[2025-03-23 12:19:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [39]) iter 19 loss:0.8170686960220337 norm:0.01887631230056286 max memory_allocated 62751.7841796875 
[2025-03-23 12:20:01 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 20, block: [39]
[2025-03-23 12:20:01 root] (main_calib_config3_attn.py 379): INFO 36873.02175140381
[2025-03-23 12:20:11 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-23 12:21:35 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.2001800537109375
[2025-03-23 12:21:35 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-23 12:23:46 root] (main_calib_config3_attn.py 161): INFO c4 : 6.747839450836182
[2025-03-23 13:16:36 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.2001800537109375, 'c4': 6.747839450836182, 'results': {'arc_easy': {'acc': 0.7327441077441077, 'acc_stderr': 0.009080463246017469, 'acc_norm': 0.5845959595959596, 'acc_norm_stderr': 0.010111869494911515}, 'piqa': {'acc': 0.7878128400435256, 'acc_stderr': 0.009539299828174044, 'acc_norm': 0.7878128400435256, 'acc_norm_stderr': 0.009539299828174048}, 'hellaswag': {'acc': 0.5823541127265485, 'acc_stderr': 0.004921632645102378, 'acc_norm': 0.7542322246564429, 'acc_norm_stderr': 0.0042966158627866305}, 'arc_challenge': {'acc': 0.4257679180887372, 'acc_stderr': 0.01444946427886881, 'acc_norm': 0.4351535836177474, 'acc_norm_stderr': 0.014487986197186045}, 'winogrande': {'acc': 0.6937647987371744, 'acc_stderr': 0.012954385972802468}, 'boolq': {'acc': 0.6697247706422018, 'acc_stderr': 0.008225810914277267}}, 'versions': {'arc_easy': 0, 'piqa': 0, 'hellaswag': 0, 'arc_challenge': 0, 'winogrande': 0, 'boolq': 1}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-23 13:16:36 root] (main_calib_config3_attn.py 175): INFO 42.58,73.27,66.97,58.24,78.78,69.38
