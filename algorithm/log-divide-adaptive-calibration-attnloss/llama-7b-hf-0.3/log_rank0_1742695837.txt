[2025-03-23 02:10:37 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/llama-7b-hf-0.3', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.3.pkl', blocks_pkl='./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-23 02:10:45 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-23 02:10:45 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-23 02:10:45 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-23 02:10:45 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.3.pkl
[2025-03-23 02:10:45 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 5), (5, 8), (8, 11), (11, 14), (14, 16), (16, 19), (19, 22), (22, 25), (25, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-23 02:10:45 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2, 3, 4], [5, 6, 7], [8, 9, 10], [11, 12, 13], [14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26], [27], [28], [29], [30], [31]]
[2025-03-23 02:10:49 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-23 02:10:49 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:11:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.011318817734718323 norm:0.014298656024038792 max memory_allocated 34630.880859375 
[2025-03-23 02:11:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.0057228077203035355 norm:0.007060237228870392 max memory_allocated 34630.880859375 
[2025-03-23 02:12:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.0037769745104014874 norm:0.0048429653979837894 max memory_allocated 34630.880859375 
[2025-03-23 02:12:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0031550386920571327 norm:0.003786667948588729 max memory_allocated 34630.880859375 
[2025-03-23 02:13:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.0028786729089915752 norm:0.0030213494319468737 max memory_allocated 34630.880859375 
[2025-03-23 02:13:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.00268411822617054 norm:0.002597321756184101 max memory_allocated 34630.880859375 
[2025-03-23 02:14:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.002580683445557952 norm:0.002174751367419958 max memory_allocated 34630.880859375 
[2025-03-23 02:14:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0024901949800550938 norm:0.0019285418093204498 max memory_allocated 34630.880859375 
[2025-03-23 02:15:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0024284019600600004 norm:0.001744749373756349 max memory_allocated 34630.880859375 
[2025-03-23 02:15:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.002396941650658846 norm:0.0015739755472168326 max memory_allocated 34630.880859375 
[2025-03-23 02:16:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.0023598787374794483 norm:0.0013907153625041246 max memory_allocated 34630.880859375 
[2025-03-23 02:16:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0023468639701604843 norm:0.0012923558242619038 max memory_allocated 34630.880859375 
[2025-03-23 02:17:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.002313740085810423 norm:0.001144860521890223 max memory_allocated 34630.880859375 
[2025-03-23 02:17:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.002291728276759386 norm:0.0010277265682816505 max memory_allocated 34630.880859375 
[2025-03-23 02:18:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0022792865056544542 norm:0.0009424103191122413 max memory_allocated 34630.880859375 
[2025-03-23 02:18:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.002260378561913967 norm:0.0008509174804203212 max memory_allocated 34630.880859375 
[2025-03-23 02:18:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.0022682517301291227 norm:0.0007787365466356277 max memory_allocated 34630.880859375 
[2025-03-23 02:19:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0022652004845440388 norm:0.0007346344646066427 max memory_allocated 34630.880859375 
[2025-03-23 02:19:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.0022797961719334126 norm:0.0007136674248613417 max memory_allocated 34630.880859375 
[2025-03-23 02:20:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.002256820211187005 norm:0.0006785187870264053 max memory_allocated 34630.880859375 
[2025-03-23 02:21:02 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-23 02:21:02 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-23 02:21:02 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:21:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.020985545590519905 norm:0.01985308900475502 max memory_allocated 35097.7724609375 
[2025-03-23 02:22:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.011920683085918427 norm:0.012155361473560333 max memory_allocated 35097.7724609375 
[2025-03-23 02:22:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.008216972462832928 norm:0.007065719924867153 max memory_allocated 35097.7724609375 
[2025-03-23 02:22:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.007168141659349203 norm:0.00505414605140686 max memory_allocated 35097.7724609375 
[2025-03-23 02:23:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.006799633614718914 norm:0.004319627769291401 max memory_allocated 35097.7724609375 
[2025-03-23 02:23:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.006536468397825956 norm:0.003885384416207671 max memory_allocated 35097.7724609375 
[2025-03-23 02:24:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.006330301985144615 norm:0.0034945933148264885 max memory_allocated 35097.7724609375 
[2025-03-23 02:24:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.006196613889187574 norm:0.0032064756378531456 max memory_allocated 35097.7724609375 
[2025-03-23 02:25:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.00599149102345109 norm:0.0029291678220033646 max memory_allocated 35097.7724609375 
[2025-03-23 02:25:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.005869755055755377 norm:0.002672565169632435 max memory_allocated 35097.7724609375 
[2025-03-23 02:26:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.005782814230769873 norm:0.0024667170364409685 max memory_allocated 35097.7724609375 
[2025-03-23 02:26:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.005752144381403923 norm:0.0022765330504626036 max memory_allocated 35097.7724609375 
[2025-03-23 02:27:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.005647622048854828 norm:0.002064646454527974 max memory_allocated 35097.7724609375 
[2025-03-23 02:27:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.00560953700914979 norm:0.0018925070762634277 max memory_allocated 35097.7724609375 
[2025-03-23 02:28:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.005566674750298262 norm:0.001714402693323791 max memory_allocated 35097.7724609375 
[2025-03-23 02:28:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.005544173996895552 norm:0.001538753043860197 max memory_allocated 35097.7724609375 
[2025-03-23 02:29:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.0055192988365888596 norm:0.0013801734894514084 max memory_allocated 35097.7724609375 
[2025-03-23 02:29:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.005503818858414888 norm:0.0012289374135434628 max memory_allocated 35097.7724609375 
[2025-03-23 02:30:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.005500898230820894 norm:0.0011027142172679305 max memory_allocated 35097.7724609375 
[2025-03-23 02:30:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.005469655618071556 norm:0.0009932512184605002 max memory_allocated 35097.7724609375 
[2025-03-23 02:31:13 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-23 02:31:13 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2, 3, 4] ===
[2025-03-23 02:31:14 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:32:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 0 loss:0.053294774144887924 norm:0.02995283156633377 max memory_allocated 47468.5419921875 
[2025-03-23 02:34:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 1 loss:0.03955778107047081 norm:0.02207440882921219 max memory_allocated 47468.5419921875 
[2025-03-23 02:35:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 2 loss:0.030907917767763138 norm:0.015801135450601578 max memory_allocated 47468.5419921875 
[2025-03-23 02:37:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 3 loss:0.026718581095337868 norm:0.010900940746068954 max memory_allocated 47468.5419921875 
[2025-03-23 02:38:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 4 loss:0.024608630686998367 norm:0.0084571223706007 max memory_allocated 47468.5419921875 
[2025-03-23 02:39:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 5 loss:0.023275895044207573 norm:0.006825809367001057 max memory_allocated 47468.5419921875 
[2025-03-23 02:41:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 6 loss:0.02259555086493492 norm:0.006215331144630909 max memory_allocated 47468.5419921875 
[2025-03-23 02:42:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 7 loss:0.0220591202378273 norm:0.005525167100131512 max memory_allocated 47468.5419921875 
[2025-03-23 02:44:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 8 loss:0.02150549925863743 norm:0.005266794469207525 max memory_allocated 47468.5419921875 
[2025-03-23 02:45:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 9 loss:0.021320058032870293 norm:0.00491487979888916 max memory_allocated 47468.5419921875 
[2025-03-23 02:47:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 10 loss:0.021397795528173447 norm:0.004304911009967327 max memory_allocated 47468.5419921875 
[2025-03-23 02:48:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 11 loss:0.02142396941781044 norm:0.0039000636897981167 max memory_allocated 47468.5419921875 
[2025-03-23 02:49:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 12 loss:0.021548809483647346 norm:0.004194747656583786 max memory_allocated 47468.5419921875 
[2025-03-23 02:51:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 13 loss:0.021405532956123352 norm:0.004109623841941357 max memory_allocated 47468.5419921875 
[2025-03-23 02:52:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 14 loss:0.02133249305188656 norm:0.0038168979808688164 max memory_allocated 47468.5419921875 
[2025-03-23 02:54:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 15 loss:0.021358177065849304 norm:0.003609835635870695 max memory_allocated 47468.5419921875 
[2025-03-23 02:55:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 16 loss:0.02135506272315979 norm:0.003718971274793148 max memory_allocated 47468.5419921875 
[2025-03-23 02:56:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 17 loss:0.02131117507815361 norm:0.003578662406653166 max memory_allocated 47468.5419921875 
[2025-03-23 02:58:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 18 loss:0.0213941503316164 norm:0.0034108778927475214 max memory_allocated 47468.5419921875 
[2025-03-23 02:59:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 19 loss:0.021358318626880646 norm:0.003450741060078144 max memory_allocated 47468.5419921875 
[2025-03-23 03:01:38 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2, 3, 4]
[2025-03-23 03:01:38 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [5, 6, 7] ===
[2025-03-23 03:03:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 0 loss:0.07530006021261215 norm:0.004709832835942507 max memory_allocated 47468.7294921875 
[2025-03-23 03:04:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 1 loss:0.05300174653530121 norm:0.0009014279348775744 max memory_allocated 47468.7294921875 
[2025-03-23 03:06:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 2 loss:0.04172055423259735 norm:0.00043005021871067584 max memory_allocated 47468.7294921875 
[2025-03-23 03:07:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 3 loss:0.036644548177719116 norm:0.0003458867722656578 max memory_allocated 47468.7294921875 
[2025-03-23 03:08:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 4 loss:0.034113116562366486 norm:0.00030868183239363134 max memory_allocated 47468.7294921875 
[2025-03-23 03:10:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 5 loss:0.03244831785559654 norm:0.00031266504083760083 max memory_allocated 47468.7294921875 
[2025-03-23 03:11:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 6 loss:0.031418927013874054 norm:0.0003035917761735618 max memory_allocated 47468.7294921875 
[2025-03-23 03:13:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 7 loss:0.030735870823264122 norm:0.0003071812097914517 max memory_allocated 47468.7294921875 
[2025-03-23 03:14:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 8 loss:0.03043239936232567 norm:0.0003127755771856755 max memory_allocated 47468.7294921875 
[2025-03-23 03:15:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 9 loss:0.030110005289316177 norm:0.00028035417199134827 max memory_allocated 47468.7294921875 
[2025-03-23 03:17:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 10 loss:0.030026953667402267 norm:0.00031993346055969596 max memory_allocated 47468.7294921875 
[2025-03-23 03:18:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 11 loss:0.0299066212028265 norm:0.0002991720102727413 max memory_allocated 47468.7294921875 
[2025-03-23 03:20:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 12 loss:0.02978733740746975 norm:0.0002893834316637367 max memory_allocated 47468.7294921875 
[2025-03-23 03:21:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 13 loss:0.029693474993109703 norm:0.000282725173747167 max memory_allocated 47468.7294921875 
[2025-03-23 03:23:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 14 loss:0.02959551475942135 norm:0.00028574830503202975 max memory_allocated 47468.7294921875 
[2025-03-23 03:24:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 15 loss:0.02954534813761711 norm:0.00028694956563413143 max memory_allocated 47468.7294921875 
[2025-03-23 03:25:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 16 loss:0.029513489454984665 norm:0.0002895694342441857 max memory_allocated 47468.7294921875 
[2025-03-23 03:27:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 17 loss:0.0294900294393301 norm:0.0002929448673967272 max memory_allocated 47468.7294921875 
[2025-03-23 03:28:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 18 loss:0.029466431587934494 norm:0.00029063355759717524 max memory_allocated 47468.7294921875 
[2025-03-23 03:30:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 19 loss:0.029464364051818848 norm:0.0003009405918419361 max memory_allocated 47468.7294921875 
[2025-03-23 03:31:59 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [5, 6, 7]
[2025-03-23 03:31:59 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [8, 9, 10] ===
[2025-03-23 03:33:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 0 loss:0.08470775187015533 norm:0.001572595676407218 max memory_allocated 47468.9169921875 
[2025-03-23 03:34:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 1 loss:0.06363477557897568 norm:0.0006461238954216242 max memory_allocated 47468.9169921875 
[2025-03-23 03:36:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 2 loss:0.05092015862464905 norm:0.00040313665522262454 max memory_allocated 47468.9169921875 
[2025-03-23 03:37:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 3 loss:0.04515577107667923 norm:0.0003098986635450274 max memory_allocated 47468.9169921875 
[2025-03-23 03:39:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 4 loss:0.04233546182513237 norm:0.0002787891717161983 max memory_allocated 47468.9169921875 
[2025-03-23 03:40:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 5 loss:0.04062410071492195 norm:0.00025874690618366003 max memory_allocated 47468.9169921875 
[2025-03-23 03:42:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 6 loss:0.03951369971036911 norm:0.00023418098862748593 max memory_allocated 47468.9169921875 
[2025-03-23 03:43:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 7 loss:0.038844190537929535 norm:0.00023024661641102284 max memory_allocated 47468.9169921875 
[2025-03-23 03:44:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 8 loss:0.038417357951402664 norm:0.00022497025202028453 max memory_allocated 47468.9169921875 
[2025-03-23 03:46:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 9 loss:0.03810859099030495 norm:0.00022512994473800063 max memory_allocated 47468.9169921875 
[2025-03-23 03:47:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 10 loss:0.03794568032026291 norm:0.0002265407529193908 max memory_allocated 47468.9169921875 
[2025-03-23 03:49:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 11 loss:0.037807025015354156 norm:0.00023198031703941524 max memory_allocated 47468.9169921875 
[2025-03-23 03:50:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 12 loss:0.0377611443400383 norm:0.0002351376024307683 max memory_allocated 47468.9169921875 
[2025-03-23 03:51:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 13 loss:0.03771234303712845 norm:0.00023221381707116961 max memory_allocated 47468.9169921875 
[2025-03-23 03:53:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 14 loss:0.03774888068437576 norm:0.00024517287965863943 max memory_allocated 47468.9169921875 
[2025-03-23 03:54:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 15 loss:0.037669241428375244 norm:0.00021913016098551452 max memory_allocated 47468.9169921875 
[2025-03-23 03:56:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 16 loss:0.03761909157037735 norm:0.0002275157457916066 max memory_allocated 47468.9169921875 
[2025-03-23 03:57:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 17 loss:0.037613559514284134 norm:0.00023817502369638532 max memory_allocated 47468.9169921875 
[2025-03-23 03:59:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 18 loss:0.0375572107732296 norm:0.00023135155788622797 max memory_allocated 47468.9169921875 
[2025-03-23 04:00:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 19 loss:0.037533726543188095 norm:0.00023633523960597813 max memory_allocated 47468.9169921875 
[2025-03-23 04:02:21 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [8, 9, 10]
[2025-03-23 04:02:21 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [11, 12, 13] ===
[2025-03-23 04:03:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 0 loss:0.08396747708320618 norm:0.0013818331062793732 max memory_allocated 47470.1044921875 
[2025-03-23 04:05:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 1 loss:0.06724072992801666 norm:0.0005881633842363954 max memory_allocated 47470.1044921875 
[2025-03-23 04:06:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 2 loss:0.055275559425354004 norm:0.00034930685069411993 max memory_allocated 47470.1044921875 
[2025-03-23 04:08:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 3 loss:0.049932993948459625 norm:0.0002505746961105615 max memory_allocated 47470.1044921875 
[2025-03-23 04:09:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 4 loss:0.047283269464969635 norm:0.00022618964430876076 max memory_allocated 47470.1044921875 
[2025-03-23 04:10:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 5 loss:0.04567147418856621 norm:0.0002069028269033879 max memory_allocated 47470.1044921875 
[2025-03-23 04:12:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 6 loss:0.044665202498435974 norm:0.00020386210235301405 max memory_allocated 47470.1044921875 
[2025-03-23 04:13:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 7 loss:0.044091515243053436 norm:0.0001977612846530974 max memory_allocated 47470.1044921875 
[2025-03-23 04:15:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 8 loss:0.04372608661651611 norm:0.00019216393411625177 max memory_allocated 47470.1044921875 
[2025-03-23 04:16:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 9 loss:0.043479062616825104 norm:0.00019115512259304523 max memory_allocated 47470.1044921875 
[2025-03-23 04:18:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 10 loss:0.04330732300877571 norm:0.00018559291493147612 max memory_allocated 47470.1044921875 
[2025-03-23 04:19:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 11 loss:0.04313879460096359 norm:0.00018375400395598263 max memory_allocated 47470.1044921875 
[2025-03-23 04:20:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 12 loss:0.04306754097342491 norm:0.00018681497022043914 max memory_allocated 47470.1044921875 
[2025-03-23 04:22:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 13 loss:0.04299325495958328 norm:0.00018685277609620243 max memory_allocated 47470.1044921875 
[2025-03-23 04:23:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 14 loss:0.042908500880002975 norm:0.0001848559477366507 max memory_allocated 47470.1044921875 
[2025-03-23 04:25:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 15 loss:0.04284265637397766 norm:0.00018256639305036515 max memory_allocated 47470.1044921875 
[2025-03-23 04:26:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 16 loss:0.04280546307563782 norm:0.00017941094120033085 max memory_allocated 47470.1044921875 
[2025-03-23 04:28:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 17 loss:0.04274344444274902 norm:0.00017793389270082116 max memory_allocated 47470.1044921875 
[2025-03-23 04:29:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 18 loss:0.04270569980144501 norm:0.00018246175022795796 max memory_allocated 47470.1044921875 
[2025-03-23 04:30:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 19 loss:0.04265502840280533 norm:0.00017726540681906044 max memory_allocated 47470.1044921875 
[2025-03-23 04:32:42 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [11, 12, 13]
[2025-03-23 04:32:42 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [14, 15] ===
[2025-03-23 04:33:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 0 loss:0.07546281069517136 norm:0.0008067898452281952 max memory_allocated 47470.1044921875 
[2025-03-23 04:34:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 1 loss:0.06264474987983704 norm:0.00037696841172873974 max memory_allocated 47470.1044921875 
[2025-03-23 04:35:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 2 loss:0.05304113030433655 norm:0.0002400282974122092 max memory_allocated 47470.1044921875 
[2025-03-23 04:36:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 3 loss:0.04951246827840805 norm:0.00020166070316918194 max memory_allocated 47470.1044921875 
[2025-03-23 04:37:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 4 loss:0.04767411947250366 norm:0.00019177442300133407 max memory_allocated 47470.1044921875 
[2025-03-23 04:38:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 5 loss:0.04662880301475525 norm:0.00016564356337767094 max memory_allocated 47470.1044921875 
[2025-03-23 04:39:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 6 loss:0.046076368540525436 norm:0.00015977829752955586 max memory_allocated 47470.1044921875 
[2025-03-23 04:40:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 7 loss:0.04574950784444809 norm:0.00015595168224535882 max memory_allocated 47470.1044921875 
[2025-03-23 04:41:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 8 loss:0.04549060016870499 norm:0.00014539317635353655 max memory_allocated 47470.1044921875 
[2025-03-23 04:42:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 9 loss:0.0454031266272068 norm:0.00015538246952928603 max memory_allocated 47470.1044921875 
[2025-03-23 04:43:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 10 loss:0.04526441916823387 norm:0.00014205032493919134 max memory_allocated 47470.1044921875 
[2025-03-23 04:44:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 11 loss:0.04517969489097595 norm:0.00013628283340949565 max memory_allocated 47470.1044921875 
[2025-03-23 04:45:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 12 loss:0.045088429003953934 norm:0.00013571392628364265 max memory_allocated 47470.1044921875 
[2025-03-23 04:46:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 13 loss:0.04498932138085365 norm:0.00013310957001522183 max memory_allocated 47470.1044921875 
[2025-03-23 04:47:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 14 loss:0.04490203410387039 norm:0.00012937340943608433 max memory_allocated 47470.1044921875 
[2025-03-23 04:47:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 15 loss:0.04491224139928818 norm:0.00013959048374090344 max memory_allocated 47470.1044921875 
[2025-03-23 04:48:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 16 loss:0.04486323893070221 norm:0.00013272497744765133 max memory_allocated 47470.1044921875 
[2025-03-23 04:49:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 17 loss:0.04479847475886345 norm:0.00012767931912094355 max memory_allocated 47470.1044921875 
[2025-03-23 04:50:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 18 loss:0.04474959522485733 norm:0.00012750345922540873 max memory_allocated 47470.1044921875 
[2025-03-23 04:51:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 19 loss:0.04472684860229492 norm:0.00012619426706805825 max memory_allocated 47470.1044921875 
[2025-03-23 04:52:56 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [14, 15]
[2025-03-23 04:52:56 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [16, 17, 18] ===
[2025-03-23 04:54:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 0 loss:0.12215349078178406 norm:0.0016553819878026843 max memory_allocated 47470.1044921875 
[2025-03-23 04:55:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 1 loss:0.09944001585245132 norm:0.0006085471250116825 max memory_allocated 47470.1044921875 
[2025-03-23 04:57:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 2 loss:0.08162280172109604 norm:0.00034734580549411476 max memory_allocated 47470.1044921875 
[2025-03-23 04:58:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 3 loss:0.07534758001565933 norm:0.0002885541180148721 max memory_allocated 47470.1044921875 
[2025-03-23 05:00:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 4 loss:0.07192946970462799 norm:0.0002654918353073299 max memory_allocated 47470.1044921875 
[2025-03-23 05:01:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 5 loss:0.07023346424102783 norm:0.00025992566952481866 max memory_allocated 47470.1044921875 
[2025-03-23 05:03:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 6 loss:0.06928963214159012 norm:0.0002497827517800033 max memory_allocated 47470.1044921875 
[2025-03-23 05:04:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 7 loss:0.06863947212696075 norm:0.00023983871506061405 max memory_allocated 47470.1044921875 
[2025-03-23 05:05:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 8 loss:0.06823079288005829 norm:0.00023801341012585908 max memory_allocated 47470.1044921875 
[2025-03-23 05:07:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 9 loss:0.0678434744477272 norm:0.00022956958855502307 max memory_allocated 47470.1044921875 
[2025-03-23 05:08:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 10 loss:0.06755182147026062 norm:0.00022397379507310688 max memory_allocated 47470.1044921875 
[2025-03-23 05:10:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 11 loss:0.06735643744468689 norm:0.00022712917416356504 max memory_allocated 47470.1044921875 
[2025-03-23 05:11:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 12 loss:0.06717037409543991 norm:0.00022618485672865063 max memory_allocated 47470.1044921875 
[2025-03-23 05:12:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 13 loss:0.06697266548871994 norm:0.000219418085180223 max memory_allocated 47470.1044921875 
[2025-03-23 05:14:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 14 loss:0.06686815619468689 norm:0.0002225503121735528 max memory_allocated 47470.1044921875 
[2025-03-23 05:15:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 15 loss:0.06670306622982025 norm:0.00021440687123686075 max memory_allocated 47470.1044921875 
[2025-03-23 05:17:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 16 loss:0.06656863540410995 norm:0.00021541480964515358 max memory_allocated 47470.1044921875 
[2025-03-23 05:18:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 17 loss:0.06648901104927063 norm:0.00021411698253359646 max memory_allocated 47470.1044921875 
[2025-03-23 05:20:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 18 loss:0.06639254838228226 norm:0.00021248214761726558 max memory_allocated 47470.1044921875 
[2025-03-23 05:21:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 19 loss:0.06631862372159958 norm:0.00021314896002877504 max memory_allocated 47470.1044921875 
[2025-03-23 05:23:21 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [16, 17, 18]
[2025-03-23 05:23:21 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [19, 20, 21] ===
[2025-03-23 05:24:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 0 loss:0.18352587521076202 norm:0.002454438479617238 max memory_allocated 47470.1044921875 
[2025-03-23 05:26:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 1 loss:0.15242978930473328 norm:0.0008785546524450183 max memory_allocated 47470.1044921875 
[2025-03-23 05:27:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 2 loss:0.12652526795864105 norm:0.0005227446090430021 max memory_allocated 47470.1044921875 
[2025-03-23 05:29:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 3 loss:0.11823055893182755 norm:0.00047506619011983275 max memory_allocated 47470.1044921875 
[2025-03-23 05:30:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 4 loss:0.11411809921264648 norm:0.0004516102490015328 max memory_allocated 47470.1044921875 
[2025-03-23 05:31:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 5 loss:0.11225277185440063 norm:0.00040167124825529754 max memory_allocated 47470.1044921875 
[2025-03-23 05:33:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 6 loss:0.11123082041740417 norm:0.0003903751203324646 max memory_allocated 47470.1044921875 
[2025-03-23 05:34:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 7 loss:0.11053350567817688 norm:0.00039099634159356356 max memory_allocated 47470.1044921875 
[2025-03-23 05:36:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 8 loss:0.10984139889478683 norm:0.00038332826807163656 max memory_allocated 47470.1044921875 
[2025-03-23 05:37:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 9 loss:0.10921388119459152 norm:0.00035707588540390134 max memory_allocated 47470.1044921875 
[2025-03-23 05:39:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 10 loss:0.1087237223982811 norm:0.00034740378032438457 max memory_allocated 47470.1044921875 
[2025-03-23 05:40:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 11 loss:0.10825133323669434 norm:0.00034130056155845523 max memory_allocated 47470.1044921875 
[2025-03-23 05:41:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 12 loss:0.10786004364490509 norm:0.00032787659438326955 max memory_allocated 47470.1044921875 
[2025-03-23 05:43:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 13 loss:0.10754333436489105 norm:0.0003368606267031282 max memory_allocated 47470.1044921875 
[2025-03-23 05:44:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 14 loss:0.10731825232505798 norm:0.0003297371731605381 max memory_allocated 47470.1044921875 
[2025-03-23 05:46:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 15 loss:0.1070704311132431 norm:0.0003309715539216995 max memory_allocated 47470.1044921875 
[2025-03-23 05:47:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 16 loss:0.1068161204457283 norm:0.0003254800394643098 max memory_allocated 47470.1044921875 
[2025-03-23 05:49:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 17 loss:0.10665477812290192 norm:0.00031535301241092384 max memory_allocated 47470.1044921875 
[2025-03-23 05:50:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 18 loss:0.10648138076066971 norm:0.0003178596671205014 max memory_allocated 47470.1044921875 
[2025-03-23 05:51:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 19 loss:0.10632892698049545 norm:0.000314759963657707 max memory_allocated 47470.1044921875 
[2025-03-23 05:53:42 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [19, 20, 21]
[2025-03-23 05:53:42 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [22, 23, 24] ===
[2025-03-23 05:55:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 0 loss:0.2651216387748718 norm:0.0025273493956774473 max memory_allocated 47470.1044921875 
[2025-03-23 05:56:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 1 loss:0.22439034283161163 norm:0.0009804037399590015 max memory_allocated 47470.1044921875 
[2025-03-23 05:58:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 2 loss:0.18875177204608917 norm:0.0005583463935181499 max memory_allocated 47470.1044921875 
[2025-03-23 05:59:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 3 loss:0.17801018059253693 norm:0.0004884428344666958 max memory_allocated 47470.1044921875 
[2025-03-23 06:00:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 4 loss:0.17360982298851013 norm:0.00044053312740288675 max memory_allocated 47470.1044921875 
[2025-03-23 06:02:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 5 loss:0.17181387543678284 norm:0.0004228566540405154 max memory_allocated 47470.1044921875 
[2025-03-23 06:03:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 6 loss:0.1705303192138672 norm:0.00041717709973454475 max memory_allocated 47470.1044921875 
[2025-03-23 06:05:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 7 loss:0.16954703629016876 norm:0.0004019006446469575 max memory_allocated 47470.1044921875 
[2025-03-23 06:06:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 8 loss:0.16874803602695465 norm:0.0003966851800214499 max memory_allocated 47470.1044921875 
[2025-03-23 06:08:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 9 loss:0.16807669401168823 norm:0.0003919150331057608 max memory_allocated 47470.1044921875 
[2025-03-23 06:09:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 10 loss:0.16741546988487244 norm:0.00038438031333498657 max memory_allocated 47470.1044921875 
[2025-03-23 06:10:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 11 loss:0.16680094599723816 norm:0.00037775689270347357 max memory_allocated 47470.1044921875 
[2025-03-23 06:12:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 12 loss:0.16634738445281982 norm:0.000373297167243436 max memory_allocated 47470.1044921875 
[2025-03-23 06:13:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 13 loss:0.16593031585216522 norm:0.0003745013091247529 max memory_allocated 47470.1044921875 
[2025-03-23 06:15:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 14 loss:0.16552066802978516 norm:0.00037567297113128006 max memory_allocated 47470.1044921875 
[2025-03-23 06:16:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 15 loss:0.1651853621006012 norm:0.0003636484907474369 max memory_allocated 47470.1044921875 
[2025-03-23 06:17:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 16 loss:0.1648286134004593 norm:0.0003660063957795501 max memory_allocated 47470.1044921875 
[2025-03-23 06:19:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 17 loss:0.16452138125896454 norm:0.00036088694469071925 max memory_allocated 47470.1044921875 
[2025-03-23 06:20:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 18 loss:0.16433970630168915 norm:0.00038052996387705207 max memory_allocated 47470.1044921875 
[2025-03-23 06:22:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 19 loss:0.16406968235969543 norm:0.00036145569174550474 max memory_allocated 47470.1044921875 
[2025-03-23 06:23:59 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [22, 23, 24]
[2025-03-23 06:23:59 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [25, 26] ===
[2025-03-23 06:25:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 0 loss:0.2812176048755646 norm:0.0024820519611239433 max memory_allocated 47470.1044921875 
[2025-03-23 06:25:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 1 loss:0.24951612949371338 norm:0.0009407688048668206 max memory_allocated 47470.1044921875 
[2025-03-23 06:26:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 2 loss:0.22070594131946564 norm:0.0003950825484935194 max memory_allocated 47470.1044921875 
[2025-03-23 06:27:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 3 loss:0.2131475806236267 norm:0.0003648417186923325 max memory_allocated 47470.1044921875 
[2025-03-23 06:28:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 4 loss:0.2112082540988922 norm:0.0003351444029249251 max memory_allocated 47470.1044921875 
[2025-03-23 06:29:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 5 loss:0.21018214523792267 norm:0.00030693085864186287 max memory_allocated 47470.1044921875 
[2025-03-23 06:30:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 6 loss:0.2094786912202835 norm:0.0002940727863460779 max memory_allocated 47470.1044921875 
[2025-03-23 06:31:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 7 loss:0.2089226394891739 norm:0.0002843636611942202 max memory_allocated 47470.1044921875 
[2025-03-23 06:32:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 8 loss:0.20836912095546722 norm:0.0002655447751749307 max memory_allocated 47470.1044921875 
[2025-03-23 06:33:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 9 loss:0.20780646800994873 norm:0.00026027890271507204 max memory_allocated 47470.1044921875 
[2025-03-23 06:34:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 10 loss:0.207443505525589 norm:0.0002692592388484627 max memory_allocated 47470.1044921875 
[2025-03-23 06:35:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 11 loss:0.2070607841014862 norm:0.00026381967472843826 max memory_allocated 47470.1044921875 
[2025-03-23 06:36:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 12 loss:0.20672988891601562 norm:0.0002582724264357239 max memory_allocated 47470.1044921875 
[2025-03-23 06:37:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 13 loss:0.20640668272972107 norm:0.0002558949345257133 max memory_allocated 47470.1044921875 
[2025-03-23 06:38:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 14 loss:0.20617511868476868 norm:0.0002580283326096833 max memory_allocated 47470.1044921875 
[2025-03-23 06:39:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 15 loss:0.205906942486763 norm:0.0002517349785193801 max memory_allocated 47470.1044921875 
[2025-03-23 06:40:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 16 loss:0.20565150678157806 norm:0.00025043266941793263 max memory_allocated 47470.1044921875 
[2025-03-23 06:41:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 17 loss:0.20550459623336792 norm:0.00024858792312443256 max memory_allocated 47470.1044921875 
[2025-03-23 06:42:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 18 loss:0.20542962849140167 norm:0.00025058063329197466 max memory_allocated 47470.1044921875 
[2025-03-23 06:43:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 19 loss:0.2052288055419922 norm:0.0002503542636986822 max memory_allocated 47470.1044921875 
[2025-03-23 06:44:13 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [25, 26]
[2025-03-23 06:44:13 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [27] ===
[2025-03-23 06:44:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 0 loss:0.256202757358551 norm:0.0006280116504058242 max memory_allocated 47470.1044921875 
[2025-03-23 06:45:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 1 loss:0.2412719875574112 norm:0.0004044384404551238 max memory_allocated 47470.1044921875 
[2025-03-23 06:45:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 2 loss:0.22773531079292297 norm:0.00026960845571011305 max memory_allocated 47470.1044921875 
[2025-03-23 06:46:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 3 loss:0.22440555691719055 norm:0.0002319979394087568 max memory_allocated 47470.1044921875 
[2025-03-23 06:46:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 4 loss:0.22361068427562714 norm:0.00022779837308917195 max memory_allocated 47470.1044921875 
[2025-03-23 06:47:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 5 loss:0.22321175038814545 norm:0.0002110671193804592 max memory_allocated 47470.1044921875 
[2025-03-23 06:47:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 6 loss:0.22288323938846588 norm:0.00019607716239988804 max memory_allocated 47470.1044921875 
[2025-03-23 06:48:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 7 loss:0.2225847989320755 norm:0.0001880499767139554 max memory_allocated 47470.1044921875 
[2025-03-23 06:48:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 8 loss:0.22238485515117645 norm:0.00017667030624579638 max memory_allocated 47470.1044921875 
[2025-03-23 06:49:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 9 loss:0.2221900075674057 norm:0.00018006324535235763 max memory_allocated 47470.1044921875 
[2025-03-23 06:49:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 10 loss:0.2220117151737213 norm:0.00016874987340997905 max memory_allocated 47470.1044921875 
[2025-03-23 06:49:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 11 loss:0.22179603576660156 norm:0.00017203891184180975 max memory_allocated 47470.1044921875 
[2025-03-23 06:50:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 12 loss:0.22163192927837372 norm:0.00016501807840541005 max memory_allocated 47470.1044921875 
[2025-03-23 06:50:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 13 loss:0.22148802876472473 norm:0.00016685803711879998 max memory_allocated 47470.1044921875 
[2025-03-23 06:51:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 14 loss:0.22136762738227844 norm:0.0001628693426027894 max memory_allocated 47470.1044921875 
[2025-03-23 06:51:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 15 loss:0.22129899263381958 norm:0.00016734747623559088 max memory_allocated 47470.1044921875 
[2025-03-23 06:52:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 16 loss:0.22120055556297302 norm:0.00016316860273946077 max memory_allocated 47470.1044921875 
[2025-03-23 06:52:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 17 loss:0.2211136370897293 norm:0.00016087554104160517 max memory_allocated 47470.1044921875 
[2025-03-23 06:53:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 18 loss:0.22106140851974487 norm:0.00015925928892102093 max memory_allocated 47470.1044921875 
[2025-03-23 06:53:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 19 loss:0.22102175652980804 norm:0.00016583804972469807 max memory_allocated 47470.1044921875 
[2025-03-23 06:54:22 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [27]
[2025-03-23 06:54:22 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [28] ===
[2025-03-23 06:54:22 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 06:54:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 0 loss:0.30159634351730347 norm:0.01734582521021366 max memory_allocated 47470.1044921875 
[2025-03-23 06:55:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 1 loss:0.28198495507240295 norm:0.013599117286503315 max memory_allocated 47470.1044921875 
[2025-03-23 06:55:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 2 loss:0.2653470039367676 norm:0.009034981951117516 max memory_allocated 47470.1044921875 
[2025-03-23 06:56:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 3 loss:0.26102814078330994 norm:0.007693230174481869 max memory_allocated 47470.1044921875 
[2025-03-23 06:56:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 4 loss:0.25988253951072693 norm:0.006678509525954723 max memory_allocated 47470.1044921875 
[2025-03-23 06:57:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 5 loss:0.2591273784637451 norm:0.005763445980846882 max memory_allocated 47470.1044921875 
[2025-03-23 06:57:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 6 loss:0.2585732936859131 norm:0.00492660328745842 max memory_allocated 47470.1044921875 
[2025-03-23 06:58:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 7 loss:0.2581964433193207 norm:0.004571824800223112 max memory_allocated 47470.1044921875 
[2025-03-23 06:58:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 8 loss:0.25798362493515015 norm:0.004500304814428091 max memory_allocated 47470.1044921875 
[2025-03-23 06:59:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 9 loss:0.2577357292175293 norm:0.004485143814235926 max memory_allocated 47470.1044921875 
[2025-03-23 06:59:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 10 loss:0.2575535178184509 norm:0.0040412768721580505 max memory_allocated 47470.1044921875 
[2025-03-23 07:00:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 11 loss:0.25714537501335144 norm:0.0038739643059670925 max memory_allocated 47470.1044921875 
[2025-03-23 07:00:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 12 loss:0.2569097876548767 norm:0.003576636780053377 max memory_allocated 47470.1044921875 
[2025-03-23 07:01:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 13 loss:0.2567884027957916 norm:0.003618214512243867 max memory_allocated 47470.1044921875 
[2025-03-23 07:01:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 14 loss:0.2566528022289276 norm:0.0034852465614676476 max memory_allocated 47470.1044921875 
[2025-03-23 07:02:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 15 loss:0.2564980983734131 norm:0.0034383407328277826 max memory_allocated 47470.1044921875 
[2025-03-23 07:02:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 16 loss:0.25633642077445984 norm:0.0032749015372246504 max memory_allocated 47470.1044921875 
[2025-03-23 07:03:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 17 loss:0.25620341300964355 norm:0.0032863193191587925 max memory_allocated 47470.1044921875 
[2025-03-23 07:03:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 18 loss:0.2561168968677521 norm:0.003145385766401887 max memory_allocated 47470.1044921875 
[2025-03-23 07:03:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 19 loss:0.2560030221939087 norm:0.0031839811708778143 max memory_allocated 47470.1044921875 
[2025-03-23 07:04:35 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [28]
[2025-03-23 07:04:35 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [29] ===
[2025-03-23 07:04:35 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 07:05:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 0 loss:0.34828025102615356 norm:0.019795432686805725 max memory_allocated 47470.1044921875 
[2025-03-23 07:05:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 1 loss:0.3262879252433777 norm:0.013995463959872723 max memory_allocated 47470.1044921875 
[2025-03-23 07:06:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 2 loss:0.3075614869594574 norm:0.009151189588010311 max memory_allocated 47470.1044921875 
[2025-03-23 07:06:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 3 loss:0.3032715916633606 norm:0.007762922905385494 max memory_allocated 47470.1044921875 
[2025-03-23 07:07:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 4 loss:0.30213212966918945 norm:0.006730652879923582 max memory_allocated 47470.1044921875 
[2025-03-23 07:07:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 5 loss:0.30132925510406494 norm:0.005790152121335268 max memory_allocated 47470.1044921875 
[2025-03-23 07:07:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 6 loss:0.30069664120674133 norm:0.004982528742402792 max memory_allocated 47470.1044921875 
[2025-03-23 07:08:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 7 loss:0.30026575922966003 norm:0.004578833002597094 max memory_allocated 47470.1044921875 
[2025-03-23 07:08:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 8 loss:0.3001173734664917 norm:0.004559076391160488 max memory_allocated 47470.1044921875 
[2025-03-23 07:09:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 9 loss:0.29976895451545715 norm:0.004439168609678745 max memory_allocated 47470.1044921875 
[2025-03-23 07:09:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 10 loss:0.29949355125427246 norm:0.003981493413448334 max memory_allocated 47470.1044921875 
[2025-03-23 07:10:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 11 loss:0.29923558235168457 norm:0.004086621105670929 max memory_allocated 47470.1044921875 
[2025-03-23 07:10:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 12 loss:0.2990291118621826 norm:0.003756412770599127 max memory_allocated 47470.1044921875 
[2025-03-23 07:11:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 13 loss:0.29884085059165955 norm:0.003909291233867407 max memory_allocated 47470.1044921875 
[2025-03-23 07:11:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 14 loss:0.2986988425254822 norm:0.0035510880406945944 max memory_allocated 47470.1044921875 
[2025-03-23 07:12:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 15 loss:0.2985498309135437 norm:0.00370386173017323 max memory_allocated 47470.1044921875 
[2025-03-23 07:12:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 16 loss:0.29839324951171875 norm:0.00333189545199275 max memory_allocated 47470.1044921875 
[2025-03-23 07:13:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 17 loss:0.298275351524353 norm:0.003513800911605358 max memory_allocated 47470.1044921875 
[2025-03-23 07:13:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 18 loss:0.2982269525527954 norm:0.003319208277389407 max memory_allocated 47470.1044921875 
[2025-03-23 07:14:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 19 loss:0.2981005311012268 norm:0.0034751808270812035 max memory_allocated 47470.1044921875 
[2025-03-23 07:14:47 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [29]
[2025-03-23 07:14:47 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [30] ===
[2025-03-23 07:14:48 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 07:15:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 0 loss:0.4687088131904602 norm:0.03421274945139885 max memory_allocated 47470.1044921875 
[2025-03-23 07:15:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 1 loss:0.4261382818222046 norm:0.022403808310627937 max memory_allocated 47470.1044921875 
[2025-03-23 07:16:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 2 loss:0.3918769955635071 norm:0.015019568614661694 max memory_allocated 47470.1044921875 
[2025-03-23 07:16:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 3 loss:0.3850870132446289 norm:0.013170366175472736 max memory_allocated 47470.1044921875 
[2025-03-23 07:17:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 4 loss:0.38258692622184753 norm:0.011150697246193886 max memory_allocated 47470.1044921875 
[2025-03-23 07:17:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 5 loss:0.3808095455169678 norm:0.00969616323709488 max memory_allocated 47470.1044921875 
[2025-03-23 07:18:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 6 loss:0.3795648217201233 norm:0.008513987064361572 max memory_allocated 47470.1044921875 
[2025-03-23 07:18:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 7 loss:0.3785952925682068 norm:0.007774593774229288 max memory_allocated 47470.1044921875 
[2025-03-23 07:19:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 8 loss:0.3779928982257843 norm:0.007648201659321785 max memory_allocated 47470.1044921875 
[2025-03-23 07:19:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 9 loss:0.37779679894447327 norm:0.007704637013375759 max memory_allocated 47470.1044921875 
[2025-03-23 07:20:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 10 loss:0.3768671452999115 norm:0.006569116376340389 max memory_allocated 47470.1044921875 
[2025-03-23 07:20:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 11 loss:0.3764244616031647 norm:0.0061534675769507885 max memory_allocated 47470.1044921875 
[2025-03-23 07:21:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 12 loss:0.3760746121406555 norm:0.006357083097100258 max memory_allocated 47470.1044921875 
[2025-03-23 07:21:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 13 loss:0.3758690655231476 norm:0.0063710599206388 max memory_allocated 47470.1044921875 
[2025-03-23 07:22:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 14 loss:0.3754615783691406 norm:0.006070874631404877 max memory_allocated 47470.1044921875 
[2025-03-23 07:22:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 15 loss:0.3756639063358307 norm:0.005654545966535807 max memory_allocated 47470.1044921875 
[2025-03-23 07:22:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 16 loss:0.37503302097320557 norm:0.005931146442890167 max memory_allocated 47470.1044921875 
[2025-03-23 07:23:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 17 loss:0.3748142123222351 norm:0.0058158813044428825 max memory_allocated 47470.1044921875 
[2025-03-23 07:23:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 18 loss:0.374539852142334 norm:0.005612164735794067 max memory_allocated 47470.1044921875 
[2025-03-23 07:24:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 19 loss:0.37451601028442383 norm:0.0055622523650527 max memory_allocated 47470.1044921875 
[2025-03-23 07:25:02 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [30]
[2025-03-23 07:25:02 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [31] ===
[2025-03-23 07:25:02 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 07:25:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 0 loss:0.806845486164093 norm:0.0851806253194809 max memory_allocated 47470.1044921875 
[2025-03-23 07:26:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 1 loss:0.7141085267066956 norm:0.0583612360060215 max memory_allocated 47470.1044921875 
[2025-03-23 07:26:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 2 loss:0.6481646299362183 norm:0.038221292197704315 max memory_allocated 47470.1044921875 
[2025-03-23 07:26:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 3 loss:0.632391095161438 norm:0.03433525562286377 max memory_allocated 47470.1044921875 
[2025-03-23 07:27:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 4 loss:0.624595582485199 norm:0.030838076025247574 max memory_allocated 47470.1044921875 
[2025-03-23 07:27:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 5 loss:0.6189351677894592 norm:0.027512326836586 max memory_allocated 47470.1044921875 
[2025-03-23 07:28:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 6 loss:0.6152431964874268 norm:0.025424731895327568 max memory_allocated 47470.1044921875 
[2025-03-23 07:28:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 7 loss:0.6124461889266968 norm:0.02403233014047146 max memory_allocated 47470.1044921875 
[2025-03-23 07:29:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 8 loss:0.6100396513938904 norm:0.023039203137159348 max memory_allocated 47470.1044921875 
[2025-03-23 07:29:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 9 loss:0.6080136895179749 norm:0.0220306646078825 max memory_allocated 47470.1044921875 
[2025-03-23 07:30:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 10 loss:0.606328010559082 norm:0.020864248275756836 max memory_allocated 47470.1044921875 
[2025-03-23 07:30:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 11 loss:0.6054846048355103 norm:0.021371634677052498 max memory_allocated 47470.1044921875 
[2025-03-23 07:31:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 12 loss:0.6054099202156067 norm:0.02198229357600212 max memory_allocated 47470.1044921875 
[2025-03-23 07:31:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 13 loss:0.6044065952301025 norm:0.02153971418738365 max memory_allocated 47470.1044921875 
[2025-03-23 07:32:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 14 loss:0.6025004386901855 norm:0.01922568306326866 max memory_allocated 47470.1044921875 
[2025-03-23 07:32:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 15 loss:0.6015021204948425 norm:0.018439866602420807 max memory_allocated 47470.1044921875 
[2025-03-23 07:33:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 16 loss:0.6005664467811584 norm:0.017950160428881645 max memory_allocated 47470.1044921875 
[2025-03-23 07:33:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 17 loss:0.6002609729766846 norm:0.018009955063462257 max memory_allocated 47470.1044921875 
[2025-03-23 07:34:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 18 loss:0.5989067554473877 norm:0.016743076965212822 max memory_allocated 47470.1044921875 
[2025-03-23 07:34:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 19 loss:0.5990001559257507 norm:0.017454184591770172 max memory_allocated 47470.1044921875 
[2025-03-23 07:35:15 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 15, block: [31]
[2025-03-23 07:35:15 root] (main_calib_config3_attn.py 379): INFO 19469.990778923035
[2025-03-23 07:35:20 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-23 07:36:10 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.826959609985352
[2025-03-23 07:36:10 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-23 07:37:28 root] (main_calib_config3_attn.py 161): INFO c4 : 7.297031402587891
[2025-03-23 08:17:49 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.826959609985352, 'c4': 7.297031402587891, 'results': {'hellaswag': {'acc': 0.5552678749253137, 'acc_stderr': 0.0049592047730462096, 'acc_norm': 0.7154949213304123, 'acc_norm_stderr': 0.004502563079349394}, 'arc_easy': {'acc': 0.6717171717171717, 'acc_stderr': 0.009635749509262161, 'acc_norm': 0.5273569023569024, 'acc_norm_stderr': 0.010244415164390534}, 'winogrande': {'acc': 0.6614048934490924, 'acc_stderr': 0.013300169865842409}, 'arc_challenge': {'acc': 0.3796928327645051, 'acc_stderr': 0.014182119866974874, 'acc_norm': 0.3916382252559727, 'acc_norm_stderr': 0.014264122124938213}, 'boolq': {'acc': 0.7302752293577982, 'acc_stderr': 0.0077624039763634954}, 'piqa': {'acc': 0.7812840043525572, 'acc_stderr': 0.009644731932667563, 'acc_norm': 0.7774755168661589, 'acc_norm_stderr': 0.009704600975718227}}, 'versions': {'hellaswag': 0, 'arc_easy': 0, 'winogrande': 0, 'arc_challenge': 0, 'boolq': 1, 'piqa': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-23 08:17:49 root] (main_calib_config3_attn.py 175): INFO 37.97,67.17,73.03,55.53,78.13,66.14
