[2025-03-20 14:58:56 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/Llama-2-7b-hf-0.6', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.6.pkl', blocks_pkl='./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-20 14:59:03 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-20 14:59:03 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-20 14:59:03 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-20 14:59:03 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.6.pkl
[2025-03-20 14:59:03 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 9), (9, 11), (11, 14), (14, 17), (17, 20), (20, 23), (23, 26), (26, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-20 14:59:03 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12, 13], [14, 15, 16], [17, 18, 19], [20, 21, 22], [23, 24, 25], [26, 27, 28], [29], [30], [31]]
[2025-03-20 14:59:04 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-20 14:59:05 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 14:59:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.007437864784151316 norm:0.010786831378936768 max memory_allocated 34633.880859375 
[2025-03-20 15:00:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.003918416798114777 norm:0.0058749085292220116 max memory_allocated 34633.880859375 
[2025-03-20 15:00:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.0025967005640268326 norm:0.004060646519064903 max memory_allocated 34633.880859375 
[2025-03-20 15:00:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0020959165412932634 norm:0.00316709466278553 max memory_allocated 34633.880859375 
[2025-03-20 15:01:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.0020001851953566074 norm:0.0027291765436530113 max memory_allocated 34633.880859375 
[2025-03-20 15:01:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0019195594359189272 norm:0.002377921249717474 max memory_allocated 34633.880859375 
[2025-03-20 15:02:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.0018879646668210626 norm:0.0021738240029662848 max memory_allocated 34633.880859375 
[2025-03-20 15:02:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0018582514021545649 norm:0.002010863972827792 max memory_allocated 34633.880859375 
[2025-03-20 15:03:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0018290472216904163 norm:0.0018726822454482317 max memory_allocated 34633.880859375 
[2025-03-20 15:03:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.0017927674343809485 norm:0.001689469558186829 max memory_allocated 34633.880859375 
[2025-03-20 15:04:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.001764196204021573 norm:0.001499031437560916 max memory_allocated 34633.880859375 
[2025-03-20 15:04:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.001742298249155283 norm:0.0013470585690811276 max memory_allocated 34633.880859375 
[2025-03-20 15:04:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.0017432206077501178 norm:0.0012005571043118834 max memory_allocated 34633.880859375 
[2025-03-20 15:05:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.0016976927872747183 norm:0.0011174739338457584 max memory_allocated 34633.880859375 
[2025-03-20 15:05:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0016725577879697084 norm:0.0010224863654002547 max memory_allocated 34633.880859375 
[2025-03-20 15:06:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.0016608581645414233 norm:0.0009453176171518862 max memory_allocated 34633.880859375 
[2025-03-20 15:06:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.00165550597012043 norm:0.0008763661026023328 max memory_allocated 34633.880859375 
[2025-03-20 15:07:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0016499269986525178 norm:0.0008096913225017488 max memory_allocated 34633.880859375 
[2025-03-20 15:07:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.0016486106906086206 norm:0.0008058587554842234 max memory_allocated 34633.880859375 
[2025-03-20 15:08:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.001640605041757226 norm:0.0007149410666897893 max memory_allocated 34633.880859375 
[2025-03-20 15:08:49 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-20 15:08:49 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-20 15:08:50 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 15:09:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.026330988854169846 norm:0.023578688502311707 max memory_allocated 35100.7724609375 
[2025-03-20 15:09:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.015415270812809467 norm:0.013706665486097336 max memory_allocated 35100.7724609375 
[2025-03-20 15:10:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.011386449448764324 norm:0.011874880641698837 max memory_allocated 35100.7724609375 
[2025-03-20 15:10:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.010168690234422684 norm:0.01237217802554369 max memory_allocated 35100.7724609375 
[2025-03-20 15:11:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.009086390025913715 norm:0.008543797768652439 max memory_allocated 35100.7724609375 
[2025-03-20 15:11:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.008821278810501099 norm:0.007234693504869938 max memory_allocated 35100.7724609375 
[2025-03-20 15:12:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.008623220026493073 norm:0.006870472803711891 max memory_allocated 35100.7724609375 
[2025-03-20 15:12:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.008872030302882195 norm:0.006301824934780598 max memory_allocated 35100.7724609375 
[2025-03-20 15:12:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.00815627072006464 norm:0.0057427347637712955 max memory_allocated 35100.7724609375 
[2025-03-20 15:13:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.008121348917484283 norm:0.0053069740533828735 max memory_allocated 35100.7724609375 
[2025-03-20 15:13:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.008179309777915478 norm:0.005343423690646887 max memory_allocated 35100.7724609375 
[2025-03-20 15:14:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.008082101121544838 norm:0.0054300385527312756 max memory_allocated 35100.7724609375 
[2025-03-20 15:14:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.00818515382707119 norm:0.0049406080506742 max memory_allocated 35100.7724609375 
[2025-03-20 15:15:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.008047044277191162 norm:0.0050817797891795635 max memory_allocated 35100.7724609375 
[2025-03-20 15:15:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.007773836143314838 norm:0.004749900661408901 max memory_allocated 35100.7724609375 
[2025-03-20 15:16:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.00827943254262209 norm:0.0050439778715372086 max memory_allocated 35100.7724609375 
[2025-03-20 15:16:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.008878378197550774 norm:0.005168274976313114 max memory_allocated 35100.7724609375 
[2025-03-20 15:16:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.008255359716713428 norm:0.004676789976656437 max memory_allocated 35100.7724609375 
[2025-03-20 15:17:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.008062542416155338 norm:0.004528728313744068 max memory_allocated 35100.7724609375 
[2025-03-20 15:17:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.008784678764641285 norm:0.004995940253138542 max memory_allocated 35100.7724609375 
[2025-03-20 15:18:31 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-20 15:18:31 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
[2025-03-20 15:18:32 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 15:19:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.016831208020448685 norm:0.005023849196732044 max memory_allocated 35101.8349609375 
[2025-03-20 15:19:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.012061874382197857 norm:0.0038202577270567417 max memory_allocated 35101.8349609375 
[2025-03-20 15:19:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.010090971365571022 norm:0.00293390522710979 max memory_allocated 35101.8349609375 
[2025-03-20 15:20:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.009525016881525517 norm:0.002533724531531334 max memory_allocated 35101.8349609375 
[2025-03-20 15:20:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.009097645990550518 norm:0.002202474046498537 max memory_allocated 35101.8349609375 
[2025-03-20 15:21:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.008758019655942917 norm:0.0019221690017729998 max memory_allocated 35101.8349609375 
[2025-03-20 15:21:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.008572428487241268 norm:0.0016483241925016046 max memory_allocated 35101.8349609375 
[2025-03-20 15:22:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.008503951132297516 norm:0.0014088088646531105 max memory_allocated 35101.8349609375 
[2025-03-20 15:22:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.008465786464512348 norm:0.0012080114101991057 max memory_allocated 35101.8349609375 
[2025-03-20 15:23:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.008439176715910435 norm:0.000999956508167088 max memory_allocated 35101.8349609375 
[2025-03-20 15:23:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.008406206034123898 norm:0.0008160801371559501 max memory_allocated 35101.8349609375 
[2025-03-20 15:24:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.008380232378840446 norm:0.0006904472829774022 max memory_allocated 35101.8349609375 
[2025-03-20 15:24:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.00838007964193821 norm:0.0006948552909307182 max memory_allocated 35101.8349609375 
[2025-03-20 15:24:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.008448107168078423 norm:0.000838541251141578 max memory_allocated 35101.8349609375 
[2025-03-20 15:25:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.00843137688934803 norm:0.0007727273623459041 max memory_allocated 35101.8349609375 
[2025-03-20 15:25:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.008397869765758514 norm:0.0006729781161993742 max memory_allocated 35101.8349609375 
[2025-03-20 15:26:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.008385601453483105 norm:0.000611672701779753 max memory_allocated 35101.8349609375 
[2025-03-20 15:26:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.008336818777024746 norm:0.0005574295064434409 max memory_allocated 35101.8349609375 
[2025-03-20 15:27:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.008376821875572205 norm:0.0005667968653142452 max memory_allocated 35101.8349609375 
[2025-03-20 15:27:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.008345779962837696 norm:0.0005329724517650902 max memory_allocated 35101.8349609375 
[2025-03-20 15:28:12 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2]
[2025-03-20 15:28:12 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4, 5] ===
[2025-03-20 15:29:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 0 loss:0.04940664395689964 norm:0.001015647780150175 max memory_allocated 47477.6044921875 
[2025-03-20 15:31:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 1 loss:0.03548568859696388 norm:0.00040917727164924145 max memory_allocated 47477.6044921875 
[2025-03-20 15:32:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 2 loss:0.027292750775814056 norm:0.00026059054653160274 max memory_allocated 47477.6044921875 
[2025-03-20 15:33:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 3 loss:0.024246821179986 norm:0.00023419316858053207 max memory_allocated 47477.6044921875 
[2025-03-20 15:35:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 4 loss:0.022535383701324463 norm:0.00022489676484838128 max memory_allocated 47477.6044921875 
[2025-03-20 15:36:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 5 loss:0.0214813482016325 norm:0.000224075629375875 max memory_allocated 47477.6044921875 
[2025-03-20 15:37:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 6 loss:0.020767483860254288 norm:0.00022132036974653602 max memory_allocated 47477.6044921875 
[2025-03-20 15:39:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 7 loss:0.02034750208258629 norm:0.00022523518418893218 max memory_allocated 47477.6044921875 
[2025-03-20 15:40:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 8 loss:0.020109597593545914 norm:0.00025523625663481653 max memory_allocated 47477.6044921875 
[2025-03-20 15:41:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 9 loss:0.019954707473516464 norm:0.00031246955040842295 max memory_allocated 47477.6044921875 
[2025-03-20 15:43:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 10 loss:0.01987409219145775 norm:0.00032798724714666605 max memory_allocated 47477.6044921875 
[2025-03-20 15:44:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 11 loss:0.019740555435419083 norm:0.0002981945581268519 max memory_allocated 47477.6044921875 
[2025-03-20 15:45:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 12 loss:0.01969391666352749 norm:0.00031127105467021465 max memory_allocated 47477.6044921875 
[2025-03-20 15:47:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 13 loss:0.019643893465399742 norm:0.00027041163411922753 max memory_allocated 47477.6044921875 
[2025-03-20 15:48:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 14 loss:0.019573960453271866 norm:0.0002798251807689667 max memory_allocated 47477.6044921875 
[2025-03-20 15:49:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 15 loss:0.019519072026014328 norm:0.00031181523809209466 max memory_allocated 47477.6044921875 
[2025-03-20 15:51:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 16 loss:0.01949683204293251 norm:0.0003266302519477904 max memory_allocated 47477.6044921875 
[2025-03-20 15:52:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 17 loss:0.01950451359152794 norm:0.00033164123306050897 max memory_allocated 47477.6044921875 
[2025-03-20 15:53:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 18 loss:0.019515039399266243 norm:0.0003242470556870103 max memory_allocated 47477.6044921875 
[2025-03-20 15:55:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 19 loss:0.01944536529481411 norm:0.00030575369601137936 max memory_allocated 47477.6044921875 
[2025-03-20 15:56:43 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [3, 4, 5]
[2025-03-20 15:56:43 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [6, 7, 8] ===
[2025-03-20 15:58:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 0 loss:0.06348370015621185 norm:0.001258740434423089 max memory_allocated 47477.7919921875 
[2025-03-20 15:59:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 1 loss:0.04735435172915459 norm:0.000589233182836324 max memory_allocated 47477.7919921875 
[2025-03-20 16:00:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 2 loss:0.03693389147520065 norm:0.00035755906719714403 max memory_allocated 47477.7919921875 
[2025-03-20 16:02:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 3 loss:0.03294288367033005 norm:0.00031890967511571944 max memory_allocated 47477.7919921875 
[2025-03-20 16:03:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 4 loss:0.03076808527112007 norm:0.00025678862584754825 max memory_allocated 47477.7919921875 
[2025-03-20 16:04:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 5 loss:0.029332369565963745 norm:0.00023524335119873285 max memory_allocated 47477.7919921875 
[2025-03-20 16:06:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 6 loss:0.028458591550588608 norm:0.00023953753407113254 max memory_allocated 47477.7919921875 
[2025-03-20 16:07:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 7 loss:0.027900224551558495 norm:0.0002291118144057691 max memory_allocated 47477.7919921875 
[2025-03-20 16:08:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 8 loss:0.02752523496747017 norm:0.00022139595239423215 max memory_allocated 47477.7919921875 
[2025-03-20 16:10:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 9 loss:0.027265887707471848 norm:0.00021569611271843314 max memory_allocated 47477.7919921875 
[2025-03-20 16:11:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 10 loss:0.02708905190229416 norm:0.00022228260058909655 max memory_allocated 47477.7919921875 
[2025-03-20 16:12:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 11 loss:0.026953084394335747 norm:0.00021799479145556688 max memory_allocated 47477.7919921875 
[2025-03-20 16:14:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 12 loss:0.026816071942448616 norm:0.0002091666974592954 max memory_allocated 47477.7919921875 
[2025-03-20 16:15:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 13 loss:0.026745952665805817 norm:0.00021338858641684055 max memory_allocated 47477.7919921875 
[2025-03-20 16:16:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 14 loss:0.026682091876864433 norm:0.00021330639719963074 max memory_allocated 47477.7919921875 
[2025-03-20 16:18:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 15 loss:0.026571469381451607 norm:0.00020011440210510045 max memory_allocated 47477.7919921875 
[2025-03-20 16:19:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 16 loss:0.026534996926784515 norm:0.00020489963935688138 max memory_allocated 47477.7919921875 
[2025-03-20 16:20:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 17 loss:0.026514975354075432 norm:0.00021897570695728064 max memory_allocated 47477.7919921875 
[2025-03-20 16:22:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 18 loss:0.026440106332302094 norm:0.00021180190378800035 max memory_allocated 47477.7919921875 
[2025-03-20 16:23:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 19 loss:0.026368338614702225 norm:0.00019765282922890037 max memory_allocated 47477.7919921875 
[2025-03-20 16:25:13 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [6, 7, 8]
[2025-03-20 16:25:13 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [9, 10] ===
[2025-03-20 16:26:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 0 loss:0.04830872640013695 norm:0.0005114162340760231 max memory_allocated 47477.7919921875 
[2025-03-20 16:27:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 1 loss:0.038435257971286774 norm:0.000327180081512779 max memory_allocated 47477.7919921875 
[2025-03-20 16:27:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 2 loss:0.031875062733888626 norm:0.0002238662855233997 max memory_allocated 47477.7919921875 
[2025-03-20 16:28:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 3 loss:0.029701458290219307 norm:0.00019273150246590376 max memory_allocated 47477.7919921875 
[2025-03-20 16:29:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 4 loss:0.02845631167292595 norm:0.00017800333444029093 max memory_allocated 47477.7919921875 
[2025-03-20 16:30:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 5 loss:0.027641169726848602 norm:0.00016404320194851607 max memory_allocated 47477.7919921875 
[2025-03-20 16:31:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 6 loss:0.02712373062968254 norm:0.0001521105587016791 max memory_allocated 47477.7919921875 
[2025-03-20 16:32:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 7 loss:0.02684171125292778 norm:0.00014529921463690698 max memory_allocated 47477.7919921875 
[2025-03-20 16:33:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 8 loss:0.02664826065301895 norm:0.00014337492757476866 max memory_allocated 47477.7919921875 
[2025-03-20 16:34:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 9 loss:0.02650861069560051 norm:0.00014291021216195077 max memory_allocated 47477.7919921875 
[2025-03-20 16:35:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 10 loss:0.026403119787573814 norm:0.00014006848505232483 max memory_allocated 47477.7919921875 
[2025-03-20 16:36:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 11 loss:0.026323815807700157 norm:0.00014251854736357927 max memory_allocated 47477.7919921875 
[2025-03-20 16:36:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 12 loss:0.026237595826387405 norm:0.00014848558930680156 max memory_allocated 47477.7919921875 
[2025-03-20 16:37:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 13 loss:0.026171835139393806 norm:0.00013642865815199912 max memory_allocated 47477.7919921875 
[2025-03-20 16:38:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 14 loss:0.026174025610089302 norm:0.0001443002838641405 max memory_allocated 47477.7919921875 
[2025-03-20 16:39:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 15 loss:0.026113977655768394 norm:0.00014169409405440092 max memory_allocated 47477.7919921875 
[2025-03-20 16:40:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 16 loss:0.026069682091474533 norm:0.0001346514472970739 max memory_allocated 47477.7919921875 
[2025-03-20 16:41:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 17 loss:0.02604869194328785 norm:0.00013375563139561564 max memory_allocated 47477.7919921875 
[2025-03-20 16:42:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 18 loss:0.026051990687847137 norm:0.00014406755508389324 max memory_allocated 47477.7919921875 
[2025-03-20 16:43:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 19 loss:0.025988515466451645 norm:0.00013296020915731788 max memory_allocated 47477.7919921875 
[2025-03-20 16:44:12 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [9, 10]
[2025-03-20 16:44:14 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [11, 12, 13] ===
[2025-03-20 16:45:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 0 loss:0.058616530150175095 norm:0.000621914747171104 max memory_allocated 47478.1044921875 
[2025-03-20 16:47:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 1 loss:0.0463886633515358 norm:0.00035781762562692165 max memory_allocated 47478.1044921875 
[2025-03-20 16:48:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 2 loss:0.03787028044462204 norm:0.00026668544160202146 max memory_allocated 47478.1044921875 
[2025-03-20 16:49:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 3 loss:0.03486175090074539 norm:0.00022700661793351173 max memory_allocated 47478.1044921875 
[2025-03-20 16:51:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 4 loss:0.03317716717720032 norm:0.00020522830891422927 max memory_allocated 47478.1044921875 
[2025-03-20 16:52:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 5 loss:0.031989894807338715 norm:0.00018990965327247977 max memory_allocated 47478.1044921875 
[2025-03-20 16:53:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 6 loss:0.03122171014547348 norm:0.00018032420484814793 max memory_allocated 47478.1044921875 
[2025-03-20 16:55:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 7 loss:0.03067503497004509 norm:0.0001815579307731241 max memory_allocated 47478.1044921875 
[2025-03-20 16:56:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 8 loss:0.03028716892004013 norm:0.0001732134842313826 max memory_allocated 47478.1044921875 
[2025-03-20 16:57:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 9 loss:0.030025025829672813 norm:0.0001747852365951985 max memory_allocated 47478.1044921875 
[2025-03-20 16:59:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 10 loss:0.02975899912416935 norm:0.00016368736396543682 max memory_allocated 47478.1044921875 
[2025-03-20 17:00:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 11 loss:0.029610132798552513 norm:0.00015986365906428546 max memory_allocated 47478.1044921875 
[2025-03-20 17:01:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 12 loss:0.029487159103155136 norm:0.00015834980877116323 max memory_allocated 47478.1044921875 
[2025-03-20 17:03:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 13 loss:0.0293600894510746 norm:0.0001559790689498186 max memory_allocated 47478.1044921875 
[2025-03-20 17:04:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 14 loss:0.02930467575788498 norm:0.00016399237210862339 max memory_allocated 47478.1044921875 
[2025-03-20 17:05:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 15 loss:0.029208313673734665 norm:0.0001605023571755737 max memory_allocated 47478.1044921875 
[2025-03-20 17:07:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 16 loss:0.029128460213541985 norm:0.00014680645836051553 max memory_allocated 47478.1044921875 
[2025-03-20 17:08:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 17 loss:0.029100995510816574 norm:0.00016073038568720222 max memory_allocated 47478.1044921875 
[2025-03-20 17:09:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 18 loss:0.029056299477815628 norm:0.00016050403064582497 max memory_allocated 47478.1044921875 
[2025-03-20 17:11:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 19 loss:0.028989039361476898 norm:0.0001464951055822894 max memory_allocated 47478.1044921875 
[2025-03-20 17:12:50 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [11, 12, 13]
[2025-03-20 17:12:50 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [14, 15, 16] ===
[2025-03-20 17:14:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 0 loss:0.06347285956144333 norm:0.0006976253353059292 max memory_allocated 47478.2919921875 
[2025-03-20 17:15:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 1 loss:0.05057267099618912 norm:0.00038755766581743956 max memory_allocated 47478.2919921875 
[2025-03-20 17:16:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 2 loss:0.04077903926372528 norm:0.00026250904193148017 max memory_allocated 47478.2919921875 
[2025-03-20 17:18:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 3 loss:0.03730756416916847 norm:0.00020706043869722635 max memory_allocated 47478.2919921875 
[2025-03-20 17:19:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 4 loss:0.035431407392024994 norm:0.0001847228268161416 max memory_allocated 47478.2919921875 
[2025-03-20 17:20:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 5 loss:0.034270256757736206 norm:0.0001724003377603367 max memory_allocated 47478.2919921875 
[2025-03-20 17:22:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 6 loss:0.033486105501651764 norm:0.0001646651653572917 max memory_allocated 47478.2919921875 
[2025-03-20 17:23:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 7 loss:0.03297771140933037 norm:0.00015856098616495728 max memory_allocated 47478.2919921875 
[2025-03-20 17:24:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 8 loss:0.032568592578172684 norm:0.00015334728232119232 max memory_allocated 47478.2919921875 
[2025-03-20 17:26:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 9 loss:0.03222823143005371 norm:0.0001452405413147062 max memory_allocated 47478.2919921875 
[2025-03-20 17:27:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 10 loss:0.03199570253491402 norm:0.00013915414456278086 max memory_allocated 47478.2919921875 
[2025-03-20 17:28:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 11 loss:0.03183894231915474 norm:0.0001338652946287766 max memory_allocated 47478.2919921875 
[2025-03-20 17:30:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 12 loss:0.03168242424726486 norm:0.00013119836512487382 max memory_allocated 47478.2919921875 
[2025-03-20 17:31:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 13 loss:0.031579554080963135 norm:0.0001318778085988015 max memory_allocated 47478.2919921875 
[2025-03-20 17:32:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 14 loss:0.03145599737763405 norm:0.00012808077735826373 max memory_allocated 47478.2919921875 
[2025-03-20 17:34:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 15 loss:0.03135670721530914 norm:0.00012759515084326267 max memory_allocated 47478.2919921875 
[2025-03-20 17:35:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 16 loss:0.031283214688301086 norm:0.00012769413297064602 max memory_allocated 47478.2919921875 
[2025-03-20 17:36:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 17 loss:0.03121957555413246 norm:0.00012014459207421169 max memory_allocated 47478.2919921875 
[2025-03-20 17:38:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 18 loss:0.0311749130487442 norm:0.00012345709546934813 max memory_allocated 47478.2919921875 
[2025-03-20 17:39:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 19 loss:0.031114712357521057 norm:0.00012210384011268616 max memory_allocated 47478.2919921875 
[2025-03-20 17:41:19 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [14, 15, 16]
[2025-03-20 17:41:19 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [17, 18, 19] ===
[2025-03-20 17:42:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 0 loss:0.060602445155382156 norm:0.0019619022496044636 max memory_allocated 47478.4794921875 
[2025-03-20 17:44:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 1 loss:0.051033370196819305 norm:0.0006168336258269846 max memory_allocated 47478.4794921875 
[2025-03-20 17:45:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 2 loss:0.04484841972589493 norm:0.0002920460538007319 max memory_allocated 47478.4794921875 
[2025-03-20 17:46:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 3 loss:0.04145333543419838 norm:0.00021378003293648362 max memory_allocated 47478.4794921875 
[2025-03-20 17:48:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 4 loss:0.03922881931066513 norm:0.0001835772709455341 max memory_allocated 47478.4794921875 
[2025-03-20 17:49:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 5 loss:0.03776209056377411 norm:0.00017086543084587902 max memory_allocated 47478.4794921875 
[2025-03-20 17:50:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 6 loss:0.036993563175201416 norm:0.00016164941189344972 max memory_allocated 47478.4794921875 
[2025-03-20 17:52:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 7 loss:0.03654273599386215 norm:0.00015828260802663863 max memory_allocated 47478.4794921875 
[2025-03-20 17:53:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 8 loss:0.03626842051744461 norm:0.0001555920171085745 max memory_allocated 47478.4794921875 
[2025-03-20 17:54:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 9 loss:0.03607786074280739 norm:0.00015100195014383644 max memory_allocated 47478.4794921875 
[2025-03-20 17:56:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 10 loss:0.035922128707170486 norm:0.0001454085868317634 max memory_allocated 47478.4794921875 
[2025-03-20 17:57:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 11 loss:0.03581545129418373 norm:0.00013726166798733175 max memory_allocated 47478.4794921875 
[2025-03-20 17:58:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 12 loss:0.03571044281125069 norm:0.00013400269381236285 max memory_allocated 47478.4794921875 
[2025-03-20 18:00:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 13 loss:0.03563180938363075 norm:0.00012809858890250325 max memory_allocated 47478.4794921875 
[2025-03-20 18:01:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 14 loss:0.03556705638766289 norm:0.00013238414248917252 max memory_allocated 47478.4794921875 
[2025-03-20 18:02:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 15 loss:0.035513609647750854 norm:0.00013188071898184717 max memory_allocated 47478.4794921875 
[2025-03-20 18:04:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 16 loss:0.03546509891748428 norm:0.00012783984129782766 max memory_allocated 47478.4794921875 
[2025-03-20 18:05:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 17 loss:0.035425152629613876 norm:0.0001247873151442036 max memory_allocated 47478.4794921875 
[2025-03-20 18:06:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 18 loss:0.03539618104696274 norm:0.00012513273395597935 max memory_allocated 47478.4794921875 
[2025-03-20 18:08:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 19 loss:0.03536836430430412 norm:0.00012757927470374852 max memory_allocated 47478.4794921875 
[2025-03-20 18:09:49 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [17, 18, 19]
[2025-03-20 18:09:49 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [20, 21, 22] ===
[2025-03-20 18:11:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 0 loss:0.07701616734266281 norm:0.001745731569826603 max memory_allocated 47478.6669921875 
[2025-03-20 18:12:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 1 loss:0.06645361334085464 norm:0.0006934261182323098 max memory_allocated 47478.6669921875 
[2025-03-20 18:13:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 2 loss:0.05885881185531616 norm:0.000330120965372771 max memory_allocated 47478.6669921875 
[2025-03-20 18:15:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 3 loss:0.055259108543395996 norm:0.0002477293601259589 max memory_allocated 47478.6669921875 
[2025-03-20 18:16:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 4 loss:0.0525984950363636 norm:0.00023299467284232378 max memory_allocated 47478.6669921875 
[2025-03-20 18:17:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 5 loss:0.05099993199110031 norm:0.00021953450050204992 max memory_allocated 47478.6669921875 
[2025-03-20 18:19:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 6 loss:0.05024508759379387 norm:0.00020793045405298471 max memory_allocated 47478.6669921875 
[2025-03-20 18:20:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 7 loss:0.04983694851398468 norm:0.00019756225810851902 max memory_allocated 47478.6669921875 
[2025-03-20 18:21:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 8 loss:0.04957437515258789 norm:0.00019035942386835814 max memory_allocated 47478.6669921875 
[2025-03-20 18:23:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 9 loss:0.04935986176133156 norm:0.00018145744979847223 max memory_allocated 47478.6669921875 
[2025-03-20 18:24:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 10 loss:0.049187541007995605 norm:0.00017764591029845178 max memory_allocated 47478.6669921875 
[2025-03-20 18:25:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 11 loss:0.04904229938983917 norm:0.00018051858933176845 max memory_allocated 47478.6669921875 
[2025-03-20 18:27:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 12 loss:0.04893065243959427 norm:0.0001679605629760772 max memory_allocated 47478.6669921875 
[2025-03-20 18:28:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 13 loss:0.048807237297296524 norm:0.00016039337788242847 max memory_allocated 47478.6669921875 
[2025-03-20 18:29:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 14 loss:0.04868951439857483 norm:0.0001650792546570301 max memory_allocated 47478.6669921875 
[2025-03-20 18:31:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 15 loss:0.04861501231789589 norm:0.00016274015069939196 max memory_allocated 47478.6669921875 
[2025-03-20 18:32:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 16 loss:0.04855971410870552 norm:0.00016370537923648953 max memory_allocated 47478.6669921875 
[2025-03-20 18:33:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 17 loss:0.048500120639801025 norm:0.00016054842853918672 max memory_allocated 47478.6669921875 
[2025-03-20 18:35:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 18 loss:0.04841151461005211 norm:0.00015020754653960466 max memory_allocated 47478.6669921875 
[2025-03-20 18:36:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 19 loss:0.048343222588300705 norm:0.00015095260459929705 max memory_allocated 47478.6669921875 
[2025-03-20 18:38:16 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [20, 21, 22]
[2025-03-20 18:38:16 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [23, 24, 25] ===
[2025-03-20 18:39:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 0 loss:0.11246076226234436 norm:0.0038527180440723896 max memory_allocated 47478.8544921875 
[2025-03-20 18:41:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 1 loss:0.09479840099811554 norm:0.000778337474912405 max memory_allocated 47478.8544921875 
[2025-03-20 18:42:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 2 loss:0.08351341634988785 norm:0.0004067567642778158 max memory_allocated 47478.8544921875 
[2025-03-20 18:43:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 3 loss:0.07848755270242691 norm:0.0003248498251195997 max memory_allocated 47478.8544921875 
[2025-03-20 18:45:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 4 loss:0.07506493479013443 norm:0.00029535795329138637 max memory_allocated 47478.8544921875 
[2025-03-20 18:46:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 5 loss:0.07358260452747345 norm:0.00027401174884289503 max memory_allocated 47478.8544921875 
[2025-03-20 18:47:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 6 loss:0.0729857087135315 norm:0.00025282311253249645 max memory_allocated 47478.8544921875 
[2025-03-20 18:49:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 7 loss:0.07261470705270767 norm:0.00025936562451533973 max memory_allocated 47478.8544921875 
[2025-03-20 18:50:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 8 loss:0.07231692969799042 norm:0.00024943353491835296 max memory_allocated 47478.8544921875 
[2025-03-20 18:51:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 9 loss:0.07208080589771271 norm:0.0002326044486835599 max memory_allocated 47478.8544921875 
[2025-03-20 18:53:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 10 loss:0.07182065397500992 norm:0.00022068720136303455 max memory_allocated 47478.8544921875 
[2025-03-20 18:54:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 11 loss:0.07162772864103317 norm:0.00022676732623949647 max memory_allocated 47478.8544921875 
[2025-03-20 18:55:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 12 loss:0.07143601030111313 norm:0.00021200370974838734 max memory_allocated 47478.8544921875 
[2025-03-20 18:57:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 13 loss:0.07132663577795029 norm:0.00021112784452270716 max memory_allocated 47478.8544921875 
[2025-03-20 18:58:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 14 loss:0.07124121487140656 norm:0.00023584575683344156 max memory_allocated 47478.8544921875 
[2025-03-20 18:59:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 15 loss:0.07109729200601578 norm:0.0002119728596881032 max memory_allocated 47478.8544921875 
[2025-03-20 19:01:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 16 loss:0.0710068792104721 norm:0.00020237996068317443 max memory_allocated 47478.8544921875 
[2025-03-20 19:02:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 17 loss:0.070933498442173 norm:0.00019925161905121058 max memory_allocated 47478.8544921875 
[2025-03-20 19:03:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 18 loss:0.07083019614219666 norm:0.00019417278235778213 max memory_allocated 47478.8544921875 
[2025-03-20 19:05:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 19 loss:0.07077358663082123 norm:0.00019206646538805217 max memory_allocated 47478.8544921875 
[2025-03-20 19:06:42 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [23, 24, 25]
[2025-03-20 19:06:42 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [26, 27, 28] ===
[2025-03-20 19:06:43 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 19:08:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 0 loss:0.15692153573036194 norm:0.010424423031508923 max memory_allocated 47479.0419921875 
[2025-03-20 19:09:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 1 loss:0.13767467439174652 norm:0.007167952135205269 max memory_allocated 47479.0419921875 
[2025-03-20 19:10:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 2 loss:0.12087053060531616 norm:0.0051229349337518215 max memory_allocated 47479.0419921875 
[2025-03-20 19:12:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 3 loss:0.11428128927946091 norm:0.004208590369671583 max memory_allocated 47479.0419921875 
[2025-03-20 19:13:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 4 loss:0.11126656085252762 norm:0.003528380300849676 max memory_allocated 47479.0419921875 
[2025-03-20 19:14:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 5 loss:0.11010796576738358 norm:0.0029503104742616415 max memory_allocated 47479.0419921875 
[2025-03-20 19:16:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 6 loss:0.10935158282518387 norm:0.0024878475815057755 max memory_allocated 47479.0419921875 
[2025-03-20 19:17:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 7 loss:0.10884205996990204 norm:0.002378487028181553 max memory_allocated 47479.0419921875 
[2025-03-20 19:18:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 8 loss:0.10840888321399689 norm:0.0022235370706766844 max memory_allocated 47479.0419921875 
[2025-03-20 19:20:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 9 loss:0.10795601457357407 norm:0.0022535035386681557 max memory_allocated 47479.0419921875 
[2025-03-20 19:21:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 10 loss:0.10766389966011047 norm:0.0020209860522300005 max memory_allocated 47479.0419921875 
[2025-03-20 19:22:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 11 loss:0.10730697959661484 norm:0.0021363841369748116 max memory_allocated 47479.0419921875 
[2025-03-20 19:24:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 12 loss:0.10697531700134277 norm:0.0019802118185907602 max memory_allocated 47479.0419921875 
[2025-03-20 19:25:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 13 loss:0.10665883868932724 norm:0.0018932149978354573 max memory_allocated 47479.0419921875 
[2025-03-20 19:26:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 14 loss:0.10649421811103821 norm:0.001827231957577169 max memory_allocated 47479.0419921875 
[2025-03-20 19:28:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 15 loss:0.10627171397209167 norm:0.0018460475839674473 max memory_allocated 47479.0419921875 
[2025-03-20 19:29:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 16 loss:0.1060628741979599 norm:0.0017117258394137025 max memory_allocated 47479.0419921875 
[2025-03-20 19:30:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 17 loss:0.10585677623748779 norm:0.0016181390965357423 max memory_allocated 47479.0419921875 
[2025-03-20 19:32:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 18 loss:0.10570485889911652 norm:0.0015401900745928288 max memory_allocated 47479.0419921875 
[2025-03-20 19:33:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 19 loss:0.10559571534395218 norm:0.0016018816968426108 max memory_allocated 47479.0419921875 
[2025-03-20 19:35:12 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [26, 27, 28]
[2025-03-20 19:35:12 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [29] ===
[2025-03-20 19:35:12 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 19:35:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 0 loss:0.14423757791519165 norm:0.011720691807568073 max memory_allocated 47479.0419921875 
[2025-03-20 19:36:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 1 loss:0.13140785694122314 norm:0.008473134599626064 max memory_allocated 47479.0419921875 
[2025-03-20 19:36:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 2 loss:0.1212930828332901 norm:0.005362070631235838 max memory_allocated 47479.0419921875 
[2025-03-20 19:37:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 3 loss:0.11833485960960388 norm:0.004529566969722509 max memory_allocated 47479.0419921875 
[2025-03-20 19:37:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 4 loss:0.1173991784453392 norm:0.003873416455462575 max memory_allocated 47479.0419921875 
[2025-03-20 19:37:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 5 loss:0.11690601706504822 norm:0.0032898150384426117 max memory_allocated 47479.0419921875 
[2025-03-20 19:38:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 6 loss:0.11654245853424072 norm:0.002800132380798459 max memory_allocated 47479.0419921875 
[2025-03-20 19:38:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 7 loss:0.11624370515346527 norm:0.002353091025725007 max memory_allocated 47479.0419921875 
[2025-03-20 19:39:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 8 loss:0.11603449285030365 norm:0.002205782337114215 max memory_allocated 47479.0419921875 
[2025-03-20 19:39:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 9 loss:0.11596009135246277 norm:0.0023073405027389526 max memory_allocated 47479.0419921875 
[2025-03-20 19:40:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 10 loss:0.11582419276237488 norm:0.002359512960538268 max memory_allocated 47479.0419921875 
[2025-03-20 19:40:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 11 loss:0.11562922596931458 norm:0.001977062551304698 max memory_allocated 47479.0419921875 
[2025-03-20 19:41:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 12 loss:0.11545468866825104 norm:0.0019304431043565273 max memory_allocated 47479.0419921875 
[2025-03-20 19:41:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 13 loss:0.11537560075521469 norm:0.0018400768749415874 max memory_allocated 47479.0419921875 
[2025-03-20 19:42:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 14 loss:0.11527349799871445 norm:0.001826632535085082 max memory_allocated 47479.0419921875 
[2025-03-20 19:42:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 15 loss:0.11517485976219177 norm:0.0016472158022224903 max memory_allocated 47479.0419921875 
[2025-03-20 19:42:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 16 loss:0.11509107053279877 norm:0.0016819266602396965 max memory_allocated 47479.0419921875 
[2025-03-20 19:43:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 17 loss:0.11504048109054565 norm:0.0016271632630378008 max memory_allocated 47479.0419921875 
[2025-03-20 19:43:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 18 loss:0.11496782302856445 norm:0.0016284743323922157 max memory_allocated 47479.0419921875 
[2025-03-20 19:44:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 19 loss:0.11494947969913483 norm:0.0015733102336525917 max memory_allocated 47479.0419921875 
[2025-03-20 19:44:47 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [29]
[2025-03-20 19:44:47 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [30] ===
[2025-03-20 19:44:48 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 19:45:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 0 loss:0.2167341411113739 norm:0.027142701670527458 max memory_allocated 47479.0419921875 
[2025-03-20 19:45:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 1 loss:0.18455517292022705 norm:0.018796348944306374 max memory_allocated 47479.0419921875 
[2025-03-20 19:46:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 2 loss:0.16434183716773987 norm:0.011699292808771133 max memory_allocated 47479.0419921875 
[2025-03-20 19:46:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 3 loss:0.15939931571483612 norm:0.010299364104866982 max memory_allocated 47479.0419921875 
[2025-03-20 19:47:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 4 loss:0.157988503575325 norm:0.009866271167993546 max memory_allocated 47479.0419921875 
[2025-03-20 19:47:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 5 loss:0.15727704763412476 norm:0.008684446103870869 max memory_allocated 47479.0419921875 
[2025-03-20 19:47:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 6 loss:0.15708690881729126 norm:0.00811725202947855 max memory_allocated 47479.0419921875 
[2025-03-20 19:48:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 7 loss:0.1562594622373581 norm:0.007784600369632244 max memory_allocated 47479.0419921875 
[2025-03-20 19:48:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 8 loss:0.15581229329109192 norm:0.0076446267776191235 max memory_allocated 47479.0419921875 
[2025-03-20 19:49:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 9 loss:0.15554317831993103 norm:0.00787917897105217 max memory_allocated 47479.0419921875 
[2025-03-20 19:49:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 10 loss:0.1557668149471283 norm:0.007603614125400782 max memory_allocated 47479.0419921875 
[2025-03-20 19:50:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 11 loss:0.15509113669395447 norm:0.00727227283641696 max memory_allocated 47479.0419921875 
[2025-03-20 19:50:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 12 loss:0.1550205498933792 norm:0.006741982884705067 max memory_allocated 47479.0419921875 
[2025-03-20 19:51:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 13 loss:0.1549479067325592 norm:0.006617645733058453 max memory_allocated 47479.0419921875 
[2025-03-20 19:51:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 14 loss:0.15525132417678833 norm:0.006875458639115095 max memory_allocated 47479.0419921875 
[2025-03-20 19:52:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 15 loss:0.1551867574453354 norm:0.007249174173921347 max memory_allocated 47479.0419921875 
[2025-03-20 19:52:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 16 loss:0.15489837527275085 norm:0.006837652064859867 max memory_allocated 47479.0419921875 
[2025-03-20 19:52:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 17 loss:0.15432748198509216 norm:0.006156991235911846 max memory_allocated 47479.0419921875 
[2025-03-20 19:53:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 18 loss:0.15426038205623627 norm:0.006309820339083672 max memory_allocated 47479.0419921875 
[2025-03-20 19:53:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 19 loss:0.1544562578201294 norm:0.006201491691172123 max memory_allocated 47479.0419921875 
[2025-03-20 19:54:24 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [30]
[2025-03-20 19:54:24 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [31] ===
[2025-03-20 19:54:24 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 19:54:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 0 loss:0.25875478982925415 norm:0.02013707160949707 max memory_allocated 47479.0419921875 
[2025-03-20 19:55:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 1 loss:0.24479764699935913 norm:0.015078241005539894 max memory_allocated 47479.0419921875 
[2025-03-20 19:55:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 2 loss:0.23611366748809814 norm:0.011514469981193542 max memory_allocated 47479.0419921875 
[2025-03-20 19:56:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 3 loss:0.2315095216035843 norm:0.009475755505263805 max memory_allocated 47479.0419921875 
[2025-03-20 19:56:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 4 loss:0.2296064794063568 norm:0.00808473490178585 max memory_allocated 47479.0419921875 
[2025-03-20 19:57:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 5 loss:0.2284887433052063 norm:0.007118538022041321 max memory_allocated 47479.0419921875 
[2025-03-20 19:57:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 6 loss:0.22763238847255707 norm:0.006422906182706356 max memory_allocated 47479.0419921875 
[2025-03-20 19:58:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 7 loss:0.22733694314956665 norm:0.005889123771339655 max memory_allocated 47479.0419921875 
[2025-03-20 19:58:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 8 loss:0.22684140503406525 norm:0.005506413523107767 max memory_allocated 47479.0419921875 
[2025-03-20 19:58:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 9 loss:0.22676312923431396 norm:0.00563107430934906 max memory_allocated 47479.0419921875 
[2025-03-20 19:59:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 10 loss:0.22664442658424377 norm:0.005541557911783457 max memory_allocated 47479.0419921875 
[2025-03-20 19:59:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 11 loss:0.22655349969863892 norm:0.0055646817199885845 max memory_allocated 47479.0419921875 
[2025-03-20 20:00:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 12 loss:0.22649496793746948 norm:0.005405276548117399 max memory_allocated 47479.0419921875 
[2025-03-20 20:00:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 13 loss:0.22636154294013977 norm:0.005465923808515072 max memory_allocated 47479.0419921875 
[2025-03-20 20:01:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 14 loss:0.22612416744232178 norm:0.0049689472652971745 max memory_allocated 47479.0419921875 
[2025-03-20 20:01:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 15 loss:0.2259959578514099 norm:0.005148554686456919 max memory_allocated 47479.0419921875 
[2025-03-20 20:02:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 16 loss:0.22586612403392792 norm:0.005207898560911417 max memory_allocated 47479.0419921875 
[2025-03-20 20:02:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 17 loss:0.225920170545578 norm:0.00542804878205061 max memory_allocated 47479.0419921875 
[2025-03-20 20:03:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 18 loss:0.22573783993721008 norm:0.005101758986711502 max memory_allocated 47479.0419921875 
[2025-03-20 20:03:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 19 loss:0.22569754719734192 norm:0.0053359707817435265 max memory_allocated 47479.0419921875 
[2025-03-20 20:03:59 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [31]
[2025-03-20 20:04:00 root] (main_calib_config3_attn.py 379): INFO 18297.07994365692
[2025-03-20 20:04:05 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-20 20:04:51 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.599102020263672
[2025-03-20 20:04:51 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-20 20:06:02 root] (main_calib_config3_attn.py 161): INFO c4 : 7.135504245758057
[2025-03-20 20:47:16 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.599102020263672, 'c4': 7.135504245758057, 'results': {'hellaswag': {'acc': 0.5574586735710018, 'acc_stderr': 0.004956724392646532, 'acc_norm': 0.7202748456482773, 'acc_norm_stderr': 0.00447946761946479}, 'winogrande': {'acc': 0.6677190213101816, 'acc_stderr': 0.013238316554236525}, 'arc_easy': {'acc': 0.6919191919191919, 'acc_stderr': 0.009473887075826332, 'acc_norm': 0.5248316498316499, 'acc_norm_stderr': 0.010247123122159287}, 'boolq': {'acc': 0.718348623853211, 'acc_stderr': 0.007867126084150702}, 'arc_challenge': {'acc': 0.4112627986348123, 'acc_stderr': 0.014379441068522078, 'acc_norm': 0.39590443686006827, 'acc_norm_stderr': 0.014291228393536588}, 'piqa': {'acc': 0.779651795429815, 'acc_stderr': 0.009670535456853136, 'acc_norm': 0.7676822633297062, 'acc_norm_stderr': 0.009853201384168241}}, 'versions': {'hellaswag': 0, 'winogrande': 0, 'arc_easy': 0, 'boolq': 1, 'arc_challenge': 0, 'piqa': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-20 20:47:16 root] (main_calib_config3_attn.py 175): INFO 41.13,69.19,71.83,55.75,77.97,66.77
