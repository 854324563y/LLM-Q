[2025-03-20 15:00:48 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/llama-7b-hf-0.6', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.6.pkl', blocks_pkl='./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-20 15:00:55 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-20 15:00:55 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-20 15:00:55 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-20 15:00:55 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.6.pkl
[2025-03-20 15:00:55 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 5), (5, 8), (8, 11), (11, 14), (14, 16), (16, 19), (19, 22), (22, 25), (25, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-20 15:00:55 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2, 3, 4], [5, 6, 7], [8, 9, 10], [11, 12, 13], [14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26], [27], [28], [29], [30], [31]]
[2025-03-20 15:00:57 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-20 15:00:57 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 15:01:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.006227750796824694 norm:0.004776592366397381 max memory_allocated 34630.880859375 
[2025-03-20 15:01:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.0033747851848602295 norm:0.0025931186974048615 max memory_allocated 34630.880859375 
[2025-03-20 15:02:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.0020922606345266104 norm:0.0016907940153032541 max memory_allocated 34630.880859375 
[2025-03-20 15:02:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.00176093855407089 norm:0.001345236087217927 max memory_allocated 34630.880859375 
[2025-03-20 15:03:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.00168273423332721 norm:0.0011838757200166583 max memory_allocated 34630.880859375 
[2025-03-20 15:03:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0016352025559172034 norm:0.0010695017408579588 max memory_allocated 34630.880859375 
[2025-03-20 15:04:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.0015976979630067945 norm:0.0009671793086454272 max memory_allocated 34630.880859375 
[2025-03-20 15:04:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0015676227631047368 norm:0.0008448752923868597 max memory_allocated 34630.880859375 
[2025-03-20 15:05:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0015336873475462198 norm:0.0007778948056511581 max memory_allocated 34630.880859375 
[2025-03-20 15:05:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.001525662373751402 norm:0.0007135884370654821 max memory_allocated 34630.880859375 
[2025-03-20 15:05:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.0015108119696378708 norm:0.0006570988916791975 max memory_allocated 34630.880859375 
[2025-03-20 15:06:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.001494327443651855 norm:0.000601167615968734 max memory_allocated 34630.880859375 
[2025-03-20 15:06:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.0014784197555854917 norm:0.0005350346327759326 max memory_allocated 34630.880859375 
[2025-03-20 15:07:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.0014666272327303886 norm:0.0005031644250266254 max memory_allocated 34630.880859375 
[2025-03-20 15:07:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0014564108569175005 norm:0.0004614515055436641 max memory_allocated 34630.880859375 
[2025-03-20 15:08:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.0014493881026282907 norm:0.00042698593460954726 max memory_allocated 34630.880859375 
[2025-03-20 15:08:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.0014393334276974201 norm:0.0003865042526740581 max memory_allocated 34630.880859375 
[2025-03-20 15:09:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0014437768841162324 norm:0.0003622942022047937 max memory_allocated 34630.880859375 
[2025-03-20 15:09:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.001434847479686141 norm:0.0003268691070843488 max memory_allocated 34630.880859375 
[2025-03-20 15:10:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0014316107844933867 norm:0.00031322278664447367 max memory_allocated 34630.880859375 
[2025-03-20 15:10:36 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-20 15:10:36 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-20 15:10:36 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 15:11:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.018118062987923622 norm:0.018134547397494316 max memory_allocated 35097.7724609375 
[2025-03-20 15:11:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.009858254343271255 norm:0.011485269293189049 max memory_allocated 35097.7724609375 
[2025-03-20 15:12:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.006501799449324608 norm:0.006757940165698528 max memory_allocated 35097.7724609375 
[2025-03-20 15:12:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.005717867519706488 norm:0.00478306645527482 max memory_allocated 35097.7724609375 
[2025-03-20 15:12:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.0053587909787893295 norm:0.004092589020729065 max memory_allocated 35097.7724609375 
[2025-03-20 15:13:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.005103037692606449 norm:0.0036262960638850927 max memory_allocated 35097.7724609375 
[2025-03-20 15:13:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.004910207819193602 norm:0.0032265475019812584 max memory_allocated 35097.7724609375 
[2025-03-20 15:14:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.004786527715623379 norm:0.003016192000359297 max memory_allocated 35097.7724609375 
[2025-03-20 15:14:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.00467071495950222 norm:0.002756014233455062 max memory_allocated 35097.7724609375 
[2025-03-20 15:15:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.004537704400718212 norm:0.0024763811379671097 max memory_allocated 35097.7724609375 
[2025-03-20 15:15:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.004454903304576874 norm:0.0023523597046732903 max memory_allocated 35097.7724609375 
[2025-03-20 15:16:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.004390723071992397 norm:0.0021070463117212057 max memory_allocated 35097.7724609375 
[2025-03-20 15:16:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.004327765665948391 norm:0.0019375550327822566 max memory_allocated 35097.7724609375 
[2025-03-20 15:16:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.004272594582289457 norm:0.0017548720352351665 max memory_allocated 35097.7724609375 
[2025-03-20 15:17:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.004233682528138161 norm:0.001628017402254045 max memory_allocated 35097.7724609375 
[2025-03-20 15:17:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.0042150975205004215 norm:0.001506378292106092 max memory_allocated 35097.7724609375 
[2025-03-20 15:18:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.004177447408437729 norm:0.0013500871136784554 max memory_allocated 35097.7724609375 
[2025-03-20 15:18:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.004154982045292854 norm:0.001215227646753192 max memory_allocated 35097.7724609375 
[2025-03-20 15:19:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.0041251517832279205 norm:0.0011078279931098223 max memory_allocated 35097.7724609375 
[2025-03-20 15:19:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.0041261520236730576 norm:0.0010198615491390228 max memory_allocated 35097.7724609375 
[2025-03-20 15:20:17 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-20 15:20:17 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2, 3, 4] ===
[2025-03-20 15:20:17 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 15:21:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 0 loss:0.038744427263736725 norm:0.016895297914743423 max memory_allocated 47468.5419921875 
[2025-03-20 15:23:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 1 loss:0.038494814187288284 norm:0.07428888976573944 max memory_allocated 47468.5419921875 
[2025-03-20 15:24:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 2 loss:0.02308511734008789 norm:0.005794592201709747 max memory_allocated 47468.5419921875 
[2025-03-20 15:25:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 3 loss:0.020694712176918983 norm:0.0071541257202625275 max memory_allocated 47468.5419921875 
[2025-03-20 15:27:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 4 loss:0.01944081112742424 norm:0.00869889184832573 max memory_allocated 47468.5419921875 
[2025-03-20 15:28:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 5 loss:0.018499301746487617 norm:0.009289048612117767 max memory_allocated 47468.5419921875 
[2025-03-20 15:29:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 6 loss:0.017987199127674103 norm:0.01019280031323433 max memory_allocated 47468.5419921875 
[2025-03-20 15:31:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 7 loss:0.01753123104572296 norm:0.009244050830602646 max memory_allocated 47468.5419921875 
[2025-03-20 15:32:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 8 loss:0.017265578731894493 norm:0.009381800889968872 max memory_allocated 47468.5419921875 
[2025-03-20 15:33:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 9 loss:0.016995588317513466 norm:0.007877607829868793 max memory_allocated 47468.5419921875 
[2025-03-20 15:35:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 10 loss:0.016788605600595474 norm:0.00761508010327816 max memory_allocated 47468.5419921875 
[2025-03-20 15:36:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 11 loss:0.016830164939165115 norm:0.007768777199089527 max memory_allocated 47468.5419921875 
[2025-03-20 15:37:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 12 loss:0.01651192083954811 norm:0.006531952880322933 max memory_allocated 47468.5419921875 
[2025-03-20 15:39:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 13 loss:0.01646355353295803 norm:0.006914668250828981 max memory_allocated 47468.5419921875 
[2025-03-20 15:40:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 14 loss:0.016286756843328476 norm:0.006250618491321802 max memory_allocated 47468.5419921875 
[2025-03-20 15:41:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 15 loss:0.01615022122859955 norm:0.006399686448276043 max memory_allocated 47468.5419921875 
[2025-03-20 15:43:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 16 loss:0.01607804372906685 norm:0.005879824515432119 max memory_allocated 47468.5419921875 
[2025-03-20 15:44:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 17 loss:0.016309481114149094 norm:0.007236517500132322 max memory_allocated 47468.5419921875 
[2025-03-20 15:45:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 18 loss:0.016005879268050194 norm:0.005907989572733641 max memory_allocated 47468.5419921875 
[2025-03-20 15:47:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 19 loss:0.015918578952550888 norm:0.006025383248925209 max memory_allocated 47468.5419921875 
[2025-03-20 15:48:50 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2, 3, 4]
[2025-03-20 15:48:50 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [5, 6, 7] ===
[2025-03-20 15:50:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 0 loss:0.05210132896900177 norm:0.0006935898563824594 max memory_allocated 47468.7294921875 
[2025-03-20 15:51:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 1 loss:0.03890668600797653 norm:0.0003565981751307845 max memory_allocated 47468.7294921875 
[2025-03-20 15:52:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 2 loss:0.030391355976462364 norm:0.00025608818395994604 max memory_allocated 47468.7294921875 
[2025-03-20 15:54:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 3 loss:0.027332566678524017 norm:0.00022931981948204339 max memory_allocated 47468.7294921875 
[2025-03-20 15:55:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 4 loss:0.025546016171574593 norm:0.0002142218581866473 max memory_allocated 47468.7294921875 
[2025-03-20 15:56:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 5 loss:0.024373190477490425 norm:0.000205418182304129 max memory_allocated 47468.7294921875 
[2025-03-20 15:58:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 6 loss:0.0236444603651762 norm:0.00021095798001624644 max memory_allocated 47468.7294921875 
[2025-03-20 15:59:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 7 loss:0.023276176303625107 norm:0.00022536756296176463 max memory_allocated 47468.7294921875 
[2025-03-20 16:00:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 8 loss:0.023001400753855705 norm:0.00020547155872918665 max memory_allocated 47468.7294921875 
[2025-03-20 16:02:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 9 loss:0.02284921519458294 norm:0.0002069596666842699 max memory_allocated 47468.7294921875 
[2025-03-20 16:03:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 10 loss:0.022750239819288254 norm:0.00020548433531075716 max memory_allocated 47468.7294921875 
[2025-03-20 16:04:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 11 loss:0.022652382031083107 norm:0.00020967914315406233 max memory_allocated 47468.7294921875 
[2025-03-20 16:06:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 12 loss:0.022583363577723503 norm:0.00021208949328865856 max memory_allocated 47468.7294921875 
[2025-03-20 16:07:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 13 loss:0.022527078166604042 norm:0.00020695821149274707 max memory_allocated 47468.7294921875 
[2025-03-20 16:08:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 14 loss:0.022488348186016083 norm:0.00022155424812808633 max memory_allocated 47468.7294921875 
[2025-03-20 16:10:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 15 loss:0.022411003708839417 norm:0.00021735527843702585 max memory_allocated 47468.7294921875 
[2025-03-20 16:11:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 16 loss:0.022332392632961273 norm:0.0002150506479665637 max memory_allocated 47468.7294921875 
[2025-03-20 16:12:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 17 loss:0.022322895005345345 norm:0.00021902234584558755 max memory_allocated 47468.7294921875 
[2025-03-20 16:14:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 18 loss:0.022304100915789604 norm:0.00020692090038210154 max memory_allocated 47468.7294921875 
[2025-03-20 16:15:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 19 loss:0.02225475013256073 norm:0.0002031957556027919 max memory_allocated 47468.7294921875 
[2025-03-20 16:17:16 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [5, 6, 7]
[2025-03-20 16:17:16 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [8, 9, 10] ===
[2025-03-20 16:18:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 0 loss:0.059898633509874344 norm:0.0007432613056153059 max memory_allocated 47468.9169921875 
[2025-03-20 16:20:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 1 loss:0.045451730489730835 norm:0.0003944298659916967 max memory_allocated 47468.9169921875 
[2025-03-20 16:21:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 2 loss:0.03629453852772713 norm:0.0002815360203385353 max memory_allocated 47468.9169921875 
[2025-03-20 16:22:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 3 loss:0.03302926942706108 norm:0.00024382141418755054 max memory_allocated 47468.9169921875 
[2025-03-20 16:24:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 4 loss:0.031147630885243416 norm:0.0002267389791086316 max memory_allocated 47468.9169921875 
[2025-03-20 16:25:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 5 loss:0.02990536391735077 norm:0.00020842993399128318 max memory_allocated 47468.9169921875 
[2025-03-20 16:26:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 6 loss:0.029055068269371986 norm:0.0001951888552866876 max memory_allocated 47468.9169921875 
[2025-03-20 16:28:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 7 loss:0.028534919023513794 norm:0.0001951179001480341 max memory_allocated 47468.9169921875 
[2025-03-20 16:29:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 8 loss:0.028126882389187813 norm:0.00018200802151113749 max memory_allocated 47468.9169921875 
[2025-03-20 16:30:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 9 loss:0.027953719720244408 norm:0.00019869884999934584 max memory_allocated 47468.9169921875 
[2025-03-20 16:32:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 10 loss:0.027743816375732422 norm:0.0001862419449025765 max memory_allocated 47468.9169921875 
[2025-03-20 16:33:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 11 loss:0.027582725510001183 norm:0.00016984340618364513 max memory_allocated 47468.9169921875 
[2025-03-20 16:34:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 12 loss:0.027440311387181282 norm:0.00016263667203020304 max memory_allocated 47468.9169921875 
[2025-03-20 16:36:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 13 loss:0.027357591316103935 norm:0.00016271413187496364 max memory_allocated 47468.9169921875 
[2025-03-20 16:37:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 14 loss:0.027289148420095444 norm:0.00015671251458115876 max memory_allocated 47468.9169921875 
[2025-03-20 16:38:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 15 loss:0.02726156823337078 norm:0.00015730489394627512 max memory_allocated 47468.9169921875 
[2025-03-20 16:40:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 16 loss:0.027213890105485916 norm:0.00016003139899112284 max memory_allocated 47468.9169921875 
[2025-03-20 16:41:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 17 loss:0.027178607881069183 norm:0.0001707319897832349 max memory_allocated 47468.9169921875 
[2025-03-20 16:42:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 18 loss:0.027129705995321274 norm:0.0001605343131814152 max memory_allocated 47468.9169921875 
[2025-03-20 16:44:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 19 loss:0.027094382792711258 norm:0.00016571323794778436 max memory_allocated 47468.9169921875 
[2025-03-20 16:45:46 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [8, 9, 10]
[2025-03-20 16:45:46 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [11, 12, 13] ===
[2025-03-20 16:47:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 0 loss:0.06156625971198082 norm:0.0005229402449913323 max memory_allocated 47469.1044921875 
[2025-03-20 16:48:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 1 loss:0.049022775143384933 norm:0.0003167883260175586 max memory_allocated 47469.1044921875 
[2025-03-20 16:49:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 2 loss:0.03955589607357979 norm:0.00023019389482215047 max memory_allocated 47469.1044921875 
[2025-03-20 16:51:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 3 loss:0.03655576705932617 norm:0.000200260168639943 max memory_allocated 47469.1044921875 
[2025-03-20 16:52:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 4 loss:0.03485426306724548 norm:0.00019074013107456267 max memory_allocated 47469.1044921875 
[2025-03-20 16:53:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 5 loss:0.0335468128323555 norm:0.00017322844360023737 max memory_allocated 47469.1044921875 
[2025-03-20 16:55:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 6 loss:0.03282223641872406 norm:0.0001691080688033253 max memory_allocated 47469.1044921875 
[2025-03-20 16:56:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 7 loss:0.03231329098343849 norm:0.0001612658379599452 max memory_allocated 47469.1044921875 
[2025-03-20 16:57:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 8 loss:0.0319206602871418 norm:0.00014639468281529844 max memory_allocated 47469.1044921875 
[2025-03-20 16:59:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 9 loss:0.03164016082882881 norm:0.00014113812358118594 max memory_allocated 47469.1044921875 
[2025-03-20 17:00:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 10 loss:0.031482383608818054 norm:0.00013838513405062258 max memory_allocated 47469.1044921875 
[2025-03-20 17:01:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 11 loss:0.03131786361336708 norm:0.00013269072223920375 max memory_allocated 47469.1044921875 
[2025-03-20 17:03:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 12 loss:0.031196925789117813 norm:0.00012919394066557288 max memory_allocated 47469.1044921875 
[2025-03-20 17:04:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 13 loss:0.0310947448015213 norm:0.00012559103197418153 max memory_allocated 47469.1044921875 
[2025-03-20 17:05:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 14 loss:0.03102283924818039 norm:0.000126140279462561 max memory_allocated 47469.1044921875 
[2025-03-20 17:07:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 15 loss:0.030982080847024918 norm:0.0001286302285734564 max memory_allocated 47469.1044921875 
[2025-03-20 17:08:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 16 loss:0.03091481141746044 norm:0.00012605264782905579 max memory_allocated 47469.1044921875 
[2025-03-20 17:09:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 17 loss:0.030858252197504044 norm:0.00012461363803595304 max memory_allocated 47469.1044921875 
[2025-03-20 17:11:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 18 loss:0.030826210975646973 norm:0.00012711877934634686 max memory_allocated 47469.1044921875 
[2025-03-20 17:12:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 19 loss:0.03075793758034706 norm:0.00012068500655004755 max memory_allocated 47469.1044921875 
[2025-03-20 17:14:13 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [11, 12, 13]
[2025-03-20 17:14:15 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [14, 15] ===
[2025-03-20 17:15:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 0 loss:0.058234915137290955 norm:0.0005067462334409356 max memory_allocated 47469.1044921875 
[2025-03-20 17:16:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 1 loss:0.04769182577729225 norm:0.00028552341973409057 max memory_allocated 47469.1044921875 
[2025-03-20 17:17:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 2 loss:0.03981686383485794 norm:0.00019691324268933386 max memory_allocated 47469.1044921875 
[2025-03-20 17:17:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 3 loss:0.03744078800082207 norm:0.00016709102783352137 max memory_allocated 47469.1044921875 
[2025-03-20 17:18:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 4 loss:0.03597230464220047 norm:0.0001493365125497803 max memory_allocated 47469.1044921875 
[2025-03-20 17:19:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 5 loss:0.03507179394364357 norm:0.00013618145021609962 max memory_allocated 47469.1044921875 
[2025-03-20 17:20:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 6 loss:0.0346064567565918 norm:0.00012976618018001318 max memory_allocated 47469.1044921875 
[2025-03-20 17:21:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 7 loss:0.03425075486302376 norm:0.00012107174552511424 max memory_allocated 47469.1044921875 
[2025-03-20 17:22:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 8 loss:0.033985286951065063 norm:0.00011110734340036288 max memory_allocated 47469.1044921875 
[2025-03-20 17:23:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 9 loss:0.03385376185178757 norm:0.00011833100143121555 max memory_allocated 47469.1044921875 
[2025-03-20 17:24:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 10 loss:0.03365188091993332 norm:0.00010792524699354544 max memory_allocated 47469.1044921875 
[2025-03-20 17:25:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 11 loss:0.03350379317998886 norm:9.877136471914127e-05 max memory_allocated 47469.1044921875 
[2025-03-20 17:25:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 12 loss:0.03341573476791382 norm:9.126898657996207e-05 max memory_allocated 47469.1044921875 
[2025-03-20 17:26:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 13 loss:0.033328987658023834 norm:9.147846867563203e-05 max memory_allocated 47469.1044921875 
[2025-03-20 17:27:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 14 loss:0.03326123207807541 norm:9.255886834580451e-05 max memory_allocated 47469.1044921875 
[2025-03-20 17:28:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 15 loss:0.033216092735528946 norm:9.238385973731056e-05 max memory_allocated 47469.1044921875 
[2025-03-20 17:29:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 16 loss:0.033156849443912506 norm:9.017460979521275e-05 max memory_allocated 47469.1044921875 
[2025-03-20 17:30:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 17 loss:0.03309686854481697 norm:8.811447332846001e-05 max memory_allocated 47469.1044921875 
[2025-03-20 17:31:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 18 loss:0.03305274620652199 norm:8.486671140417457e-05 max memory_allocated 47469.1044921875 
[2025-03-20 17:32:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 19 loss:0.03301050513982773 norm:8.504356810590252e-05 max memory_allocated 47469.1044921875 
[2025-03-20 17:33:14 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [14, 15]
[2025-03-20 17:33:14 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [16, 17, 18] ===
[2025-03-20 17:34:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 0 loss:0.09552206099033356 norm:0.0006424507009796798 max memory_allocated 47469.4169921875 
[2025-03-20 17:36:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 1 loss:0.07694235444068909 norm:0.00038726854836568236 max memory_allocated 47469.4169921875 
[2025-03-20 17:37:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 2 loss:0.0619804672896862 norm:0.0002668073575478047 max memory_allocated 47469.4169921875 
[2025-03-20 17:38:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 3 loss:0.05796002969145775 norm:0.00024875413510017097 max memory_allocated 47469.4169921875 
[2025-03-20 17:40:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 4 loss:0.05546341836452484 norm:0.00023428592248819768 max memory_allocated 47469.4169921875 
[2025-03-20 17:41:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 5 loss:0.05418409779667854 norm:0.00022082585201133043 max memory_allocated 47469.4169921875 
[2025-03-20 17:42:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 6 loss:0.05339360982179642 norm:0.00020950514590367675 max memory_allocated 47469.4169921875 
[2025-03-20 17:44:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 7 loss:0.052909526973962784 norm:0.0002046956360572949 max memory_allocated 47469.4169921875 
[2025-03-20 17:45:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 8 loss:0.05242098122835159 norm:0.00019578800129238516 max memory_allocated 47469.4169921875 
[2025-03-20 17:46:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 9 loss:0.05201205238699913 norm:0.0001920756621984765 max memory_allocated 47469.4169921875 
[2025-03-20 17:48:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 10 loss:0.051732342690229416 norm:0.0001885945093818009 max memory_allocated 47469.4169921875 
[2025-03-20 17:49:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 11 loss:0.05150957778096199 norm:0.00018706302216742188 max memory_allocated 47469.4169921875 
[2025-03-20 17:50:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 12 loss:0.0512392595410347 norm:0.00017895136261358857 max memory_allocated 47469.4169921875 
[2025-03-20 17:52:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 13 loss:0.051085054874420166 norm:0.00017735379515215755 max memory_allocated 47469.4169921875 
[2025-03-20 17:53:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 14 loss:0.050936922430992126 norm:0.00017295520228799433 max memory_allocated 47469.4169921875 
[2025-03-20 17:54:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 15 loss:0.050785358995199203 norm:0.00017344103252980858 max memory_allocated 47469.4169921875 
[2025-03-20 17:55:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 16 loss:0.05065193399786949 norm:0.00017248009680770338 max memory_allocated 47469.4169921875 
[2025-03-20 17:57:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 17 loss:0.050525322556495667 norm:0.00017333202413283288 max memory_allocated 47469.4169921875 
[2025-03-20 17:58:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 18 loss:0.05043388530611992 norm:0.00017077512165997177 max memory_allocated 47469.4169921875 
[2025-03-20 17:59:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 19 loss:0.050361622124910355 norm:0.00016977317864075303 max memory_allocated 47469.4169921875 
[2025-03-20 18:01:41 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [16, 17, 18]
[2025-03-20 18:01:41 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [19, 20, 21] ===
[2025-03-20 18:03:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 0 loss:0.122672438621521 norm:0.0024188600946217775 max memory_allocated 47469.6044921875 
[2025-03-20 18:04:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 1 loss:0.1067613884806633 norm:0.0008286351221613586 max memory_allocated 47469.6044921875 
[2025-03-20 18:05:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 2 loss:0.0935441106557846 norm:0.00045978164416737854 max memory_allocated 47469.6044921875 
[2025-03-20 18:07:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 3 loss:0.08673109114170074 norm:0.00040844036266207695 max memory_allocated 47469.6044921875 
[2025-03-20 18:08:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 4 loss:0.0835399478673935 norm:0.00038923215470276773 max memory_allocated 47469.6044921875 
[2025-03-20 18:09:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 5 loss:0.08218248933553696 norm:0.00033867580350488424 max memory_allocated 47469.6044921875 
[2025-03-20 18:11:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 6 loss:0.0815809965133667 norm:0.0003321040130686015 max memory_allocated 47469.6044921875 
[2025-03-20 18:12:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 7 loss:0.08130615949630737 norm:0.00035818840842694044 max memory_allocated 47469.6044921875 
[2025-03-20 18:13:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 8 loss:0.08082489669322968 norm:0.00029667431954294443 max memory_allocated 47469.6044921875 
[2025-03-20 18:15:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 9 loss:0.08053315430879593 norm:0.0002879602834582329 max memory_allocated 47469.6044921875 
[2025-03-20 18:16:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 10 loss:0.08029879629611969 norm:0.00028325465973466635 max memory_allocated 47469.6044921875 
[2025-03-20 18:17:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 11 loss:0.08014243841171265 norm:0.00026679167058318853 max memory_allocated 47469.6044921875 
[2025-03-20 18:19:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 12 loss:0.07998622208833694 norm:0.0002713299181777984 max memory_allocated 47469.6044921875 
[2025-03-20 18:20:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 13 loss:0.07985204458236694 norm:0.00027840552502311766 max memory_allocated 47469.6044921875 
[2025-03-20 18:21:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 14 loss:0.07967360317707062 norm:0.0002719615586102009 max memory_allocated 47469.6044921875 
[2025-03-20 18:23:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 15 loss:0.07958374917507172 norm:0.0002705929218791425 max memory_allocated 47469.6044921875 
[2025-03-20 18:24:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 16 loss:0.07951873540878296 norm:0.0002793860621750355 max memory_allocated 47469.6044921875 
[2025-03-20 18:25:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 17 loss:0.07948867976665497 norm:0.0002867015136871487 max memory_allocated 47469.6044921875 
[2025-03-20 18:27:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 18 loss:0.07944463938474655 norm:0.0002679485478438437 max memory_allocated 47469.6044921875 
[2025-03-20 18:28:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 19 loss:0.07935256510972977 norm:0.0002682677295524627 max memory_allocated 47469.6044921875 
[2025-03-20 18:30:08 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [19, 20, 21]
[2025-03-20 18:30:08 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [22, 23, 24] ===
[2025-03-20 18:31:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 0 loss:0.17733661830425262 norm:0.0021929729264229536 max memory_allocated 47469.7919921875 
[2025-03-20 18:32:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 1 loss:0.15686647593975067 norm:0.0008812284795567393 max memory_allocated 47469.7919921875 
[2025-03-20 18:34:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 2 loss:0.13902518153190613 norm:0.0004903146764263511 max memory_allocated 47469.7919921875 
[2025-03-20 18:35:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 3 loss:0.1305670291185379 norm:0.0004123846592847258 max memory_allocated 47469.7919921875 
[2025-03-20 18:36:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 4 loss:0.1275394707918167 norm:0.0003875515831168741 max memory_allocated 47469.7919921875 
[2025-03-20 18:38:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 5 loss:0.12636560201644897 norm:0.0003648761485237628 max memory_allocated 47469.7919921875 
[2025-03-20 18:39:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 6 loss:0.12568937242031097 norm:0.00034248799784108996 max memory_allocated 47469.7919921875 
[2025-03-20 18:40:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 7 loss:0.12522684037685394 norm:0.0003392279031686485 max memory_allocated 47469.7919921875 
[2025-03-20 18:42:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 8 loss:0.12491142004728317 norm:0.0003335488145239651 max memory_allocated 47469.7919921875 
[2025-03-20 18:43:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 9 loss:0.12451577186584473 norm:0.0003281567187514156 max memory_allocated 47469.7919921875 
[2025-03-20 18:44:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 10 loss:0.12412908673286438 norm:0.00031934582511894405 max memory_allocated 47469.7919921875 
[2025-03-20 18:46:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 11 loss:0.1238776221871376 norm:0.0003209057031199336 max memory_allocated 47469.7919921875 
[2025-03-20 18:47:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 12 loss:0.12369538098573685 norm:0.00031579274218529463 max memory_allocated 47469.7919921875 
[2025-03-20 18:48:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 13 loss:0.12351173907518387 norm:0.0003222294326405972 max memory_allocated 47469.7919921875 
[2025-03-20 18:50:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 14 loss:0.12325017154216766 norm:0.0003008879139088094 max memory_allocated 47469.7919921875 
[2025-03-20 18:51:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 15 loss:0.12311157584190369 norm:0.00029493222245946527 max memory_allocated 47469.7919921875 
[2025-03-20 18:52:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 16 loss:0.1229681521654129 norm:0.0002918545505963266 max memory_allocated 47469.7919921875 
[2025-03-20 18:54:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 17 loss:0.12289038300514221 norm:0.0003005166945513338 max memory_allocated 47469.7919921875 
[2025-03-20 18:55:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 18 loss:0.12285108864307404 norm:0.0002989005297422409 max memory_allocated 47469.7919921875 
[2025-03-20 18:56:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 19 loss:0.12275327742099762 norm:0.0003008175117429346 max memory_allocated 47469.7919921875 
[2025-03-20 18:58:34 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [22, 23, 24]
[2025-03-20 18:58:34 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [25, 26] ===
[2025-03-20 18:59:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 0 loss:0.1996835619211197 norm:0.0010445188963785768 max memory_allocated 47469.7919921875 
[2025-03-20 19:00:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 1 loss:0.18218612670898438 norm:0.0005268928944133222 max memory_allocated 47469.7919921875 
[2025-03-20 19:01:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 2 loss:0.1648956537246704 norm:0.00033570354571565986 max memory_allocated 47469.7919921875 
[2025-03-20 19:02:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 3 loss:0.15921296179294586 norm:0.00030147144570946693 max memory_allocated 47469.7919921875 
[2025-03-20 19:03:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 4 loss:0.15802446007728577 norm:0.00027975073317065835 max memory_allocated 47469.7919921875 
[2025-03-20 19:04:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 5 loss:0.1573614627122879 norm:0.0002448159211780876 max memory_allocated 47469.7919921875 
[2025-03-20 19:04:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 6 loss:0.15695098042488098 norm:0.00023915132624097168 max memory_allocated 47469.7919921875 
[2025-03-20 19:05:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 7 loss:0.15654394030570984 norm:0.00022685949807055295 max memory_allocated 47469.7919921875 
[2025-03-20 19:06:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 8 loss:0.15617087483406067 norm:0.00021235097665339708 max memory_allocated 47469.7919921875 
[2025-03-20 19:07:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 9 loss:0.15589343011379242 norm:0.0002227040531579405 max memory_allocated 47469.7919921875 
[2025-03-20 19:08:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 10 loss:0.15568439662456512 norm:0.0002110915957018733 max memory_allocated 47469.7919921875 
[2025-03-20 19:09:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 11 loss:0.15545013546943665 norm:0.0002058623795164749 max memory_allocated 47469.7919921875 
[2025-03-20 19:10:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 12 loss:0.15524111688137054 norm:0.00020553349168039858 max memory_allocated 47469.7919921875 
[2025-03-20 19:11:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 13 loss:0.15502801537513733 norm:0.00020134785154368728 max memory_allocated 47469.7919921875 
[2025-03-20 19:12:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 14 loss:0.15489140152931213 norm:0.00020644848700612783 max memory_allocated 47469.7919921875 
[2025-03-20 19:12:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 15 loss:0.15473535656929016 norm:0.00020395423052832484 max memory_allocated 47469.7919921875 
[2025-03-20 19:13:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 16 loss:0.15465384721755981 norm:0.0002014262427110225 max memory_allocated 47469.7919921875 
[2025-03-20 19:14:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 17 loss:0.15452270209789276 norm:0.00020418365602381527 max memory_allocated 47469.7919921875 
[2025-03-20 19:15:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 18 loss:0.15443071722984314 norm:0.00020362452778499573 max memory_allocated 47469.7919921875 
[2025-03-20 19:16:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 19 loss:0.15436074137687683 norm:0.0002014079364016652 max memory_allocated 47469.7919921875 
[2025-03-20 19:17:33 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [25, 26]
[2025-03-20 19:17:33 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [27] ===
[2025-03-20 19:18:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 0 loss:0.2038140743970871 norm:0.0009401587303727865 max memory_allocated 47469.7919921875 
[2025-03-20 19:18:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 1 loss:0.18933619558811188 norm:0.0005250473041087389 max memory_allocated 47469.7919921875 
[2025-03-20 19:18:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 2 loss:0.17605683207511902 norm:0.00030161181348375976 max memory_allocated 47469.7919921875 
[2025-03-20 19:19:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 3 loss:0.17307093739509583 norm:0.0002233756531495601 max memory_allocated 47469.7919921875 
[2025-03-20 19:19:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 4 loss:0.1724245846271515 norm:0.00021231890423223376 max memory_allocated 47469.7919921875 
[2025-03-20 19:20:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 5 loss:0.172023743391037 norm:0.00019462262571323663 max memory_allocated 47469.7919921875 
[2025-03-20 19:20:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 6 loss:0.17168457806110382 norm:0.00017158297123387456 max memory_allocated 47469.7919921875 
[2025-03-20 19:21:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 7 loss:0.17141316831111908 norm:0.00015860877465456724 max memory_allocated 47469.7919921875 
[2025-03-20 19:21:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 8 loss:0.17115283012390137 norm:0.0001476556935813278 max memory_allocated 47469.7919921875 
[2025-03-20 19:22:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 9 loss:0.17095470428466797 norm:0.00014140391431283206 max memory_allocated 47469.7919921875 
[2025-03-20 19:22:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 10 loss:0.17077940702438354 norm:0.00013993321044836193 max memory_allocated 47469.7919921875 
[2025-03-20 19:22:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 11 loss:0.17061109840869904 norm:0.000135077178128995 max memory_allocated 47469.7919921875 
[2025-03-20 19:23:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 12 loss:0.17050258815288544 norm:0.00013265488087199628 max memory_allocated 47469.7919921875 
[2025-03-20 19:23:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 13 loss:0.1703709363937378 norm:0.00014278950402513146 max memory_allocated 47469.7919921875 
[2025-03-20 19:24:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 14 loss:0.17026659846305847 norm:0.00014334380102809519 max memory_allocated 47469.7919921875 
[2025-03-20 19:24:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 15 loss:0.17017456889152527 norm:0.00014184745668899268 max memory_allocated 47469.7919921875 
[2025-03-20 19:25:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 16 loss:0.17008106410503387 norm:0.0001381783513352275 max memory_allocated 47469.7919921875 
[2025-03-20 19:25:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 17 loss:0.1700054109096527 norm:0.00013971106091048568 max memory_allocated 47469.7919921875 
[2025-03-20 19:26:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 18 loss:0.16993564367294312 norm:0.00015231454744935036 max memory_allocated 47469.7919921875 
[2025-03-20 19:26:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 19 loss:0.1698761284351349 norm:0.0001501796068623662 max memory_allocated 47469.7919921875 
[2025-03-20 19:27:05 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [27]
[2025-03-20 19:27:05 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [28] ===
[2025-03-20 19:27:05 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 19:27:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 0 loss:0.23822739720344543 norm:0.012349839322268963 max memory_allocated 47469.7919921875 
[2025-03-20 19:28:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 1 loss:0.22110973298549652 norm:0.009553627111017704 max memory_allocated 47469.7919921875 
[2025-03-20 19:28:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 2 loss:0.20590709149837494 norm:0.006525884382426739 max memory_allocated 47469.7919921875 
[2025-03-20 19:28:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 3 loss:0.20249098539352417 norm:0.005676145199686289 max memory_allocated 47469.7919921875 
[2025-03-20 19:29:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 4 loss:0.2016468644142151 norm:0.00484975753352046 max memory_allocated 47469.7919921875 
[2025-03-20 19:29:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 5 loss:0.20099101960659027 norm:0.004069440998136997 max memory_allocated 47469.7919921875 
[2025-03-20 19:30:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 6 loss:0.20049674808979034 norm:0.0034717696253210306 max memory_allocated 47469.7919921875 
[2025-03-20 19:30:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 7 loss:0.20010924339294434 norm:0.0033043597359210253 max memory_allocated 47469.7919921875 
[2025-03-20 19:31:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 8 loss:0.19989581406116486 norm:0.003309395629912615 max memory_allocated 47469.7919921875 
[2025-03-20 19:31:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 9 loss:0.19966860115528107 norm:0.0033705076202750206 max memory_allocated 47469.7919921875 
[2025-03-20 19:32:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 10 loss:0.19945120811462402 norm:0.002965793013572693 max memory_allocated 47469.7919921875 
[2025-03-20 19:32:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 11 loss:0.19916018843650818 norm:0.002869363408535719 max memory_allocated 47469.7919921875 
[2025-03-20 19:32:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 12 loss:0.1989409625530243 norm:0.0026762697380036116 max memory_allocated 47469.7919921875 
[2025-03-20 19:33:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 13 loss:0.1987849473953247 norm:0.0027246233075857162 max memory_allocated 47469.7919921875 
[2025-03-20 19:33:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 14 loss:0.19863584637641907 norm:0.002617734018713236 max memory_allocated 47469.7919921875 
[2025-03-20 19:34:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 15 loss:0.1984846293926239 norm:0.0025724817533046007 max memory_allocated 47469.7919921875 
[2025-03-20 19:34:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 16 loss:0.19832582771778107 norm:0.0023662769235670567 max memory_allocated 47469.7919921875 
[2025-03-20 19:35:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 17 loss:0.19819703698158264 norm:0.002378578530624509 max memory_allocated 47469.7919921875 
[2025-03-20 19:35:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 18 loss:0.19812165200710297 norm:0.0022378014400601387 max memory_allocated 47469.7919921875 
[2025-03-20 19:36:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 19 loss:0.19804830849170685 norm:0.0023084189742803574 max memory_allocated 47469.7919921875 
[2025-03-20 19:36:41 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [28]
[2025-03-20 19:36:41 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [29] ===
[2025-03-20 19:36:41 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 19:37:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 0 loss:0.27741920948028564 norm:0.012598982080817223 max memory_allocated 47469.7919921875 
[2025-03-20 19:37:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 1 loss:0.2576403021812439 norm:0.008734429255127907 max memory_allocated 47469.7919921875 
[2025-03-20 19:38:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 2 loss:0.24090808629989624 norm:0.006189353298395872 max memory_allocated 47469.7919921875 
[2025-03-20 19:38:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 3 loss:0.23759007453918457 norm:0.005344097968190908 max memory_allocated 47469.7919921875 
[2025-03-20 19:38:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 4 loss:0.23674187064170837 norm:0.0045487042516469955 max memory_allocated 47469.7919921875 
[2025-03-20 19:39:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 5 loss:0.23601993918418884 norm:0.003817716147750616 max memory_allocated 47469.7919921875 
[2025-03-20 19:39:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 6 loss:0.23555836081504822 norm:0.003417905420064926 max memory_allocated 47469.7919921875 
[2025-03-20 19:40:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 7 loss:0.23511627316474915 norm:0.0033032468054443598 max memory_allocated 47469.7919921875 
[2025-03-20 19:40:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 8 loss:0.23497061431407928 norm:0.00303532462567091 max memory_allocated 47469.7919921875 
[2025-03-20 19:41:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 9 loss:0.23456576466560364 norm:0.0030142590403556824 max memory_allocated 47469.7919921875 
[2025-03-20 19:41:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 10 loss:0.23427215218544006 norm:0.002706534694880247 max memory_allocated 47469.7919921875 
[2025-03-20 19:42:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 11 loss:0.23396702110767365 norm:0.00267922249622643 max memory_allocated 47469.7919921875 
[2025-03-20 19:42:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 12 loss:0.23374749720096588 norm:0.002450695727020502 max memory_allocated 47469.7919921875 
[2025-03-20 19:43:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 13 loss:0.23355042934417725 norm:0.0024817688390612602 max memory_allocated 47469.7919921875 
[2025-03-20 19:43:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 14 loss:0.2333868443965912 norm:0.00231750076636672 max memory_allocated 47469.7919921875 
[2025-03-20 19:43:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 15 loss:0.23322917520999908 norm:0.0024044280871748924 max memory_allocated 47469.7919921875 
[2025-03-20 19:44:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 16 loss:0.23311389982700348 norm:0.0022988934069871902 max memory_allocated 47469.7919921875 
[2025-03-20 19:44:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 17 loss:0.23293079435825348 norm:0.0022717039100825787 max memory_allocated 47469.7919921875 
[2025-03-20 19:45:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 18 loss:0.23288258910179138 norm:0.0021602388005703688 max memory_allocated 47469.7919921875 
[2025-03-20 19:45:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 19 loss:0.2327694296836853 norm:0.002221145434305072 max memory_allocated 47469.7919921875 
[2025-03-20 19:46:15 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [29]
[2025-03-20 19:46:15 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [30] ===
[2025-03-20 19:46:16 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 19:46:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 0 loss:0.3048269748687744 norm:0.017805056646466255 max memory_allocated 47469.7919921875 
[2025-03-20 19:47:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 1 loss:0.29501873254776 norm:0.013385348953306675 max memory_allocated 47469.7919921875 
[2025-03-20 19:47:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 2 loss:0.28545665740966797 norm:0.011288155801594257 max memory_allocated 47469.7919921875 
[2025-03-20 19:48:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 3 loss:0.28184089064598083 norm:0.009057736024260521 max memory_allocated 47469.7919921875 
[2025-03-20 19:48:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 4 loss:0.28087612986564636 norm:0.007939881645143032 max memory_allocated 47469.7919921875 
[2025-03-20 19:49:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 5 loss:0.2802479863166809 norm:0.007491775322705507 max memory_allocated 47469.7919921875 
[2025-03-20 19:49:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 6 loss:0.2797885537147522 norm:0.006516651716083288 max memory_allocated 47469.7919921875 
[2025-03-20 19:49:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 7 loss:0.2795630693435669 norm:0.006128381006419659 max memory_allocated 47469.7919921875 
[2025-03-20 19:50:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 8 loss:0.27911096811294556 norm:0.005668116733431816 max memory_allocated 47469.7919921875 
[2025-03-20 19:50:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 9 loss:0.27885833382606506 norm:0.005282703321427107 max memory_allocated 47469.7919921875 
[2025-03-20 19:51:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 10 loss:0.27869266271591187 norm:0.005096890032291412 max memory_allocated 47469.7919921875 
[2025-03-20 19:51:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 11 loss:0.27890655398368835 norm:0.004999143071472645 max memory_allocated 47469.7919921875 
[2025-03-20 19:52:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 12 loss:0.27839162945747375 norm:0.004525621421635151 max memory_allocated 47469.7919921875 
[2025-03-20 19:52:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 13 loss:0.2784966230392456 norm:0.004512893036007881 max memory_allocated 47469.7919921875 
[2025-03-20 19:53:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 14 loss:0.27845847606658936 norm:0.004337341524660587 max memory_allocated 47469.7919921875 
[2025-03-20 19:53:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 15 loss:0.2783815860748291 norm:0.0044022114016115665 max memory_allocated 47469.7919921875 
[2025-03-20 19:53:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 16 loss:0.27830392122268677 norm:0.004006533417850733 max memory_allocated 47469.7919921875 
[2025-03-20 19:54:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 17 loss:0.27811485528945923 norm:0.004064719658344984 max memory_allocated 47469.7919921875 
[2025-03-20 19:54:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 18 loss:0.27806469798088074 norm:0.003819603007286787 max memory_allocated 47469.7919921875 
[2025-03-20 19:55:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 19 loss:0.2779093384742737 norm:0.003645471762865782 max memory_allocated 47469.7919921875 
[2025-03-20 19:55:50 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [30]
[2025-03-20 19:55:50 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [31] ===
[2025-03-20 19:55:50 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 19:56:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 0 loss:0.4786491394042969 norm:0.030968185514211655 max memory_allocated 47469.7919921875 
[2025-03-20 19:56:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 1 loss:0.45376697182655334 norm:0.022431248798966408 max memory_allocated 47469.7919921875 
[2025-03-20 19:57:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 2 loss:0.43897882103919983 norm:0.01690840534865856 max memory_allocated 47469.7919921875 
[2025-03-20 19:57:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 3 loss:0.4311264753341675 norm:0.013914196752011776 max memory_allocated 47469.7919921875 
[2025-03-20 19:58:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 4 loss:0.4281565248966217 norm:0.012137806043028831 max memory_allocated 47469.7919921875 
[2025-03-20 19:58:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 5 loss:0.4263628125190735 norm:0.0108136385679245 max memory_allocated 47469.7919921875 
[2025-03-20 19:59:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 6 loss:0.42461055517196655 norm:0.009612229652702808 max memory_allocated 47469.7919921875 
[2025-03-20 19:59:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 7 loss:0.42381906509399414 norm:0.009244855493307114 max memory_allocated 47469.7919921875 
[2025-03-20 19:59:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 8 loss:0.42355209589004517 norm:0.009087802842259407 max memory_allocated 47469.7919921875 
[2025-03-20 20:00:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 9 loss:0.4229232370853424 norm:0.00951545499265194 max memory_allocated 47469.7919921875 
[2025-03-20 20:00:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 10 loss:0.42281782627105713 norm:0.009781494736671448 max memory_allocated 47469.7919921875 
[2025-03-20 20:01:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 11 loss:0.42210614681243896 norm:0.009168365970253944 max memory_allocated 47469.7919921875 
[2025-03-20 20:01:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 12 loss:0.4221222996711731 norm:0.0091570308431983 max memory_allocated 47469.7919921875 
[2025-03-20 20:02:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 13 loss:0.4217422902584076 norm:0.008924922905862331 max memory_allocated 47469.7919921875 
[2025-03-20 20:02:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 14 loss:0.4216836988925934 norm:0.009195865131914616 max memory_allocated 47469.7919921875 
[2025-03-20 20:03:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 15 loss:0.42112764716148376 norm:0.008349746465682983 max memory_allocated 47469.7919921875 
[2025-03-20 20:03:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 16 loss:0.4213412404060364 norm:0.008750318549573421 max memory_allocated 47469.7919921875 
[2025-03-20 20:03:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 17 loss:0.42075085639953613 norm:0.008790779858827591 max memory_allocated 47469.7919921875 
[2025-03-20 20:04:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 18 loss:0.4211752414703369 norm:0.009556775912642479 max memory_allocated 47469.7919921875 
[2025-03-20 20:04:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 19 loss:0.4209938645362854 norm:0.008909663185477257 max memory_allocated 47469.7919921875 
[2025-03-20 20:05:27 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 15, block: [31]
[2025-03-20 20:05:27 root] (main_calib_config3_attn.py 379): INFO 18272.1486222744
[2025-03-20 20:05:32 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-20 20:06:18 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.780422210693359
[2025-03-20 20:06:18 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-20 20:07:30 root] (main_calib_config3_attn.py 161): INFO c4 : 7.219733238220215
[2025-03-20 20:45:58 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.780422210693359, 'c4': 7.219733238220215, 'results': {'piqa': {'acc': 0.7823721436343852, 'acc_stderr': 0.009627407474840878, 'acc_norm': 0.7725788900979326, 'acc_norm_stderr': 0.009779850767847226}, 'arc_challenge': {'acc': 0.38822525597269625, 'acc_stderr': 0.014241614207414037, 'acc_norm': 0.4069965870307167, 'acc_norm_stderr': 0.014356399418009124}, 'arc_easy': {'acc': 0.6611952861952862, 'acc_stderr': 0.009711980224301647, 'acc_norm': 0.515993265993266, 'acc_norm_stderr': 0.010254533589288184}, 'winogrande': {'acc': 0.6614048934490924, 'acc_stderr': 0.013300169865842409}, 'hellaswag': {'acc': 0.5578570005974905, 'acc_stderr': 0.004956262919324398, 'acc_norm': 0.7208723361880104, 'acc_norm_stderr': 0.004476536569056588}, 'boolq': {'acc': 0.7149847094801223, 'acc_stderr': 0.007895415622528962}}, 'versions': {'piqa': 0, 'arc_challenge': 0, 'arc_easy': 0, 'winogrande': 0, 'hellaswag': 0, 'boolq': 1}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-20 20:45:58 root] (main_calib_config3_attn.py 175): INFO 38.82,66.12,71.50,55.79,78.24,66.14
