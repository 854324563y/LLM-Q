[2025-03-23 01:50:18 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/Llama-2-7b-hf-0.8', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.8.pkl', blocks_pkl='./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-23 01:51:40 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-23 01:51:40 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-23 01:51:40 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-23 01:51:40 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.8.pkl
[2025-03-23 01:51:40 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 9), (9, 11), (11, 14), (14, 17), (17, 20), (20, 23), (23, 26), (26, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-23 01:51:40 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12, 13], [14, 15, 16], [17, 18, 19], [20, 21, 22], [23, 24, 25], [26, 27, 28], [29], [30], [31]]
[2025-03-23 01:51:43 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-23 01:51:43 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 01:52:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.001646880293264985 norm:0.005481779109686613 max memory_allocated 34633.880859375 
[2025-03-23 01:52:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.0006905249902047217 norm:0.002468099817633629 max memory_allocated 34633.880859375 
[2025-03-23 01:53:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.0005904923309572041 norm:0.0019022306660190225 max memory_allocated 34633.880859375 
[2025-03-23 01:53:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0005571761284954846 norm:0.0016812117537483573 max memory_allocated 34633.880859375 
[2025-03-23 01:54:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.0005477633676491678 norm:0.0015103479381650686 max memory_allocated 34633.880859375 
[2025-03-23 01:54:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0005378435016609728 norm:0.0014722503256052732 max memory_allocated 34633.880859375 
[2025-03-23 01:55:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.0005239768070168793 norm:0.0013091089203953743 max memory_allocated 34633.880859375 
[2025-03-23 01:55:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0005143635207787156 norm:0.0012548048980534077 max memory_allocated 34633.880859375 
[2025-03-23 01:56:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0005060707917436957 norm:0.001154877245426178 max memory_allocated 34633.880859375 
[2025-03-23 01:56:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.0004965780535712838 norm:0.00104379968252033 max memory_allocated 34633.880859375 
[2025-03-23 01:57:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.0004902085056528449 norm:0.0010303840972483158 max memory_allocated 34633.880859375 
[2025-03-23 01:57:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0004848612588830292 norm:0.0009522775653749704 max memory_allocated 34633.880859375 
[2025-03-23 01:58:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.00047463871305808425 norm:0.0008624987676739693 max memory_allocated 34633.880859375 
[2025-03-23 01:58:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.0004697894037235528 norm:0.0008078549290075898 max memory_allocated 34633.880859375 
[2025-03-23 01:59:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0004595099890138954 norm:0.0007512163138017058 max memory_allocated 34633.880859375 
[2025-03-23 01:59:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.0004485922690946609 norm:0.0006756079383194447 max memory_allocated 34633.880859375 
[2025-03-23 01:59:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.00044460606295615435 norm:0.0006519460584968328 max memory_allocated 34633.880859375 
[2025-03-23 02:00:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0004472354776225984 norm:0.000646424014121294 max memory_allocated 34633.880859375 
[2025-03-23 02:00:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.00044829558464698493 norm:0.0005959707777947187 max memory_allocated 34633.880859375 
[2025-03-23 02:01:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0004407578962855041 norm:0.00057517911773175 max memory_allocated 34633.880859375 
[2025-03-23 02:02:05 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-23 02:02:05 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-23 02:02:05 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:02:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.019997332245111465 norm:0.013045408762991428 max memory_allocated 35100.7724609375 
[2025-03-23 02:03:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.012379325926303864 norm:0.010368408635258675 max memory_allocated 35100.7724609375 
[2025-03-23 02:03:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.009234786964952946 norm:0.011665391735732555 max memory_allocated 35100.7724609375 
[2025-03-23 02:04:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.007907364517450333 norm:0.010822172276675701 max memory_allocated 35100.7724609375 
[2025-03-23 02:04:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.007161897141486406 norm:0.007719370536506176 max memory_allocated 35100.7724609375 
[2025-03-23 02:05:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.006935914512723684 norm:0.006168747320771217 max memory_allocated 35100.7724609375 
[2025-03-23 02:05:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.006927717477083206 norm:0.0055550383403897285 max memory_allocated 35100.7724609375 
[2025-03-23 02:05:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.006697274744510651 norm:0.004931269679218531 max memory_allocated 35100.7724609375 
[2025-03-23 02:06:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.006804545409977436 norm:0.00488212238997221 max memory_allocated 35100.7724609375 
[2025-03-23 02:06:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.006703343708068132 norm:0.004925396293401718 max memory_allocated 35100.7724609375 
[2025-03-23 02:07:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.006539315916597843 norm:0.0046605924144387245 max memory_allocated 35100.7724609375 
[2025-03-23 02:07:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.0064530097879469395 norm:0.004397484939545393 max memory_allocated 35100.7724609375 
[2025-03-23 02:08:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.006339059676975012 norm:0.004203554708510637 max memory_allocated 35100.7724609375 
[2025-03-23 02:08:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.006701335776597261 norm:0.00442138547077775 max memory_allocated 35100.7724609375 
[2025-03-23 02:09:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.007097452878952026 norm:0.0055260746739804745 max memory_allocated 35100.7724609375 
[2025-03-23 02:09:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.008305588737130165 norm:0.00813346542418003 max memory_allocated 35100.7724609375 
[2025-03-23 02:10:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.006352658849209547 norm:0.005980010610073805 max memory_allocated 35100.7724609375 
[2025-03-23 02:10:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.006314116530120373 norm:0.005523199215531349 max memory_allocated 35100.7724609375 
[2025-03-23 02:11:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.006368696689605713 norm:0.005370819941163063 max memory_allocated 35100.7724609375 
[2025-03-23 02:11:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.00644927890971303 norm:0.005516005214303732 max memory_allocated 35100.7724609375 
[2025-03-23 02:12:21 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-23 02:12:21 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
[2025-03-23 02:12:21 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:12:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.01123273279517889 norm:0.005269140936434269 max memory_allocated 35100.8349609375 
[2025-03-23 02:13:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.008469857275485992 norm:0.0040510003454983234 max memory_allocated 35100.8349609375 
[2025-03-23 02:13:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.007627128157764673 norm:0.003068511374294758 max memory_allocated 35100.8349609375 
[2025-03-23 02:14:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.007073301821947098 norm:0.0025745308957993984 max memory_allocated 35100.8349609375 
[2025-03-23 02:14:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.006694361101835966 norm:0.002234020736068487 max memory_allocated 35100.8349609375 
[2025-03-23 02:15:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.006384551525115967 norm:0.0019445179495960474 max memory_allocated 35100.8349609375 
[2025-03-23 02:15:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.006214573513716459 norm:0.0016869562678039074 max memory_allocated 35100.8349609375 
[2025-03-23 02:16:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.006145543418824673 norm:0.0014378309715539217 max memory_allocated 35100.8349609375 
[2025-03-23 02:16:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.006109223701059818 norm:0.00120948301628232 max memory_allocated 35100.8349609375 
[2025-03-23 02:17:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.006073488388210535 norm:0.000998809584416449 max memory_allocated 35100.8349609375 
[2025-03-23 02:17:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.00604974664747715 norm:0.0008104215376079082 max memory_allocated 35100.8349609375 
[2025-03-23 02:18:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.006052233278751373 norm:0.0007336597773246467 max memory_allocated 35100.8349609375 
[2025-03-23 02:18:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.00610320782288909 norm:0.0007722800364717841 max memory_allocated 35100.8349609375 
[2025-03-23 02:19:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.006021687760949135 norm:0.0006238053902052343 max memory_allocated 35100.8349609375 
[2025-03-23 02:19:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.006028586998581886 norm:0.0006228568963706493 max memory_allocated 35100.8349609375 
[2025-03-23 02:20:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.006060851737856865 norm:0.0006803203141316772 max memory_allocated 35100.8349609375 
[2025-03-23 02:20:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.006195015273988247 norm:0.0008959532715380192 max memory_allocated 35100.8349609375 
[2025-03-23 02:21:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.006249629892408848 norm:0.0008750483975745738 max memory_allocated 35100.8349609375 
[2025-03-23 02:21:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.006044068839401007 norm:0.000547770643606782 max memory_allocated 35100.8349609375 
[2025-03-23 02:22:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.005964010488241911 norm:0.0003437469422351569 max memory_allocated 35100.8349609375 
[2025-03-23 02:22:38 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2]
[2025-03-23 02:22:38 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4, 5] ===
[2025-03-23 02:24:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 0 loss:0.01885981857776642 norm:0.0002917726233135909 max memory_allocated 47477.6044921875 
[2025-03-23 02:25:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 1 loss:0.016628548502922058 norm:0.00020847997802775353 max memory_allocated 47477.6044921875 
[2025-03-23 02:27:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 2 loss:0.015289390459656715 norm:0.00016815509297885 max memory_allocated 47477.6044921875 
[2025-03-23 02:28:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 3 loss:0.013929877430200577 norm:0.00014683970948681235 max memory_allocated 47477.6044921875 
[2025-03-23 02:29:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 4 loss:0.012794043868780136 norm:0.00014803583326283842 max memory_allocated 47477.6044921875 
[2025-03-23 02:31:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 5 loss:0.012071375735104084 norm:0.00013827322982251644 max memory_allocated 47477.6044921875 
[2025-03-23 02:32:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 6 loss:0.011645051650702953 norm:0.00014248202205635607 max memory_allocated 47477.6044921875 
[2025-03-23 02:34:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 7 loss:0.011433541774749756 norm:0.00015471891674678773 max memory_allocated 47477.6044921875 
[2025-03-23 02:35:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 8 loss:0.01132405549287796 norm:0.00014406454283744097 max memory_allocated 47477.6044921875 
[2025-03-23 02:37:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 9 loss:0.011286155320703983 norm:0.00016314238018821925 max memory_allocated 47477.6044921875 
[2025-03-23 02:38:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 10 loss:0.011253099888563156 norm:0.00017299712635576725 max memory_allocated 47477.6044921875 
[2025-03-23 02:39:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 11 loss:0.011237331666052341 norm:0.00018108272342942655 max memory_allocated 47477.6044921875 
[2025-03-23 02:41:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 12 loss:0.011217370629310608 norm:0.0001599045644979924 max memory_allocated 47477.6044921875 
[2025-03-23 02:42:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 13 loss:0.011194356717169285 norm:0.00016228470485657454 max memory_allocated 47477.6044921875 
[2025-03-23 02:44:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 14 loss:0.01117772702127695 norm:0.00021667443797923625 max memory_allocated 47477.6044921875 
[2025-03-23 02:45:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 15 loss:0.01117704901844263 norm:0.0002353210875298828 max memory_allocated 47477.6044921875 
[2025-03-23 02:47:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 16 loss:0.011165972799062729 norm:0.0002356524346396327 max memory_allocated 47477.6044921875 
[2025-03-23 02:48:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 17 loss:0.011127489618957043 norm:0.00021704749087803066 max memory_allocated 47477.6044921875 
[2025-03-23 02:50:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 18 loss:0.011121832765638828 norm:0.00021577472216449678 max memory_allocated 47477.6044921875 
[2025-03-23 02:51:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4, 5]) iter 19 loss:0.01111689954996109 norm:0.00022091779101174325 max memory_allocated 47477.6044921875 
[2025-03-23 02:53:23 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [3, 4, 5]
[2025-03-23 02:53:23 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [6, 7, 8] ===
[2025-03-23 02:54:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 0 loss:0.026061182841658592 norm:0.000509183737449348 max memory_allocated 47477.7919921875 
[2025-03-23 02:56:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 1 loss:0.02241801656782627 norm:0.00028523860964924097 max memory_allocated 47477.7919921875 
[2025-03-23 02:57:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 2 loss:0.020408257842063904 norm:0.000232747828704305 max memory_allocated 47477.7919921875 
[2025-03-23 02:59:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 3 loss:0.018554210662841797 norm:0.00019626604625955224 max memory_allocated 47477.7919921875 
[2025-03-23 03:00:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 4 loss:0.017137881368398666 norm:0.00017614771786611527 max memory_allocated 47477.7919921875 
[2025-03-23 03:02:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 5 loss:0.016265785321593285 norm:0.00016987218987196684 max memory_allocated 47477.7919921875 
[2025-03-23 03:03:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 6 loss:0.015771014615893364 norm:0.00017207908967975527 max memory_allocated 47477.7919921875 
[2025-03-23 03:04:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 7 loss:0.015488101169466972 norm:0.0001691985671641305 max memory_allocated 47477.7919921875 
[2025-03-23 03:06:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 8 loss:0.015294177457690239 norm:0.00016498945478815585 max memory_allocated 47477.7919921875 
[2025-03-23 03:07:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 9 loss:0.015176916494965553 norm:0.00016272450739052147 max memory_allocated 47477.7919921875 
[2025-03-23 03:09:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 10 loss:0.015116948634386063 norm:0.00016297775437124074 max memory_allocated 47477.7919921875 
[2025-03-23 03:10:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 11 loss:0.015065494924783707 norm:0.00015562075714115053 max memory_allocated 47477.7919921875 
[2025-03-23 03:12:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 12 loss:0.015008890070021152 norm:0.00015094754053279757 max memory_allocated 47477.7919921875 
[2025-03-23 03:13:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 13 loss:0.014948591589927673 norm:0.00014651792298536748 max memory_allocated 47477.7919921875 
[2025-03-23 03:14:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 14 loss:0.014906964264810085 norm:0.000145758138387464 max memory_allocated 47477.7919921875 
[2025-03-23 03:16:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 15 loss:0.01488126814365387 norm:0.0001407986128469929 max memory_allocated 47477.7919921875 
[2025-03-23 03:17:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 16 loss:0.014861887320876122 norm:0.00014992785872891545 max memory_allocated 47477.7919921875 
[2025-03-23 03:19:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 17 loss:0.014825358986854553 norm:0.0001407674135407433 max memory_allocated 47477.7919921875 
[2025-03-23 03:20:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 18 loss:0.014804764650762081 norm:0.00014513946371152997 max memory_allocated 47477.7919921875 
[2025-03-23 03:22:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [6, 7, 8]) iter 19 loss:0.014774714596569538 norm:0.00013302153092809021 max memory_allocated 47477.7919921875 
[2025-03-23 03:23:56 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [6, 7, 8]
[2025-03-23 03:23:56 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [9, 10] ===
[2025-03-23 03:24:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 0 loss:0.022444643080234528 norm:0.0004058841150254011 max memory_allocated 47477.7919921875 
[2025-03-23 03:25:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 1 loss:0.019942231476306915 norm:0.0002687395899556577 max memory_allocated 47477.7919921875 
[2025-03-23 03:26:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 2 loss:0.018532050773501396 norm:0.00020157943072263151 max memory_allocated 47477.7919921875 
[2025-03-23 03:27:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 3 loss:0.017280885949730873 norm:0.00017239426961168647 max memory_allocated 47477.7919921875 
[2025-03-23 03:28:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 4 loss:0.016325535252690315 norm:0.00015975556743796915 max memory_allocated 47477.7919921875 
[2025-03-23 03:29:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 5 loss:0.01572468876838684 norm:0.00014659689622931182 max memory_allocated 47477.7919921875 
[2025-03-23 03:30:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 6 loss:0.015358214266598225 norm:0.0001280000724364072 max memory_allocated 47477.7919921875 
[2025-03-23 03:31:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 7 loss:0.015196803957223892 norm:0.0001282125012949109 max memory_allocated 47477.7919921875 
[2025-03-23 03:32:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 8 loss:0.015081081539392471 norm:0.00011903754784725606 max memory_allocated 47477.7919921875 
[2025-03-23 03:33:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 9 loss:0.014987118542194366 norm:0.00011461014946689829 max memory_allocated 47477.7919921875 
[2025-03-23 03:34:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 10 loss:0.014910100027918816 norm:0.00011260262544965371 max memory_allocated 47477.7919921875 
[2025-03-23 03:35:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 11 loss:0.014863040298223495 norm:0.00010964273678837344 max memory_allocated 47477.7919921875 
[2025-03-23 03:36:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 12 loss:0.014816605485975742 norm:0.00010106467379955575 max memory_allocated 47477.7919921875 
[2025-03-23 03:37:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 13 loss:0.014790009707212448 norm:0.00010344982001697645 max memory_allocated 47477.7919921875 
[2025-03-23 03:38:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 14 loss:0.014782898128032684 norm:0.00010799665324157104 max memory_allocated 47477.7919921875 
[2025-03-23 03:39:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 15 loss:0.014750834554433823 norm:0.00010979895159834996 max memory_allocated 47477.7919921875 
[2025-03-23 03:40:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 16 loss:0.014746613800525665 norm:0.00011836535850306973 max memory_allocated 47477.7919921875 
[2025-03-23 03:41:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 17 loss:0.01470878068357706 norm:0.00010693319200072438 max memory_allocated 47477.7919921875 
[2025-03-23 03:42:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 18 loss:0.014688483439385891 norm:0.0001082190647139214 max memory_allocated 47477.7919921875 
[2025-03-23 03:43:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [9, 10]) iter 19 loss:0.014710033312439919 norm:0.00012629789125639945 max memory_allocated 47477.7919921875 
[2025-03-23 03:44:24 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [9, 10]
[2025-03-23 03:44:24 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [11, 12, 13] ===
[2025-03-23 03:45:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 0 loss:0.026873424649238586 norm:0.0005212620599195361 max memory_allocated 47478.1044921875 
[2025-03-23 03:47:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 1 loss:0.022991321980953217 norm:0.00031136596226133406 max memory_allocated 47478.1044921875 
[2025-03-23 03:48:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 2 loss:0.021124614402651787 norm:0.00024786623544059694 max memory_allocated 47478.1044921875 
[2025-03-23 03:50:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 3 loss:0.019726021215319633 norm:0.00021303108951542526 max memory_allocated 47478.1044921875 
[2025-03-23 03:51:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 4 loss:0.018607858568429947 norm:0.0001922729570651427 max memory_allocated 47478.1044921875 
[2025-03-23 03:53:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 5 loss:0.017741188406944275 norm:0.00017232002574019134 max memory_allocated 47478.1044921875 
[2025-03-23 03:54:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 6 loss:0.017181169241666794 norm:0.0001703258603811264 max memory_allocated 47478.1044921875 
[2025-03-23 03:55:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 7 loss:0.016766460612416267 norm:0.00016029702965170145 max memory_allocated 47478.1044921875 
[2025-03-23 03:57:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 8 loss:0.016487261280417442 norm:0.00015059643192216754 max memory_allocated 47478.1044921875 
[2025-03-23 03:58:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 9 loss:0.0162678062915802 norm:0.000147780665429309 max memory_allocated 47478.1044921875 
[2025-03-23 04:00:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 10 loss:0.016114449128508568 norm:0.0001361351751256734 max memory_allocated 47478.1044921875 
[2025-03-23 04:01:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 11 loss:0.015998033806681633 norm:0.0001315850531682372 max memory_allocated 47478.1044921875 
[2025-03-23 04:03:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 12 loss:0.015919161960482597 norm:0.00013214215869084 max memory_allocated 47478.1044921875 
[2025-03-23 04:04:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 13 loss:0.01585511863231659 norm:0.00013953291636426002 max memory_allocated 47478.1044921875 
[2025-03-23 04:05:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 14 loss:0.015790844336152077 norm:0.0001304071774939075 max memory_allocated 47478.1044921875 
[2025-03-23 04:07:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 15 loss:0.015719275921583176 norm:0.00012226610851939768 max memory_allocated 47478.1044921875 
[2025-03-23 04:08:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 16 loss:0.015690520405769348 norm:0.00012352188059594482 max memory_allocated 47478.1044921875 
[2025-03-23 04:10:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 17 loss:0.015658898279070854 norm:0.00012517659342847764 max memory_allocated 47478.1044921875 
[2025-03-23 04:11:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 18 loss:0.015624171122908592 norm:0.00012228393461555243 max memory_allocated 47478.1044921875 
[2025-03-23 04:13:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [11, 12, 13]) iter 19 loss:0.015606331638991833 norm:0.00011689977691275999 max memory_allocated 47478.1044921875 
[2025-03-23 04:15:00 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [11, 12, 13]
[2025-03-23 04:15:00 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [14, 15, 16] ===
[2025-03-23 04:16:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 0 loss:0.02670440264046192 norm:0.0005021447432227433 max memory_allocated 47479.1669921875 
[2025-03-23 04:17:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 1 loss:0.02344154380261898 norm:0.0003032833628822118 max memory_allocated 47479.1669921875 
[2025-03-23 04:19:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 2 loss:0.02166362851858139 norm:0.00022125532268546522 max memory_allocated 47479.1669921875 
[2025-03-23 04:20:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 3 loss:0.02023172751069069 norm:0.00018038127745967358 max memory_allocated 47479.1669921875 
[2025-03-23 04:22:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 4 loss:0.019083108752965927 norm:0.00016148461145348847 max memory_allocated 47479.1669921875 
[2025-03-23 04:23:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 5 loss:0.018257979303598404 norm:0.0001492848969064653 max memory_allocated 47479.1669921875 
[2025-03-23 04:25:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 6 loss:0.01771174743771553 norm:0.00014313828432932496 max memory_allocated 47479.1669921875 
[2025-03-23 04:26:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 7 loss:0.01734812930226326 norm:0.00013103059609420598 max memory_allocated 47479.1669921875 
[2025-03-23 04:28:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 8 loss:0.017118312418460846 norm:0.00012535594578366727 max memory_allocated 47479.1669921875 
[2025-03-23 04:29:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 9 loss:0.016957957297563553 norm:0.00012185205559944734 max memory_allocated 47479.1669921875 
[2025-03-23 04:30:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 10 loss:0.016822559759020805 norm:0.00011533524957485497 max memory_allocated 47479.1669921875 
[2025-03-23 04:32:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 11 loss:0.01672329567372799 norm:0.00010986155393766239 max memory_allocated 47479.1669921875 
[2025-03-23 04:33:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 12 loss:0.0166497603058815 norm:0.0001057530171237886 max memory_allocated 47479.1669921875 
[2025-03-23 04:35:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 13 loss:0.01657293736934662 norm:0.00010437400487717241 max memory_allocated 47479.1669921875 
[2025-03-23 04:36:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 14 loss:0.016500767320394516 norm:0.00010024158837040886 max memory_allocated 47479.1669921875 
[2025-03-23 04:38:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 15 loss:0.016442961990833282 norm:0.00010177034710068256 max memory_allocated 47479.1669921875 
[2025-03-23 04:39:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 16 loss:0.016398487612605095 norm:9.99763433355838e-05 max memory_allocated 47479.1669921875 
[2025-03-23 04:40:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 17 loss:0.016348915174603462 norm:9.207909897668287e-05 max memory_allocated 47479.1669921875 
[2025-03-23 04:42:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 18 loss:0.016303295269608498 norm:9.188698459183797e-05 max memory_allocated 47479.1669921875 
[2025-03-23 04:43:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [14, 15, 16]) iter 19 loss:0.01627851463854313 norm:9.257750934921205e-05 max memory_allocated 47479.1669921875 
[2025-03-23 04:45:34 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [14, 15, 16]
[2025-03-23 04:45:34 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [17, 18, 19] ===
[2025-03-23 04:47:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 0 loss:0.027875514701008797 norm:0.0003418750420678407 max memory_allocated 47479.4794921875 
[2025-03-23 04:48:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 1 loss:0.02573114074766636 norm:0.00021641362400259823 max memory_allocated 47479.4794921875 
[2025-03-23 04:49:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 2 loss:0.02432650700211525 norm:0.00016644703282509 max memory_allocated 47479.4794921875 
[2025-03-23 04:51:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 3 loss:0.022992068901658058 norm:0.00014191065565682948 max memory_allocated 47479.4794921875 
[2025-03-23 04:52:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 4 loss:0.021682370454072952 norm:0.0001304793549934402 max memory_allocated 47479.4794921875 
[2025-03-23 04:54:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 5 loss:0.020780472084879875 norm:0.0001229258195962757 max memory_allocated 47479.4794921875 
[2025-03-23 04:55:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 6 loss:0.020365701988339424 norm:0.00011880499368999153 max memory_allocated 47479.4794921875 
[2025-03-23 04:57:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 7 loss:0.02011924982070923 norm:0.00010591487807687372 max memory_allocated 47479.4794921875 
[2025-03-23 04:58:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 8 loss:0.019962668418884277 norm:0.00010097495396621525 max memory_allocated 47479.4794921875 
[2025-03-23 05:00:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 9 loss:0.019852828234434128 norm:9.243206295650452e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:01:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 10 loss:0.019777068868279457 norm:8.82241438375786e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:02:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 11 loss:0.019701382145285606 norm:8.478846575599164e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:04:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 12 loss:0.01964462921023369 norm:8.362310472875834e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:05:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 13 loss:0.019607307389378548 norm:8.558542322134599e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:07:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 14 loss:0.019549120217561722 norm:7.992239989107475e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:08:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 15 loss:0.019519072026014328 norm:8.241604518843815e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:10:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 16 loss:0.01949683204293251 norm:7.888674008427188e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:11:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 17 loss:0.01944219507277012 norm:7.480243948521093e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:12:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 18 loss:0.01942623034119606 norm:7.72071216488257e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:14:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [17, 18, 19]) iter 19 loss:0.019386686384677887 norm:7.32584303477779e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:16:09 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [17, 18, 19]
[2025-03-23 05:16:09 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [20, 21, 22] ===
[2025-03-23 05:17:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 0 loss:0.037762682884931564 norm:0.0007251927745528519 max memory_allocated 47479.4794921875 
[2025-03-23 05:19:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 1 loss:0.03469263017177582 norm:0.00035956379724666476 max memory_allocated 47479.4794921875 
[2025-03-23 05:20:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 2 loss:0.03281141817569733 norm:0.0002458635426592082 max memory_allocated 47479.4794921875 
[2025-03-23 05:22:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 3 loss:0.0309298075735569 norm:0.0001928800338646397 max memory_allocated 47479.4794921875 
[2025-03-23 05:23:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 4 loss:0.029233241453766823 norm:0.00017628514615353197 max memory_allocated 47479.4794921875 
[2025-03-23 05:24:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 5 loss:0.028298087418079376 norm:0.00016336936096195132 max memory_allocated 47479.4794921875 
[2025-03-23 05:26:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 6 loss:0.027980118989944458 norm:0.00016189657617360353 max memory_allocated 47479.4794921875 
[2025-03-23 05:27:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 7 loss:0.027805142104625702 norm:0.00014195014955475926 max memory_allocated 47479.4794921875 
[2025-03-23 05:29:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 8 loss:0.02766634337604046 norm:0.00014145382738206536 max memory_allocated 47479.4794921875 
[2025-03-23 05:30:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 9 loss:0.027549192309379578 norm:0.00013389057130552828 max memory_allocated 47479.4794921875 
[2025-03-23 05:32:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 10 loss:0.02746998891234398 norm:0.0001331526436842978 max memory_allocated 47479.4794921875 
[2025-03-23 05:33:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 11 loss:0.027374841272830963 norm:0.0001252203219337389 max memory_allocated 47479.4794921875 
[2025-03-23 05:34:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 12 loss:0.027296941727399826 norm:0.00012482755118981004 max memory_allocated 47479.4794921875 
[2025-03-23 05:36:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 13 loss:0.027233034372329712 norm:0.00011463902774266899 max memory_allocated 47479.4794921875 
[2025-03-23 05:37:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 14 loss:0.02718203514814377 norm:0.00011365484533598647 max memory_allocated 47479.4794921875 
[2025-03-23 05:39:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 15 loss:0.02710839919745922 norm:0.00011144389281980693 max memory_allocated 47479.4794921875 
[2025-03-23 05:40:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 16 loss:0.02704940363764763 norm:0.00010839701280929148 max memory_allocated 47479.4794921875 
[2025-03-23 05:42:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 17 loss:0.026990141719579697 norm:0.00010561817180132493 max memory_allocated 47479.4794921875 
[2025-03-23 05:43:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 18 loss:0.02694406360387802 norm:0.00010153894254472107 max memory_allocated 47479.4794921875 
[2025-03-23 05:44:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [20, 21, 22]) iter 19 loss:0.026917608454823494 norm:9.840990242082626e-05 max memory_allocated 47479.4794921875 
[2025-03-23 05:46:44 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [20, 21, 22]
[2025-03-23 05:46:45 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [23, 24, 25] ===
[2025-03-23 05:48:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 0 loss:0.05535558611154556 norm:0.001562854740768671 max memory_allocated 47479.4794921875 
[2025-03-23 05:49:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 1 loss:0.050439540296792984 norm:0.000497407338116318 max memory_allocated 47479.4794921875 
[2025-03-23 05:51:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 2 loss:0.04751797765493393 norm:0.00034831176162697375 max memory_allocated 47479.4794921875 
[2025-03-23 05:52:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 3 loss:0.04453587904572487 norm:0.0002787630073726177 max memory_allocated 47479.4794921875 
[2025-03-23 05:54:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 4 loss:0.042379625141620636 norm:0.0002515286032576114 max memory_allocated 47479.4794921875 
[2025-03-23 05:55:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 5 loss:0.041761789470911026 norm:0.0002411598397884518 max memory_allocated 47479.4794921875 
[2025-03-23 05:56:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 6 loss:0.04150193929672241 norm:0.0002266931114718318 max memory_allocated 47479.4794921875 
[2025-03-23 05:58:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 7 loss:0.04130806773900986 norm:0.0002093568618874997 max memory_allocated 47479.4794921875 
[2025-03-23 05:59:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 8 loss:0.04118061438202858 norm:0.00020430440781638026 max memory_allocated 47479.4794921875 
[2025-03-23 06:01:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 9 loss:0.04102194309234619 norm:0.00018926852499134839 max memory_allocated 47479.4794921875 
[2025-03-23 06:02:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 10 loss:0.040954917669296265 norm:0.00019812236132565886 max memory_allocated 47479.4794921875 
[2025-03-23 06:04:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 11 loss:0.04081632196903229 norm:0.00017694183043204248 max memory_allocated 47479.4794921875 
[2025-03-23 06:05:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 12 loss:0.0407286062836647 norm:0.00017151236534118652 max memory_allocated 47479.4794921875 
[2025-03-23 06:06:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 13 loss:0.040630895644426346 norm:0.00017342880892101675 max memory_allocated 47479.4794921875 
[2025-03-23 06:08:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 14 loss:0.04054480046033859 norm:0.00016555702313780785 max memory_allocated 47479.4794921875 
[2025-03-23 06:09:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 15 loss:0.04045158252120018 norm:0.00015949604858178645 max memory_allocated 47479.4794921875 
[2025-03-23 06:11:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 16 loss:0.040397025644779205 norm:0.0001558760995976627 max memory_allocated 47479.4794921875 
[2025-03-23 06:12:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 17 loss:0.040364183485507965 norm:0.00014910625759512186 max memory_allocated 47479.4794921875 
[2025-03-23 06:14:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 18 loss:0.04032950475811958 norm:0.00016006140504032373 max memory_allocated 47479.4794921875 
[2025-03-23 06:15:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [23, 24, 25]) iter 19 loss:0.04028774052858353 norm:0.00015391134365927428 max memory_allocated 47479.4794921875 
[2025-03-23 06:17:22 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [23, 24, 25]
[2025-03-23 06:17:22 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [26, 27, 28] ===
[2025-03-23 06:17:23 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 06:18:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 0 loss:0.07682256400585175 norm:0.004158841911703348 max memory_allocated 47479.4794921875 
[2025-03-23 06:20:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 1 loss:0.07167704403400421 norm:0.003051223000511527 max memory_allocated 47479.4794921875 
[2025-03-23 06:21:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 2 loss:0.06781598180532455 norm:0.0024185264483094215 max memory_allocated 47479.4794921875 
[2025-03-23 06:23:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 3 loss:0.06386371701955795 norm:0.0019485754892230034 max memory_allocated 47479.4794921875 
[2025-03-23 06:24:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 4 loss:0.06219135969877243 norm:0.0016543288948014379 max memory_allocated 47479.4794921875 
[2025-03-23 06:26:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 5 loss:0.0616462305188179 norm:0.0014050842728465796 max memory_allocated 47479.4794921875 
[2025-03-23 06:27:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 6 loss:0.06132534146308899 norm:0.0012778909876942635 max memory_allocated 47479.4794921875 
[2025-03-23 06:28:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 7 loss:0.06114134192466736 norm:0.0011829897994175553 max memory_allocated 47479.4794921875 
[2025-03-23 06:30:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 8 loss:0.06094159930944443 norm:0.0012533057015389204 max memory_allocated 47479.4794921875 
[2025-03-23 06:31:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 9 loss:0.06077516824007034 norm:0.0009420958813279867 max memory_allocated 47479.4794921875 
[2025-03-23 06:33:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 10 loss:0.06053396314382553 norm:0.001077683991752565 max memory_allocated 47479.4794921875 
[2025-03-23 06:34:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 11 loss:0.06044231727719307 norm:0.0009579053148627281 max memory_allocated 47479.4794921875 
[2025-03-23 06:36:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 12 loss:0.06026832386851311 norm:0.0010440826881676912 max memory_allocated 47479.4794921875 
[2025-03-23 06:37:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 13 loss:0.06020139157772064 norm:0.0009282925166189671 max memory_allocated 47479.4794921875 
[2025-03-23 06:39:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 14 loss:0.060104381293058395 norm:0.0009421625290997326 max memory_allocated 47479.4794921875 
[2025-03-23 06:40:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 15 loss:0.05996683984994888 norm:0.0009037971612997353 max memory_allocated 47479.4794921875 
[2025-03-23 06:41:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 16 loss:0.059851258993148804 norm:0.0008767676190473139 max memory_allocated 47479.4794921875 
[2025-03-23 06:43:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 17 loss:0.059800948947668076 norm:0.0008642892935313284 max memory_allocated 47479.4794921875 
[2025-03-23 06:44:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 18 loss:0.05975016579031944 norm:0.0008367772679775953 max memory_allocated 47479.4794921875 
[2025-03-23 06:46:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [26, 27, 28]) iter 19 loss:0.05971073359251022 norm:0.0008148596389219165 max memory_allocated 47479.4794921875 
[2025-03-23 06:48:01 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [26, 27, 28]
[2025-03-23 06:48:01 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [29] ===
[2025-03-23 06:48:02 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 06:48:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 0 loss:0.07443132251501083 norm:0.004617026541382074 max memory_allocated 47479.4794921875 
[2025-03-23 06:49:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 1 loss:0.07093024998903275 norm:0.0036793285980820656 max memory_allocated 47479.4794921875 
[2025-03-23 06:49:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 2 loss:0.0681774914264679 norm:0.002735479734838009 max memory_allocated 47479.4794921875 
[2025-03-23 06:49:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 3 loss:0.06636321544647217 norm:0.002212500898167491 max memory_allocated 47479.4794921875 
[2025-03-23 06:50:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 4 loss:0.06595766544342041 norm:0.0019332633819431067 max memory_allocated 47479.4794921875 
[2025-03-23 06:50:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 5 loss:0.06579062342643738 norm:0.001663627801463008 max memory_allocated 47479.4794921875 
[2025-03-23 06:51:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 6 loss:0.06563335657119751 norm:0.001409997814334929 max memory_allocated 47479.4794921875 
[2025-03-23 06:51:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 7 loss:0.06553281098604202 norm:0.0012315066996961832 max memory_allocated 47479.4794921875 
[2025-03-23 06:52:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 8 loss:0.0654667466878891 norm:0.001146547612734139 max memory_allocated 47479.4794921875 
[2025-03-23 06:52:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 9 loss:0.06539580225944519 norm:0.0011168060591444373 max memory_allocated 47479.4794921875 
[2025-03-23 06:53:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 10 loss:0.06542110443115234 norm:0.0010817149886861444 max memory_allocated 47479.4794921875 
[2025-03-23 06:53:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 11 loss:0.06534865498542786 norm:0.0010681002167984843 max memory_allocated 47479.4794921875 
[2025-03-23 06:54:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 12 loss:0.06533791124820709 norm:0.0008846708224155009 max memory_allocated 47479.4794921875 
[2025-03-23 06:54:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 13 loss:0.06527459621429443 norm:0.001043774769641459 max memory_allocated 47479.4794921875 
[2025-03-23 06:55:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 14 loss:0.06529540568590164 norm:0.0008231651736423373 max memory_allocated 47479.4794921875 
[2025-03-23 06:55:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 15 loss:0.0651744082570076 norm:0.0009212063159793615 max memory_allocated 47479.4794921875 
[2025-03-23 06:56:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 16 loss:0.06520268321037292 norm:0.00082716642646119 max memory_allocated 47479.4794921875 
[2025-03-23 06:56:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 17 loss:0.06513465195894241 norm:0.0008743939106352627 max memory_allocated 47479.4794921875 
[2025-03-23 06:57:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 18 loss:0.06511692702770233 norm:0.0008111090864986181 max memory_allocated 47479.4794921875 
[2025-03-23 06:57:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [29]) iter 19 loss:0.06509348750114441 norm:0.0008247519144788384 max memory_allocated 47479.4794921875 
[2025-03-23 06:58:19 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [29]
[2025-03-23 06:58:19 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [30] ===
[2025-03-23 06:58:20 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 06:58:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 0 loss:0.11208675801753998 norm:0.015015057288110256 max memory_allocated 47479.4794921875 
[2025-03-23 06:59:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 1 loss:0.09733442217111588 norm:0.010187406092882156 max memory_allocated 47479.4794921875 
[2025-03-23 06:59:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 2 loss:0.09038436412811279 norm:0.007028961554169655 max memory_allocated 47479.4794921875 
[2025-03-23 07:00:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 3 loss:0.08717374503612518 norm:0.005821993108838797 max memory_allocated 47479.4794921875 
[2025-03-23 07:00:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 4 loss:0.08629346638917923 norm:0.004978764336556196 max memory_allocated 47479.4794921875 
[2025-03-23 07:01:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 5 loss:0.08577921986579895 norm:0.004424295853823423 max memory_allocated 47479.4794921875 
[2025-03-23 07:01:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 6 loss:0.08544366806745529 norm:0.003995959181338549 max memory_allocated 47479.4794921875 
[2025-03-23 07:02:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 7 loss:0.08525686711072922 norm:0.003538526827469468 max memory_allocated 47479.4794921875 
[2025-03-23 07:02:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 8 loss:0.08501389622688293 norm:0.003049625549465418 max memory_allocated 47479.4794921875 
[2025-03-23 07:03:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 9 loss:0.08486916124820709 norm:0.0028065978549420834 max memory_allocated 47479.4794921875 
[2025-03-23 07:03:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 10 loss:0.08477611094713211 norm:0.0026378510519862175 max memory_allocated 47479.4794921875 
[2025-03-23 07:04:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 11 loss:0.08476456999778748 norm:0.0026320579927414656 max memory_allocated 47479.4794921875 
[2025-03-23 07:04:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 12 loss:0.08475631475448608 norm:0.0027227357495576143 max memory_allocated 47479.4794921875 
[2025-03-23 07:05:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 13 loss:0.08484190702438354 norm:0.002629521768540144 max memory_allocated 47479.4794921875 
[2025-03-23 07:05:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 14 loss:0.08459111303091049 norm:0.0024400583934038877 max memory_allocated 47479.4794921875 
[2025-03-23 07:06:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 15 loss:0.08454012870788574 norm:0.0021908266935497522 max memory_allocated 47479.4794921875 
[2025-03-23 07:06:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 16 loss:0.08450954407453537 norm:0.0021574816200882196 max memory_allocated 47479.4794921875 
[2025-03-23 07:07:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 17 loss:0.08451128005981445 norm:0.0021044323220849037 max memory_allocated 47479.4794921875 
[2025-03-23 07:07:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 18 loss:0.08451858907938004 norm:0.0022281473502516747 max memory_allocated 47479.4794921875 
[2025-03-23 07:08:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [30]) iter 19 loss:0.08449653536081314 norm:0.002197868889197707 max memory_allocated 47479.4794921875 
[2025-03-23 07:08:36 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [30]
[2025-03-23 07:08:36 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [31] ===
[2025-03-23 07:08:37 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 07:09:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 0 loss:0.15229052305221558 norm:0.015072263777256012 max memory_allocated 47479.4794921875 
[2025-03-23 07:09:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 1 loss:0.14489176869392395 norm:0.01137389987707138 max memory_allocated 47479.4794921875 
[2025-03-23 07:10:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 2 loss:0.13988561928272247 norm:0.008572826161980629 max memory_allocated 47479.4794921875 
[2025-03-23 07:10:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 3 loss:0.13688795268535614 norm:0.006829538848251104 max memory_allocated 47479.4794921875 
[2025-03-23 07:11:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 4 loss:0.13560424745082855 norm:0.00590863823890686 max memory_allocated 47479.4794921875 
[2025-03-23 07:11:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 5 loss:0.13491956889629364 norm:0.00522914994508028 max memory_allocated 47479.4794921875 
[2025-03-23 07:12:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 6 loss:0.13436323404312134 norm:0.004670052323490381 max memory_allocated 47479.4794921875 
[2025-03-23 07:12:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 7 loss:0.1341005265712738 norm:0.0042808642610907555 max memory_allocated 47479.4794921875 
[2025-03-23 07:12:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 8 loss:0.1338154673576355 norm:0.004020031541585922 max memory_allocated 47479.4794921875 
[2025-03-23 07:13:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 9 loss:0.13367611169815063 norm:0.0039600105956196785 max memory_allocated 47479.4794921875 
[2025-03-23 07:13:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 10 loss:0.1337234377861023 norm:0.004174534231424332 max memory_allocated 47479.4794921875 
[2025-03-23 07:14:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 11 loss:0.13344819843769073 norm:0.003870847402140498 max memory_allocated 47479.4794921875 
[2025-03-23 07:14:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 12 loss:0.1333610862493515 norm:0.0038313153199851513 max memory_allocated 47479.4794921875 
[2025-03-23 07:15:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 13 loss:0.13324587047100067 norm:0.0036832797341048717 max memory_allocated 47479.4794921875 
[2025-03-23 07:15:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 14 loss:0.13317491114139557 norm:0.003647885285317898 max memory_allocated 47479.4794921875 
[2025-03-23 07:16:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 15 loss:0.13301856815814972 norm:0.0034875720739364624 max memory_allocated 47479.4794921875 
[2025-03-23 07:16:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 16 loss:0.13304279744625092 norm:0.0036929382476955652 max memory_allocated 47479.4794921875 
[2025-03-23 07:17:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 17 loss:0.1330907940864563 norm:0.0036672414280474186 max memory_allocated 47479.4794921875 
[2025-03-23 07:17:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 18 loss:0.13309940695762634 norm:0.003915281966328621 max memory_allocated 47479.4794921875 
[2025-03-23 07:18:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [31]) iter 19 loss:0.1328762024641037 norm:0.0034644140396267176 max memory_allocated 47479.4794921875 
[2025-03-23 07:18:53 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [31]
[2025-03-23 07:18:53 root] (main_calib_config3_attn.py 379): INFO 19633.766493558884
[2025-03-23 07:18:59 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-23 07:19:49 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.541950225830078
[2025-03-23 07:19:50 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-23 07:21:08 root] (main_calib_config3_attn.py 161): INFO c4 : 7.060171604156494
