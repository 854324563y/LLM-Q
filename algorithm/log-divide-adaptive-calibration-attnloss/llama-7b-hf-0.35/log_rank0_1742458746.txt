[2025-03-20 08:19:06 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/llama-7b-hf-0.35', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.35.pkl', blocks_pkl='./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-20 08:19:36 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-20 08:19:36 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-20 08:19:37 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-20 08:19:37 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.35.pkl
[2025-03-20 08:19:37 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 5), (5, 8), (8, 11), (11, 14), (14, 16), (16, 19), (19, 22), (22, 25), (25, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-20 08:19:37 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2, 3, 4], [5, 6, 7], [8, 9, 10], [11, 12, 13], [14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26], [27], [28], [29], [30], [31]]
[2025-03-20 08:19:38 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-20 08:19:38 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 08:20:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.011318817734718323 norm:0.014298656024038792 max memory_allocated 34630.880859375 
[2025-03-20 08:20:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.0057228077203035355 norm:0.007060237228870392 max memory_allocated 34630.880859375 
[2025-03-20 08:21:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.0037769745104014874 norm:0.0048429653979837894 max memory_allocated 34630.880859375 
[2025-03-20 08:21:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0031550386920571327 norm:0.003786667948588729 max memory_allocated 34630.880859375 
[2025-03-20 08:21:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.0028786729089915752 norm:0.0030213494319468737 max memory_allocated 34630.880859375 
[2025-03-20 08:22:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.00268411822617054 norm:0.002597321756184101 max memory_allocated 34630.880859375 
[2025-03-20 08:22:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.002580683445557952 norm:0.002174751367419958 max memory_allocated 34630.880859375 
[2025-03-20 08:23:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0024901949800550938 norm:0.0019285418093204498 max memory_allocated 34630.880859375 
[2025-03-20 08:23:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0024284019600600004 norm:0.001744749373756349 max memory_allocated 34630.880859375 
[2025-03-20 08:24:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.002396941650658846 norm:0.0015739755472168326 max memory_allocated 34630.880859375 
[2025-03-20 08:24:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.0023598787374794483 norm:0.0013907153625041246 max memory_allocated 34630.880859375 
[2025-03-20 08:25:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0023468639701604843 norm:0.0012923558242619038 max memory_allocated 34630.880859375 
[2025-03-20 08:25:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.002313740085810423 norm:0.001144860521890223 max memory_allocated 34630.880859375 
[2025-03-20 08:25:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.002291728276759386 norm:0.0010277265682816505 max memory_allocated 34630.880859375 
[2025-03-20 08:26:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0022792865056544542 norm:0.0009424103191122413 max memory_allocated 34630.880859375 
[2025-03-20 08:26:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.002260378561913967 norm:0.0008509174804203212 max memory_allocated 34630.880859375 
[2025-03-20 08:27:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.0022682517301291227 norm:0.0007787365466356277 max memory_allocated 34630.880859375 
[2025-03-20 08:27:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0022652004845440388 norm:0.0007346344646066427 max memory_allocated 34630.880859375 
[2025-03-20 08:28:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.0022797961719334126 norm:0.0007136674248613417 max memory_allocated 34630.880859375 
[2025-03-20 08:28:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.002256820211187005 norm:0.0006785187870264053 max memory_allocated 34630.880859375 
[2025-03-20 08:29:14 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-20 08:29:14 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-20 08:29:16 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 08:29:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.020985545590519905 norm:0.01985308900475502 max memory_allocated 35097.7724609375 
[2025-03-20 08:30:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.011920683085918427 norm:0.012155361473560333 max memory_allocated 35097.7724609375 
[2025-03-20 08:30:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.008216972462832928 norm:0.007065719924867153 max memory_allocated 35097.7724609375 
[2025-03-20 08:31:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.007168141659349203 norm:0.00505414605140686 max memory_allocated 35097.7724609375 
[2025-03-20 08:31:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.006799633614718914 norm:0.004319627769291401 max memory_allocated 35097.7724609375 
[2025-03-20 08:32:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.006536468397825956 norm:0.003885384416207671 max memory_allocated 35097.7724609375 
[2025-03-20 08:32:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.006330301985144615 norm:0.0034945933148264885 max memory_allocated 35097.7724609375 
[2025-03-20 08:32:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.006196613889187574 norm:0.0032064756378531456 max memory_allocated 35097.7724609375 
[2025-03-20 08:33:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.00599149102345109 norm:0.0029291678220033646 max memory_allocated 35097.7724609375 
[2025-03-20 08:33:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.005869755055755377 norm:0.002672565169632435 max memory_allocated 35097.7724609375 
[2025-03-20 08:34:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.005782814230769873 norm:0.0024667170364409685 max memory_allocated 35097.7724609375 
[2025-03-20 08:34:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.005752144381403923 norm:0.0022765330504626036 max memory_allocated 35097.7724609375 
[2025-03-20 08:35:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.005647622048854828 norm:0.002064646454527974 max memory_allocated 35097.7724609375 
[2025-03-20 08:35:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.00560953700914979 norm:0.0018925070762634277 max memory_allocated 35097.7724609375 
[2025-03-20 08:36:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.005566674750298262 norm:0.001714402693323791 max memory_allocated 35097.7724609375 
[2025-03-20 08:36:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.005544173996895552 norm:0.001538753043860197 max memory_allocated 35097.7724609375 
[2025-03-20 08:37:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.0055192988365888596 norm:0.0013801734894514084 max memory_allocated 35097.7724609375 
[2025-03-20 08:37:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.005503818858414888 norm:0.0012289374135434628 max memory_allocated 35097.7724609375 
[2025-03-20 08:37:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.005500898230820894 norm:0.0011027142172679305 max memory_allocated 35097.7724609375 
[2025-03-20 08:38:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.005469655618071556 norm:0.0009932512184605002 max memory_allocated 35097.7724609375 
[2025-03-20 08:38:54 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-20 08:38:54 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2, 3, 4] ===
[2025-03-20 08:38:54 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 08:40:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 0 loss:0.053294774144887924 norm:0.02995283156633377 max memory_allocated 47468.5419921875 
[2025-03-20 08:41:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 1 loss:0.03955778107047081 norm:0.02207440882921219 max memory_allocated 47468.5419921875 
[2025-03-20 08:43:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 2 loss:0.030907917767763138 norm:0.015801135450601578 max memory_allocated 47468.5419921875 
[2025-03-20 08:44:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 3 loss:0.026718581095337868 norm:0.010900940746068954 max memory_allocated 47468.5419921875 
[2025-03-20 08:45:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 4 loss:0.024608630686998367 norm:0.0084571223706007 max memory_allocated 47468.5419921875 
[2025-03-20 08:47:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 5 loss:0.023275895044207573 norm:0.006825809367001057 max memory_allocated 47468.5419921875 
[2025-03-20 08:48:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 6 loss:0.02259555086493492 norm:0.006215331144630909 max memory_allocated 47468.5419921875 
[2025-03-20 08:49:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 7 loss:0.0220591202378273 norm:0.005525167100131512 max memory_allocated 47468.5419921875 
[2025-03-20 08:51:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 8 loss:0.02150549925863743 norm:0.005266794469207525 max memory_allocated 47468.5419921875 
[2025-03-20 08:52:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 9 loss:0.021320058032870293 norm:0.00491487979888916 max memory_allocated 47468.5419921875 
[2025-03-20 08:53:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 10 loss:0.021397795528173447 norm:0.004304911009967327 max memory_allocated 47468.5419921875 
[2025-03-20 08:55:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 11 loss:0.02142396941781044 norm:0.0039000636897981167 max memory_allocated 47468.5419921875 
[2025-03-20 08:56:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 12 loss:0.021548809483647346 norm:0.004194747656583786 max memory_allocated 47468.5419921875 
[2025-03-20 08:57:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 13 loss:0.021405532956123352 norm:0.004109623841941357 max memory_allocated 47468.5419921875 
[2025-03-20 08:59:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 14 loss:0.02133249305188656 norm:0.0038168979808688164 max memory_allocated 47468.5419921875 
[2025-03-20 09:00:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 15 loss:0.021358177065849304 norm:0.003609835635870695 max memory_allocated 47468.5419921875 
[2025-03-20 09:01:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 16 loss:0.02135506272315979 norm:0.003718971274793148 max memory_allocated 47468.5419921875 
[2025-03-20 09:03:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 17 loss:0.02131117507815361 norm:0.003578662406653166 max memory_allocated 47468.5419921875 
[2025-03-20 09:04:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 18 loss:0.0213941503316164 norm:0.0034108778927475214 max memory_allocated 47468.5419921875 
[2025-03-20 09:05:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 19 loss:0.021358318626880646 norm:0.003450741060078144 max memory_allocated 47468.5419921875 
[2025-03-20 09:07:29 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2, 3, 4]
[2025-03-20 09:07:29 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [5, 6, 7] ===
[2025-03-20 09:08:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 0 loss:0.07530006021261215 norm:0.004709832835942507 max memory_allocated 47468.7294921875 
[2025-03-20 09:10:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 1 loss:0.05300174653530121 norm:0.0009014279348775744 max memory_allocated 47468.7294921875 
[2025-03-20 09:11:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 2 loss:0.04172055423259735 norm:0.00043005021871067584 max memory_allocated 47468.7294921875 
[2025-03-20 09:12:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 3 loss:0.036644548177719116 norm:0.0003458867722656578 max memory_allocated 47468.7294921875 
[2025-03-20 09:14:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 4 loss:0.034113116562366486 norm:0.00030868183239363134 max memory_allocated 47468.7294921875 
[2025-03-20 09:15:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 5 loss:0.03244831785559654 norm:0.00031266504083760083 max memory_allocated 47468.7294921875 
[2025-03-20 09:16:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 6 loss:0.031418927013874054 norm:0.0003035917761735618 max memory_allocated 47468.7294921875 
[2025-03-20 09:18:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 7 loss:0.030735870823264122 norm:0.0003071812097914517 max memory_allocated 47468.7294921875 
[2025-03-20 09:19:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 8 loss:0.03043239936232567 norm:0.0003127755771856755 max memory_allocated 47468.7294921875 
[2025-03-20 09:20:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 9 loss:0.030110005289316177 norm:0.00028035417199134827 max memory_allocated 47468.7294921875 
[2025-03-20 09:22:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 10 loss:0.030026953667402267 norm:0.00031993346055969596 max memory_allocated 47468.7294921875 
[2025-03-20 09:23:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 11 loss:0.0299066212028265 norm:0.0002991720102727413 max memory_allocated 47468.7294921875 
[2025-03-20 09:24:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 12 loss:0.02978733740746975 norm:0.0002893834316637367 max memory_allocated 47468.7294921875 
[2025-03-20 09:26:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 13 loss:0.029693474993109703 norm:0.000282725173747167 max memory_allocated 47468.7294921875 
[2025-03-20 09:27:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 14 loss:0.02959551475942135 norm:0.00028574830503202975 max memory_allocated 47468.7294921875 
[2025-03-20 09:28:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 15 loss:0.02954534813761711 norm:0.00028694956563413143 max memory_allocated 47468.7294921875 
[2025-03-20 09:30:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 16 loss:0.029513489454984665 norm:0.0002895694342441857 max memory_allocated 47468.7294921875 
[2025-03-20 09:31:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 17 loss:0.0294900294393301 norm:0.0002929448673967272 max memory_allocated 47468.7294921875 
[2025-03-20 09:32:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 18 loss:0.029466431587934494 norm:0.00029063355759717524 max memory_allocated 47468.7294921875 
[2025-03-20 09:34:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 19 loss:0.029464364051818848 norm:0.0003009405918419361 max memory_allocated 47468.7294921875 
[2025-03-20 09:35:54 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [5, 6, 7]
[2025-03-20 09:35:54 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [8, 9, 10] ===
[2025-03-20 09:37:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 0 loss:0.08470775187015533 norm:0.001572595676407218 max memory_allocated 47468.9169921875 
[2025-03-20 09:38:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 1 loss:0.06363477557897568 norm:0.0006461238954216242 max memory_allocated 47468.9169921875 
[2025-03-20 09:40:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 2 loss:0.05092015862464905 norm:0.00040313665522262454 max memory_allocated 47468.9169921875 
[2025-03-20 09:41:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 3 loss:0.04515577107667923 norm:0.0003098986635450274 max memory_allocated 47468.9169921875 
[2025-03-20 09:42:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 4 loss:0.04233546182513237 norm:0.0002787891717161983 max memory_allocated 47468.9169921875 
[2025-03-20 09:44:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 5 loss:0.04062410071492195 norm:0.00025874690618366003 max memory_allocated 47468.9169921875 
[2025-03-20 09:45:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 6 loss:0.03951369971036911 norm:0.00023418098862748593 max memory_allocated 47468.9169921875 
[2025-03-20 09:46:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 7 loss:0.038844190537929535 norm:0.00023024661641102284 max memory_allocated 47468.9169921875 
[2025-03-20 09:48:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 8 loss:0.038417357951402664 norm:0.00022497025202028453 max memory_allocated 47468.9169921875 
[2025-03-20 09:49:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 9 loss:0.03810859099030495 norm:0.00022512994473800063 max memory_allocated 47468.9169921875 
[2025-03-20 09:50:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 10 loss:0.03794568032026291 norm:0.0002265407529193908 max memory_allocated 47468.9169921875 
[2025-03-20 09:52:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 11 loss:0.037807025015354156 norm:0.00023198031703941524 max memory_allocated 47468.9169921875 
[2025-03-20 09:53:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 12 loss:0.0377611443400383 norm:0.0002351376024307683 max memory_allocated 47468.9169921875 
[2025-03-20 09:54:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 13 loss:0.03771234303712845 norm:0.00023221381707116961 max memory_allocated 47468.9169921875 
[2025-03-20 09:56:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 14 loss:0.03774888068437576 norm:0.00024517287965863943 max memory_allocated 47468.9169921875 
[2025-03-20 09:57:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 15 loss:0.037669241428375244 norm:0.00021913016098551452 max memory_allocated 47468.9169921875 
[2025-03-20 09:58:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 16 loss:0.03761909157037735 norm:0.0002275157457916066 max memory_allocated 47468.9169921875 
[2025-03-20 10:00:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 17 loss:0.037613559514284134 norm:0.00023817502369638532 max memory_allocated 47468.9169921875 
[2025-03-20 10:01:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 18 loss:0.0375572107732296 norm:0.00023135155788622797 max memory_allocated 47468.9169921875 
[2025-03-20 10:02:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 19 loss:0.037533726543188095 norm:0.00023633523960597813 max memory_allocated 47468.9169921875 
[2025-03-20 10:04:19 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [8, 9, 10]
[2025-03-20 10:04:22 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [11, 12, 13] ===
[2025-03-20 10:05:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 0 loss:0.08396747708320618 norm:0.0013818331062793732 max memory_allocated 47469.1044921875 
[2025-03-20 10:07:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 1 loss:0.06724072992801666 norm:0.0005881633842363954 max memory_allocated 47469.1044921875 
[2025-03-20 10:08:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 2 loss:0.055275559425354004 norm:0.00034930685069411993 max memory_allocated 47469.1044921875 
[2025-03-20 10:09:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 3 loss:0.049932993948459625 norm:0.0002505746961105615 max memory_allocated 47469.1044921875 
[2025-03-20 10:11:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 4 loss:0.047283269464969635 norm:0.00022618964430876076 max memory_allocated 47469.1044921875 
[2025-03-20 10:12:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 5 loss:0.04567147418856621 norm:0.0002069028269033879 max memory_allocated 47469.1044921875 
[2025-03-20 10:13:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 6 loss:0.044665202498435974 norm:0.00020386210235301405 max memory_allocated 47469.1044921875 
[2025-03-20 10:15:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 7 loss:0.044091515243053436 norm:0.0001977612846530974 max memory_allocated 47469.1044921875 
[2025-03-20 10:16:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 8 loss:0.04372608661651611 norm:0.00019216393411625177 max memory_allocated 47469.1044921875 
[2025-03-20 10:17:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 9 loss:0.043479062616825104 norm:0.00019115512259304523 max memory_allocated 47469.1044921875 
[2025-03-20 10:19:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 10 loss:0.04330732300877571 norm:0.00018559291493147612 max memory_allocated 47469.1044921875 
[2025-03-20 10:20:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 11 loss:0.04313879460096359 norm:0.00018375400395598263 max memory_allocated 47469.1044921875 
[2025-03-20 10:21:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 12 loss:0.04306754097342491 norm:0.00018681497022043914 max memory_allocated 47469.1044921875 
[2025-03-20 10:23:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 13 loss:0.04299325495958328 norm:0.00018685277609620243 max memory_allocated 47469.1044921875 
[2025-03-20 10:24:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 14 loss:0.042908500880002975 norm:0.0001848559477366507 max memory_allocated 47469.1044921875 
[2025-03-20 10:25:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 15 loss:0.04284265637397766 norm:0.00018256639305036515 max memory_allocated 47469.1044921875 
[2025-03-20 10:27:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 16 loss:0.04280546307563782 norm:0.00017941094120033085 max memory_allocated 47469.1044921875 
[2025-03-20 10:28:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 17 loss:0.04274344444274902 norm:0.00017793389270082116 max memory_allocated 47469.1044921875 
[2025-03-20 10:29:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 18 loss:0.04270569980144501 norm:0.00018246175022795796 max memory_allocated 47469.1044921875 
[2025-03-20 10:31:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 19 loss:0.04265502840280533 norm:0.00017726540681906044 max memory_allocated 47469.1044921875 
[2025-03-20 10:32:50 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [11, 12, 13]
[2025-03-20 10:32:50 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [14, 15] ===
[2025-03-20 10:33:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 0 loss:0.07546281069517136 norm:0.0008067898452281952 max memory_allocated 47469.1044921875 
[2025-03-20 10:34:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 1 loss:0.06264474987983704 norm:0.00037696841172873974 max memory_allocated 47469.1044921875 
[2025-03-20 10:35:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 2 loss:0.05304113030433655 norm:0.0002400282974122092 max memory_allocated 47469.1044921875 
[2025-03-20 10:36:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 3 loss:0.04951246827840805 norm:0.00020166070316918194 max memory_allocated 47469.1044921875 
[2025-03-20 10:37:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 4 loss:0.04767411947250366 norm:0.00019177442300133407 max memory_allocated 47469.1044921875 
[2025-03-20 10:38:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 5 loss:0.04662880301475525 norm:0.00016564356337767094 max memory_allocated 47469.1044921875 
[2025-03-20 10:39:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 6 loss:0.046076368540525436 norm:0.00015977829752955586 max memory_allocated 47469.1044921875 
[2025-03-20 10:40:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 7 loss:0.04574950784444809 norm:0.00015595168224535882 max memory_allocated 47469.1044921875 
[2025-03-20 10:40:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 8 loss:0.04549060016870499 norm:0.00014539317635353655 max memory_allocated 47469.1044921875 
[2025-03-20 10:41:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 9 loss:0.0454031266272068 norm:0.00015538246952928603 max memory_allocated 47469.1044921875 
[2025-03-20 10:42:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 10 loss:0.04526441916823387 norm:0.00014205032493919134 max memory_allocated 47469.1044921875 
[2025-03-20 10:43:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 11 loss:0.04517969489097595 norm:0.00013628283340949565 max memory_allocated 47469.1044921875 
[2025-03-20 10:44:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 12 loss:0.045088429003953934 norm:0.00013571392628364265 max memory_allocated 47469.1044921875 
[2025-03-20 10:45:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 13 loss:0.04498932138085365 norm:0.00013310957001522183 max memory_allocated 47469.1044921875 
[2025-03-20 10:46:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 14 loss:0.04490203410387039 norm:0.00012937340943608433 max memory_allocated 47469.1044921875 
[2025-03-20 10:47:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 15 loss:0.04491224139928818 norm:0.00013959048374090344 max memory_allocated 47469.1044921875 
[2025-03-20 10:48:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 16 loss:0.04486323893070221 norm:0.00013272497744765133 max memory_allocated 47469.1044921875 
[2025-03-20 10:48:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 17 loss:0.04479847475886345 norm:0.00012767931912094355 max memory_allocated 47469.1044921875 
[2025-03-20 10:49:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 18 loss:0.04474959522485733 norm:0.00012750345922540873 max memory_allocated 47469.1044921875 
[2025-03-20 10:50:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 19 loss:0.04472684860229492 norm:0.00012619426706805825 max memory_allocated 47469.1044921875 
[2025-03-20 10:51:48 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [14, 15]
[2025-03-20 10:51:48 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [16, 17, 18] ===
[2025-03-20 10:53:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 0 loss:0.12215349078178406 norm:0.0016553819878026843 max memory_allocated 47469.4169921875 
[2025-03-20 10:54:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 1 loss:0.09944001585245132 norm:0.0006085471250116825 max memory_allocated 47469.4169921875 
[2025-03-20 10:55:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 2 loss:0.08162280172109604 norm:0.00034734580549411476 max memory_allocated 47469.4169921875 
[2025-03-20 10:57:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 3 loss:0.07534758001565933 norm:0.0002885541180148721 max memory_allocated 47469.4169921875 
[2025-03-20 10:58:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 4 loss:0.07192946970462799 norm:0.0002654918353073299 max memory_allocated 47469.4169921875 
[2025-03-20 10:59:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 5 loss:0.07023346424102783 norm:0.00025992566952481866 max memory_allocated 47469.4169921875 
[2025-03-20 11:01:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 6 loss:0.06928963214159012 norm:0.0002497827517800033 max memory_allocated 47469.4169921875 
[2025-03-20 11:02:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 7 loss:0.06863947212696075 norm:0.00023983871506061405 max memory_allocated 47469.4169921875 
[2025-03-20 11:03:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 8 loss:0.06823079288005829 norm:0.00023801341012585908 max memory_allocated 47469.4169921875 
[2025-03-20 11:05:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 9 loss:0.0678434744477272 norm:0.00022956958855502307 max memory_allocated 47469.4169921875 
[2025-03-20 11:06:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 10 loss:0.06755182147026062 norm:0.00022397379507310688 max memory_allocated 47469.4169921875 
[2025-03-20 11:07:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 11 loss:0.06735643744468689 norm:0.00022712917416356504 max memory_allocated 47469.4169921875 
[2025-03-20 11:09:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 12 loss:0.06717037409543991 norm:0.00022618485672865063 max memory_allocated 47469.4169921875 
[2025-03-20 11:10:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 13 loss:0.06697266548871994 norm:0.000219418085180223 max memory_allocated 47469.4169921875 
[2025-03-20 11:11:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 14 loss:0.06686815619468689 norm:0.0002225503121735528 max memory_allocated 47469.4169921875 
[2025-03-20 11:13:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 15 loss:0.06670306622982025 norm:0.00021440687123686075 max memory_allocated 47469.4169921875 
[2025-03-20 11:14:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 16 loss:0.06656863540410995 norm:0.00021541480964515358 max memory_allocated 47469.4169921875 
[2025-03-20 11:15:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 17 loss:0.06648901104927063 norm:0.00021411698253359646 max memory_allocated 47469.4169921875 
[2025-03-20 11:17:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 18 loss:0.06639254838228226 norm:0.00021248214761726558 max memory_allocated 47469.4169921875 
[2025-03-20 11:18:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 19 loss:0.06631862372159958 norm:0.00021314896002877504 max memory_allocated 47469.4169921875 
[2025-03-20 11:20:22 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [16, 17, 18]
[2025-03-20 11:20:22 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [19, 20, 21] ===
[2025-03-20 11:21:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 0 loss:0.18352587521076202 norm:0.002454438479617238 max memory_allocated 47469.6044921875 
[2025-03-20 11:23:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 1 loss:0.15242978930473328 norm:0.0008785546524450183 max memory_allocated 47469.6044921875 
[2025-03-20 11:24:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 2 loss:0.12652526795864105 norm:0.0005227446090430021 max memory_allocated 47469.6044921875 
[2025-03-20 11:25:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 3 loss:0.11823055893182755 norm:0.00047506619011983275 max memory_allocated 47469.6044921875 
[2025-03-20 11:27:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 4 loss:0.11411809921264648 norm:0.0004516102490015328 max memory_allocated 47469.6044921875 
[2025-03-20 11:28:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 5 loss:0.11225277185440063 norm:0.00040167124825529754 max memory_allocated 47469.6044921875 
[2025-03-20 11:29:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 6 loss:0.11123082041740417 norm:0.0003903751203324646 max memory_allocated 47469.6044921875 
[2025-03-20 11:31:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 7 loss:0.11053350567817688 norm:0.00039099634159356356 max memory_allocated 47469.6044921875 
[2025-03-20 11:32:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 8 loss:0.10984139889478683 norm:0.00038332826807163656 max memory_allocated 47469.6044921875 
[2025-03-20 11:33:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 9 loss:0.10921388119459152 norm:0.00035707588540390134 max memory_allocated 47469.6044921875 
[2025-03-20 11:35:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 10 loss:0.1087237223982811 norm:0.00034740378032438457 max memory_allocated 47469.6044921875 
[2025-03-20 11:36:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 11 loss:0.10825133323669434 norm:0.00034130056155845523 max memory_allocated 47469.6044921875 
[2025-03-20 11:37:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 12 loss:0.10786004364490509 norm:0.00032787659438326955 max memory_allocated 47469.6044921875 
[2025-03-20 11:39:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 13 loss:0.10754333436489105 norm:0.0003368606267031282 max memory_allocated 47469.6044921875 
[2025-03-20 11:40:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 14 loss:0.10731825232505798 norm:0.0003297371731605381 max memory_allocated 47469.6044921875 
[2025-03-20 11:41:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 15 loss:0.1070704311132431 norm:0.0003309715539216995 max memory_allocated 47469.6044921875 
[2025-03-20 11:43:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 16 loss:0.1068161204457283 norm:0.0003254800394643098 max memory_allocated 47469.6044921875 
[2025-03-20 11:44:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 17 loss:0.10665477812290192 norm:0.00031535301241092384 max memory_allocated 47469.6044921875 
[2025-03-20 11:45:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 18 loss:0.10648138076066971 norm:0.0003178596671205014 max memory_allocated 47469.6044921875 
[2025-03-20 11:47:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 19 loss:0.10632892698049545 norm:0.000314759963657707 max memory_allocated 47469.6044921875 
[2025-03-20 11:48:48 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [19, 20, 21]
[2025-03-20 11:48:48 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [22, 23, 24] ===
[2025-03-20 11:50:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 0 loss:0.2651216387748718 norm:0.0025273493956774473 max memory_allocated 47469.7919921875 
[2025-03-20 11:51:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 1 loss:0.22439034283161163 norm:0.0009804037399590015 max memory_allocated 47469.7919921875 
[2025-03-20 11:52:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 2 loss:0.18875177204608917 norm:0.0005583463935181499 max memory_allocated 47469.7919921875 
[2025-03-20 11:54:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 3 loss:0.17801018059253693 norm:0.0004884428344666958 max memory_allocated 47469.7919921875 
[2025-03-20 11:55:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 4 loss:0.17360982298851013 norm:0.00044053312740288675 max memory_allocated 47469.7919921875 
[2025-03-20 11:56:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 5 loss:0.17181387543678284 norm:0.0004228566540405154 max memory_allocated 47469.7919921875 
[2025-03-20 11:58:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 6 loss:0.1705303192138672 norm:0.00041717709973454475 max memory_allocated 47469.7919921875 
[2025-03-20 11:59:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 7 loss:0.16954703629016876 norm:0.0004019006446469575 max memory_allocated 47469.7919921875 
[2025-03-20 12:00:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 8 loss:0.16874803602695465 norm:0.0003966851800214499 max memory_allocated 47469.7919921875 
[2025-03-20 12:02:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 9 loss:0.16807669401168823 norm:0.0003919150331057608 max memory_allocated 47469.7919921875 
[2025-03-20 12:03:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 10 loss:0.16741546988487244 norm:0.00038438031333498657 max memory_allocated 47469.7919921875 
[2025-03-20 12:04:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 11 loss:0.16680094599723816 norm:0.00037775689270347357 max memory_allocated 47469.7919921875 
[2025-03-20 12:06:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 12 loss:0.16634738445281982 norm:0.000373297167243436 max memory_allocated 47469.7919921875 
[2025-03-20 12:07:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 13 loss:0.16593031585216522 norm:0.0003745013091247529 max memory_allocated 47469.7919921875 
[2025-03-20 12:08:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 14 loss:0.16552066802978516 norm:0.00037567297113128006 max memory_allocated 47469.7919921875 
[2025-03-20 12:10:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 15 loss:0.1651853621006012 norm:0.0003636484907474369 max memory_allocated 47469.7919921875 
[2025-03-20 12:11:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 16 loss:0.1648286134004593 norm:0.0003660063957795501 max memory_allocated 47469.7919921875 
[2025-03-20 12:12:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 17 loss:0.16452138125896454 norm:0.00036088694469071925 max memory_allocated 47469.7919921875 
[2025-03-20 12:14:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 18 loss:0.16433970630168915 norm:0.00038052996387705207 max memory_allocated 47469.7919921875 
[2025-03-20 12:15:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 19 loss:0.16406968235969543 norm:0.00036145569174550474 max memory_allocated 47469.7919921875 
[2025-03-20 12:17:14 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [22, 23, 24]
[2025-03-20 12:17:15 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [25, 26] ===
[2025-03-20 12:18:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 0 loss:0.2812176048755646 norm:0.0024820519611239433 max memory_allocated 47469.7919921875 
[2025-03-20 12:19:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 1 loss:0.24951612949371338 norm:0.0009407688048668206 max memory_allocated 47469.7919921875 
[2025-03-20 12:20:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 2 loss:0.22070594131946564 norm:0.0003950825484935194 max memory_allocated 47469.7919921875 
[2025-03-20 12:20:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 3 loss:0.2131475806236267 norm:0.0003648417186923325 max memory_allocated 47469.7919921875 
[2025-03-20 12:21:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 4 loss:0.2112082540988922 norm:0.0003351444029249251 max memory_allocated 47469.7919921875 
[2025-03-20 12:22:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 5 loss:0.21018214523792267 norm:0.00030693085864186287 max memory_allocated 47469.7919921875 
[2025-03-20 12:23:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 6 loss:0.2094786912202835 norm:0.0002940727863460779 max memory_allocated 47469.7919921875 
[2025-03-20 12:24:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 7 loss:0.2089226394891739 norm:0.0002843636611942202 max memory_allocated 47469.7919921875 
[2025-03-20 12:25:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 8 loss:0.20836912095546722 norm:0.0002655447751749307 max memory_allocated 47469.7919921875 
[2025-03-20 12:26:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 9 loss:0.20780646800994873 norm:0.00026027890271507204 max memory_allocated 47469.7919921875 
[2025-03-20 12:27:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 10 loss:0.207443505525589 norm:0.0002692592388484627 max memory_allocated 47469.7919921875 
[2025-03-20 12:28:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 11 loss:0.2070607841014862 norm:0.00026381967472843826 max memory_allocated 47469.7919921875 
[2025-03-20 12:28:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 12 loss:0.20672988891601562 norm:0.0002582724264357239 max memory_allocated 47469.7919921875 
[2025-03-20 12:29:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 13 loss:0.20640668272972107 norm:0.0002558949345257133 max memory_allocated 47469.7919921875 
[2025-03-20 12:30:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 14 loss:0.20617511868476868 norm:0.0002580283326096833 max memory_allocated 47469.7919921875 
[2025-03-20 12:31:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 15 loss:0.205906942486763 norm:0.0002517349785193801 max memory_allocated 47469.7919921875 
[2025-03-20 12:32:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 16 loss:0.20565150678157806 norm:0.00025043266941793263 max memory_allocated 47469.7919921875 
[2025-03-20 12:33:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 17 loss:0.20550459623336792 norm:0.00024858792312443256 max memory_allocated 47469.7919921875 
[2025-03-20 12:34:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 18 loss:0.20542962849140167 norm:0.00025058063329197466 max memory_allocated 47469.7919921875 
[2025-03-20 12:35:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 19 loss:0.2052288055419922 norm:0.0002503542636986822 max memory_allocated 47469.7919921875 
[2025-03-20 12:36:19 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [25, 26]
[2025-03-20 12:36:19 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [27] ===
[2025-03-20 12:36:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 0 loss:0.256202757358551 norm:0.0006280116504058242 max memory_allocated 47469.7919921875 
[2025-03-20 12:37:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 1 loss:0.2412719875574112 norm:0.0004044384404551238 max memory_allocated 47469.7919921875 
[2025-03-20 12:37:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 2 loss:0.22773531079292297 norm:0.00026960845571011305 max memory_allocated 47469.7919921875 
[2025-03-20 12:38:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 3 loss:0.22440555691719055 norm:0.0002319979394087568 max memory_allocated 47469.7919921875 
[2025-03-20 12:38:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 4 loss:0.22361068427562714 norm:0.00022779837308917195 max memory_allocated 47469.7919921875 
[2025-03-20 12:39:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 5 loss:0.22321175038814545 norm:0.0002110671193804592 max memory_allocated 47469.7919921875 
[2025-03-20 12:39:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 6 loss:0.22288323938846588 norm:0.00019607716239988804 max memory_allocated 47469.7919921875 
[2025-03-20 12:39:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 7 loss:0.2225847989320755 norm:0.0001880499767139554 max memory_allocated 47469.7919921875 
[2025-03-20 12:40:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 8 loss:0.22238485515117645 norm:0.00017667030624579638 max memory_allocated 47469.7919921875 
[2025-03-20 12:40:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 9 loss:0.2221900075674057 norm:0.00018006324535235763 max memory_allocated 47469.7919921875 
[2025-03-20 12:41:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 10 loss:0.2220117151737213 norm:0.00016874987340997905 max memory_allocated 47469.7919921875 
[2025-03-20 12:41:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 11 loss:0.22179603576660156 norm:0.00017203891184180975 max memory_allocated 47469.7919921875 
[2025-03-20 12:42:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 12 loss:0.22163192927837372 norm:0.00016501807840541005 max memory_allocated 47469.7919921875 
[2025-03-20 12:42:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 13 loss:0.22148802876472473 norm:0.00016685803711879998 max memory_allocated 47469.7919921875 
[2025-03-20 12:43:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 14 loss:0.22136762738227844 norm:0.0001628693426027894 max memory_allocated 47469.7919921875 
[2025-03-20 12:43:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 15 loss:0.22129899263381958 norm:0.00016734747623559088 max memory_allocated 47469.7919921875 
[2025-03-20 12:43:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 16 loss:0.22120055556297302 norm:0.00016316860273946077 max memory_allocated 47469.7919921875 
[2025-03-20 12:44:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 17 loss:0.2211136370897293 norm:0.00016087554104160517 max memory_allocated 47469.7919921875 
[2025-03-20 12:44:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 18 loss:0.22106140851974487 norm:0.00015925928892102093 max memory_allocated 47469.7919921875 
[2025-03-20 12:45:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 19 loss:0.22102175652980804 norm:0.00016583804972469807 max memory_allocated 47469.7919921875 
[2025-03-20 12:45:54 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [27]
[2025-03-20 12:45:54 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [28] ===
[2025-03-20 12:45:54 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 12:46:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 0 loss:0.30159634351730347 norm:0.01734582521021366 max memory_allocated 47469.7919921875 
[2025-03-20 12:46:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 1 loss:0.28198495507240295 norm:0.013599117286503315 max memory_allocated 47469.7919921875 
[2025-03-20 12:47:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 2 loss:0.2653470039367676 norm:0.009034981951117516 max memory_allocated 47469.7919921875 
[2025-03-20 12:47:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 3 loss:0.26102814078330994 norm:0.007693230174481869 max memory_allocated 47469.7919921875 
[2025-03-20 12:48:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 4 loss:0.25988253951072693 norm:0.006678509525954723 max memory_allocated 47469.7919921875 
[2025-03-20 12:48:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 5 loss:0.2591273784637451 norm:0.005763445980846882 max memory_allocated 47469.7919921875 
[2025-03-20 12:49:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 6 loss:0.2585732936859131 norm:0.00492660328745842 max memory_allocated 47469.7919921875 
[2025-03-20 12:49:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 7 loss:0.2581964433193207 norm:0.004571824800223112 max memory_allocated 47469.7919921875 
[2025-03-20 12:50:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 8 loss:0.25798362493515015 norm:0.004500304814428091 max memory_allocated 47469.7919921875 
[2025-03-20 12:50:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 9 loss:0.2577357292175293 norm:0.004485143814235926 max memory_allocated 47469.7919921875 
[2025-03-20 12:50:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 10 loss:0.2575535178184509 norm:0.0040412768721580505 max memory_allocated 47469.7919921875 
[2025-03-20 12:51:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 11 loss:0.25714537501335144 norm:0.0038739643059670925 max memory_allocated 47469.7919921875 
[2025-03-20 12:51:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 12 loss:0.2569097876548767 norm:0.003576636780053377 max memory_allocated 47469.7919921875 
[2025-03-20 12:52:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 13 loss:0.2567884027957916 norm:0.003618214512243867 max memory_allocated 47469.7919921875 
[2025-03-20 12:52:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 14 loss:0.2566528022289276 norm:0.0034852465614676476 max memory_allocated 47469.7919921875 
[2025-03-20 12:53:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 15 loss:0.2564980983734131 norm:0.0034383407328277826 max memory_allocated 47469.7919921875 
[2025-03-20 12:53:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 16 loss:0.25633642077445984 norm:0.0032749015372246504 max memory_allocated 47469.7919921875 
[2025-03-20 12:54:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 17 loss:0.25620341300964355 norm:0.0032863193191587925 max memory_allocated 47469.7919921875 
[2025-03-20 12:54:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 18 loss:0.2561168968677521 norm:0.003145385766401887 max memory_allocated 47469.7919921875 
[2025-03-20 12:54:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 19 loss:0.2560030221939087 norm:0.0031839811708778143 max memory_allocated 47469.7919921875 
[2025-03-20 12:55:31 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [28]
[2025-03-20 12:55:31 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [29] ===
[2025-03-20 12:55:31 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 12:56:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 0 loss:0.34828025102615356 norm:0.019795432686805725 max memory_allocated 47469.7919921875 
[2025-03-20 12:56:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 1 loss:0.3262879252433777 norm:0.013995463959872723 max memory_allocated 47469.7919921875 
[2025-03-20 12:56:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 2 loss:0.3075614869594574 norm:0.009151189588010311 max memory_allocated 47469.7919921875 
[2025-03-20 12:57:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 3 loss:0.3032715916633606 norm:0.007762922905385494 max memory_allocated 47469.7919921875 
[2025-03-20 12:57:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 4 loss:0.30213212966918945 norm:0.006730652879923582 max memory_allocated 47469.7919921875 
[2025-03-20 12:58:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 5 loss:0.30132925510406494 norm:0.005790152121335268 max memory_allocated 47469.7919921875 
[2025-03-20 12:58:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 6 loss:0.30069664120674133 norm:0.004982528742402792 max memory_allocated 47469.7919921875 
[2025-03-20 12:59:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 7 loss:0.30026575922966003 norm:0.004578833002597094 max memory_allocated 47469.7919921875 
[2025-03-20 12:59:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 8 loss:0.3001173734664917 norm:0.004559076391160488 max memory_allocated 47469.7919921875 
[2025-03-20 13:00:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 9 loss:0.29976895451545715 norm:0.004439168609678745 max memory_allocated 47469.7919921875 
[2025-03-20 13:00:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 10 loss:0.29949355125427246 norm:0.003981493413448334 max memory_allocated 47469.7919921875 
[2025-03-20 13:00:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 11 loss:0.29923558235168457 norm:0.004086621105670929 max memory_allocated 47469.7919921875 
[2025-03-20 13:01:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 12 loss:0.2990291118621826 norm:0.003756412770599127 max memory_allocated 47469.7919921875 
[2025-03-20 13:01:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 13 loss:0.29884085059165955 norm:0.003909291233867407 max memory_allocated 47469.7919921875 
[2025-03-20 13:02:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 14 loss:0.2986988425254822 norm:0.0035510880406945944 max memory_allocated 47469.7919921875 
[2025-03-20 13:02:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 15 loss:0.2985498309135437 norm:0.00370386173017323 max memory_allocated 47469.7919921875 
[2025-03-20 13:03:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 16 loss:0.29839324951171875 norm:0.00333189545199275 max memory_allocated 47469.7919921875 
[2025-03-20 13:03:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 17 loss:0.298275351524353 norm:0.003513800911605358 max memory_allocated 47469.7919921875 
[2025-03-20 13:04:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 18 loss:0.2982269525527954 norm:0.003319208277389407 max memory_allocated 47469.7919921875 
[2025-03-20 13:04:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 19 loss:0.2981005311012268 norm:0.0034751808270812035 max memory_allocated 47469.7919921875 
[2025-03-20 13:05:08 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [29]
[2025-03-20 13:05:08 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [30] ===
[2025-03-20 13:05:08 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 13:05:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 0 loss:0.4687088131904602 norm:0.03421274945139885 max memory_allocated 47469.7919921875 
[2025-03-20 13:06:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 1 loss:0.4261382818222046 norm:0.022403808310627937 max memory_allocated 47469.7919921875 
[2025-03-20 13:06:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 2 loss:0.3918769955635071 norm:0.015019568614661694 max memory_allocated 47469.7919921875 
[2025-03-20 13:06:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 3 loss:0.3850870132446289 norm:0.013170366175472736 max memory_allocated 47469.7919921875 
[2025-03-20 13:07:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 4 loss:0.38258692622184753 norm:0.011150697246193886 max memory_allocated 47469.7919921875 
[2025-03-20 13:07:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 5 loss:0.3808095455169678 norm:0.00969616323709488 max memory_allocated 47469.7919921875 
[2025-03-20 13:08:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 6 loss:0.3795648217201233 norm:0.008513987064361572 max memory_allocated 47469.7919921875 
[2025-03-20 13:08:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 7 loss:0.3785952925682068 norm:0.007774593774229288 max memory_allocated 47469.7919921875 
[2025-03-20 13:09:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 8 loss:0.3779928982257843 norm:0.007648201659321785 max memory_allocated 47469.7919921875 
[2025-03-20 13:09:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 9 loss:0.37779679894447327 norm:0.007704637013375759 max memory_allocated 47469.7919921875 
[2025-03-20 13:10:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 10 loss:0.3768671452999115 norm:0.006569116376340389 max memory_allocated 47469.7919921875 
[2025-03-20 13:10:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 11 loss:0.3764244616031647 norm:0.0061534675769507885 max memory_allocated 47469.7919921875 
[2025-03-20 13:11:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 12 loss:0.3760746121406555 norm:0.006357083097100258 max memory_allocated 47469.7919921875 
[2025-03-20 13:11:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 13 loss:0.3758690655231476 norm:0.0063710599206388 max memory_allocated 47469.7919921875 
[2025-03-20 13:11:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 14 loss:0.3754615783691406 norm:0.006070874631404877 max memory_allocated 47469.7919921875 
[2025-03-20 13:12:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 15 loss:0.3756639063358307 norm:0.005654545966535807 max memory_allocated 47469.7919921875 
[2025-03-20 13:12:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 16 loss:0.37503302097320557 norm:0.005931146442890167 max memory_allocated 47469.7919921875 
[2025-03-20 13:13:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 17 loss:0.3748142123222351 norm:0.0058158813044428825 max memory_allocated 47469.7919921875 
[2025-03-20 13:13:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 18 loss:0.374539852142334 norm:0.005612164735794067 max memory_allocated 47469.7919921875 
[2025-03-20 13:14:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 19 loss:0.37451601028442383 norm:0.0055622523650527 max memory_allocated 47469.7919921875 
[2025-03-20 13:14:45 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [30]
[2025-03-20 13:14:45 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [31] ===
[2025-03-20 13:14:45 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 13:15:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 0 loss:0.806845486164093 norm:0.0851806253194809 max memory_allocated 47469.7919921875 
[2025-03-20 13:15:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 1 loss:0.7141085267066956 norm:0.0583612360060215 max memory_allocated 47469.7919921875 
[2025-03-20 13:16:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 2 loss:0.6481646299362183 norm:0.038221292197704315 max memory_allocated 47469.7919921875 
[2025-03-20 13:16:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 3 loss:0.632391095161438 norm:0.03433525562286377 max memory_allocated 47469.7919921875 
[2025-03-20 13:17:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 4 loss:0.624595582485199 norm:0.030838076025247574 max memory_allocated 47469.7919921875 
[2025-03-20 13:17:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 5 loss:0.6189351677894592 norm:0.027512326836586 max memory_allocated 47469.7919921875 
[2025-03-20 13:17:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 6 loss:0.6152431964874268 norm:0.025424731895327568 max memory_allocated 47469.7919921875 
[2025-03-20 13:18:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 7 loss:0.6124461889266968 norm:0.02403233014047146 max memory_allocated 47469.7919921875 
[2025-03-20 13:18:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 8 loss:0.6100396513938904 norm:0.023039203137159348 max memory_allocated 47469.7919921875 
[2025-03-20 13:19:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 9 loss:0.6080136895179749 norm:0.0220306646078825 max memory_allocated 47469.7919921875 
[2025-03-20 13:19:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 10 loss:0.606328010559082 norm:0.020864248275756836 max memory_allocated 47469.7919921875 
[2025-03-20 13:20:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 11 loss:0.6054846048355103 norm:0.021371634677052498 max memory_allocated 47469.7919921875 
[2025-03-20 13:20:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 12 loss:0.6054099202156067 norm:0.02198229357600212 max memory_allocated 47469.7919921875 
[2025-03-20 13:21:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 13 loss:0.6044065952301025 norm:0.02153971418738365 max memory_allocated 47469.7919921875 
[2025-03-20 13:21:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 14 loss:0.6025004386901855 norm:0.01922568306326866 max memory_allocated 47469.7919921875 
[2025-03-20 13:22:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 15 loss:0.6015021204948425 norm:0.018439866602420807 max memory_allocated 47469.7919921875 
[2025-03-20 13:22:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 16 loss:0.6005664467811584 norm:0.017950160428881645 max memory_allocated 47469.7919921875 
[2025-03-20 13:22:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 17 loss:0.6002609729766846 norm:0.018009955063462257 max memory_allocated 47469.7919921875 
[2025-03-20 13:23:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 18 loss:0.5989067554473877 norm:0.016743076965212822 max memory_allocated 47469.7919921875 
[2025-03-20 13:23:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 19 loss:0.5990001559257507 norm:0.017454184591770172 max memory_allocated 47469.7919921875 
[2025-03-20 13:24:21 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 15, block: [31]
[2025-03-20 13:24:21 root] (main_calib_config3_attn.py 379): INFO 18285.017528295517
[2025-03-20 13:24:35 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-20 13:25:21 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.826959609985352
[2025-03-20 13:25:21 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-20 13:26:33 root] (main_calib_config3_attn.py 161): INFO c4 : 7.297031402587891
[2025-03-20 14:05:08 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.826959609985352, 'c4': 7.297031402587891, 'results': {'boolq': {'acc': 0.7302752293577982, 'acc_stderr': 0.0077624039763634954}, 'hellaswag': {'acc': 0.5552678749253137, 'acc_stderr': 0.0049592047730462096, 'acc_norm': 0.7154949213304123, 'acc_norm_stderr': 0.004502563079349394}, 'piqa': {'acc': 0.7812840043525572, 'acc_stderr': 0.009644731932667563, 'acc_norm': 0.7774755168661589, 'acc_norm_stderr': 0.009704600975718227}, 'winogrande': {'acc': 0.6614048934490924, 'acc_stderr': 0.013300169865842409}, 'arc_challenge': {'acc': 0.3796928327645051, 'acc_stderr': 0.014182119866974874, 'acc_norm': 0.3916382252559727, 'acc_norm_stderr': 0.014264122124938213}, 'arc_easy': {'acc': 0.6717171717171717, 'acc_stderr': 0.009635749509262161, 'acc_norm': 0.5273569023569024, 'acc_norm_stderr': 0.010244415164390534}}, 'versions': {'boolq': 1, 'hellaswag': 0, 'piqa': 0, 'winogrande': 0, 'arc_challenge': 0, 'arc_easy': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-20 14:05:08 root] (main_calib_config3_attn.py 175): INFO 37.97,67.17,73.03,55.53,78.13,66.14
