[2025-03-23 02:09:59 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/llama-7b-hf-0.25', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.25.pkl', blocks_pkl='./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-23 02:10:07 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-23 02:10:07 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-23 02:10:07 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-23 02:10:07 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.25.pkl
[2025-03-23 02:10:07 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 5), (5, 8), (8, 11), (11, 14), (14, 16), (16, 19), (19, 22), (22, 25), (25, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-23 02:10:07 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2, 3, 4], [5, 6, 7], [8, 9, 10], [11, 12, 13], [14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26], [27], [28], [29], [30], [31]]
[2025-03-23 02:10:10 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-23 02:10:10 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:10:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.011318817734718323 norm:0.014298656024038792 max memory_allocated 34630.880859375 
[2025-03-23 02:11:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.0057228077203035355 norm:0.007060237228870392 max memory_allocated 34630.880859375 
[2025-03-23 02:11:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.0037769745104014874 norm:0.0048429653979837894 max memory_allocated 34630.880859375 
[2025-03-23 02:12:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0031550386920571327 norm:0.003786667948588729 max memory_allocated 34630.880859375 
[2025-03-23 02:12:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.0028786729089915752 norm:0.0030213494319468737 max memory_allocated 34630.880859375 
[2025-03-23 02:13:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.00268411822617054 norm:0.002597321756184101 max memory_allocated 34630.880859375 
[2025-03-23 02:13:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.002580683445557952 norm:0.002174751367419958 max memory_allocated 34630.880859375 
[2025-03-23 02:14:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0024901949800550938 norm:0.0019285418093204498 max memory_allocated 34630.880859375 
[2025-03-23 02:14:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0024284019600600004 norm:0.001744749373756349 max memory_allocated 34630.880859375 
[2025-03-23 02:15:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.002396941650658846 norm:0.0015739755472168326 max memory_allocated 34630.880859375 
[2025-03-23 02:15:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.0023598787374794483 norm:0.0013907153625041246 max memory_allocated 34630.880859375 
[2025-03-23 02:15:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0023468639701604843 norm:0.0012923558242619038 max memory_allocated 34630.880859375 
[2025-03-23 02:16:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.002313740085810423 norm:0.001144860521890223 max memory_allocated 34630.880859375 
[2025-03-23 02:16:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.002291728276759386 norm:0.0010277265682816505 max memory_allocated 34630.880859375 
[2025-03-23 02:17:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0022792865056544542 norm:0.0009424103191122413 max memory_allocated 34630.880859375 
[2025-03-23 02:17:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.002260378561913967 norm:0.0008509174804203212 max memory_allocated 34630.880859375 
[2025-03-23 02:18:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.0022682517301291227 norm:0.0007787365466356277 max memory_allocated 34630.880859375 
[2025-03-23 02:18:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0022652004845440388 norm:0.0007346344646066427 max memory_allocated 34630.880859375 
[2025-03-23 02:19:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.0022797961719334126 norm:0.0007136674248613417 max memory_allocated 34630.880859375 
[2025-03-23 02:19:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.002256820211187005 norm:0.0006785187870264053 max memory_allocated 34630.880859375 
[2025-03-23 02:20:42 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-23 02:20:42 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-23 02:20:42 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:21:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.020985545590519905 norm:0.01985308900475502 max memory_allocated 35097.7724609375 
[2025-03-23 02:21:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.011920683085918427 norm:0.012155361473560333 max memory_allocated 35097.7724609375 
[2025-03-23 02:22:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.008216972462832928 norm:0.007065719924867153 max memory_allocated 35097.7724609375 
[2025-03-23 02:22:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.007168141659349203 norm:0.00505414605140686 max memory_allocated 35097.7724609375 
[2025-03-23 02:23:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.006799633614718914 norm:0.004319627769291401 max memory_allocated 35097.7724609375 
[2025-03-23 02:23:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.006536468397825956 norm:0.003885384416207671 max memory_allocated 35097.7724609375 
[2025-03-23 02:24:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.006330301985144615 norm:0.0034945933148264885 max memory_allocated 35097.7724609375 
[2025-03-23 02:24:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.006196613889187574 norm:0.0032064756378531456 max memory_allocated 35097.7724609375 
[2025-03-23 02:25:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.00599149102345109 norm:0.0029291678220033646 max memory_allocated 35097.7724609375 
[2025-03-23 02:25:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.005869755055755377 norm:0.002672565169632435 max memory_allocated 35097.7724609375 
[2025-03-23 02:26:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.005782814230769873 norm:0.0024667170364409685 max memory_allocated 35097.7724609375 
[2025-03-23 02:26:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.005752144381403923 norm:0.0022765330504626036 max memory_allocated 35097.7724609375 
[2025-03-23 02:27:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.005647622048854828 norm:0.002064646454527974 max memory_allocated 35097.7724609375 
[2025-03-23 02:27:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.00560953700914979 norm:0.0018925070762634277 max memory_allocated 35097.7724609375 
[2025-03-23 02:27:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.005566674750298262 norm:0.001714402693323791 max memory_allocated 35097.7724609375 
[2025-03-23 02:28:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.005544173996895552 norm:0.001538753043860197 max memory_allocated 35097.7724609375 
[2025-03-23 02:28:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.0055192988365888596 norm:0.0013801734894514084 max memory_allocated 35097.7724609375 
[2025-03-23 02:29:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.005503818858414888 norm:0.0012289374135434628 max memory_allocated 35097.7724609375 
[2025-03-23 02:29:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.005500898230820894 norm:0.0011027142172679305 max memory_allocated 35097.7724609375 
[2025-03-23 02:30:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.005469655618071556 norm:0.0009932512184605002 max memory_allocated 35097.7724609375 
[2025-03-23 02:31:00 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-23 02:31:00 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2, 3, 4] ===
[2025-03-23 02:31:01 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 02:32:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 0 loss:0.053294774144887924 norm:0.02995283156633377 max memory_allocated 47468.5419921875 
[2025-03-23 02:34:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 1 loss:0.03955778107047081 norm:0.02207440882921219 max memory_allocated 47468.5419921875 
[2025-03-23 02:35:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 2 loss:0.030907917767763138 norm:0.015801135450601578 max memory_allocated 47468.5419921875 
[2025-03-23 02:36:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 3 loss:0.026718581095337868 norm:0.010900940746068954 max memory_allocated 47468.5419921875 
[2025-03-23 02:38:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 4 loss:0.024608630686998367 norm:0.0084571223706007 max memory_allocated 47468.5419921875 
[2025-03-23 02:39:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 5 loss:0.023275895044207573 norm:0.006825809367001057 max memory_allocated 47468.5419921875 
[2025-03-23 02:41:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 6 loss:0.02259555086493492 norm:0.006215331144630909 max memory_allocated 47468.5419921875 
[2025-03-23 02:42:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 7 loss:0.0220591202378273 norm:0.005525167100131512 max memory_allocated 47468.5419921875 
[2025-03-23 02:44:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 8 loss:0.02150549925863743 norm:0.005266794469207525 max memory_allocated 47468.5419921875 
[2025-03-23 02:45:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 9 loss:0.021320058032870293 norm:0.00491487979888916 max memory_allocated 47468.5419921875 
[2025-03-23 02:46:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 10 loss:0.021397795528173447 norm:0.004304911009967327 max memory_allocated 47468.5419921875 
[2025-03-23 02:48:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 11 loss:0.02142396941781044 norm:0.0039000636897981167 max memory_allocated 47468.5419921875 
[2025-03-23 02:49:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 12 loss:0.021548809483647346 norm:0.004194747656583786 max memory_allocated 47468.5419921875 
[2025-03-23 02:51:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 13 loss:0.021405532956123352 norm:0.004109623841941357 max memory_allocated 47468.5419921875 
[2025-03-23 02:52:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 14 loss:0.02133249305188656 norm:0.0038168979808688164 max memory_allocated 47468.5419921875 
[2025-03-23 02:54:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 15 loss:0.021358177065849304 norm:0.003609835635870695 max memory_allocated 47468.5419921875 
[2025-03-23 02:55:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 16 loss:0.02135506272315979 norm:0.003718971274793148 max memory_allocated 47468.5419921875 
[2025-03-23 02:56:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 17 loss:0.02131117507815361 norm:0.003578662406653166 max memory_allocated 47468.5419921875 
[2025-03-23 02:58:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 18 loss:0.0213941503316164 norm:0.0034108778927475214 max memory_allocated 47468.5419921875 
[2025-03-23 02:59:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 19 loss:0.021358318626880646 norm:0.003450741060078144 max memory_allocated 47468.5419921875 
[2025-03-23 03:01:44 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2, 3, 4]
[2025-03-23 03:01:44 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [5, 6, 7] ===
[2025-03-23 03:03:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 0 loss:0.07530006021261215 norm:0.004709832835942507 max memory_allocated 47468.7294921875 
[2025-03-23 03:04:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 1 loss:0.05300174653530121 norm:0.0009014279348775744 max memory_allocated 47468.7294921875 
[2025-03-23 03:06:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 2 loss:0.04172055423259735 norm:0.00043005021871067584 max memory_allocated 47468.7294921875 
[2025-03-23 03:07:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 3 loss:0.036644548177719116 norm:0.0003458867722656578 max memory_allocated 47468.7294921875 
[2025-03-23 03:09:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 4 loss:0.034113116562366486 norm:0.00030868183239363134 max memory_allocated 47468.7294921875 
[2025-03-23 03:10:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 5 loss:0.03244831785559654 norm:0.00031266504083760083 max memory_allocated 47468.7294921875 
[2025-03-23 03:11:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 6 loss:0.031418927013874054 norm:0.0003035917761735618 max memory_allocated 47468.7294921875 
[2025-03-23 03:13:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 7 loss:0.030735870823264122 norm:0.0003071812097914517 max memory_allocated 47468.7294921875 
[2025-03-23 03:14:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 8 loss:0.03043239936232567 norm:0.0003127755771856755 max memory_allocated 47468.7294921875 
[2025-03-23 03:16:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 9 loss:0.030110005289316177 norm:0.00028035417199134827 max memory_allocated 47468.7294921875 
[2025-03-23 03:17:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 10 loss:0.030026953667402267 norm:0.00031993346055969596 max memory_allocated 47468.7294921875 
[2025-03-23 03:19:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 11 loss:0.0299066212028265 norm:0.0002991720102727413 max memory_allocated 47468.7294921875 
[2025-03-23 03:20:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 12 loss:0.02978733740746975 norm:0.0002893834316637367 max memory_allocated 47468.7294921875 
[2025-03-23 03:21:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 13 loss:0.029693474993109703 norm:0.000282725173747167 max memory_allocated 47468.7294921875 
[2025-03-23 03:23:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 14 loss:0.02959551475942135 norm:0.00028574830503202975 max memory_allocated 47468.7294921875 
[2025-03-23 03:24:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 15 loss:0.02954534813761711 norm:0.00028694956563413143 max memory_allocated 47468.7294921875 
[2025-03-23 03:26:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 16 loss:0.029513489454984665 norm:0.0002895694342441857 max memory_allocated 47468.7294921875 
[2025-03-23 03:27:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 17 loss:0.0294900294393301 norm:0.0002929448673967272 max memory_allocated 47468.7294921875 
[2025-03-23 03:29:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 18 loss:0.029466431587934494 norm:0.00029063355759717524 max memory_allocated 47468.7294921875 
[2025-03-23 03:30:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 19 loss:0.029464364051818848 norm:0.0003009405918419361 max memory_allocated 47468.7294921875 
[2025-03-23 03:32:27 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [5, 6, 7]
[2025-03-23 03:32:27 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [8, 9, 10] ===
[2025-03-23 03:34:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 0 loss:0.08470775187015533 norm:0.001572595676407218 max memory_allocated 47468.9169921875 
[2025-03-23 03:35:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 1 loss:0.06363477557897568 norm:0.0006461238954216242 max memory_allocated 47468.9169921875 
[2025-03-23 03:36:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 2 loss:0.05092015862464905 norm:0.00040313665522262454 max memory_allocated 47468.9169921875 
[2025-03-23 03:38:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 3 loss:0.04515577107667923 norm:0.0003098986635450274 max memory_allocated 47468.9169921875 
[2025-03-23 03:39:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 4 loss:0.04233546182513237 norm:0.0002787891717161983 max memory_allocated 47468.9169921875 
[2025-03-23 03:41:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 5 loss:0.04062410071492195 norm:0.00025874690618366003 max memory_allocated 47468.9169921875 
[2025-03-23 03:42:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 6 loss:0.03951369971036911 norm:0.00023418098862748593 max memory_allocated 47468.9169921875 
[2025-03-23 03:44:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 7 loss:0.038844190537929535 norm:0.00023024661641102284 max memory_allocated 47468.9169921875 
[2025-03-23 03:45:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 8 loss:0.038417357951402664 norm:0.00022497025202028453 max memory_allocated 47468.9169921875 
[2025-03-23 03:46:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 9 loss:0.03810859099030495 norm:0.00022512994473800063 max memory_allocated 47468.9169921875 
[2025-03-23 03:48:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 10 loss:0.03794568032026291 norm:0.0002265407529193908 max memory_allocated 47468.9169921875 
[2025-03-23 03:49:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 11 loss:0.037807025015354156 norm:0.00023198031703941524 max memory_allocated 47468.9169921875 
[2025-03-23 03:51:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 12 loss:0.0377611443400383 norm:0.0002351376024307683 max memory_allocated 47468.9169921875 
[2025-03-23 03:52:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 13 loss:0.03771234303712845 norm:0.00023221381707116961 max memory_allocated 47468.9169921875 
[2025-03-23 03:54:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 14 loss:0.03774888068437576 norm:0.00024517287965863943 max memory_allocated 47468.9169921875 
[2025-03-23 03:55:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 15 loss:0.037669241428375244 norm:0.00021913016098551452 max memory_allocated 47468.9169921875 
[2025-03-23 03:56:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 16 loss:0.03761909157037735 norm:0.0002275157457916066 max memory_allocated 47468.9169921875 
[2025-03-23 03:58:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 17 loss:0.037613559514284134 norm:0.00023817502369638532 max memory_allocated 47468.9169921875 
[2025-03-23 03:59:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 18 loss:0.0375572107732296 norm:0.00023135155788622797 max memory_allocated 47468.9169921875 
[2025-03-23 04:01:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 19 loss:0.037533726543188095 norm:0.00023633523960597813 max memory_allocated 47468.9169921875 
[2025-03-23 04:03:06 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [8, 9, 10]
[2025-03-23 04:03:06 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [11, 12, 13] ===
[2025-03-23 04:04:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 0 loss:0.08396747708320618 norm:0.0013818331062793732 max memory_allocated 47469.1044921875 
[2025-03-23 04:06:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 1 loss:0.06724072992801666 norm:0.0005881633842363954 max memory_allocated 47469.1044921875 
[2025-03-23 04:07:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 2 loss:0.055275559425354004 norm:0.00034930685069411993 max memory_allocated 47469.1044921875 
[2025-03-23 04:08:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 3 loss:0.049932993948459625 norm:0.0002505746961105615 max memory_allocated 47469.1044921875 
[2025-03-23 04:10:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 4 loss:0.047283269464969635 norm:0.00022618964430876076 max memory_allocated 47469.1044921875 
[2025-03-23 04:11:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 5 loss:0.04567147418856621 norm:0.0002069028269033879 max memory_allocated 47469.1044921875 
[2025-03-23 04:13:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 6 loss:0.044665202498435974 norm:0.00020386210235301405 max memory_allocated 47469.1044921875 
[2025-03-23 04:14:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 7 loss:0.044091515243053436 norm:0.0001977612846530974 max memory_allocated 47469.1044921875 
[2025-03-23 04:16:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 8 loss:0.04372608661651611 norm:0.00019216393411625177 max memory_allocated 47469.1044921875 
[2025-03-23 04:17:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 9 loss:0.043479062616825104 norm:0.00019115512259304523 max memory_allocated 47469.1044921875 
[2025-03-23 04:18:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 10 loss:0.04330732300877571 norm:0.00018559291493147612 max memory_allocated 47469.1044921875 
[2025-03-23 04:20:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 11 loss:0.04313879460096359 norm:0.00018375400395598263 max memory_allocated 47469.1044921875 
[2025-03-23 04:21:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 12 loss:0.04306754097342491 norm:0.00018681497022043914 max memory_allocated 47469.1044921875 
[2025-03-23 04:23:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 13 loss:0.04299325495958328 norm:0.00018685277609620243 max memory_allocated 47469.1044921875 
[2025-03-23 04:24:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 14 loss:0.042908500880002975 norm:0.0001848559477366507 max memory_allocated 47469.1044921875 
[2025-03-23 04:26:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 15 loss:0.04284265637397766 norm:0.00018256639305036515 max memory_allocated 47469.1044921875 
[2025-03-23 04:27:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 16 loss:0.04280546307563782 norm:0.00017941094120033085 max memory_allocated 47469.1044921875 
[2025-03-23 04:29:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 17 loss:0.04274344444274902 norm:0.00017793389270082116 max memory_allocated 47469.1044921875 
[2025-03-23 04:30:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 18 loss:0.04270569980144501 norm:0.00018246175022795796 max memory_allocated 47469.1044921875 
[2025-03-23 04:31:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 19 loss:0.04265502840280533 norm:0.00017726540681906044 max memory_allocated 47469.1044921875 
[2025-03-23 04:33:42 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [11, 12, 13]
[2025-03-23 04:33:42 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [14, 15] ===
[2025-03-23 04:34:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 0 loss:0.07546281069517136 norm:0.0008067898452281952 max memory_allocated 47469.1044921875 
[2025-03-23 04:35:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 1 loss:0.06264474987983704 norm:0.00037696841172873974 max memory_allocated 47469.1044921875 
[2025-03-23 04:36:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 2 loss:0.05304113030433655 norm:0.0002400282974122092 max memory_allocated 47469.1044921875 
[2025-03-23 04:37:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 3 loss:0.04951246827840805 norm:0.00020166070316918194 max memory_allocated 47469.1044921875 
[2025-03-23 04:38:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 4 loss:0.04767411947250366 norm:0.00019177442300133407 max memory_allocated 47469.1044921875 
[2025-03-23 04:39:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 5 loss:0.04662880301475525 norm:0.00016564356337767094 max memory_allocated 47469.1044921875 
[2025-03-23 04:40:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 6 loss:0.046076368540525436 norm:0.00015977829752955586 max memory_allocated 47469.1044921875 
[2025-03-23 04:41:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 7 loss:0.04574950784444809 norm:0.00015595168224535882 max memory_allocated 47469.1044921875 
[2025-03-23 04:42:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 8 loss:0.04549060016870499 norm:0.00014539317635353655 max memory_allocated 47469.1044921875 
[2025-03-23 04:43:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 9 loss:0.0454031266272068 norm:0.00015538246952928603 max memory_allocated 47469.1044921875 
[2025-03-23 04:44:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 10 loss:0.04526441916823387 norm:0.00014205032493919134 max memory_allocated 47469.1044921875 
[2025-03-23 04:45:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 11 loss:0.04517969489097595 norm:0.00013628283340949565 max memory_allocated 47469.1044921875 
[2025-03-23 04:46:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 12 loss:0.045088429003953934 norm:0.00013571392628364265 max memory_allocated 47469.1044921875 
[2025-03-23 04:47:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 13 loss:0.04498932138085365 norm:0.00013310957001522183 max memory_allocated 47469.1044921875 
[2025-03-23 04:48:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 14 loss:0.04490203410387039 norm:0.00012937340943608433 max memory_allocated 47469.1044921875 
[2025-03-23 04:49:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 15 loss:0.04491224139928818 norm:0.00013959048374090344 max memory_allocated 47469.1044921875 
[2025-03-23 04:50:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 16 loss:0.04486323893070221 norm:0.00013272497744765133 max memory_allocated 47469.1044921875 
[2025-03-23 04:50:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 17 loss:0.04479847475886345 norm:0.00012767931912094355 max memory_allocated 47469.1044921875 
[2025-03-23 04:51:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 18 loss:0.04474959522485733 norm:0.00012750345922540873 max memory_allocated 47469.1044921875 
[2025-03-23 04:52:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 19 loss:0.04472684860229492 norm:0.00012619426706805825 max memory_allocated 47469.1044921875 
[2025-03-23 04:54:08 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [14, 15]
[2025-03-23 04:54:08 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [16, 17, 18] ===
[2025-03-23 04:55:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 0 loss:0.12215349078178406 norm:0.0016553819878026843 max memory_allocated 47469.4169921875 
[2025-03-23 04:57:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 1 loss:0.09944001585245132 norm:0.0006085471250116825 max memory_allocated 47469.4169921875 
[2025-03-23 04:58:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 2 loss:0.08162280172109604 norm:0.00034734580549411476 max memory_allocated 47469.4169921875 
[2025-03-23 04:59:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 3 loss:0.07534758001565933 norm:0.0002885541180148721 max memory_allocated 47469.4169921875 
[2025-03-23 05:01:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 4 loss:0.07192946970462799 norm:0.0002654918353073299 max memory_allocated 47469.4169921875 
[2025-03-23 05:02:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 5 loss:0.07023346424102783 norm:0.00025992566952481866 max memory_allocated 47469.4169921875 
[2025-03-23 05:04:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 6 loss:0.06928963214159012 norm:0.0002497827517800033 max memory_allocated 47469.4169921875 
[2025-03-23 05:05:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 7 loss:0.06863947212696075 norm:0.00023983871506061405 max memory_allocated 47469.4169921875 
[2025-03-23 05:07:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 8 loss:0.06823079288005829 norm:0.00023801341012585908 max memory_allocated 47469.4169921875 
[2025-03-23 05:08:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 9 loss:0.0678434744477272 norm:0.00022956958855502307 max memory_allocated 47469.4169921875 
[2025-03-23 05:10:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 10 loss:0.06755182147026062 norm:0.00022397379507310688 max memory_allocated 47469.4169921875 
[2025-03-23 05:11:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 11 loss:0.06735643744468689 norm:0.00022712917416356504 max memory_allocated 47469.4169921875 
[2025-03-23 05:12:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 12 loss:0.06717037409543991 norm:0.00022618485672865063 max memory_allocated 47469.4169921875 
[2025-03-23 05:14:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 13 loss:0.06697266548871994 norm:0.000219418085180223 max memory_allocated 47469.4169921875 
[2025-03-23 05:15:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 14 loss:0.06686815619468689 norm:0.0002225503121735528 max memory_allocated 47469.4169921875 
[2025-03-23 05:17:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 15 loss:0.06670306622982025 norm:0.00021440687123686075 max memory_allocated 47469.4169921875 
[2025-03-23 05:18:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 16 loss:0.06656863540410995 norm:0.00021541480964515358 max memory_allocated 47469.4169921875 
[2025-03-23 05:20:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 17 loss:0.06648901104927063 norm:0.00021411698253359646 max memory_allocated 47469.4169921875 
[2025-03-23 05:21:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 18 loss:0.06639254838228226 norm:0.00021248214761726558 max memory_allocated 47469.4169921875 
[2025-03-23 05:22:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 19 loss:0.06631862372159958 norm:0.00021314896002877504 max memory_allocated 47469.4169921875 
[2025-03-23 05:24:53 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [16, 17, 18]
[2025-03-23 05:24:53 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [19, 20, 21] ===
[2025-03-23 05:26:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 0 loss:0.18352587521076202 norm:0.002454438479617238 max memory_allocated 47469.6044921875 
[2025-03-23 05:27:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 1 loss:0.15242978930473328 norm:0.0008785546524450183 max memory_allocated 47469.6044921875 
[2025-03-23 05:29:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 2 loss:0.12652526795864105 norm:0.0005227446090430021 max memory_allocated 47469.6044921875 
[2025-03-23 05:30:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 3 loss:0.11823055893182755 norm:0.00047506619011983275 max memory_allocated 47469.6044921875 
[2025-03-23 05:32:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 4 loss:0.11411809921264648 norm:0.0004516102490015328 max memory_allocated 47469.6044921875 
[2025-03-23 05:33:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 5 loss:0.11225277185440063 norm:0.00040167124825529754 max memory_allocated 47469.6044921875 
[2025-03-23 05:35:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 6 loss:0.11123082041740417 norm:0.0003903751203324646 max memory_allocated 47469.6044921875 
[2025-03-23 05:36:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 7 loss:0.11053350567817688 norm:0.00039099634159356356 max memory_allocated 47469.6044921875 
[2025-03-23 05:37:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 8 loss:0.10984139889478683 norm:0.00038332826807163656 max memory_allocated 47469.6044921875 
[2025-03-23 05:39:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 9 loss:0.10921388119459152 norm:0.00035707588540390134 max memory_allocated 47469.6044921875 
[2025-03-23 05:40:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 10 loss:0.1087237223982811 norm:0.00034740378032438457 max memory_allocated 47469.6044921875 
[2025-03-23 05:42:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 11 loss:0.10825133323669434 norm:0.00034130056155845523 max memory_allocated 47469.6044921875 
[2025-03-23 05:43:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 12 loss:0.10786004364490509 norm:0.00032787659438326955 max memory_allocated 47469.6044921875 
[2025-03-23 05:45:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 13 loss:0.10754333436489105 norm:0.0003368606267031282 max memory_allocated 47469.6044921875 
[2025-03-23 05:46:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 14 loss:0.10731825232505798 norm:0.0003297371731605381 max memory_allocated 47469.6044921875 
[2025-03-23 05:47:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 15 loss:0.1070704311132431 norm:0.0003309715539216995 max memory_allocated 47469.6044921875 
[2025-03-23 05:49:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 16 loss:0.1068161204457283 norm:0.0003254800394643098 max memory_allocated 47469.6044921875 
[2025-03-23 05:50:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 17 loss:0.10665477812290192 norm:0.00031535301241092384 max memory_allocated 47469.6044921875 
[2025-03-23 05:52:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 18 loss:0.10648138076066971 norm:0.0003178596671205014 max memory_allocated 47469.6044921875 
[2025-03-23 05:53:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 19 loss:0.10632892698049545 norm:0.000314759963657707 max memory_allocated 47469.6044921875 
[2025-03-23 05:55:40 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [19, 20, 21]
[2025-03-23 05:55:40 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [22, 23, 24] ===
[2025-03-23 05:57:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 0 loss:0.2651216387748718 norm:0.0025273493956774473 max memory_allocated 47469.7919921875 
[2025-03-23 05:58:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 1 loss:0.22439034283161163 norm:0.0009804037399590015 max memory_allocated 47469.7919921875 
[2025-03-23 06:00:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 2 loss:0.18875177204608917 norm:0.0005583463935181499 max memory_allocated 47469.7919921875 
[2025-03-23 06:01:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 3 loss:0.17801018059253693 norm:0.0004884428344666958 max memory_allocated 47469.7919921875 
[2025-03-23 06:02:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 4 loss:0.17360982298851013 norm:0.00044053312740288675 max memory_allocated 47469.7919921875 
[2025-03-23 06:04:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 5 loss:0.17181387543678284 norm:0.0004228566540405154 max memory_allocated 47469.7919921875 
[2025-03-23 06:05:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 6 loss:0.1705303192138672 norm:0.00041717709973454475 max memory_allocated 47469.7919921875 
[2025-03-23 06:07:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 7 loss:0.16954703629016876 norm:0.0004019006446469575 max memory_allocated 47469.7919921875 
[2025-03-23 06:08:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 8 loss:0.16874803602695465 norm:0.0003966851800214499 max memory_allocated 47469.7919921875 
[2025-03-23 06:10:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 9 loss:0.16807669401168823 norm:0.0003919150331057608 max memory_allocated 47469.7919921875 
[2025-03-23 06:11:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 10 loss:0.16741546988487244 norm:0.00038438031333498657 max memory_allocated 47469.7919921875 
[2025-03-23 06:13:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 11 loss:0.16680094599723816 norm:0.00037775689270347357 max memory_allocated 47469.7919921875 
[2025-03-23 06:14:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 12 loss:0.16634738445281982 norm:0.000373297167243436 max memory_allocated 47469.7919921875 
[2025-03-23 06:15:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 13 loss:0.16593031585216522 norm:0.0003745013091247529 max memory_allocated 47469.7919921875 
[2025-03-23 06:17:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 14 loss:0.16552066802978516 norm:0.00037567297113128006 max memory_allocated 47469.7919921875 
[2025-03-23 06:18:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 15 loss:0.1651853621006012 norm:0.0003636484907474369 max memory_allocated 47469.7919921875 
[2025-03-23 06:20:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 16 loss:0.1648286134004593 norm:0.0003660063957795501 max memory_allocated 47469.7919921875 
[2025-03-23 06:21:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 17 loss:0.16452138125896454 norm:0.00036088694469071925 max memory_allocated 47469.7919921875 
[2025-03-23 06:23:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 18 loss:0.16433970630168915 norm:0.00038052996387705207 max memory_allocated 47469.7919921875 
[2025-03-23 06:24:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 19 loss:0.16406968235969543 norm:0.00036145569174550474 max memory_allocated 47469.7919921875 
[2025-03-23 06:26:15 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [22, 23, 24]
[2025-03-23 06:26:15 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [25, 26] ===
[2025-03-23 06:27:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 0 loss:0.2812176048755646 norm:0.0024820519611239433 max memory_allocated 47469.7919921875 
[2025-03-23 06:28:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 1 loss:0.24951612949371338 norm:0.0009407688048668206 max memory_allocated 47469.7919921875 
[2025-03-23 06:29:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 2 loss:0.22070594131946564 norm:0.0003950825484935194 max memory_allocated 47469.7919921875 
[2025-03-23 06:30:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 3 loss:0.2131475806236267 norm:0.0003648417186923325 max memory_allocated 47469.7919921875 
[2025-03-23 06:31:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 4 loss:0.2112082540988922 norm:0.0003351444029249251 max memory_allocated 47469.7919921875 
[2025-03-23 06:32:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 5 loss:0.21018214523792267 norm:0.00030693085864186287 max memory_allocated 47469.7919921875 
[2025-03-23 06:33:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 6 loss:0.2094786912202835 norm:0.0002940727863460779 max memory_allocated 47469.7919921875 
[2025-03-23 06:33:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 7 loss:0.2089226394891739 norm:0.0002843636611942202 max memory_allocated 47469.7919921875 
[2025-03-23 06:34:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 8 loss:0.20836912095546722 norm:0.0002655447751749307 max memory_allocated 47469.7919921875 
[2025-03-23 06:35:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 9 loss:0.20780646800994873 norm:0.00026027890271507204 max memory_allocated 47469.7919921875 
[2025-03-23 06:36:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 10 loss:0.207443505525589 norm:0.0002692592388484627 max memory_allocated 47469.7919921875 
[2025-03-23 06:37:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 11 loss:0.2070607841014862 norm:0.00026381967472843826 max memory_allocated 47469.7919921875 
[2025-03-23 06:38:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 12 loss:0.20672988891601562 norm:0.0002582724264357239 max memory_allocated 47469.7919921875 
[2025-03-23 06:39:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 13 loss:0.20640668272972107 norm:0.0002558949345257133 max memory_allocated 47469.7919921875 
[2025-03-23 06:40:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 14 loss:0.20617511868476868 norm:0.0002580283326096833 max memory_allocated 47469.7919921875 
[2025-03-23 06:41:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 15 loss:0.205906942486763 norm:0.0002517349785193801 max memory_allocated 47469.7919921875 
[2025-03-23 06:42:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 16 loss:0.20565150678157806 norm:0.00025043266941793263 max memory_allocated 47469.7919921875 
[2025-03-23 06:43:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 17 loss:0.20550459623336792 norm:0.00024858792312443256 max memory_allocated 47469.7919921875 
[2025-03-23 06:44:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 18 loss:0.20542962849140167 norm:0.00025058063329197466 max memory_allocated 47469.7919921875 
[2025-03-23 06:45:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 19 loss:0.2052288055419922 norm:0.0002503542636986822 max memory_allocated 47469.7919921875 
[2025-03-23 06:46:39 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [25, 26]
[2025-03-23 06:46:39 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [27] ===
[2025-03-23 06:47:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 0 loss:0.256202757358551 norm:0.0006280116504058242 max memory_allocated 47469.7919921875 
[2025-03-23 06:47:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 1 loss:0.2412719875574112 norm:0.0004044384404551238 max memory_allocated 47469.7919921875 
[2025-03-23 06:48:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 2 loss:0.22773531079292297 norm:0.00026960845571011305 max memory_allocated 47469.7919921875 
[2025-03-23 06:48:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 3 loss:0.22440555691719055 norm:0.0002319979394087568 max memory_allocated 47469.7919921875 
[2025-03-23 06:49:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 4 loss:0.22361068427562714 norm:0.00022779837308917195 max memory_allocated 47469.7919921875 
[2025-03-23 06:49:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 5 loss:0.22321175038814545 norm:0.0002110671193804592 max memory_allocated 47469.7919921875 
[2025-03-23 06:50:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 6 loss:0.22288323938846588 norm:0.00019607716239988804 max memory_allocated 47469.7919921875 
[2025-03-23 06:50:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 7 loss:0.2225847989320755 norm:0.0001880499767139554 max memory_allocated 47469.7919921875 
[2025-03-23 06:51:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 8 loss:0.22238485515117645 norm:0.00017667030624579638 max memory_allocated 47469.7919921875 
[2025-03-23 06:51:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 9 loss:0.2221900075674057 norm:0.00018006324535235763 max memory_allocated 47469.7919921875 
[2025-03-23 06:51:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 10 loss:0.2220117151737213 norm:0.00016874987340997905 max memory_allocated 47469.7919921875 
[2025-03-23 06:52:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 11 loss:0.22179603576660156 norm:0.00017203891184180975 max memory_allocated 47469.7919921875 
[2025-03-23 06:52:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 12 loss:0.22163192927837372 norm:0.00016501807840541005 max memory_allocated 47469.7919921875 
[2025-03-23 06:53:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 13 loss:0.22148802876472473 norm:0.00016685803711879998 max memory_allocated 47469.7919921875 
[2025-03-23 06:53:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 14 loss:0.22136762738227844 norm:0.0001628693426027894 max memory_allocated 47469.7919921875 
[2025-03-23 06:54:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 15 loss:0.22129899263381958 norm:0.00016734747623559088 max memory_allocated 47469.7919921875 
[2025-03-23 06:54:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 16 loss:0.22120055556297302 norm:0.00016316860273946077 max memory_allocated 47469.7919921875 
[2025-03-23 06:55:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 17 loss:0.2211136370897293 norm:0.00016087554104160517 max memory_allocated 47469.7919921875 
[2025-03-23 06:55:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 18 loss:0.22106140851974487 norm:0.00015925928892102093 max memory_allocated 47469.7919921875 
[2025-03-23 06:56:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 19 loss:0.22102175652980804 norm:0.00016583804972469807 max memory_allocated 47469.7919921875 
[2025-03-23 06:57:07 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [27]
[2025-03-23 06:57:08 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [28] ===
[2025-03-23 06:57:08 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 06:57:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 0 loss:0.30159634351730347 norm:0.01734582521021366 max memory_allocated 47469.7919921875 
[2025-03-23 06:58:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 1 loss:0.28198495507240295 norm:0.013599117286503315 max memory_allocated 47469.7919921875 
[2025-03-23 06:58:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 2 loss:0.2653470039367676 norm:0.009034981951117516 max memory_allocated 47469.7919921875 
[2025-03-23 06:59:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 3 loss:0.26102814078330994 norm:0.007693230174481869 max memory_allocated 47469.7919921875 
[2025-03-23 06:59:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 4 loss:0.25988253951072693 norm:0.006678509525954723 max memory_allocated 47469.7919921875 
[2025-03-23 07:00:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 5 loss:0.2591273784637451 norm:0.005763445980846882 max memory_allocated 47469.7919921875 
[2025-03-23 07:00:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 6 loss:0.2585732936859131 norm:0.00492660328745842 max memory_allocated 47469.7919921875 
[2025-03-23 07:01:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 7 loss:0.2581964433193207 norm:0.004571824800223112 max memory_allocated 47469.7919921875 
[2025-03-23 07:01:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 8 loss:0.25798362493515015 norm:0.004500304814428091 max memory_allocated 47469.7919921875 
[2025-03-23 07:01:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 9 loss:0.2577357292175293 norm:0.004485143814235926 max memory_allocated 47469.7919921875 
[2025-03-23 07:02:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 10 loss:0.2575535178184509 norm:0.0040412768721580505 max memory_allocated 47469.7919921875 
[2025-03-23 07:02:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 11 loss:0.25714537501335144 norm:0.0038739643059670925 max memory_allocated 47469.7919921875 
[2025-03-23 07:03:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 12 loss:0.2569097876548767 norm:0.003576636780053377 max memory_allocated 47469.7919921875 
[2025-03-23 07:03:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 13 loss:0.2567884027957916 norm:0.003618214512243867 max memory_allocated 47469.7919921875 
[2025-03-23 07:04:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 14 loss:0.2566528022289276 norm:0.0034852465614676476 max memory_allocated 47469.7919921875 
[2025-03-23 07:04:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 15 loss:0.2564980983734131 norm:0.0034383407328277826 max memory_allocated 47469.7919921875 
[2025-03-23 07:05:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 16 loss:0.25633642077445984 norm:0.0032749015372246504 max memory_allocated 47469.7919921875 
[2025-03-23 07:05:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 17 loss:0.25620341300964355 norm:0.0032863193191587925 max memory_allocated 47469.7919921875 
[2025-03-23 07:06:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 18 loss:0.2561168968677521 norm:0.003145385766401887 max memory_allocated 47469.7919921875 
[2025-03-23 07:06:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 19 loss:0.2560030221939087 norm:0.0031839811708778143 max memory_allocated 47469.7919921875 
[2025-03-23 07:07:24 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [28]
[2025-03-23 07:07:24 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [29] ===
[2025-03-23 07:07:25 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 07:07:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 0 loss:0.34828025102615356 norm:0.019795432686805725 max memory_allocated 47469.7919921875 
[2025-03-23 07:08:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 1 loss:0.3262879252433777 norm:0.013995463959872723 max memory_allocated 47469.7919921875 
[2025-03-23 07:08:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 2 loss:0.3075614869594574 norm:0.009151189588010311 max memory_allocated 47469.7919921875 
[2025-03-23 07:09:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 3 loss:0.3032715916633606 norm:0.007762922905385494 max memory_allocated 47469.7919921875 
[2025-03-23 07:09:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 4 loss:0.30213212966918945 norm:0.006730652879923582 max memory_allocated 47469.7919921875 
[2025-03-23 07:10:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 5 loss:0.30132925510406494 norm:0.005790152121335268 max memory_allocated 47469.7919921875 
[2025-03-23 07:10:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 6 loss:0.30069664120674133 norm:0.004982528742402792 max memory_allocated 47469.7919921875 
[2025-03-23 07:11:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 7 loss:0.30026575922966003 norm:0.004578833002597094 max memory_allocated 47469.7919921875 
[2025-03-23 07:11:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 8 loss:0.3001173734664917 norm:0.004559076391160488 max memory_allocated 47469.7919921875 
[2025-03-23 07:12:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 9 loss:0.29976895451545715 norm:0.004439168609678745 max memory_allocated 47469.7919921875 
[2025-03-23 07:12:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 10 loss:0.29949355125427246 norm:0.003981493413448334 max memory_allocated 47469.7919921875 
[2025-03-23 07:13:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 11 loss:0.29923558235168457 norm:0.004086621105670929 max memory_allocated 47469.7919921875 
[2025-03-23 07:13:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 12 loss:0.2990291118621826 norm:0.003756412770599127 max memory_allocated 47469.7919921875 
[2025-03-23 07:14:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 13 loss:0.29884085059165955 norm:0.003909291233867407 max memory_allocated 47469.7919921875 
[2025-03-23 07:14:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 14 loss:0.2986988425254822 norm:0.0035510880406945944 max memory_allocated 47469.7919921875 
[2025-03-23 07:15:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 15 loss:0.2985498309135437 norm:0.00370386173017323 max memory_allocated 47469.7919921875 
[2025-03-23 07:15:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 16 loss:0.29839324951171875 norm:0.00333189545199275 max memory_allocated 47469.7919921875 
[2025-03-23 07:16:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 17 loss:0.298275351524353 norm:0.003513800911605358 max memory_allocated 47469.7919921875 
[2025-03-23 07:16:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 18 loss:0.2982269525527954 norm:0.003319208277389407 max memory_allocated 47469.7919921875 
[2025-03-23 07:17:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 19 loss:0.2981005311012268 norm:0.0034751808270812035 max memory_allocated 47469.7919921875 
[2025-03-23 07:17:41 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [29]
[2025-03-23 07:17:41 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [30] ===
[2025-03-23 07:17:42 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 07:18:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 0 loss:0.4687088131904602 norm:0.03421274945139885 max memory_allocated 47469.7919921875 
[2025-03-23 07:18:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 1 loss:0.4261382818222046 norm:0.022403808310627937 max memory_allocated 47469.7919921875 
[2025-03-23 07:19:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 2 loss:0.3918769955635071 norm:0.015019568614661694 max memory_allocated 47469.7919921875 
[2025-03-23 07:19:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 3 loss:0.3850870132446289 norm:0.013170366175472736 max memory_allocated 47469.7919921875 
[2025-03-23 07:20:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 4 loss:0.38258692622184753 norm:0.011150697246193886 max memory_allocated 47469.7919921875 
[2025-03-23 07:20:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 5 loss:0.3808095455169678 norm:0.00969616323709488 max memory_allocated 47469.7919921875 
[2025-03-23 07:21:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 6 loss:0.3795648217201233 norm:0.008513987064361572 max memory_allocated 47469.7919921875 
[2025-03-23 07:21:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 7 loss:0.3785952925682068 norm:0.007774593774229288 max memory_allocated 47469.7919921875 
[2025-03-23 07:22:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 8 loss:0.3779928982257843 norm:0.007648201659321785 max memory_allocated 47469.7919921875 
[2025-03-23 07:22:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 9 loss:0.37779679894447327 norm:0.007704637013375759 max memory_allocated 47469.7919921875 
[2025-03-23 07:23:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 10 loss:0.3768671452999115 norm:0.006569116376340389 max memory_allocated 47469.7919921875 
[2025-03-23 07:23:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 11 loss:0.3764244616031647 norm:0.0061534675769507885 max memory_allocated 47469.7919921875 
[2025-03-23 07:24:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 12 loss:0.3760746121406555 norm:0.006357083097100258 max memory_allocated 47469.7919921875 
[2025-03-23 07:24:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 13 loss:0.3758690655231476 norm:0.0063710599206388 max memory_allocated 47469.7919921875 
[2025-03-23 07:24:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 14 loss:0.3754615783691406 norm:0.006070874631404877 max memory_allocated 47469.7919921875 
[2025-03-23 07:25:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 15 loss:0.3756639063358307 norm:0.005654545966535807 max memory_allocated 47469.7919921875 
[2025-03-23 07:25:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 16 loss:0.37503302097320557 norm:0.005931146442890167 max memory_allocated 47469.7919921875 
[2025-03-23 07:26:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 17 loss:0.3748142123222351 norm:0.0058158813044428825 max memory_allocated 47469.7919921875 
[2025-03-23 07:26:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 18 loss:0.374539852142334 norm:0.005612164735794067 max memory_allocated 47469.7919921875 
[2025-03-23 07:27:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 19 loss:0.37451601028442383 norm:0.0055622523650527 max memory_allocated 47469.7919921875 
[2025-03-23 07:28:00 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [30]
[2025-03-23 07:28:00 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [31] ===
[2025-03-23 07:28:01 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-23 07:28:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 0 loss:0.806845486164093 norm:0.0851806253194809 max memory_allocated 47469.7919921875 
[2025-03-23 07:29:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 1 loss:0.7141085267066956 norm:0.0583612360060215 max memory_allocated 47469.7919921875 
[2025-03-23 07:29:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 2 loss:0.6481646299362183 norm:0.038221292197704315 max memory_allocated 47469.7919921875 
[2025-03-23 07:29:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 3 loss:0.632391095161438 norm:0.03433525562286377 max memory_allocated 47469.7919921875 
[2025-03-23 07:30:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 4 loss:0.624595582485199 norm:0.030838076025247574 max memory_allocated 47469.7919921875 
[2025-03-23 07:30:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 5 loss:0.6189351677894592 norm:0.027512326836586 max memory_allocated 47469.7919921875 
[2025-03-23 07:31:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 6 loss:0.6152431964874268 norm:0.025424731895327568 max memory_allocated 47469.7919921875 
[2025-03-23 07:31:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 7 loss:0.6124461889266968 norm:0.02403233014047146 max memory_allocated 47469.7919921875 
[2025-03-23 07:32:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 8 loss:0.6100396513938904 norm:0.023039203137159348 max memory_allocated 47469.7919921875 
[2025-03-23 07:32:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 9 loss:0.6080136895179749 norm:0.0220306646078825 max memory_allocated 47469.7919921875 
[2025-03-23 07:33:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 10 loss:0.606328010559082 norm:0.020864248275756836 max memory_allocated 47469.7919921875 
[2025-03-23 07:33:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 11 loss:0.6054846048355103 norm:0.021371634677052498 max memory_allocated 47469.7919921875 
[2025-03-23 07:34:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 12 loss:0.6054099202156067 norm:0.02198229357600212 max memory_allocated 47469.7919921875 
[2025-03-23 07:34:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 13 loss:0.6044065952301025 norm:0.02153971418738365 max memory_allocated 47469.7919921875 
[2025-03-23 07:35:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 14 loss:0.6025004386901855 norm:0.01922568306326866 max memory_allocated 47469.7919921875 
[2025-03-23 07:35:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 15 loss:0.6015021204948425 norm:0.018439866602420807 max memory_allocated 47469.7919921875 
[2025-03-23 07:36:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 16 loss:0.6005664467811584 norm:0.017950160428881645 max memory_allocated 47469.7919921875 
[2025-03-23 07:36:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 17 loss:0.6002609729766846 norm:0.018009955063462257 max memory_allocated 47469.7919921875 
[2025-03-23 07:37:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 18 loss:0.5989067554473877 norm:0.016743076965212822 max memory_allocated 47469.7919921875 
[2025-03-23 07:37:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 19 loss:0.5990001559257507 norm:0.017454184591770172 max memory_allocated 47469.7919921875 
[2025-03-23 07:38:20 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 15, block: [31]
[2025-03-23 07:38:20 root] (main_calib_config3_attn.py 379): INFO 19693.07346057892
[2025-03-23 07:38:32 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-23 07:39:23 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.826959609985352
[2025-03-23 07:39:23 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-23 07:40:41 root] (main_calib_config3_attn.py 161): INFO c4 : 7.297031402587891
