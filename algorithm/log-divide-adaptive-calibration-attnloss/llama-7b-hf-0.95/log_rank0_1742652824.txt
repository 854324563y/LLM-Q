[2025-03-22 14:13:44 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/llama-7b-hf-0.95', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.95.pkl', blocks_pkl='./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-22 14:15:05 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-22 14:15:05 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-22 14:15:06 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-22 14:15:06 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.95.pkl
[2025-03-22 14:15:06 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 5), (5, 8), (8, 11), (11, 14), (14, 16), (16, 19), (19, 22), (22, 25), (25, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-22 14:15:06 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2, 3, 4], [5, 6, 7], [8, 9, 10], [11, 12, 13], [14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26], [27], [28], [29], [30], [31]]
[2025-03-22 14:15:09 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-22 14:15:09 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:15:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.0008254675194621086 norm:0.001997064333409071 max memory_allocated 34630.880859375 
[2025-03-22 14:16:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.0004639995750039816 norm:0.0005118025583215058 max memory_allocated 34630.880859375 
[2025-03-22 14:16:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.0004401081532705575 norm:0.0009487208444625139 max memory_allocated 34630.880859375 
[2025-03-22 14:17:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.00045045441947877407 norm:0.0010298506822437048 max memory_allocated 34630.880859375 
[2025-03-22 14:17:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.0004464888188522309 norm:0.0010405853390693665 max memory_allocated 34630.880859375 
[2025-03-22 14:18:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.00043531524715945125 norm:0.0009048773208633065 max memory_allocated 34630.880859375 
[2025-03-22 14:18:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.00042746029794216156 norm:0.0008443691185675561 max memory_allocated 34630.880859375 
[2025-03-22 14:19:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.00041868496919050813 norm:0.0007638663519173861 max memory_allocated 34630.880859375 
[2025-03-22 14:19:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0004078693164046854 norm:0.0006867435877211392 max memory_allocated 34630.880859375 
[2025-03-22 14:20:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.00039567629573866725 norm:0.0006288630538620055 max memory_allocated 34630.880859375 
[2025-03-22 14:20:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.0003961219044867903 norm:0.0006091616814956069 max memory_allocated 34630.880859375 
[2025-03-22 14:21:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0003909928200300783 norm:0.0005653216503560543 max memory_allocated 34630.880859375 
[2025-03-22 14:21:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.000390970817534253 norm:0.0005334458546712995 max memory_allocated 34630.880859375 
[2025-03-22 14:21:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.0003881151496898383 norm:0.0004990075249224901 max memory_allocated 34630.880859375 
[2025-03-22 14:22:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0003809858753811568 norm:0.00043501678737811744 max memory_allocated 34630.880859375 
[2025-03-22 14:22:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.000374651572201401 norm:0.0004047841648571193 max memory_allocated 34630.880859375 
[2025-03-22 14:23:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.00037208409048616886 norm:0.000377257470972836 max memory_allocated 34630.880859375 
[2025-03-22 14:23:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.00036440620897337794 norm:0.0003387534525245428 max memory_allocated 34630.880859375 
[2025-03-22 14:24:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.0003656573826447129 norm:0.00032792898127809167 max memory_allocated 34630.880859375 
[2025-03-22 14:24:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0003628080594353378 norm:0.00030941839213483036 max memory_allocated 34630.880859375 
[2025-03-22 14:25:30 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-22 14:25:30 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-22 14:25:31 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:26:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.00845391396433115 norm:0.014471180737018585 max memory_allocated 35097.7724609375 
[2025-03-22 14:26:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.0040710922330617905 norm:0.008900845423340797 max memory_allocated 35097.7724609375 
[2025-03-22 14:27:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.0032012006267905235 norm:0.005249516572803259 max memory_allocated 35097.7724609375 
[2025-03-22 14:27:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.0029262141324579716 norm:0.00411144969984889 max memory_allocated 35097.7724609375 
[2025-03-22 14:27:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.0027523550670593977 norm:0.003629811806604266 max memory_allocated 35097.7724609375 
[2025-03-22 14:28:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.002658919198438525 norm:0.0033434834331274033 max memory_allocated 35097.7724609375 
[2025-03-22 14:28:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.002577231964096427 norm:0.003090236335992813 max memory_allocated 35097.7724609375 
[2025-03-22 14:29:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.002502468414604664 norm:0.0027887425385415554 max memory_allocated 35097.7724609375 
[2025-03-22 14:29:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.0024258214980363846 norm:0.002549735363572836 max memory_allocated 35097.7724609375 
[2025-03-22 14:30:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.002374017145484686 norm:0.002320529194548726 max memory_allocated 35097.7724609375 
[2025-03-22 14:30:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.0023188339546322823 norm:0.0021279100328683853 max memory_allocated 35097.7724609375 
[2025-03-22 14:31:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.0022572434972971678 norm:0.0019250340992584825 max memory_allocated 35097.7724609375 
[2025-03-22 14:31:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.00227652071043849 norm:0.0017432535532861948 max memory_allocated 35097.7724609375 
[2025-03-22 14:32:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.0022486348170787096 norm:0.0014885369455441833 max memory_allocated 35097.7724609375 
[2025-03-22 14:32:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.0022081653587520123 norm:0.0012797616655007005 max memory_allocated 35097.7724609375 
[2025-03-22 14:33:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.0022093032021075487 norm:0.001132499543018639 max memory_allocated 35097.7724609375 
[2025-03-22 14:33:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.00214857654646039 norm:0.0010224488796666265 max memory_allocated 35097.7724609375 
[2025-03-22 14:34:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.002126151928678155 norm:0.0008485298021696508 max memory_allocated 35097.7724609375 
[2025-03-22 14:34:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.002110469853505492 norm:0.0007691584760323167 max memory_allocated 35097.7724609375 
[2025-03-22 14:35:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.0021434093359857798 norm:0.000762424897402525 max memory_allocated 35097.7724609375 
[2025-03-22 14:35:54 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-22 14:35:54 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2, 3, 4] ===
[2025-03-22 14:35:54 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 14:37:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 0 loss:0.007477479055523872 norm:0.010788464918732643 max memory_allocated 47468.5419921875 
[2025-03-22 14:38:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 1 loss:0.0056082396768033504 norm:0.0037018274888396263 max memory_allocated 47468.5419921875 
[2025-03-22 14:40:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 2 loss:0.005296006798744202 norm:0.003561550285667181 max memory_allocated 47468.5419921875 
[2025-03-22 14:41:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 3 loss:0.004983068909496069 norm:0.003022121964022517 max memory_allocated 47468.5419921875 
[2025-03-22 14:43:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 4 loss:0.004657936282455921 norm:0.002731316490098834 max memory_allocated 47468.5419921875 
[2025-03-22 14:44:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 5 loss:0.0043482063338160515 norm:0.0026036184281110764 max memory_allocated 47468.5419921875 
[2025-03-22 14:46:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 6 loss:0.00411341805011034 norm:0.00230423198081553 max memory_allocated 47468.5419921875 
[2025-03-22 14:47:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 7 loss:0.003998567350208759 norm:0.002072602044790983 max memory_allocated 47468.5419921875 
[2025-03-22 14:49:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 8 loss:0.003919750917702913 norm:0.0020920257084071636 max memory_allocated 47468.5419921875 
[2025-03-22 14:50:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 9 loss:0.003896885085850954 norm:0.001697719213552773 max memory_allocated 47468.5419921875 
[2025-03-22 14:51:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 10 loss:0.003914705477654934 norm:0.0018881198484450579 max memory_allocated 47468.5419921875 
[2025-03-22 14:53:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 11 loss:0.003832027781754732 norm:0.0018216542666777968 max memory_allocated 47468.5419921875 
[2025-03-22 14:54:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 12 loss:0.003829211462289095 norm:0.0016503805527463555 max memory_allocated 47468.5419921875 
[2025-03-22 14:56:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 13 loss:0.0038257052656263113 norm:0.0015906289918348193 max memory_allocated 47468.5419921875 
[2025-03-22 14:57:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 14 loss:0.003811956848949194 norm:0.0014453012263402343 max memory_allocated 47468.5419921875 
[2025-03-22 14:59:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 15 loss:0.0037915625143796206 norm:0.0014194799587130547 max memory_allocated 47468.5419921875 
[2025-03-22 15:00:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 16 loss:0.003770590526983142 norm:0.0012448938796296716 max memory_allocated 47468.5419921875 
[2025-03-22 15:02:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 17 loss:0.003772399388253689 norm:0.00116319814696908 max memory_allocated 47468.5419921875 
[2025-03-22 15:03:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 18 loss:0.00378269050270319 norm:0.0011771739227697253 max memory_allocated 47468.5419921875 
[2025-03-22 15:05:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 19 loss:0.003789086127653718 norm:0.0011978839756920934 max memory_allocated 47468.5419921875 
[2025-03-22 15:06:52 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2, 3, 4]
[2025-03-22 15:06:52 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [5, 6, 7] ===
[2025-03-22 15:08:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 0 loss:0.009801055304706097 norm:0.0004074662283528596 max memory_allocated 47468.7294921875 
[2025-03-22 15:09:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 1 loss:0.008283266797661781 norm:0.00021941681916359812 max memory_allocated 47468.7294921875 
[2025-03-22 15:11:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 2 loss:0.007728522643446922 norm:0.00018838426331058145 max memory_allocated 47468.7294921875 
[2025-03-22 15:12:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 3 loss:0.007167896255850792 norm:0.00014995459059718996 max memory_allocated 47468.7294921875 
[2025-03-22 15:14:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 4 loss:0.006638779770582914 norm:0.00012378222891129553 max memory_allocated 47468.7294921875 
[2025-03-22 15:15:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 5 loss:0.006212117616087198 norm:0.0001166605215985328 max memory_allocated 47468.7294921875 
[2025-03-22 15:17:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 6 loss:0.005915795918554068 norm:0.00011774242739193141 max memory_allocated 47468.7294921875 
[2025-03-22 15:18:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 7 loss:0.0057634226977825165 norm:0.00010939374624285847 max memory_allocated 47468.7294921875 
[2025-03-22 15:20:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 8 loss:0.005681437440216541 norm:0.00010735752584878355 max memory_allocated 47468.7294921875 
[2025-03-22 15:21:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 9 loss:0.0056314775720238686 norm:0.00011480334069347009 max memory_allocated 47468.7294921875 
[2025-03-22 15:22:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 10 loss:0.0055986614897847176 norm:9.6083153039217e-05 max memory_allocated 47468.7294921875 
[2025-03-22 15:24:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 11 loss:0.0055745323188602924 norm:0.00010073694284074008 max memory_allocated 47468.7294921875 
[2025-03-22 15:25:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 12 loss:0.005546667613089085 norm:0.00010355847916798666 max memory_allocated 47468.7294921875 
[2025-03-22 15:27:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 13 loss:0.005548069253563881 norm:0.00010249079059576616 max memory_allocated 47468.7294921875 
[2025-03-22 15:28:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 14 loss:0.0055291661992669106 norm:0.00010896235471591353 max memory_allocated 47468.7294921875 
[2025-03-22 15:30:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 15 loss:0.0055091893300414085 norm:0.00011068355524912477 max memory_allocated 47468.7294921875 
[2025-03-22 15:31:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 16 loss:0.005490200128406286 norm:0.00010245580051559955 max memory_allocated 47468.7294921875 
[2025-03-22 15:33:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 17 loss:0.005484886933118105 norm:0.00011251732212258503 max memory_allocated 47468.7294921875 
[2025-03-22 15:34:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 18 loss:0.005488418973982334 norm:0.00011166693002451211 max memory_allocated 47468.7294921875 
[2025-03-22 15:35:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 19 loss:0.005463470239192247 norm:0.00010838601156137884 max memory_allocated 47468.7294921875 
[2025-03-22 15:37:47 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [5, 6, 7]
[2025-03-22 15:37:47 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [8, 9, 10] ===
[2025-03-22 15:39:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 0 loss:0.013888634741306305 norm:0.0005804686807096004 max memory_allocated 47468.9169921875 
[2025-03-22 15:40:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 1 loss:0.011156399734318256 norm:0.00029296925640664995 max memory_allocated 47468.9169921875 
[2025-03-22 15:42:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 2 loss:0.010141241364181042 norm:0.0002084872976411134 max memory_allocated 47468.9169921875 
[2025-03-22 15:43:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 3 loss:0.009382033720612526 norm:0.0001717615668894723 max memory_allocated 47468.9169921875 
[2025-03-22 15:45:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 4 loss:0.008696728385984898 norm:0.00013953939196653664 max memory_allocated 47468.9169921875 
[2025-03-22 15:46:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 5 loss:0.008174548856914043 norm:0.00012649748532567173 max memory_allocated 47468.9169921875 
[2025-03-22 15:48:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 6 loss:0.007841796614229679 norm:0.00011603729944908991 max memory_allocated 47468.9169921875 
[2025-03-22 15:49:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 7 loss:0.0076110875234007835 norm:0.00010908173862844706 max memory_allocated 47468.9169921875 
[2025-03-22 15:50:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 8 loss:0.007466524839401245 norm:0.00010162510443478823 max memory_allocated 47468.9169921875 
[2025-03-22 15:52:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 9 loss:0.007359819486737251 norm:9.47894441196695e-05 max memory_allocated 47468.9169921875 
[2025-03-22 15:53:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 10 loss:0.007278352044522762 norm:9.092131949728355e-05 max memory_allocated 47468.9169921875 
[2025-03-22 15:55:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 11 loss:0.00722709484398365 norm:8.876421634340659e-05 max memory_allocated 47468.9169921875 
[2025-03-22 15:56:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 12 loss:0.007166970521211624 norm:8.536960376659408e-05 max memory_allocated 47468.9169921875 
[2025-03-22 15:58:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 13 loss:0.007124916184693575 norm:8.30201170174405e-05 max memory_allocated 47468.9169921875 
[2025-03-22 15:59:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 14 loss:0.007082285359501839 norm:7.842545164749026e-05 max memory_allocated 47468.9169921875 
[2025-03-22 16:01:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 15 loss:0.007060011848807335 norm:7.326441118493676e-05 max memory_allocated 47468.9169921875 
[2025-03-22 16:02:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 16 loss:0.0070408848114311695 norm:7.357595313806087e-05 max memory_allocated 47468.9169921875 
[2025-03-22 16:03:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 17 loss:0.007019044365733862 norm:7.268686749739572e-05 max memory_allocated 47468.9169921875 
[2025-03-22 16:05:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 18 loss:0.007019546814262867 norm:7.334627298405394e-05 max memory_allocated 47468.9169921875 
[2025-03-22 16:06:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 19 loss:0.006998020224273205 norm:7.624557474628091e-05 max memory_allocated 47468.9169921875 
[2025-03-22 16:08:44 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [8, 9, 10]
[2025-03-22 16:08:44 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [11, 12, 13] ===
[2025-03-22 16:10:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 0 loss:0.01337737962603569 norm:0.000360487203579396 max memory_allocated 47469.1044921875 
[2025-03-22 16:11:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 1 loss:0.011787819676101208 norm:0.0002530521887820214 max memory_allocated 47469.1044921875 
[2025-03-22 16:13:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 2 loss:0.010781554505228996 norm:0.0001841326738940552 max memory_allocated 47469.1044921875 
[2025-03-22 16:14:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 3 loss:0.010062817484140396 norm:0.00015134664135985076 max memory_allocated 47469.1044921875 
[2025-03-22 16:16:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 4 loss:0.009405793622136116 norm:0.00012783778947778046 max memory_allocated 47469.1044921875 
[2025-03-22 16:17:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 5 loss:0.00892368983477354 norm:0.00011363488010829315 max memory_allocated 47469.1044921875 
[2025-03-22 16:18:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 6 loss:0.008594691753387451 norm:0.00010147949797101319 max memory_allocated 47469.1044921875 
[2025-03-22 16:20:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 7 loss:0.008367465808987617 norm:9.10105518414639e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:21:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 8 loss:0.008227781392633915 norm:8.40207576402463e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:23:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 9 loss:0.00813199020922184 norm:8.242514741141349e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:24:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 10 loss:0.008072130382061005 norm:7.735634426353499e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:26:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 11 loss:0.008027121424674988 norm:7.273595838341862e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:27:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 12 loss:0.007975342683494091 norm:6.864499300718307e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:29:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 13 loss:0.007935543544590473 norm:6.240163929760456e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:30:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 14 loss:0.007901802659034729 norm:5.8936351706506684e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:32:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 15 loss:0.007873253896832466 norm:5.825551488669589e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:33:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 16 loss:0.007851715199649334 norm:5.593030800810084e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:34:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 17 loss:0.007836217060685158 norm:5.697208325727843e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:36:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 18 loss:0.007834167219698429 norm:5.770118877990171e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:37:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 19 loss:0.007810867857187986 norm:5.5071202950784937e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:39:42 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [11, 12, 13]
[2025-03-22 16:39:42 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [14, 15] ===
[2025-03-22 16:40:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 0 loss:0.012556422501802444 norm:0.0003237246419303119 max memory_allocated 47469.1044921875 
[2025-03-22 16:41:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 1 loss:0.011233620345592499 norm:0.0001914104213938117 max memory_allocated 47469.1044921875 
[2025-03-22 16:42:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 2 loss:0.010479495860636234 norm:0.00013866332301404327 max memory_allocated 47469.1044921875 
[2025-03-22 16:43:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 3 loss:0.00990188866853714 norm:0.00012429429625626653 max memory_allocated 47469.1044921875 
[2025-03-22 16:44:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 4 loss:0.009352978318929672 norm:0.0001000509291770868 max memory_allocated 47469.1044921875 
[2025-03-22 16:45:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 5 loss:0.009010173380374908 norm:9.187698015011847e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:46:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 6 loss:0.00880960188806057 norm:8.136366523103788e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:47:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 7 loss:0.008691897615790367 norm:7.505528628826141e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:48:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 8 loss:0.008616844192147255 norm:6.983325147302821e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:49:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 9 loss:0.00856252945959568 norm:6.310050230240449e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:50:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 10 loss:0.008502962999045849 norm:5.6405522627756e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:51:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 11 loss:0.008454455994069576 norm:5.1277584134368226e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:52:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 12 loss:0.008418695069849491 norm:4.802289549843408e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:53:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 13 loss:0.008396454155445099 norm:4.6657194616273046e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:54:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 14 loss:0.008374685421586037 norm:4.418723256094381e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:55:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 15 loss:0.008351970463991165 norm:4.079130303580314e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:56:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 16 loss:0.008327051065862179 norm:3.580372504075058e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:57:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 17 loss:0.00831630453467369 norm:3.530165849952027e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:58:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 18 loss:0.008305970579385757 norm:3.363185896887444e-05 max memory_allocated 47469.1044921875 
[2025-03-22 16:59:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 19 loss:0.008312314748764038 norm:3.489780647214502e-05 max memory_allocated 47469.1044921875 
[2025-03-22 17:00:19 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [14, 15]
[2025-03-22 17:00:19 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [16, 17, 18] ===
[2025-03-22 17:01:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 0 loss:0.01872609555721283 norm:0.0003393403021618724 max memory_allocated 47469.4169921875 
[2025-03-22 17:03:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 1 loss:0.01709061861038208 norm:0.0002108374610543251 max memory_allocated 47469.4169921875 
[2025-03-22 17:04:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 2 loss:0.015903625637292862 norm:0.00016916630556806922 max memory_allocated 47469.4169921875 
[2025-03-22 17:06:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 3 loss:0.014753947965800762 norm:0.00014671542157884687 max memory_allocated 47469.4169921875 
[2025-03-22 17:07:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 4 loss:0.013826088048517704 norm:0.00012045626499457285 max memory_allocated 47469.4169921875 
[2025-03-22 17:09:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 5 loss:0.013411381281912327 norm:0.00010845030919881538 max memory_allocated 47469.4169921875 
[2025-03-22 17:10:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 6 loss:0.0132022425532341 norm:9.802037675399333e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:12:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 7 loss:0.013077803887426853 norm:9.254494943888858e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:13:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 8 loss:0.012975803576409817 norm:8.801475632935762e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:14:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 9 loss:0.012901943176984787 norm:8.384029933949932e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:16:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 10 loss:0.012810164131224155 norm:7.230837945826352e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:17:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 11 loss:0.012749806046485901 norm:6.884337199153379e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:19:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 12 loss:0.012710020877420902 norm:6.596824823645875e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:20:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 13 loss:0.012658830732107162 norm:6.334349018288776e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:22:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 14 loss:0.012616290710866451 norm:5.97188227402512e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:23:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 15 loss:0.01259578112512827 norm:5.784921086160466e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:25:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 16 loss:0.012569937855005264 norm:5.3545005357591435e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:26:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 17 loss:0.012555974535644054 norm:5.481696280185133e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:27:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 18 loss:0.01253420952707529 norm:5.2945422794437036e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:29:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 19 loss:0.012521983124315739 norm:5.2746654546353966e-05 max memory_allocated 47469.4169921875 
[2025-03-22 17:31:13 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [16, 17, 18]
[2025-03-22 17:31:14 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [19, 20, 21] ===
[2025-03-22 17:32:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 0 loss:0.028844129294157028 norm:0.0004625073925126344 max memory_allocated 47469.6044921875 
[2025-03-22 17:34:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 1 loss:0.026727555319666862 norm:0.00027417141245678067 max memory_allocated 47469.6044921875 
[2025-03-22 17:35:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 2 loss:0.024842245504260063 norm:0.00022205477580428123 max memory_allocated 47469.6044921875 
[2025-03-22 17:37:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 3 loss:0.0228281207382679 norm:0.00018534596893005073 max memory_allocated 47469.6044921875 
[2025-03-22 17:38:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 4 loss:0.022002730518579483 norm:0.00016472733113914728 max memory_allocated 47469.6044921875 
[2025-03-22 17:40:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 5 loss:0.021701142191886902 norm:0.00014781628851778805 max memory_allocated 47469.6044921875 
[2025-03-22 17:41:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 6 loss:0.02150564268231392 norm:0.00013727103942073882 max memory_allocated 47469.6044921875 
[2025-03-22 17:42:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 7 loss:0.021346839144825935 norm:0.00012748813605867326 max memory_allocated 47469.6044921875 
[2025-03-22 17:44:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 8 loss:0.02121405489742756 norm:0.00011643169273156673 max memory_allocated 47469.6044921875 
[2025-03-22 17:45:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 9 loss:0.021115481853485107 norm:0.00011223691399209201 max memory_allocated 47469.6044921875 
[2025-03-22 17:47:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 10 loss:0.02101624570786953 norm:0.00010860423208214343 max memory_allocated 47469.6044921875 
[2025-03-22 17:48:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 11 loss:0.02093898318707943 norm:9.706576383905485e-05 max memory_allocated 47469.6044921875 
[2025-03-22 17:50:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 12 loss:0.020881786942481995 norm:9.762409899849445e-05 max memory_allocated 47469.6044921875 
[2025-03-22 17:51:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 13 loss:0.020838920027017593 norm:9.540955943521112e-05 max memory_allocated 47469.6044921875 
[2025-03-22 17:53:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 14 loss:0.020807987079024315 norm:9.534494893159717e-05 max memory_allocated 47469.6044921875 
[2025-03-22 17:54:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 15 loss:0.020774008706212044 norm:9.084930206881836e-05 max memory_allocated 47469.6044921875 
[2025-03-22 17:55:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 16 loss:0.02073870599269867 norm:7.944313256302848e-05 max memory_allocated 47469.6044921875 
[2025-03-22 17:57:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 17 loss:0.020706743001937866 norm:7.463879592251033e-05 max memory_allocated 47469.6044921875 
[2025-03-22 17:58:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 18 loss:0.020683806389570236 norm:6.944709457457066e-05 max memory_allocated 47469.6044921875 
[2025-03-22 18:00:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 19 loss:0.02066546119749546 norm:6.847841723356396e-05 max memory_allocated 47469.6044921875 
[2025-03-22 18:02:07 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [19, 20, 21]
[2025-03-22 18:02:07 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [22, 23, 24] ===
[2025-03-22 18:03:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 0 loss:0.043282400816679 norm:0.00040016885031946003 max memory_allocated 47469.7919921875 
[2025-03-22 18:05:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 1 loss:0.040800027549266815 norm:0.000308207148918882 max memory_allocated 47469.7919921875 
[2025-03-22 18:06:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 2 loss:0.037713099271059036 norm:0.000272176053840667 max memory_allocated 47469.7919921875 
[2025-03-22 18:08:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 3 loss:0.03528411686420441 norm:0.00023943705309648067 max memory_allocated 47469.7919921875 
[2025-03-22 18:09:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 4 loss:0.03475974500179291 norm:0.00021725377882830799 max memory_allocated 47469.7919921875 
[2025-03-22 18:10:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 5 loss:0.03451187163591385 norm:0.00020299031166359782 max memory_allocated 47469.7919921875 
[2025-03-22 18:12:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 6 loss:0.034299030900001526 norm:0.00017748064419720322 max memory_allocated 47469.7919921875 
[2025-03-22 18:13:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 7 loss:0.03415059670805931 norm:0.0001646188902668655 max memory_allocated 47469.7919921875 
[2025-03-22 18:15:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 8 loss:0.03400678560137749 norm:0.00016060343477874994 max memory_allocated 47469.7919921875 
[2025-03-22 18:16:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 9 loss:0.033897195011377335 norm:0.00015523804177064449 max memory_allocated 47469.7919921875 
[2025-03-22 18:18:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 10 loss:0.033820655196905136 norm:0.00014867990103084594 max memory_allocated 47469.7919921875 
[2025-03-22 18:19:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 11 loss:0.033733561635017395 norm:0.00013748339551966637 max memory_allocated 47469.7919921875 
[2025-03-22 18:21:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 12 loss:0.03363987058401108 norm:0.00012835863162763417 max memory_allocated 47469.7919921875 
[2025-03-22 18:22:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 13 loss:0.03355932608246803 norm:0.00012331476318649948 max memory_allocated 47469.7919921875 
[2025-03-22 18:23:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 14 loss:0.03353479504585266 norm:0.00011017604992957786 max memory_allocated 47469.7919921875 
[2025-03-22 18:25:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 15 loss:0.033505819737911224 norm:0.00011572407674975693 max memory_allocated 47469.7919921875 
[2025-03-22 18:26:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 16 loss:0.033480193465948105 norm:0.00012578417954500765 max memory_allocated 47469.7919921875 
[2025-03-22 18:28:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 17 loss:0.033456508070230484 norm:0.00011876350617967546 max memory_allocated 47469.7919921875 
[2025-03-22 18:29:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 18 loss:0.03342677280306816 norm:0.0001186724693980068 max memory_allocated 47469.7919921875 
[2025-03-22 18:31:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 19 loss:0.03339556232094765 norm:0.00010906470561167225 max memory_allocated 47469.7919921875 
[2025-03-22 18:33:01 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [22, 23, 24]
[2025-03-22 18:33:01 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [25, 26] ===
[2025-03-22 18:34:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 0 loss:0.050105560570955276 norm:0.0003283121914137155 max memory_allocated 47469.7919921875 
[2025-03-22 18:35:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 1 loss:0.04815535247325897 norm:0.00021851177734788507 max memory_allocated 47469.7919921875 
[2025-03-22 18:35:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 2 loss:0.04529527574777603 norm:0.0001805560168577358 max memory_allocated 47469.7919921875 
[2025-03-22 18:36:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 3 loss:0.04376444220542908 norm:0.00014635575644206256 max memory_allocated 47469.7919921875 
[2025-03-22 18:37:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 4 loss:0.043542906641960144 norm:0.0001357939327135682 max memory_allocated 47469.7919921875 
[2025-03-22 18:38:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 5 loss:0.043392032384872437 norm:0.00012941405293531716 max memory_allocated 47469.7919921875 
[2025-03-22 18:39:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 6 loss:0.04328782111406326 norm:0.00012337943189777434 max memory_allocated 47469.7919921875 
[2025-03-22 18:40:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 7 loss:0.04320654273033142 norm:0.00011572705261642113 max memory_allocated 47469.7919921875 
[2025-03-22 18:41:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 8 loss:0.04311269894242287 norm:0.00010887530515901744 max memory_allocated 47469.7919921875 
[2025-03-22 18:42:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 9 loss:0.043079979717731476 norm:0.00011179505963809788 max memory_allocated 47469.7919921875 
[2025-03-22 18:43:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 10 loss:0.04303230717778206 norm:0.00011307549721095711 max memory_allocated 47469.7919921875 
[2025-03-22 18:44:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 11 loss:0.042986705899238586 norm:0.0001058236084645614 max memory_allocated 47469.7919921875 
[2025-03-22 18:45:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 12 loss:0.0429406613111496 norm:0.000101227626146283 max memory_allocated 47469.7919921875 
[2025-03-22 18:46:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 13 loss:0.042898230254650116 norm:9.999889152823016e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:47:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 14 loss:0.04287455976009369 norm:0.0001010227351798676 max memory_allocated 47469.7919921875 
[2025-03-22 18:48:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 15 loss:0.04283767193555832 norm:9.999873145716265e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:49:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 16 loss:0.0428096204996109 norm:9.429872443433851e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:50:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 17 loss:0.04279469698667526 norm:9.388228500029072e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:51:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 18 loss:0.042774051427841187 norm:9.231105650542304e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:52:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 19 loss:0.0427699014544487 norm:9.331267210654914e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:53:38 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [25, 26]
[2025-03-22 18:53:38 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [27] ===
[2025-03-22 18:54:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 0 loss:0.050454653799533844 norm:0.00024090676743071526 max memory_allocated 47469.7919921875 
[2025-03-22 18:54:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 1 loss:0.04930940270423889 norm:0.00015565029752906412 max memory_allocated 47469.7919921875 
[2025-03-22 18:55:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 2 loss:0.047656215727329254 norm:0.00011568801710382104 max memory_allocated 47469.7919921875 
[2025-03-22 18:55:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 3 loss:0.047026026993989944 norm:9.690735896583647e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:56:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 4 loss:0.04693903401494026 norm:8.945252920966595e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:56:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 5 loss:0.046879805624485016 norm:8.069109753705561e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:57:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 6 loss:0.04683270677924156 norm:7.847540109651163e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:57:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 7 loss:0.046784184873104095 norm:7.505844405386597e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:58:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 8 loss:0.046754706650972366 norm:7.31512118363753e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:58:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 9 loss:0.046719785779714584 norm:7.07819199305959e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:59:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 10 loss:0.04669816419482231 norm:7.169003947637975e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:59:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 11 loss:0.046675894409418106 norm:6.368196045514196e-05 max memory_allocated 47469.7919921875 
[2025-03-22 18:59:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 12 loss:0.0466596782207489 norm:5.51402335986495e-05 max memory_allocated 47469.7919921875 
[2025-03-22 19:00:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 13 loss:0.04664083942770958 norm:5.506628076545894e-05 max memory_allocated 47469.7919921875 
[2025-03-22 19:00:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 14 loss:0.04664214700460434 norm:6.331835174933076e-05 max memory_allocated 47469.7919921875 
[2025-03-22 19:01:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 15 loss:0.04662271961569786 norm:6.044252950232476e-05 max memory_allocated 47469.7919921875 
[2025-03-22 19:01:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 16 loss:0.04660149663686752 norm:5.689022509614006e-05 max memory_allocated 47469.7919921875 
[2025-03-22 19:02:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 17 loss:0.04659446328878403 norm:5.2808551117777824e-05 max memory_allocated 47469.7919921875 
[2025-03-22 19:02:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 18 loss:0.04658377170562744 norm:4.978116339771077e-05 max memory_allocated 47469.7919921875 
[2025-03-22 19:03:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 19 loss:0.046562545001506805 norm:5.589191277977079e-05 max memory_allocated 47469.7919921875 
[2025-03-22 19:03:58 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [27]
[2025-03-22 19:03:58 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [28] ===
[2025-03-22 19:03:58 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:04:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 0 loss:0.06013850122690201 norm:0.0023190146312117577 max memory_allocated 47469.7919921875 
[2025-03-22 19:04:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 1 loss:0.05838102102279663 norm:0.001962551148608327 max memory_allocated 47469.7919921875 
[2025-03-22 19:05:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 2 loss:0.05622973293066025 norm:0.001462412066757679 max memory_allocated 47469.7919921875 
[2025-03-22 19:05:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 3 loss:0.05545008182525635 norm:0.0012041942682117224 max memory_allocated 47469.7919921875 
[2025-03-22 19:06:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 4 loss:0.055268414318561554 norm:0.001086503965780139 max memory_allocated 47469.7919921875 
[2025-03-22 19:06:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 5 loss:0.05512930825352669 norm:0.000913129944819957 max memory_allocated 47469.7919921875 
[2025-03-22 19:07:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 6 loss:0.05503151938319206 norm:0.0008807508274912834 max memory_allocated 47469.7919921875 
[2025-03-22 19:07:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 7 loss:0.05506860464811325 norm:0.0009291199967265129 max memory_allocated 47469.7919921875 
[2025-03-22 19:08:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 8 loss:0.055189669132232666 norm:0.0011808460112661123 max memory_allocated 47469.7919921875 
[2025-03-22 19:08:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 9 loss:0.05496295541524887 norm:0.0009829637128859758 max memory_allocated 47469.7919921875 
[2025-03-22 19:09:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 10 loss:0.05482301861047745 norm:0.0007287444896064699 max memory_allocated 47469.7919921875 
[2025-03-22 19:09:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 11 loss:0.05470816791057587 norm:0.0006262545939534903 max memory_allocated 47469.7919921875 
[2025-03-22 19:10:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 12 loss:0.05464504286646843 norm:0.0005694621941074729 max memory_allocated 47469.7919921875 
[2025-03-22 19:10:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 13 loss:0.05461038649082184 norm:0.0005251716356724501 max memory_allocated 47469.7919921875 
[2025-03-22 19:11:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 14 loss:0.05457310751080513 norm:0.00049216253682971 max memory_allocated 47469.7919921875 
[2025-03-22 19:11:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 15 loss:0.05455760285258293 norm:0.00046574955922551453 max memory_allocated 47469.7919921875 
[2025-03-22 19:12:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 16 loss:0.054541923105716705 norm:0.0004486408724915236 max memory_allocated 47469.7919921875 
[2025-03-22 19:12:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 17 loss:0.05454191192984581 norm:0.00044696556869894266 max memory_allocated 47469.7919921875 
[2025-03-22 19:13:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 18 loss:0.0545329712331295 norm:0.0004773777036461979 max memory_allocated 47469.7919921875 
[2025-03-22 19:13:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 19 loss:0.05452796816825867 norm:0.0004793562402483076 max memory_allocated 47469.7919921875 
[2025-03-22 19:14:22 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [28]
[2025-03-22 19:14:22 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [29] ===
[2025-03-22 19:14:22 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:14:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 0 loss:0.0697304829955101 norm:0.0023305381182581186 max memory_allocated 47469.7919921875 
[2025-03-22 19:15:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 1 loss:0.06792844086885452 norm:0.0017072353512048721 max memory_allocated 47469.7919921875 
[2025-03-22 19:15:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 2 loss:0.06569978594779968 norm:0.0013875621370971203 max memory_allocated 47469.7919921875 
[2025-03-22 19:16:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 3 loss:0.06506045907735825 norm:0.0012582215713337064 max memory_allocated 47469.7919921875 
[2025-03-22 19:16:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 4 loss:0.06491395831108093 norm:0.0011041616089642048 max memory_allocated 47469.7919921875 
[2025-03-22 19:17:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 5 loss:0.06482420116662979 norm:0.0010310932993888855 max memory_allocated 47469.7919921875 
[2025-03-22 19:17:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 6 loss:0.06473641097545624 norm:0.000891850155312568 max memory_allocated 47469.7919921875 
[2025-03-22 19:18:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 7 loss:0.06473202258348465 norm:0.0009151533013209701 max memory_allocated 47469.7919921875 
[2025-03-22 19:18:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 8 loss:0.06469571590423584 norm:0.0009354199282824993 max memory_allocated 47469.7919921875 
[2025-03-22 19:19:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 9 loss:0.06464332342147827 norm:0.0007457485189661384 max memory_allocated 47469.7919921875 
[2025-03-22 19:19:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 10 loss:0.06454133987426758 norm:0.0006870785728096962 max memory_allocated 47469.7919921875 
[2025-03-22 19:20:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 11 loss:0.06444293260574341 norm:0.0006203040247783065 max memory_allocated 47469.7919921875 
[2025-03-22 19:20:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 12 loss:0.06443247199058533 norm:0.000586431473493576 max memory_allocated 47469.7919921875 
[2025-03-22 19:21:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 13 loss:0.06441701203584671 norm:0.0005985705647617579 max memory_allocated 47469.7919921875 
[2025-03-22 19:21:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 14 loss:0.06446409970521927 norm:0.0006238146452233195 max memory_allocated 47469.7919921875 
[2025-03-22 19:22:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 15 loss:0.06459172070026398 norm:0.0008364560781046748 max memory_allocated 47469.7919921875 
[2025-03-22 19:22:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 16 loss:0.06480176746845245 norm:0.00118924374692142 max memory_allocated 47469.7919921875 
[2025-03-22 19:23:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 17 loss:0.06500055640935898 norm:0.0016191471368074417 max memory_allocated 47469.7919921875 
[2025-03-22 19:23:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 18 loss:0.0647750273346901 norm:0.0012725014239549637 max memory_allocated 47469.7919921875 
[2025-03-22 19:24:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 19 loss:0.06487154215574265 norm:0.001428959658369422 max memory_allocated 47469.7919921875 
[2025-03-22 19:24:46 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [29]
[2025-03-22 19:24:46 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [30] ===
[2025-03-22 19:24:46 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:25:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 0 loss:0.09005947411060333 norm:0.006388491950929165 max memory_allocated 47469.7919921875 
[2025-03-22 19:25:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 1 loss:0.08635954558849335 norm:0.005914753302931786 max memory_allocated 47469.7919921875 
[2025-03-22 19:26:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 2 loss:0.0832379087805748 norm:0.005389271769672632 max memory_allocated 47469.7919921875 
[2025-03-22 19:26:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 3 loss:0.08228198438882828 norm:0.004894820041954517 max memory_allocated 47469.7919921875 
[2025-03-22 19:27:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 4 loss:0.08196070790290833 norm:0.004685928113758564 max memory_allocated 47469.7919921875 
[2025-03-22 19:27:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 5 loss:0.08175057172775269 norm:0.00455472944304347 max memory_allocated 47469.7919921875 
[2025-03-22 19:28:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 6 loss:0.08157368749380112 norm:0.004127084743231535 max memory_allocated 47469.7919921875 
[2025-03-22 19:28:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 7 loss:0.081414595246315 norm:0.003867201739922166 max memory_allocated 47469.7919921875 
[2025-03-22 19:29:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 8 loss:0.08125894516706467 norm:0.0037266819272190332 max memory_allocated 47469.7919921875 
[2025-03-22 19:29:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 9 loss:0.08112799376249313 norm:0.0031469459645450115 max memory_allocated 47469.7919921875 
[2025-03-22 19:30:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 10 loss:0.08101396262645721 norm:0.0028171008452773094 max memory_allocated 47469.7919921875 
[2025-03-22 19:30:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 11 loss:0.08093301951885223 norm:0.0025038185995072126 max memory_allocated 47469.7919921875 
[2025-03-22 19:31:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 12 loss:0.08087573945522308 norm:0.0026068224105983973 max memory_allocated 47469.7919921875 
[2025-03-22 19:31:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 13 loss:0.08078967034816742 norm:0.00221699895337224 max memory_allocated 47469.7919921875 
[2025-03-22 19:32:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 14 loss:0.08071427792310715 norm:0.0019088463159278035 max memory_allocated 47469.7919921875 
[2025-03-22 19:32:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 15 loss:0.0806841105222702 norm:0.0019103226950392127 max memory_allocated 47469.7919921875 
[2025-03-22 19:33:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 16 loss:0.08063669502735138 norm:0.001712699537165463 max memory_allocated 47469.7919921875 
[2025-03-22 19:33:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 17 loss:0.08058357983827591 norm:0.0016822764882817864 max memory_allocated 47469.7919921875 
[2025-03-22 19:34:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 18 loss:0.08056559413671494 norm:0.0015210388228297234 max memory_allocated 47469.7919921875 
[2025-03-22 19:34:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 19 loss:0.08057713508605957 norm:0.0015810964396223426 max memory_allocated 47469.7919921875 
[2025-03-22 19:35:09 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [30]
[2025-03-22 19:35:09 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [31] ===
[2025-03-22 19:35:09 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-22 19:35:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 0 loss:0.1412624716758728 norm:0.012293556705117226 max memory_allocated 47469.7919921875 
[2025-03-22 19:36:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 1 loss:0.13409267365932465 norm:0.008657366037368774 max memory_allocated 47469.7919921875 
[2025-03-22 19:36:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 2 loss:0.12940746545791626 norm:0.0059150937013328075 max memory_allocated 47469.7919921875 
[2025-03-22 19:37:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 3 loss:0.12790752947330475 norm:0.0052205659449100494 max memory_allocated 47469.7919921875 
[2025-03-22 19:37:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 4 loss:0.1271955966949463 norm:0.004767227452248335 max memory_allocated 47469.7919921875 
[2025-03-22 19:38:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 5 loss:0.12661689519882202 norm:0.0043509346432983875 max memory_allocated 47469.7919921875 
[2025-03-22 19:38:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 6 loss:0.1263786107301712 norm:0.00406977254897356 max memory_allocated 47469.7919921875 
[2025-03-22 19:39:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 7 loss:0.1260877549648285 norm:0.003838327480480075 max memory_allocated 47469.7919921875 
[2025-03-22 19:39:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 8 loss:0.12584532797336578 norm:0.003991924691945314 max memory_allocated 47469.7919921875 
[2025-03-22 19:40:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 9 loss:0.12561142444610596 norm:0.003692066762596369 max memory_allocated 47469.7919921875 
[2025-03-22 19:40:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 10 loss:0.1255437433719635 norm:0.0036139292642474174 max memory_allocated 47469.7919921875 
[2025-03-22 19:41:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 11 loss:0.12549686431884766 norm:0.0034630303271114826 max memory_allocated 47469.7919921875 
[2025-03-22 19:41:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 12 loss:0.1254226267337799 norm:0.003708058502525091 max memory_allocated 47469.7919921875 
[2025-03-22 19:42:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 13 loss:0.12523584067821503 norm:0.0034031125251203775 max memory_allocated 47469.7919921875 
[2025-03-22 19:42:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 14 loss:0.12514419853687286 norm:0.0034322855062782764 max memory_allocated 47469.7919921875 
[2025-03-22 19:43:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 15 loss:0.12501265108585358 norm:0.003120019566267729 max memory_allocated 47469.7919921875 
[2025-03-22 19:43:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 16 loss:0.12493254989385605 norm:0.0032369624823331833 max memory_allocated 47469.7919921875 
[2025-03-22 19:43:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 17 loss:0.1249038577079773 norm:0.0030012240167707205 max memory_allocated 47469.7919921875 
[2025-03-22 19:44:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 18 loss:0.12479757517576218 norm:0.0031545062083750963 max memory_allocated 47469.7919921875 
[2025-03-22 19:44:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 19 loss:0.12476058304309845 norm:0.002874801168218255 max memory_allocated 47469.7919921875 
[2025-03-22 19:45:37 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 15, block: [31]
[2025-03-22 19:45:37 root] (main_calib_config3_attn.py 379): INFO 19832.00948381424
[2025-03-22 19:45:44 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-22 19:46:35 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.711062431335449
[2025-03-22 19:46:35 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-22 19:47:55 root] (main_calib_config3_attn.py 161): INFO c4 : 7.118311882019043
[2025-03-22 20:48:33 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.711062431335449, 'c4': 7.118311882019043, 'results': {'boolq': {'acc': 0.7330275229357798, 'acc_stderr': 0.007737237462219758}, 'arc_easy': {'acc': 0.6750841750841751, 'acc_stderr': 0.009610203604504817, 'acc_norm': 0.5294612794612794, 'acc_norm_stderr': 0.01024195772840968}, 'piqa': {'acc': 0.7834602829162133, 'acc_stderr': 0.009609984714384609, 'acc_norm': 0.7747551686615887, 'acc_norm_stderr': 0.00974664347103214}, 'winogrande': {'acc': 0.6708760852407262, 'acc_stderr': 0.013206387089091451}, 'hellaswag': {'acc': 0.5615415255925115, 'acc_stderr': 0.004951840978219685, 'acc_norm': 0.7266480780720972, 'acc_norm_stderr': 0.004447691405592724}, 'arc_challenge': {'acc': 0.3822525597269625, 'acc_stderr': 0.014200454049979293, 'acc_norm': 0.4112627986348123, 'acc_norm_stderr': 0.014379441068522084}}, 'versions': {'boolq': 1, 'arc_easy': 0, 'piqa': 0, 'winogrande': 0, 'hellaswag': 0, 'arc_challenge': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 20:48:33 root] (main_calib_config3_attn.py 175): INFO 38.23,67.51,73.30,56.15,78.35,67.09
