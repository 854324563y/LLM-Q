[2025-03-20 15:01:21 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-attnloss/llama-7b-hf-0.65', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.65.pkl', blocks_pkl='./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-20 15:01:30 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-20 15:01:30 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-20 15:01:30 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-20 15:01:30 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.65.pkl
[2025-03-20 15:01:30 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 5), (5, 8), (8, 11), (11, 14), (14, 16), (16, 19), (19, 22), (22, 25), (25, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-20 15:01:30 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2, 3, 4], [5, 6, 7], [8, 9, 10], [11, 12, 13], [14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26], [27], [28], [29], [30], [31]]
[2025-03-20 15:01:31 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-20 15:01:31 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 15:02:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.006873763166368008 norm:0.006145629100501537 max memory_allocated 34630.880859375 
[2025-03-20 15:02:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.003933205269277096 norm:0.0037376550026237965 max memory_allocated 34630.880859375 
[2025-03-20 15:02:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.0025963785592466593 norm:0.002648614812642336 max memory_allocated 34630.880859375 
[2025-03-20 15:03:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0022055809386074543 norm:0.0020545453298836946 max memory_allocated 34630.880859375 
[2025-03-20 15:03:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.002089950954541564 norm:0.0017110781045630574 max memory_allocated 34630.880859375 
[2025-03-20 15:04:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0019882209599018097 norm:0.001422035857103765 max memory_allocated 34630.880859375 
[2025-03-20 15:04:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.0019340801518410444 norm:0.0012297064531594515 max memory_allocated 34630.880859375 
[2025-03-20 15:05:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0018685025861486793 norm:0.0010517154587432742 max memory_allocated 34630.880859375 
[2025-03-20 15:05:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0018442395376041532 norm:0.0009196551982313395 max memory_allocated 34630.880859375 
[2025-03-20 15:06:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.0018169463146477938 norm:0.0008159966091625392 max memory_allocated 34630.880859375 
[2025-03-20 15:06:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.001791484304703772 norm:0.0007251707720570266 max memory_allocated 34630.880859375 
[2025-03-20 15:07:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0017680361634120345 norm:0.0006471154047176242 max memory_allocated 34630.880859375 
[2025-03-20 15:07:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.001750590861774981 norm:0.0005884356796741486 max memory_allocated 34630.880859375 
[2025-03-20 15:07:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.0017436078051105142 norm:0.0005268817767500877 max memory_allocated 34630.880859375 
[2025-03-20 15:08:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0017455387860536575 norm:0.0004966718843206763 max memory_allocated 34630.880859375 
[2025-03-20 15:08:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.0017413734458386898 norm:0.0004705683677457273 max memory_allocated 34630.880859375 
[2025-03-20 15:09:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.001724672270938754 norm:0.00042996008414775133 max memory_allocated 34630.880859375 
[2025-03-20 15:09:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0017052170587703586 norm:0.0004079570062458515 max memory_allocated 34630.880859375 
[2025-03-20 15:10:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.0017623613821342587 norm:0.00042843958362936974 max memory_allocated 34630.880859375 
[2025-03-20 15:10:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0017519978573545814 norm:0.0004153873596806079 max memory_allocated 34630.880859375 
[2025-03-20 15:11:12 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-20 15:11:12 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-20 15:11:12 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 15:11:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.01695907488465309 norm:0.016216332092881203 max memory_allocated 35097.7724609375 
[2025-03-20 15:12:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.009145017713308334 norm:0.009053435176610947 max memory_allocated 35097.7724609375 
[2025-03-20 15:12:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.006138649769127369 norm:0.005321180447936058 max memory_allocated 35097.7724609375 
[2025-03-20 15:13:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.005537255201488733 norm:0.004237963818013668 max memory_allocated 35097.7724609375 
[2025-03-20 15:13:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.0052359383553266525 norm:0.0037401034496724606 max memory_allocated 35097.7724609375 
[2025-03-20 15:13:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.005049663595855236 norm:0.0033966104965656996 max memory_allocated 35097.7724609375 
[2025-03-20 15:14:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.004912110045552254 norm:0.0031263662967830896 max memory_allocated 35097.7724609375 
[2025-03-20 15:14:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.004791375249624252 norm:0.0028781902510672808 max memory_allocated 35097.7724609375 
[2025-03-20 15:15:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.004680708982050419 norm:0.002591943833976984 max memory_allocated 35097.7724609375 
[2025-03-20 15:15:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.004630932584404945 norm:0.0024630222469568253 max memory_allocated 35097.7724609375 
[2025-03-20 15:16:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.00455454783514142 norm:0.002192434621974826 max memory_allocated 35097.7724609375 
[2025-03-20 15:16:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.004520921036601067 norm:0.001988035626709461 max memory_allocated 35097.7724609375 
[2025-03-20 15:17:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.004445616155862808 norm:0.001778273843228817 max memory_allocated 35097.7724609375 
[2025-03-20 15:17:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.004384808242321014 norm:0.0015941536985337734 max memory_allocated 35097.7724609375 
[2025-03-20 15:18:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.004362689796835184 norm:0.0014411136507987976 max memory_allocated 35097.7724609375 
[2025-03-20 15:18:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.004404276143759489 norm:0.0012702394742518663 max memory_allocated 35097.7724609375 
[2025-03-20 15:18:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.00434393435716629 norm:0.0011286032386124134 max memory_allocated 35097.7724609375 
[2025-03-20 15:19:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.00431118905544281 norm:0.0010155781637877226 max memory_allocated 35097.7724609375 
[2025-03-20 15:19:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.004303267225623131 norm:0.0009857056429609656 max memory_allocated 35097.7724609375 
[2025-03-20 15:20:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.004300419706851244 norm:0.0010193942580372095 max memory_allocated 35097.7724609375 
[2025-03-20 15:20:59 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-20 15:20:59 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2, 3, 4] ===
[2025-03-20 15:20:59 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 15:22:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 0 loss:0.0355076789855957 norm:0.0167478546500206 max memory_allocated 47468.5419921875 
[2025-03-20 15:23:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 1 loss:0.2176450490951538 norm:0.7113639116287231 max memory_allocated 47468.5419921875 
[2025-03-20 15:25:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 2 loss:0.057513169944286346 norm:0.03437558189034462 max memory_allocated 47468.5419921875 
[2025-03-20 15:26:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 3 loss:0.03151857107877731 norm:0.012950530275702477 max memory_allocated 47468.5419921875 
[2025-03-20 15:27:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 4 loss:0.025054961442947388 norm:0.006599050480872393 max memory_allocated 47468.5419921875 
[2025-03-20 15:29:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 5 loss:0.02246004343032837 norm:0.0042093489319086075 max memory_allocated 47468.5419921875 
[2025-03-20 15:30:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 6 loss:0.021126164123415947 norm:0.0032079729717224836 max memory_allocated 47468.5419921875 
[2025-03-20 15:31:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 7 loss:0.020282117649912834 norm:0.0027509010396897793 max memory_allocated 47468.5419921875 
[2025-03-20 15:33:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 8 loss:0.019696570932865143 norm:0.0026651190128177404 max memory_allocated 47468.5419921875 
[2025-03-20 15:34:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 9 loss:0.019249027594923973 norm:0.002930551068857312 max memory_allocated 47468.5419921875 
[2025-03-20 15:35:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 10 loss:0.01885904371738434 norm:0.003060202347114682 max memory_allocated 47468.5419921875 
[2025-03-20 15:37:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 11 loss:0.018528420478105545 norm:0.0033807570580393076 max memory_allocated 47468.5419921875 
[2025-03-20 15:38:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 12 loss:0.018153907731175423 norm:0.0034923749044537544 max memory_allocated 47468.5419921875 
[2025-03-20 15:39:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 13 loss:0.0178225040435791 norm:0.0037811999209225178 max memory_allocated 47468.5419921875 
[2025-03-20 15:41:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 14 loss:0.017545759677886963 norm:0.004054151009768248 max memory_allocated 47468.5419921875 
[2025-03-20 15:42:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 15 loss:0.017309293150901794 norm:0.004312749486416578 max memory_allocated 47468.5419921875 
[2025-03-20 15:43:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 16 loss:0.0171615332365036 norm:0.004619593266397715 max memory_allocated 47468.5419921875 
[2025-03-20 15:45:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 17 loss:0.0169682614505291 norm:0.005032402463257313 max memory_allocated 47468.5419921875 
[2025-03-20 15:46:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 18 loss:0.01676112599670887 norm:0.005399385932832956 max memory_allocated 47468.5419921875 
[2025-03-20 15:47:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2, 3, 4]) iter 19 loss:0.016597922891378403 norm:0.005732817109674215 max memory_allocated 47468.5419921875 
[2025-03-20 15:49:39 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2, 3, 4]
[2025-03-20 15:49:39 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [5, 6, 7] ===
[2025-03-20 15:51:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 0 loss:0.04709139093756676 norm:0.0006649773567914963 max memory_allocated 47468.7294921875 
[2025-03-20 15:52:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 1 loss:0.03538453206419945 norm:0.0003561598714441061 max memory_allocated 47468.7294921875 
[2025-03-20 15:53:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 2 loss:0.027679020538926125 norm:0.00023160481941886246 max memory_allocated 47468.7294921875 
[2025-03-20 15:55:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 3 loss:0.025081001222133636 norm:0.0001891469000838697 max memory_allocated 47468.7294921875 
[2025-03-20 15:56:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 4 loss:0.023558642715215683 norm:0.0001751926465658471 max memory_allocated 47468.7294921875 
[2025-03-20 15:57:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 5 loss:0.02251501753926277 norm:0.0001735231780912727 max memory_allocated 47468.7294921875 
[2025-03-20 15:59:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 6 loss:0.02190171740949154 norm:0.00017299341561738402 max memory_allocated 47468.7294921875 
[2025-03-20 16:00:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 7 loss:0.0215646643191576 norm:0.00016477200551889837 max memory_allocated 47468.7294921875 
[2025-03-20 16:01:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 8 loss:0.021366864442825317 norm:0.00017802858201321214 max memory_allocated 47468.7294921875 
[2025-03-20 16:03:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 9 loss:0.021254366263747215 norm:0.00016888853861019015 max memory_allocated 47468.7294921875 
[2025-03-20 16:04:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 10 loss:0.021161925047636032 norm:0.0001586462021805346 max memory_allocated 47468.7294921875 
[2025-03-20 16:05:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 11 loss:0.021078672260046005 norm:0.00016379982116632164 max memory_allocated 47468.7294921875 
[2025-03-20 16:07:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 12 loss:0.021012749522924423 norm:0.00017329334514215589 max memory_allocated 47468.7294921875 
[2025-03-20 16:08:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 13 loss:0.020941734313964844 norm:0.00016465189401060343 max memory_allocated 47468.7294921875 
[2025-03-20 16:09:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 14 loss:0.020894855260849 norm:0.00016337644774466753 max memory_allocated 47468.7294921875 
[2025-03-20 16:11:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 15 loss:0.02086072415113449 norm:0.00015995968715287745 max memory_allocated 47468.7294921875 
[2025-03-20 16:12:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 16 loss:0.02083713375031948 norm:0.00017063619452528656 max memory_allocated 47468.7294921875 
[2025-03-20 16:13:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 17 loss:0.020807918161153793 norm:0.00015670550055801868 max memory_allocated 47468.7294921875 
[2025-03-20 16:15:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 18 loss:0.020794205367565155 norm:0.00016661681002005935 max memory_allocated 47468.7294921875 
[2025-03-20 16:16:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [5, 6, 7]) iter 19 loss:0.02076440304517746 norm:0.0001621993724256754 max memory_allocated 47468.7294921875 
[2025-03-20 16:18:15 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [5, 6, 7]
[2025-03-20 16:18:15 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [8, 9, 10] ===
[2025-03-20 16:19:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 0 loss:0.0533633716404438 norm:0.0006845706375315785 max memory_allocated 47468.9169921875 
[2025-03-20 16:21:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 1 loss:0.040687594562768936 norm:0.00037761643761768937 max memory_allocated 47468.9169921875 
[2025-03-20 16:22:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 2 loss:0.032454460859298706 norm:0.0002585039474070072 max memory_allocated 47468.9169921875 
[2025-03-20 16:23:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 3 loss:0.029764557257294655 norm:0.00020936380315106362 max memory_allocated 47468.9169921875 
[2025-03-20 16:25:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 4 loss:0.028222838416695595 norm:0.00019440765026956797 max memory_allocated 47468.9169921875 
[2025-03-20 16:26:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 5 loss:0.027102172374725342 norm:0.00017274540732614696 max memory_allocated 47468.9169921875 
[2025-03-20 16:27:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 6 loss:0.02640128880739212 norm:0.0001643127907300368 max memory_allocated 47468.9169921875 
[2025-03-20 16:29:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 7 loss:0.02596263401210308 norm:0.00016032287385314703 max memory_allocated 47468.9169921875 
[2025-03-20 16:30:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 8 loss:0.025683624669909477 norm:0.00015177545719780028 max memory_allocated 47468.9169921875 
[2025-03-20 16:31:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 9 loss:0.025444533675909042 norm:0.00014606390323024243 max memory_allocated 47468.9169921875 
[2025-03-20 16:33:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 10 loss:0.025359276682138443 norm:0.00016180875536520034 max memory_allocated 47468.9169921875 
[2025-03-20 16:34:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 11 loss:0.025189857929944992 norm:0.00014036869106348604 max memory_allocated 47468.9169921875 
[2025-03-20 16:35:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 12 loss:0.02507886476814747 norm:0.00014272029511630535 max memory_allocated 47468.9169921875 
[2025-03-20 16:37:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 13 loss:0.024994730949401855 norm:0.00013546731497626752 max memory_allocated 47468.9169921875 
[2025-03-20 16:38:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 14 loss:0.024944178760051727 norm:0.00012981859617866576 max memory_allocated 47468.9169921875 
[2025-03-20 16:39:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 15 loss:0.02490359917283058 norm:0.00013187280273996294 max memory_allocated 47468.9169921875 
[2025-03-20 16:41:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 16 loss:0.024844450876116753 norm:0.0001299325522268191 max memory_allocated 47468.9169921875 
[2025-03-20 16:42:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 17 loss:0.02481030486524105 norm:0.00013260803825687617 max memory_allocated 47468.9169921875 
[2025-03-20 16:43:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 18 loss:0.0247647687792778 norm:0.00013145661796443164 max memory_allocated 47468.9169921875 
[2025-03-20 16:45:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [8, 9, 10]) iter 19 loss:0.024710480123758316 norm:0.000129630250739865 max memory_allocated 47468.9169921875 
[2025-03-20 16:46:55 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [8, 9, 10]
[2025-03-20 16:46:55 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [11, 12, 13] ===
[2025-03-20 16:48:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 0 loss:0.050863467156887054 norm:0.0005031001637689769 max memory_allocated 47470.1044921875 
[2025-03-20 16:49:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 1 loss:0.04192766919732094 norm:0.0003065596101805568 max memory_allocated 47470.1044921875 
[2025-03-20 16:51:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 2 loss:0.03521285206079483 norm:0.00022728004842065275 max memory_allocated 47470.1044921875 
[2025-03-20 16:52:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 3 loss:0.03250383958220482 norm:0.00019589050498325378 max memory_allocated 47470.1044921875 
[2025-03-20 16:53:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 4 loss:0.03092224895954132 norm:0.00018930723308585584 max memory_allocated 47470.1044921875 
[2025-03-20 16:55:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 5 loss:0.029756836593151093 norm:0.00016871646221261472 max memory_allocated 47470.1044921875 
[2025-03-20 16:56:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 6 loss:0.029042229056358337 norm:0.00015943108883220702 max memory_allocated 47470.1044921875 
[2025-03-20 16:57:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 7 loss:0.028597990050911903 norm:0.00015004986198619008 max memory_allocated 47470.1044921875 
[2025-03-20 16:59:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 8 loss:0.02830389514565468 norm:0.00015045187319628894 max memory_allocated 47470.1044921875 
[2025-03-20 17:00:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 9 loss:0.02811715006828308 norm:0.0001407658855896443 max memory_allocated 47470.1044921875 
[2025-03-20 17:01:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 10 loss:0.027922803536057472 norm:0.0001316986745223403 max memory_allocated 47470.1044921875 
[2025-03-20 17:03:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 11 loss:0.02779628150165081 norm:0.00013076013419777155 max memory_allocated 47470.1044921875 
[2025-03-20 17:04:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 12 loss:0.02770422399044037 norm:0.00012520834570750594 max memory_allocated 47470.1044921875 
[2025-03-20 17:05:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 13 loss:0.027637725695967674 norm:0.000126146842376329 max memory_allocated 47470.1044921875 
[2025-03-20 17:07:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 14 loss:0.02755912020802498 norm:0.00012142684863647446 max memory_allocated 47470.1044921875 
[2025-03-20 17:08:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 15 loss:0.027501922100782394 norm:0.00012187509128125384 max memory_allocated 47470.1044921875 
[2025-03-20 17:09:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 16 loss:0.027467241510748863 norm:0.00012268002319615334 max memory_allocated 47470.1044921875 
[2025-03-20 17:11:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 17 loss:0.027428440749645233 norm:0.00012256907939445227 max memory_allocated 47470.1044921875 
[2025-03-20 17:12:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 18 loss:0.02740185707807541 norm:0.00012119794700993225 max memory_allocated 47470.1044921875 
[2025-03-20 17:13:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [11, 12, 13]) iter 19 loss:0.027369514107704163 norm:0.00011916934454347938 max memory_allocated 47470.1044921875 
[2025-03-20 17:15:32 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [11, 12, 13]
[2025-03-20 17:15:33 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [14, 15] ===
[2025-03-20 17:16:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 0 loss:0.05250123515725136 norm:0.0004892618162557483 max memory_allocated 47470.1044921875 
[2025-03-20 17:17:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 1 loss:0.04293166100978851 norm:0.00027124519692733884 max memory_allocated 47470.1044921875 
[2025-03-20 17:18:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 2 loss:0.03562679886817932 norm:0.0001737925922498107 max memory_allocated 47470.1044921875 
[2025-03-20 17:19:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 3 loss:0.03369367867708206 norm:0.00015189594705589116 max memory_allocated 47470.1044921875 
[2025-03-20 17:20:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 4 loss:0.032347407191991806 norm:0.0001323242613580078 max memory_allocated 47470.1044921875 
[2025-03-20 17:20:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 5 loss:0.03152143955230713 norm:0.00012392536154948175 max memory_allocated 47470.1044921875 
[2025-03-20 17:21:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 6 loss:0.031063701957464218 norm:0.00010904231749009341 max memory_allocated 47470.1044921875 
[2025-03-20 17:22:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 7 loss:0.030793335288763046 norm:0.00010663568536983803 max memory_allocated 47470.1044921875 
[2025-03-20 17:23:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 8 loss:0.0305510051548481 norm:9.65293002082035e-05 max memory_allocated 47470.1044921875 
[2025-03-20 17:24:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 9 loss:0.030389128252863884 norm:9.15517593966797e-05 max memory_allocated 47470.1044921875 
[2025-03-20 17:25:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 10 loss:0.030235065147280693 norm:8.875497587723657e-05 max memory_allocated 47470.1044921875 
[2025-03-20 17:26:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 11 loss:0.030103005468845367 norm:8.521647396264598e-05 max memory_allocated 47470.1044921875 
[2025-03-20 17:27:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 12 loss:0.030020657926797867 norm:7.96744498074986e-05 max memory_allocated 47470.1044921875 
[2025-03-20 17:28:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 13 loss:0.029936930164694786 norm:7.943301170598716e-05 max memory_allocated 47470.1044921875 
[2025-03-20 17:29:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 14 loss:0.029866833239793777 norm:7.731684308964759e-05 max memory_allocated 47470.1044921875 
[2025-03-20 17:29:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 15 loss:0.029813529923558235 norm:7.64181386330165e-05 max memory_allocated 47470.1044921875 
[2025-03-20 17:30:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 16 loss:0.029757872223854065 norm:7.603704580105841e-05 max memory_allocated 47470.1044921875 
[2025-03-20 17:31:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 17 loss:0.029718611389398575 norm:7.221313717309386e-05 max memory_allocated 47470.1044921875 
[2025-03-20 17:32:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 18 loss:0.029684055596590042 norm:7.13002445991151e-05 max memory_allocated 47470.1044921875 
[2025-03-20 17:33:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [14, 15]) iter 19 loss:0.029635872691869736 norm:7.24500569049269e-05 max memory_allocated 47470.1044921875 
[2025-03-20 17:34:36 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [14, 15]
[2025-03-20 17:34:36 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [16, 17, 18] ===
[2025-03-20 17:36:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 0 loss:0.06788094341754913 norm:0.0004890651907771826 max memory_allocated 47470.1044921875 
[2025-03-20 17:37:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 1 loss:0.05902238190174103 norm:0.00032035887124948204 max memory_allocated 47470.1044921875 
[2025-03-20 17:38:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 2 loss:0.0517655685544014 norm:0.0002345956163480878 max memory_allocated 47470.1044921875 
[2025-03-20 17:40:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 3 loss:0.047933198511600494 norm:0.0002086343156406656 max memory_allocated 47470.1044921875 
[2025-03-20 17:41:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 4 loss:0.04573018476366997 norm:0.00019230115867685527 max memory_allocated 47470.1044921875 
[2025-03-20 17:42:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 5 loss:0.04474315047264099 norm:0.0001855875743785873 max memory_allocated 47470.1044921875 
[2025-03-20 17:44:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 6 loss:0.04424396902322769 norm:0.00018049858044832945 max memory_allocated 47470.1044921875 
[2025-03-20 17:45:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 7 loss:0.043948400765657425 norm:0.0001711127843009308 max memory_allocated 47470.1044921875 
[2025-03-20 17:46:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 8 loss:0.04367409646511078 norm:0.00016371841775253415 max memory_allocated 47470.1044921875 
[2025-03-20 17:48:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 9 loss:0.04346610978245735 norm:0.00015897618141025305 max memory_allocated 47470.1044921875 
[2025-03-20 17:49:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 10 loss:0.043323975056409836 norm:0.00015993270790204406 max memory_allocated 47470.1044921875 
[2025-03-20 17:50:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 11 loss:0.043249938637018204 norm:0.00015730880841147155 max memory_allocated 47470.1044921875 
[2025-03-20 17:52:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 12 loss:0.04311278089880943 norm:0.00015701765369158238 max memory_allocated 47470.1044921875 
[2025-03-20 17:53:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 13 loss:0.04299723356962204 norm:0.00015517401334363967 max memory_allocated 47470.1044921875 
[2025-03-20 17:54:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 14 loss:0.04290705546736717 norm:0.0001521533413324505 max memory_allocated 47470.1044921875 
[2025-03-20 17:56:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 15 loss:0.04286400228738785 norm:0.00015210233686957508 max memory_allocated 47470.1044921875 
[2025-03-20 17:57:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 16 loss:0.04279295727610588 norm:0.00015681370859965682 max memory_allocated 47470.1044921875 
[2025-03-20 17:58:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 17 loss:0.042753979563713074 norm:0.00015458717825822532 max memory_allocated 47470.1044921875 
[2025-03-20 18:00:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 18 loss:0.04271936044096947 norm:0.00015719083603471518 max memory_allocated 47470.1044921875 
[2025-03-20 18:01:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [16, 17, 18]) iter 19 loss:0.042703863233327866 norm:0.00015791738405823708 max memory_allocated 47470.1044921875 
[2025-03-20 18:03:10 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [16, 17, 18]
[2025-03-20 18:03:10 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [19, 20, 21] ===
[2025-03-20 18:04:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 0 loss:0.10429246723651886 norm:0.0010569461155682802 max memory_allocated 47470.1044921875 
[2025-03-20 18:05:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 1 loss:0.09180373698472977 norm:0.0005711980629712343 max memory_allocated 47470.1044921875 
[2025-03-20 18:07:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 2 loss:0.08092325180768967 norm:0.00037821006844751537 max memory_allocated 47470.1044921875 
[2025-03-20 18:08:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 3 loss:0.07532227039337158 norm:0.0003435307589825243 max memory_allocated 47470.1044921875 
[2025-03-20 18:09:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 4 loss:0.07268403470516205 norm:0.00028842303436249495 max memory_allocated 47470.1044921875 
[2025-03-20 18:11:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 5 loss:0.0719093531370163 norm:0.0002799146459437907 max memory_allocated 47470.1044921875 
[2025-03-20 18:12:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 6 loss:0.07131975144147873 norm:0.0002466469304636121 max memory_allocated 47470.1044921875 
[2025-03-20 18:14:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 7 loss:0.07089641690254211 norm:0.00023607270850334316 max memory_allocated 47470.1044921875 
[2025-03-20 18:15:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 8 loss:0.0706007182598114 norm:0.0002186579949920997 max memory_allocated 47470.1044921875 
[2025-03-20 18:16:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 9 loss:0.07036104053258896 norm:0.0002147698833141476 max memory_allocated 47470.1044921875 
[2025-03-20 18:18:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 10 loss:0.07009755074977875 norm:0.0002142623416148126 max memory_allocated 47470.1044921875 
[2025-03-20 18:19:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 11 loss:0.06991104781627655 norm:0.00021889701019972563 max memory_allocated 47470.1044921875 
[2025-03-20 18:20:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 12 loss:0.06971841305494308 norm:0.0002076997043332085 max memory_allocated 47470.1044921875 
[2025-03-20 18:22:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 13 loss:0.06955762207508087 norm:0.00019763023010455072 max memory_allocated 47470.1044921875 
[2025-03-20 18:23:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 14 loss:0.06943055987358093 norm:0.00020098724053241313 max memory_allocated 47470.1044921875 
[2025-03-20 18:24:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 15 loss:0.06931285560131073 norm:0.00020097129163332283 max memory_allocated 47470.1044921875 
[2025-03-20 18:26:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 16 loss:0.0692182406783104 norm:0.00020600776770152152 max memory_allocated 47470.1044921875 
[2025-03-20 18:27:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 17 loss:0.06913439929485321 norm:0.0002011079341173172 max memory_allocated 47470.1044921875 
[2025-03-20 18:28:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 18 loss:0.0690920352935791 norm:0.0002076998061966151 max memory_allocated 47470.1044921875 
[2025-03-20 18:30:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [19, 20, 21]) iter 19 loss:0.06904655694961548 norm:0.00021616305457428098 max memory_allocated 47470.1044921875 
[2025-03-20 18:31:44 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [19, 20, 21]
[2025-03-20 18:31:44 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [22, 23, 24] ===
[2025-03-20 18:33:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 0 loss:0.152042955160141 norm:0.000762860057875514 max memory_allocated 47470.1044921875 
[2025-03-20 18:34:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 1 loss:0.13620591163635254 norm:0.0005221073515713215 max memory_allocated 47470.1044921875 
[2025-03-20 18:35:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 2 loss:0.12056031823158264 norm:0.0003876436094287783 max memory_allocated 47470.1044921875 
[2025-03-20 18:37:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 3 loss:0.11350126564502716 norm:0.00034958429750986397 max memory_allocated 47470.1044921875 
[2025-03-20 18:38:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 4 loss:0.11151231080293655 norm:0.00032804306829348207 max memory_allocated 47470.1044921875 
[2025-03-20 18:39:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 5 loss:0.1106787770986557 norm:0.00030215404694899917 max memory_allocated 47470.1044921875 
[2025-03-20 18:41:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 6 loss:0.11011403799057007 norm:0.0002936130913440138 max memory_allocated 47470.1044921875 
[2025-03-20 18:42:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 7 loss:0.10963466018438339 norm:0.0002841996029019356 max memory_allocated 47470.1044921875 
[2025-03-20 18:43:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 8 loss:0.10924579203128815 norm:0.0002702515630517155 max memory_allocated 47470.1044921875 
[2025-03-20 18:45:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 9 loss:0.10896812379360199 norm:0.0002660006866790354 max memory_allocated 47470.1044921875 
[2025-03-20 18:46:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 10 loss:0.10868914425373077 norm:0.0002653095871210098 max memory_allocated 47470.1044921875 
[2025-03-20 18:47:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 11 loss:0.10844629257917404 norm:0.00026521834661252797 max memory_allocated 47470.1044921875 
[2025-03-20 18:49:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 12 loss:0.10826089233160019 norm:0.00026660479488782585 max memory_allocated 47470.1044921875 
[2025-03-20 18:50:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 13 loss:0.10810869187116623 norm:0.00026129826437681913 max memory_allocated 47470.1044921875 
[2025-03-20 18:51:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 14 loss:0.10791508853435516 norm:0.0002547251060605049 max memory_allocated 47470.1044921875 
[2025-03-20 18:53:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 15 loss:0.1077750027179718 norm:0.00025224199634976685 max memory_allocated 47470.1044921875 
[2025-03-20 18:54:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 16 loss:0.10765686631202698 norm:0.0002511120110284537 max memory_allocated 47470.1044921875 
[2025-03-20 18:55:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 17 loss:0.1075427383184433 norm:0.00024790200404822826 max memory_allocated 47470.1044921875 
[2025-03-20 18:57:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 18 loss:0.10743564367294312 norm:0.0002572725643403828 max memory_allocated 47470.1044921875 
[2025-03-20 18:58:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [22, 23, 24]) iter 19 loss:0.10733900964260101 norm:0.0002512939099688083 max memory_allocated 47470.1044921875 
[2025-03-20 19:00:17 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [22, 23, 24]
[2025-03-20 19:00:18 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [25, 26] ===
[2025-03-20 19:01:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 0 loss:0.16042202711105347 norm:0.0021951058879494667 max memory_allocated 47470.1044921875 
[2025-03-20 19:02:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 1 loss:0.1518920660018921 norm:0.0008250618120655417 max memory_allocated 47470.1044921875 
[2025-03-20 19:03:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 2 loss:0.14211301505565643 norm:0.000347701832652092 max memory_allocated 47470.1044921875 
[2025-03-20 19:03:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 3 loss:0.13704067468643188 norm:0.0003139491018373519 max memory_allocated 47470.1044921875 
[2025-03-20 19:04:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 4 loss:0.1361028552055359 norm:0.00028657488292083144 max memory_allocated 47470.1044921875 
[2025-03-20 19:05:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 5 loss:0.13573148846626282 norm:0.0002588010102044791 max memory_allocated 47470.1044921875 
[2025-03-20 19:06:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 6 loss:0.13548505306243896 norm:0.0002318803162779659 max memory_allocated 47470.1044921875 
[2025-03-20 19:07:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 7 loss:0.13542607426643372 norm:0.0002305717789568007 max memory_allocated 47470.1044921875 
[2025-03-20 19:08:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 8 loss:0.13526102900505066 norm:0.00020718695304822177 max memory_allocated 47470.1044921875 
[2025-03-20 19:09:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 9 loss:0.13514336943626404 norm:0.0002135111571988091 max memory_allocated 47470.1044921875 
[2025-03-20 19:10:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 10 loss:0.13512609899044037 norm:0.00021852587815374136 max memory_allocated 47470.1044921875 
[2025-03-20 19:11:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 11 loss:0.13501301407814026 norm:0.00021169078536331654 max memory_allocated 47470.1044921875 
[2025-03-20 19:12:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 12 loss:0.1349562406539917 norm:0.00020625110482797027 max memory_allocated 47470.1044921875 
[2025-03-20 19:12:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 13 loss:0.13491253554821014 norm:0.00020320444309618324 max memory_allocated 47470.1044921875 
[2025-03-20 19:13:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 14 loss:0.13486851751804352 norm:0.00020519387908279896 max memory_allocated 47470.1044921875 
[2025-03-20 19:14:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 15 loss:0.13489976525306702 norm:0.0002102822036249563 max memory_allocated 47470.1044921875 
[2025-03-20 19:15:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 16 loss:0.13485294580459595 norm:0.00020656011474784464 max memory_allocated 47470.1044921875 
[2025-03-20 19:16:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 17 loss:0.13484175503253937 norm:0.00020684071932919323 max memory_allocated 47470.1044921875 
[2025-03-20 19:17:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 18 loss:0.134783536195755 norm:0.00020433760073501617 max memory_allocated 47470.1044921875 
[2025-03-20 19:18:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [25, 26]) iter 19 loss:0.13475868105888367 norm:0.0002058664249489084 max memory_allocated 47470.1044921875 
[2025-03-20 19:19:21 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [25, 26]
[2025-03-20 19:19:22 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [27] ===
[2025-03-20 19:19:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 0 loss:0.15622474253177643 norm:0.0004306243790779263 max memory_allocated 47470.1044921875 
[2025-03-20 19:20:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 1 loss:0.15274177491664886 norm:0.0003063940384890884 max memory_allocated 47470.1044921875 
[2025-03-20 19:20:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 2 loss:0.14851897954940796 norm:0.00024347423459403217 max memory_allocated 47470.1044921875 
[2025-03-20 19:21:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 3 loss:0.14629708230495453 norm:0.0002085175656247884 max memory_allocated 47470.1044921875 
[2025-03-20 19:21:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 4 loss:0.14599457383155823 norm:0.00020967988530173898 max memory_allocated 47470.1044921875 
[2025-03-20 19:22:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 5 loss:0.14585189521312714 norm:0.00017394831229466945 max memory_allocated 47470.1044921875 
[2025-03-20 19:22:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 6 loss:0.14571283757686615 norm:0.00015924144827295095 max memory_allocated 47470.1044921875 
[2025-03-20 19:23:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 7 loss:0.14566902816295624 norm:0.00015375568182207644 max memory_allocated 47470.1044921875 
[2025-03-20 19:23:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 8 loss:0.14560160040855408 norm:0.00014559975534211844 max memory_allocated 47470.1044921875 
[2025-03-20 19:23:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 9 loss:0.14554518461227417 norm:0.00014170924259815365 max memory_allocated 47470.1044921875 
[2025-03-20 19:24:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 10 loss:0.14547929167747498 norm:0.00014102717977948487 max memory_allocated 47470.1044921875 
[2025-03-20 19:24:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 11 loss:0.14545086026191711 norm:0.00014589718193747103 max memory_allocated 47470.1044921875 
[2025-03-20 19:25:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 12 loss:0.1454189121723175 norm:0.00013868907990399748 max memory_allocated 47470.1044921875 
[2025-03-20 19:25:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 13 loss:0.14538423717021942 norm:0.0001390559773426503 max memory_allocated 47470.1044921875 
[2025-03-20 19:26:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 14 loss:0.14535529911518097 norm:0.00013564361142925918 max memory_allocated 47470.1044921875 
[2025-03-20 19:26:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 15 loss:0.14534993469715118 norm:0.00013581602252088487 max memory_allocated 47470.1044921875 
[2025-03-20 19:27:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 16 loss:0.14532725512981415 norm:0.00012992462143301964 max memory_allocated 47470.1044921875 
[2025-03-20 19:27:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 17 loss:0.14531739056110382 norm:0.00014258346345741302 max memory_allocated 47470.1044921875 
[2025-03-20 19:27:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 18 loss:0.1453125774860382 norm:0.00013586730347014964 max memory_allocated 47470.1044921875 
[2025-03-20 19:28:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [27]) iter 19 loss:0.14530354738235474 norm:0.00014119534171186388 max memory_allocated 47470.1044921875 
[2025-03-20 19:28:57 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [27]
[2025-03-20 19:28:57 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [28] ===
[2025-03-20 19:28:58 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 19:29:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 0 loss:0.1863069236278534 norm:0.013612082228064537 max memory_allocated 47470.1044921875 
[2025-03-20 19:29:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 1 loss:0.17987899482250214 norm:0.010233473964035511 max memory_allocated 47470.1044921875 
[2025-03-20 19:30:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 2 loss:0.17363032698631287 norm:0.0070886495523154736 max memory_allocated 47470.1044921875 
[2025-03-20 19:30:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 3 loss:0.17068226635456085 norm:0.005777553655207157 max memory_allocated 47470.1044921875 
[2025-03-20 19:31:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 4 loss:0.17008665204048157 norm:0.0050740973092615604 max memory_allocated 47470.1044921875 
[2025-03-20 19:31:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 5 loss:0.16973882913589478 norm:0.004443370271474123 max memory_allocated 47470.1044921875 
[2025-03-20 19:32:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 6 loss:0.16948364675045013 norm:0.003816579934209585 max memory_allocated 47470.1044921875 
[2025-03-20 19:32:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 7 loss:0.16929936408996582 norm:0.0034134886227548122 max memory_allocated 47470.1044921875 
[2025-03-20 19:33:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 8 loss:0.16937220096588135 norm:0.0034313818905502558 max memory_allocated 47470.1044921875 
[2025-03-20 19:33:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 9 loss:0.1694101244211197 norm:0.0036352507304400206 max memory_allocated 47470.1044921875 
[2025-03-20 19:33:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 10 loss:0.1694975644350052 norm:0.0029071185272186995 max memory_allocated 47470.1044921875 
[2025-03-20 19:34:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 11 loss:0.1691584587097168 norm:0.00311410054564476 max memory_allocated 47470.1044921875 
[2025-03-20 19:34:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 12 loss:0.16907459497451782 norm:0.0028674895875155926 max memory_allocated 47470.1044921875 
[2025-03-20 19:35:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 13 loss:0.16906337440013885 norm:0.0028751050122082233 max memory_allocated 47470.1044921875 
[2025-03-20 19:35:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 14 loss:0.1689905822277069 norm:0.002766153309494257 max memory_allocated 47470.1044921875 
[2025-03-20 19:36:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 15 loss:0.16898022592067719 norm:0.0027154134586453438 max memory_allocated 47470.1044921875 
[2025-03-20 19:36:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 16 loss:0.16895228624343872 norm:0.0026112371124327183 max memory_allocated 47470.1044921875 
[2025-03-20 19:37:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 17 loss:0.1689494550228119 norm:0.00264930073171854 max memory_allocated 47470.1044921875 
[2025-03-20 19:37:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 18 loss:0.16894155740737915 norm:0.0025224226992577314 max memory_allocated 47470.1044921875 
[2025-03-20 19:38:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [28]) iter 19 loss:0.16896739602088928 norm:0.002632955089211464 max memory_allocated 47470.1044921875 
[2025-03-20 19:38:35 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [28]
[2025-03-20 19:38:35 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [29] ===
[2025-03-20 19:38:35 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 19:39:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 0 loss:0.21444371342658997 norm:0.015087687410414219 max memory_allocated 47470.1044921875 
[2025-03-20 19:39:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 1 loss:0.20780128240585327 norm:0.011168374679982662 max memory_allocated 47470.1044921875 
[2025-03-20 19:39:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 2 loss:0.20114465057849884 norm:0.007378101348876953 max memory_allocated 47470.1044921875 
[2025-03-20 19:40:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 3 loss:0.19833965599536896 norm:0.005955214612185955 max memory_allocated 47470.1044921875 
[2025-03-20 19:40:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 4 loss:0.1977773755788803 norm:0.0051689548417925835 max memory_allocated 47470.1044921875 
[2025-03-20 19:41:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 5 loss:0.197395920753479 norm:0.004465421196073294 max memory_allocated 47470.1044921875 
[2025-03-20 19:41:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 6 loss:0.1972399204969406 norm:0.003869543084874749 max memory_allocated 47470.1044921875 
[2025-03-20 19:42:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 7 loss:0.19708719849586487 norm:0.003589059691876173 max memory_allocated 47470.1044921875 
[2025-03-20 19:42:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 8 loss:0.19709841907024384 norm:0.003677074098959565 max memory_allocated 47470.1044921875 
[2025-03-20 19:43:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 9 loss:0.1969936490058899 norm:0.003551600966602564 max memory_allocated 47470.1044921875 
[2025-03-20 19:43:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 10 loss:0.19698959589004517 norm:0.0032777043525129557 max memory_allocated 47470.1044921875 
[2025-03-20 19:44:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 11 loss:0.19687272608280182 norm:0.003213983029127121 max memory_allocated 47470.1044921875 
[2025-03-20 19:44:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 12 loss:0.1967790424823761 norm:0.002898829523473978 max memory_allocated 47470.1044921875 
[2025-03-20 19:44:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 13 loss:0.1967877298593521 norm:0.0030839925166219473 max memory_allocated 47470.1044921875 
[2025-03-20 19:45:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 14 loss:0.196768119931221 norm:0.00286775641143322 max memory_allocated 47470.1044921875 
[2025-03-20 19:45:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 15 loss:0.196759432554245 norm:0.002995401853695512 max memory_allocated 47470.1044921875 
[2025-03-20 19:46:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 16 loss:0.19671329855918884 norm:0.0027261432260274887 max memory_allocated 47470.1044921875 
[2025-03-20 19:46:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 17 loss:0.19674009084701538 norm:0.002953911665827036 max memory_allocated 47470.1044921875 
[2025-03-20 19:47:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 18 loss:0.19677504897117615 norm:0.002668678527697921 max memory_allocated 47470.1044921875 
[2025-03-20 19:47:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [29]) iter 19 loss:0.19680838286876678 norm:0.0028893137350678444 max memory_allocated 47470.1044921875 
[2025-03-20 19:48:13 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [29]
[2025-03-20 19:48:13 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [30] ===
[2025-03-20 19:48:13 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 19:48:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 0 loss:0.26812636852264404 norm:0.017355134710669518 max memory_allocated 47470.1044921875 
[2025-03-20 19:49:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 1 loss:0.2587014436721802 norm:0.012779179029166698 max memory_allocated 47470.1044921875 
[2025-03-20 19:49:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 2 loss:0.2502235472202301 norm:0.009361661039292812 max memory_allocated 47470.1044921875 
[2025-03-20 19:50:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 3 loss:0.2464718073606491 norm:0.007522049825638533 max memory_allocated 47470.1044921875 
[2025-03-20 19:50:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 4 loss:0.24493372440338135 norm:0.006341729778796434 max memory_allocated 47470.1044921875 
[2025-03-20 19:50:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 5 loss:0.24422287940979004 norm:0.005421649664640427 max memory_allocated 47470.1044921875 
[2025-03-20 19:51:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 6 loss:0.24380157887935638 norm:0.0048379721119999886 max memory_allocated 47470.1044921875 
[2025-03-20 19:51:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 7 loss:0.24360604584217072 norm:0.00452028401196003 max memory_allocated 47470.1044921875 
[2025-03-20 19:52:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 8 loss:0.24333466589450836 norm:0.004188908729702234 max memory_allocated 47470.1044921875 
[2025-03-20 19:52:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 9 loss:0.2432590276002884 norm:0.0040329513140022755 max memory_allocated 47470.1044921875 
[2025-03-20 19:53:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 10 loss:0.24312755465507507 norm:0.004086388275027275 max memory_allocated 47470.1044921875 
[2025-03-20 19:53:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 11 loss:0.24315015971660614 norm:0.003858714597299695 max memory_allocated 47470.1044921875 
[2025-03-20 19:54:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 12 loss:0.2429002970457077 norm:0.00378767610527575 max memory_allocated 47470.1044921875 
[2025-03-20 19:54:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 13 loss:0.24281653761863708 norm:0.0034415582194924355 max memory_allocated 47470.1044921875 
[2025-03-20 19:55:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 14 loss:0.24284568428993225 norm:0.0038496707566082478 max memory_allocated 47470.1044921875 
[2025-03-20 19:55:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 15 loss:0.24291352927684784 norm:0.003707020776346326 max memory_allocated 47470.1044921875 
[2025-03-20 19:55:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 16 loss:0.24279165267944336 norm:0.003745571244508028 max memory_allocated 47470.1044921875 
[2025-03-20 19:56:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 17 loss:0.24275359511375427 norm:0.003529070410877466 max memory_allocated 47470.1044921875 
[2025-03-20 19:56:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 18 loss:0.2426803857088089 norm:0.003413135651499033 max memory_allocated 47470.1044921875 
[2025-03-20 19:57:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [30]) iter 19 loss:0.2425825297832489 norm:0.003187627997249365 max memory_allocated 47470.1044921875 
[2025-03-20 19:57:50 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [30]
[2025-03-20 19:57:50 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [31] ===
[2025-03-20 19:57:50 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-20 19:58:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 0 loss:0.43654152750968933 norm:0.032426342368125916 max memory_allocated 47470.1044921875 
[2025-03-20 19:58:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 1 loss:0.4110177755355835 norm:0.022722236812114716 max memory_allocated 47470.1044921875 
[2025-03-20 19:59:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 2 loss:0.3951931297779083 norm:0.017512544989585876 max memory_allocated 47470.1044921875 
[2025-03-20 19:59:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 3 loss:0.38757041096687317 norm:0.01397119089961052 max memory_allocated 47470.1044921875 
[2025-03-20 20:00:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 4 loss:0.384451687335968 norm:0.0126118715852499 max memory_allocated 47470.1044921875 
[2025-03-20 20:00:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 5 loss:0.382226824760437 norm:0.010872744023799896 max memory_allocated 47470.1044921875 
[2025-03-20 20:01:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 6 loss:0.38061439990997314 norm:0.0098929014056921 max memory_allocated 47470.1044921875 
[2025-03-20 20:01:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 7 loss:0.37975257635116577 norm:0.009289290755987167 max memory_allocated 47470.1044921875 
[2025-03-20 20:01:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 8 loss:0.37926381826400757 norm:0.009248766116797924 max memory_allocated 47470.1044921875 
[2025-03-20 20:02:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 9 loss:0.37894248962402344 norm:0.009380602277815342 max memory_allocated 47470.1044921875 
[2025-03-20 20:02:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 10 loss:0.37860962748527527 norm:0.00955630000680685 max memory_allocated 47470.1044921875 
[2025-03-20 20:03:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 11 loss:0.37812426686286926 norm:0.009482424706220627 max memory_allocated 47470.1044921875 
[2025-03-20 20:03:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 12 loss:0.3777167499065399 norm:0.009229245595633984 max memory_allocated 47470.1044921875 
[2025-03-20 20:04:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 13 loss:0.37778374552726746 norm:0.009020056575536728 max memory_allocated 47470.1044921875 
[2025-03-20 20:04:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 14 loss:0.37771299481391907 norm:0.009394528344273567 max memory_allocated 47470.1044921875 
[2025-03-20 20:05:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 15 loss:0.37768450379371643 norm:0.009814674034714699 max memory_allocated 47470.1044921875 
[2025-03-20 20:05:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 16 loss:0.3776003420352936 norm:0.009272823110222816 max memory_allocated 47470.1044921875 
[2025-03-20 20:06:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 17 loss:0.37744367122650146 norm:0.00952514074742794 max memory_allocated 47470.1044921875 
[2025-03-20 20:06:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 18 loss:0.37738072872161865 norm:0.009287816472351551 max memory_allocated 47470.1044921875 
[2025-03-20 20:06:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [31]) iter 19 loss:0.3772939145565033 norm:0.009320651181042194 max memory_allocated 47470.1044921875 
[2025-03-20 20:07:31 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 15, block: [31]
[2025-03-20 20:07:31 root] (main_calib_config3_attn.py 379): INFO 18361.57887983322
[2025-03-20 20:07:36 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-20 20:08:22 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.777329444885254
[2025-03-20 20:08:22 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-20 20:09:33 root] (main_calib_config3_attn.py 161): INFO c4 : 7.213320732116699
[2025-03-20 20:49:55 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.777329444885254, 'c4': 7.213320732116699, 'results': {'arc_challenge': {'acc': 0.386518771331058, 'acc_stderr': 0.01423008476191048, 'acc_norm': 0.40784982935153585, 'acc_norm_stderr': 0.014361097288449703}, 'boolq': {'acc': 0.7259938837920489, 'acc_stderr': 0.00780080010345601}, 'hellaswag': {'acc': 0.5590519816769568, 'acc_stderr': 0.004954859106781655, 'acc_norm': 0.7225652260505875, 'acc_norm_stderr': 0.00446817827366566}, 'arc_easy': {'acc': 0.6624579124579124, 'acc_stderr': 0.009703117820790298, 'acc_norm': 0.5176767676767676, 'acc_norm_stderr': 0.010253369805698962}, 'winogrande': {'acc': 0.65982636148382, 'acc_stderr': 0.013315218762417399}, 'piqa': {'acc': 0.7818280739934712, 'acc_stderr': 0.009636081958374381, 'acc_norm': 0.7747551686615887, 'acc_norm_stderr': 0.009746643471032141}}, 'versions': {'arc_challenge': 0, 'boolq': 1, 'hellaswag': 0, 'arc_easy': 0, 'winogrande': 0, 'piqa': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-20 20:49:55 root] (main_calib_config3_attn.py 175): INFO 38.65,66.25,72.60,55.91,78.18,65.98
