[2025-03-27 12:37:50 root] (main.py 265): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-abq-llm/MMLU/Llama-2-7b-hf-w4a7', save_dir=None, resume='./log-abq-llm/Llama-2-7b-hf-w4a7/abq_parameters.pth', real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hendrycksTest*', eval_ppl=False, num_fewshot=0, wbits=4, abits=7, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=0, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None)
[2025-03-27 12:37:59 root] (main.py 332): INFO === start quantization ===
[2025-03-27 12:37:59 root] (main.py 338): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-27 12:37:59 root] (abq_llm.py 62): INFO Starting ...
[2025-03-27 12:38:02 root] (abq_llm.py 212): INFO === Start quantize layer 0 ===
[2025-03-27 12:38:02 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:38:04 root] (abq_llm.py 212): INFO === Start quantize layer 1 ===
[2025-03-27 12:38:04 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:38:04 root] (abq_llm.py 212): INFO === Start quantize layer 2 ===
[2025-03-27 12:38:04 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:38:04 root] (abq_llm.py 212): INFO === Start quantize layer 3 ===
[2025-03-27 12:38:05 root] (abq_llm.py 212): INFO === Start quantize layer 4 ===
[2025-03-27 12:38:05 root] (abq_llm.py 212): INFO === Start quantize layer 5 ===
[2025-03-27 12:38:06 root] (abq_llm.py 212): INFO === Start quantize layer 6 ===
[2025-03-27 12:38:06 root] (abq_llm.py 212): INFO === Start quantize layer 7 ===
[2025-03-27 12:38:06 root] (abq_llm.py 212): INFO === Start quantize layer 8 ===
[2025-03-27 12:38:08 root] (abq_llm.py 212): INFO === Start quantize layer 9 ===
[2025-03-27 12:38:08 root] (abq_llm.py 212): INFO === Start quantize layer 10 ===
[2025-03-27 12:38:08 root] (abq_llm.py 212): INFO === Start quantize layer 11 ===
[2025-03-27 12:38:09 root] (abq_llm.py 212): INFO === Start quantize layer 12 ===
[2025-03-27 12:38:09 root] (abq_llm.py 212): INFO === Start quantize layer 13 ===
[2025-03-27 12:38:10 root] (abq_llm.py 212): INFO === Start quantize layer 14 ===
[2025-03-27 12:38:10 root] (abq_llm.py 212): INFO === Start quantize layer 15 ===
[2025-03-27 12:38:10 root] (abq_llm.py 212): INFO === Start quantize layer 16 ===
[2025-03-27 12:38:11 root] (abq_llm.py 212): INFO === Start quantize layer 17 ===
[2025-03-27 12:38:11 root] (abq_llm.py 212): INFO === Start quantize layer 18 ===
[2025-03-27 12:38:12 root] (abq_llm.py 212): INFO === Start quantize layer 19 ===
[2025-03-27 12:38:12 root] (abq_llm.py 212): INFO === Start quantize layer 20 ===
[2025-03-27 12:38:12 root] (abq_llm.py 212): INFO === Start quantize layer 21 ===
[2025-03-27 12:38:13 root] (abq_llm.py 212): INFO === Start quantize layer 22 ===
[2025-03-27 12:38:13 root] (abq_llm.py 212): INFO === Start quantize layer 23 ===
[2025-03-27 12:38:14 root] (abq_llm.py 212): INFO === Start quantize layer 24 ===
[2025-03-27 12:38:14 root] (abq_llm.py 212): INFO === Start quantize layer 25 ===
[2025-03-27 12:38:14 root] (abq_llm.py 212): INFO === Start quantize layer 26 ===
[2025-03-27 12:38:15 root] (abq_llm.py 212): INFO === Start quantize layer 27 ===
[2025-03-27 12:38:15 root] (abq_llm.py 212): INFO === Start quantize layer 28 ===
[2025-03-27 12:38:15 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:38:16 root] (abq_llm.py 212): INFO === Start quantize layer 29 ===
[2025-03-27 12:38:16 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:38:16 root] (abq_llm.py 212): INFO === Start quantize layer 30 ===
[2025-03-27 12:38:16 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:38:17 root] (abq_llm.py 212): INFO === Start quantize layer 31 ===
[2025-03-27 12:38:17 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:38:17 root] (main.py 361): INFO 18.32172703742981
[2025-03-27 14:18:44 root] (main.py 169): INFO {'results': {'hendrycksTest-international_law': {'acc': 0.4132231404958678, 'acc_stderr': 0.04495087843548408, 'acc_norm': 0.5785123966942148, 'acc_norm_stderr': 0.045077322787750874}, 'hendrycksTest-high_school_biology': {'acc': 0.31290322580645163, 'acc_stderr': 0.02637756702864586, 'acc_norm': 0.3096774193548387, 'acc_norm_stderr': 0.026302774983517418}, 'hendrycksTest-college_physics': {'acc': 0.23529411764705882, 'acc_stderr': 0.042207736591714534, 'acc_norm': 0.28431372549019607, 'acc_norm_stderr': 0.04488482852329017}, 'hendrycksTest-college_biology': {'acc': 0.2638888888888889, 'acc_stderr': 0.03685651095897532, 'acc_norm': 0.24305555555555555, 'acc_norm_stderr': 0.0358687928008034}, 'hendrycksTest-electrical_engineering': {'acc': 0.2827586206896552, 'acc_stderr': 0.03752833958003337, 'acc_norm': 0.3448275862068966, 'acc_norm_stderr': 0.039609335494512087}, 'hendrycksTest-high_school_physics': {'acc': 0.25165562913907286, 'acc_stderr': 0.035433042343899844, 'acc_norm': 0.271523178807947, 'acc_norm_stderr': 0.03631329803969654}, 'hendrycksTest-elementary_mathematics': {'acc': 0.291005291005291, 'acc_stderr': 0.023393826500484875, 'acc_norm': 0.30423280423280424, 'acc_norm_stderr': 0.023695415009463087}, 'hendrycksTest-formal_logic': {'acc': 0.2857142857142857, 'acc_stderr': 0.04040610178208841, 'acc_norm': 0.25396825396825395, 'acc_norm_stderr': 0.03893259610604674}, 'hendrycksTest-human_aging': {'acc': 0.3004484304932735, 'acc_stderr': 0.030769352008229132, 'acc_norm': 0.2242152466367713, 'acc_norm_stderr': 0.02799153425851952}, 'hendrycksTest-jurisprudence': {'acc': 0.2962962962962963, 'acc_stderr': 0.04414343666854933, 'acc_norm': 0.4166666666666667, 'acc_norm_stderr': 0.04766075165356461}, 'hendrycksTest-moral_scenarios': {'acc': 0.2547486033519553, 'acc_stderr': 0.014572650383409155, 'acc_norm': 0.27262569832402234, 'acc_norm_stderr': 0.014893391735249588}, 'hendrycksTest-astronomy': {'acc': 0.35526315789473684, 'acc_stderr': 0.03894734487013316, 'acc_norm': 0.40131578947368424, 'acc_norm_stderr': 0.039889037033362836}, 'hendrycksTest-conceptual_physics': {'acc': 0.2851063829787234, 'acc_stderr': 0.029513196625539355, 'acc_norm': 0.23404255319148937, 'acc_norm_stderr': 0.027678452578212397}, 'hendrycksTest-college_chemistry': {'acc': 0.22, 'acc_stderr': 0.04163331998932269, 'acc_norm': 0.28, 'acc_norm_stderr': 0.045126085985421276}, 'hendrycksTest-human_sexuality': {'acc': 0.46564885496183206, 'acc_stderr': 0.04374928560599738, 'acc_norm': 0.33587786259541985, 'acc_norm_stderr': 0.04142313771996665}, 'hendrycksTest-sociology': {'acc': 0.3333333333333333, 'acc_stderr': 0.033333333333333354, 'acc_norm': 0.31840796019900497, 'acc_norm_stderr': 0.03294118479054095}, 'hendrycksTest-prehistory': {'acc': 0.36728395061728397, 'acc_stderr': 0.026822801759507898, 'acc_norm': 0.24382716049382716, 'acc_norm_stderr': 0.023891879541959607}, 'hendrycksTest-professional_law': {'acc': 0.2796610169491525, 'acc_stderr': 0.011463397393861962, 'acc_norm': 0.29595827900912647, 'acc_norm_stderr': 0.011658518525277049}, 'hendrycksTest-virology': {'acc': 0.3313253012048193, 'acc_stderr': 0.03664314777288085, 'acc_norm': 0.28313253012048195, 'acc_norm_stderr': 0.03507295431370518}, 'hendrycksTest-machine_learning': {'acc': 0.3125, 'acc_stderr': 0.043994650575715215, 'acc_norm': 0.25, 'acc_norm_stderr': 0.04109974682633932}, 'hendrycksTest-marketing': {'acc': 0.4358974358974359, 'acc_stderr': 0.032485775115784, 'acc_norm': 0.4230769230769231, 'acc_norm_stderr': 0.03236612176220202}, 'hendrycksTest-us_foreign_policy': {'acc': 0.42, 'acc_stderr': 0.049604496374885836, 'acc_norm': 0.38, 'acc_norm_stderr': 0.048783173121456316}, 'hendrycksTest-high_school_european_history': {'acc': 0.37575757575757573, 'acc_stderr': 0.03781887353205982, 'acc_norm': 0.3333333333333333, 'acc_norm_stderr': 0.0368105086916155}, 'hendrycksTest-high_school_macroeconomics': {'acc': 0.3230769230769231, 'acc_stderr': 0.02371088850197057, 'acc_norm': 0.2743589743589744, 'acc_norm_stderr': 0.02262276576749322}, 'hendrycksTest-logical_fallacies': {'acc': 0.2147239263803681, 'acc_stderr': 0.03226219377286773, 'acc_norm': 0.32515337423312884, 'acc_norm_stderr': 0.03680350371286461}, 'hendrycksTest-abstract_algebra': {'acc': 0.23, 'acc_stderr': 0.04229525846816506, 'acc_norm': 0.22, 'acc_norm_stderr': 0.0416333199893227}, 'hendrycksTest-high_school_government_and_politics': {'acc': 0.33678756476683935, 'acc_stderr': 0.03410780251836184, 'acc_norm': 0.31088082901554404, 'acc_norm_stderr': 0.03340361906276588}, 'hendrycksTest-moral_disputes': {'acc': 0.30057803468208094, 'acc_stderr': 0.024685316867257796, 'acc_norm': 0.3468208092485549, 'acc_norm_stderr': 0.025624723994030457}, 'hendrycksTest-high_school_world_history': {'acc': 0.3755274261603376, 'acc_stderr': 0.03152256243091156, 'acc_norm': 0.3628691983122363, 'acc_norm_stderr': 0.03129920825530213}, 'hendrycksTest-high_school_microeconomics': {'acc': 0.31512605042016806, 'acc_stderr': 0.03017680828897434, 'acc_norm': 0.3865546218487395, 'acc_norm_stderr': 0.03163145807552379}, 'hendrycksTest-college_medicine': {'acc': 0.2947976878612717, 'acc_stderr': 0.034765996075164785, 'acc_norm': 0.28901734104046245, 'acc_norm_stderr': 0.034564257450869995}, 'hendrycksTest-security_studies': {'acc': 0.4489795918367347, 'acc_stderr': 0.03184213866687579, 'acc_norm': 0.3346938775510204, 'acc_norm_stderr': 0.03020923522624231}, 'hendrycksTest-high_school_geography': {'acc': 0.23737373737373738, 'acc_stderr': 0.0303137105381989, 'acc_norm': 0.29292929292929293, 'acc_norm_stderr': 0.03242497958178815}, 'hendrycksTest-world_religions': {'acc': 0.42105263157894735, 'acc_stderr': 0.03786720706234214, 'acc_norm': 0.4093567251461988, 'acc_norm_stderr': 0.03771283107626545}, 'hendrycksTest-business_ethics': {'acc': 0.4, 'acc_stderr': 0.04923659639173309, 'acc_norm': 0.34, 'acc_norm_stderr': 0.04760952285695236}, 'hendrycksTest-professional_accounting': {'acc': 0.2695035460992908, 'acc_stderr': 0.02646903681859063, 'acc_norm': 0.25886524822695034, 'acc_norm_stderr': 0.026129572527180848}, 'hendrycksTest-global_facts': {'acc': 0.24, 'acc_stderr': 0.042923469599092816, 'acc_norm': 0.18, 'acc_norm_stderr': 0.038612291966536955}, 'hendrycksTest-high_school_us_history': {'acc': 0.3333333333333333, 'acc_stderr': 0.033086111132364336, 'acc_norm': 0.3088235294117647, 'acc_norm_stderr': 0.03242661719827218}, 'hendrycksTest-professional_psychology': {'acc': 0.31862745098039214, 'acc_stderr': 0.018850084696468712, 'acc_norm': 0.2647058823529412, 'acc_norm_stderr': 0.017848089574913222}, 'hendrycksTest-college_computer_science': {'acc': 0.29, 'acc_stderr': 0.04560480215720685, 'acc_norm': 0.27, 'acc_norm_stderr': 0.044619604333847394}, 'hendrycksTest-philosophy': {'acc': 0.3086816720257235, 'acc_stderr': 0.026236965881153262, 'acc_norm': 0.3215434083601286, 'acc_norm_stderr': 0.026527724079528872}, 'hendrycksTest-anatomy': {'acc': 0.3037037037037037, 'acc_stderr': 0.039725528847851375, 'acc_norm': 0.23703703703703705, 'acc_norm_stderr': 0.03673731683969506}, 'hendrycksTest-nutrition': {'acc': 0.3660130718954248, 'acc_stderr': 0.027582811415159603, 'acc_norm': 0.40522875816993464, 'acc_norm_stderr': 0.02811092849280907}, 'hendrycksTest-computer_security': {'acc': 0.35, 'acc_stderr': 0.047937248544110196, 'acc_norm': 0.36, 'acc_norm_stderr': 0.048241815132442176}, 'hendrycksTest-econometrics': {'acc': 0.2894736842105263, 'acc_stderr': 0.04266339443159393, 'acc_norm': 0.24561403508771928, 'acc_norm_stderr': 0.04049339297748141}, 'hendrycksTest-clinical_knowledge': {'acc': 0.3169811320754717, 'acc_stderr': 0.02863723563980091, 'acc_norm': 0.3660377358490566, 'acc_norm_stderr': 0.029647813539365252}, 'hendrycksTest-high_school_mathematics': {'acc': 0.21851851851851853, 'acc_stderr': 0.02519575225182379, 'acc_norm': 0.26666666666666666, 'acc_norm_stderr': 0.026962424325073824}, 'hendrycksTest-medical_genetics': {'acc': 0.29, 'acc_stderr': 0.04560480215720684, 'acc_norm': 0.34, 'acc_norm_stderr': 0.04760952285695236}, 'hendrycksTest-public_relations': {'acc': 0.2909090909090909, 'acc_stderr': 0.04350271442923243, 'acc_norm': 0.20909090909090908, 'acc_norm_stderr': 0.03895091015724137}, 'hendrycksTest-miscellaneous': {'acc': 0.36015325670498083, 'acc_stderr': 0.017166362471369292, 'acc_norm': 0.29246487867177523, 'acc_norm_stderr': 0.016267000684598645}, 'hendrycksTest-management': {'acc': 0.33980582524271846, 'acc_stderr': 0.046897659372781335, 'acc_norm': 0.3106796116504854, 'acc_norm_stderr': 0.04582124160161552}, 'hendrycksTest-professional_medicine': {'acc': 0.2610294117647059, 'acc_stderr': 0.026679252270103124, 'acc_norm': 0.28308823529411764, 'acc_norm_stderr': 0.02736586113151381}, 'hendrycksTest-high_school_computer_science': {'acc': 0.3, 'acc_stderr': 0.046056618647183814, 'acc_norm': 0.32, 'acc_norm_stderr': 0.04688261722621503}, 'hendrycksTest-high_school_psychology': {'acc': 0.29908256880733947, 'acc_stderr': 0.019630417285415175, 'acc_norm': 0.24770642201834864, 'acc_norm_stderr': 0.018508143602547815}, 'hendrycksTest-high_school_chemistry': {'acc': 0.2315270935960591, 'acc_stderr': 0.029678333141444437, 'acc_norm': 0.32019704433497537, 'acc_norm_stderr': 0.032826493853041504}, 'hendrycksTest-college_mathematics': {'acc': 0.2, 'acc_stderr': 0.04020151261036843, 'acc_norm': 0.32, 'acc_norm_stderr': 0.046882617226215034}, 'hendrycksTest-high_school_statistics': {'acc': 0.30092592592592593, 'acc_stderr': 0.031280390843298825, 'acc_norm': 0.28703703703703703, 'acc_norm_stderr': 0.030851992993257013}}, 'versions': {'hendrycksTest-international_law': 0, 'hendrycksTest-high_school_biology': 0, 'hendrycksTest-college_physics': 0, 'hendrycksTest-college_biology': 0, 'hendrycksTest-electrical_engineering': 0, 'hendrycksTest-high_school_physics': 0, 'hendrycksTest-elementary_mathematics': 0, 'hendrycksTest-formal_logic': 0, 'hendrycksTest-human_aging': 0, 'hendrycksTest-jurisprudence': 0, 'hendrycksTest-moral_scenarios': 0, 'hendrycksTest-astronomy': 0, 'hendrycksTest-conceptual_physics': 0, 'hendrycksTest-college_chemistry': 0, 'hendrycksTest-human_sexuality': 0, 'hendrycksTest-sociology': 0, 'hendrycksTest-prehistory': 0, 'hendrycksTest-professional_law': 0, 'hendrycksTest-virology': 0, 'hendrycksTest-machine_learning': 0, 'hendrycksTest-marketing': 0, 'hendrycksTest-us_foreign_policy': 0, 'hendrycksTest-high_school_european_history': 0, 'hendrycksTest-high_school_macroeconomics': 0, 'hendrycksTest-logical_fallacies': 0, 'hendrycksTest-abstract_algebra': 0, 'hendrycksTest-high_school_government_and_politics': 0, 'hendrycksTest-moral_disputes': 0, 'hendrycksTest-high_school_world_history': 0, 'hendrycksTest-high_school_microeconomics': 0, 'hendrycksTest-college_medicine': 0, 'hendrycksTest-security_studies': 0, 'hendrycksTest-high_school_geography': 0, 'hendrycksTest-world_religions': 0, 'hendrycksTest-business_ethics': 0, 'hendrycksTest-professional_accounting': 0, 'hendrycksTest-global_facts': 0, 'hendrycksTest-high_school_us_history': 0, 'hendrycksTest-professional_psychology': 0, 'hendrycksTest-college_computer_science': 0, 'hendrycksTest-philosophy': 0, 'hendrycksTest-anatomy': 0, 'hendrycksTest-nutrition': 0, 'hendrycksTest-computer_security': 0, 'hendrycksTest-econometrics': 0, 'hendrycksTest-clinical_knowledge': 0, 'hendrycksTest-high_school_mathematics': 0, 'hendrycksTest-medical_genetics': 0, 'hendrycksTest-public_relations': 0, 'hendrycksTest-miscellaneous': 0, 'hendrycksTest-management': 0, 'hendrycksTest-professional_medicine': 0, 'hendrycksTest-high_school_computer_science': 0, 'hendrycksTest-high_school_psychology': 0, 'hendrycksTest-high_school_chemistry': 0, 'hendrycksTest-college_mathematics': 0, 'hendrycksTest-high_school_statistics': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-27 14:18:44 root] (main.py 196): INFO Average accuracy 0.2740 - STEM
[2025-03-27 14:18:44 root] (main.py 196): INFO Average accuracy 0.3251 - humanities
[2025-03-27 14:18:44 root] (main.py 196): INFO Average accuracy 0.3399 - social sciences
[2025-03-27 14:18:44 root] (main.py 196): INFO Average accuracy 0.3221 - other (business, health, misc.)
[2025-03-27 14:18:44 root] (main.py 198): INFO Average accuracy: 0.3113
