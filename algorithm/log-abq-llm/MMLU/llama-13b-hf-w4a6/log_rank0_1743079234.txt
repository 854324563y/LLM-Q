[2025-03-27 12:40:34 root] (main.py 265): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-abq-llm/MMLU/llama-13b-hf-w4a6', save_dir=None, resume='./log-abq-llm/llama-13b-hf-w4a6/abq_parameters.pth', real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hendrycksTest*', eval_ppl=False, num_fewshot=0, wbits=4, abits=6, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=0, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None)
[2025-03-27 12:49:37 root] (main.py 332): INFO === start quantization ===
[2025-03-27 12:49:37 root] (main.py 338): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-27 12:49:37 root] (abq_llm.py 62): INFO Starting ...
[2025-03-27 12:49:41 root] (abq_llm.py 212): INFO === Start quantize layer 0 ===
[2025-03-27 12:49:41 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:49:43 root] (abq_llm.py 212): INFO === Start quantize layer 1 ===
[2025-03-27 12:49:43 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:49:44 root] (abq_llm.py 212): INFO === Start quantize layer 2 ===
[2025-03-27 12:49:44 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:49:44 root] (abq_llm.py 212): INFO === Start quantize layer 3 ===
[2025-03-27 12:49:45 root] (abq_llm.py 212): INFO === Start quantize layer 4 ===
[2025-03-27 12:49:46 root] (abq_llm.py 212): INFO === Start quantize layer 5 ===
[2025-03-27 12:49:46 root] (abq_llm.py 212): INFO === Start quantize layer 6 ===
[2025-03-27 12:49:47 root] (abq_llm.py 212): INFO === Start quantize layer 7 ===
[2025-03-27 12:49:48 root] (abq_llm.py 212): INFO === Start quantize layer 8 ===
[2025-03-27 12:49:48 root] (abq_llm.py 212): INFO === Start quantize layer 9 ===
[2025-03-27 12:49:49 root] (abq_llm.py 212): INFO === Start quantize layer 10 ===
[2025-03-27 12:49:50 root] (abq_llm.py 212): INFO === Start quantize layer 11 ===
[2025-03-27 12:49:50 root] (abq_llm.py 212): INFO === Start quantize layer 12 ===
[2025-03-27 12:49:51 root] (abq_llm.py 212): INFO === Start quantize layer 13 ===
[2025-03-27 12:49:52 root] (abq_llm.py 212): INFO === Start quantize layer 14 ===
[2025-03-27 12:49:52 root] (abq_llm.py 212): INFO === Start quantize layer 15 ===
[2025-03-27 12:49:53 root] (abq_llm.py 212): INFO === Start quantize layer 16 ===
[2025-03-27 12:49:54 root] (abq_llm.py 212): INFO === Start quantize layer 17 ===
[2025-03-27 12:49:55 root] (abq_llm.py 212): INFO === Start quantize layer 18 ===
[2025-03-27 12:49:56 root] (abq_llm.py 212): INFO === Start quantize layer 19 ===
[2025-03-27 12:49:57 root] (abq_llm.py 212): INFO === Start quantize layer 20 ===
[2025-03-27 12:49:58 root] (abq_llm.py 212): INFO === Start quantize layer 21 ===
[2025-03-27 12:49:59 root] (abq_llm.py 212): INFO === Start quantize layer 22 ===
[2025-03-27 12:50:00 root] (abq_llm.py 212): INFO === Start quantize layer 23 ===
[2025-03-27 12:50:00 root] (abq_llm.py 212): INFO === Start quantize layer 24 ===
[2025-03-27 12:50:01 root] (abq_llm.py 212): INFO === Start quantize layer 25 ===
[2025-03-27 12:50:02 root] (abq_llm.py 212): INFO === Start quantize layer 26 ===
[2025-03-27 12:50:03 root] (abq_llm.py 212): INFO === Start quantize layer 27 ===
[2025-03-27 12:50:03 root] (abq_llm.py 212): INFO === Start quantize layer 28 ===
[2025-03-27 12:50:04 root] (abq_llm.py 212): INFO === Start quantize layer 29 ===
[2025-03-27 12:50:05 root] (abq_llm.py 212): INFO === Start quantize layer 30 ===
[2025-03-27 12:50:05 root] (abq_llm.py 212): INFO === Start quantize layer 31 ===
[2025-03-27 12:50:06 root] (abq_llm.py 212): INFO === Start quantize layer 32 ===
[2025-03-27 12:50:08 root] (abq_llm.py 212): INFO === Start quantize layer 33 ===
[2025-03-27 12:50:09 root] (abq_llm.py 212): INFO === Start quantize layer 34 ===
[2025-03-27 12:50:09 root] (abq_llm.py 212): INFO === Start quantize layer 35 ===
[2025-03-27 12:50:10 root] (abq_llm.py 212): INFO === Start quantize layer 36 ===
[2025-03-27 12:50:10 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:50:11 root] (abq_llm.py 212): INFO === Start quantize layer 37 ===
[2025-03-27 12:50:11 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:50:11 root] (abq_llm.py 212): INFO === Start quantize layer 38 ===
[2025-03-27 12:50:12 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:50:12 root] (abq_llm.py 212): INFO === Start quantize layer 39 ===
[2025-03-27 12:50:12 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:50:13 root] (main.py 361): INFO 36.090651512145996
[2025-03-27 14:59:31 root] (main.py 169): INFO {'results': {'hendrycksTest-international_law': {'acc': 0.4214876033057851, 'acc_stderr': 0.045077322787750944, 'acc_norm': 0.5371900826446281, 'acc_norm_stderr': 0.04551711196104218}, 'hendrycksTest-high_school_biology': {'acc': 0.3096774193548387, 'acc_stderr': 0.026302774983517418, 'acc_norm': 0.3032258064516129, 'acc_norm_stderr': 0.026148685930671753}, 'hendrycksTest-college_physics': {'acc': 0.21568627450980393, 'acc_stderr': 0.040925639582376556, 'acc_norm': 0.28431372549019607, 'acc_norm_stderr': 0.04488482852329017}, 'hendrycksTest-college_biology': {'acc': 0.24305555555555555, 'acc_stderr': 0.03586879280080341, 'acc_norm': 0.2569444444444444, 'acc_norm_stderr': 0.03653946969442099}, 'hendrycksTest-electrical_engineering': {'acc': 0.33793103448275863, 'acc_stderr': 0.039417076320648906, 'acc_norm': 0.3103448275862069, 'acc_norm_stderr': 0.03855289616378948}, 'hendrycksTest-high_school_physics': {'acc': 0.31788079470198677, 'acc_stderr': 0.03802039760107903, 'acc_norm': 0.2781456953642384, 'acc_norm_stderr': 0.03658603262763743}, 'hendrycksTest-elementary_mathematics': {'acc': 0.2857142857142857, 'acc_stderr': 0.02326651221373057, 'acc_norm': 0.30158730158730157, 'acc_norm_stderr': 0.023636975996101803}, 'hendrycksTest-formal_logic': {'acc': 0.2698412698412698, 'acc_stderr': 0.03970158273235172, 'acc_norm': 0.31746031746031744, 'acc_norm_stderr': 0.04163453031302859}, 'hendrycksTest-human_aging': {'acc': 0.3094170403587444, 'acc_stderr': 0.03102441174057222, 'acc_norm': 0.21973094170403587, 'acc_norm_stderr': 0.027790177064383602}, 'hendrycksTest-jurisprudence': {'acc': 0.2962962962962963, 'acc_stderr': 0.044143436668549335, 'acc_norm': 0.39814814814814814, 'acc_norm_stderr': 0.04732332615978813}, 'hendrycksTest-moral_scenarios': {'acc': 0.2659217877094972, 'acc_stderr': 0.014776765066438893, 'acc_norm': 0.27262569832402234, 'acc_norm_stderr': 0.014893391735249588}, 'hendrycksTest-astronomy': {'acc': 0.3815789473684211, 'acc_stderr': 0.03953173377749194, 'acc_norm': 0.39473684210526316, 'acc_norm_stderr': 0.039777499346220734}, 'hendrycksTest-conceptual_physics': {'acc': 0.30638297872340425, 'acc_stderr': 0.03013590647851756, 'acc_norm': 0.225531914893617, 'acc_norm_stderr': 0.02732107841738754}, 'hendrycksTest-college_chemistry': {'acc': 0.21, 'acc_stderr': 0.040936018074033256, 'acc_norm': 0.3, 'acc_norm_stderr': 0.046056618647183814}, 'hendrycksTest-human_sexuality': {'acc': 0.40458015267175573, 'acc_stderr': 0.043046937953806645, 'acc_norm': 0.3053435114503817, 'acc_norm_stderr': 0.04039314978724562}, 'hendrycksTest-sociology': {'acc': 0.34328358208955223, 'acc_stderr': 0.03357379665433431, 'acc_norm': 0.29850746268656714, 'acc_norm_stderr': 0.03235743789355042}, 'hendrycksTest-prehistory': {'acc': 0.32407407407407407, 'acc_stderr': 0.026041766202717167, 'acc_norm': 0.2808641975308642, 'acc_norm_stderr': 0.025006469755799215}, 'hendrycksTest-professional_law': {'acc': 0.29139504563233376, 'acc_stderr': 0.01160572021425758, 'acc_norm': 0.30182529335071706, 'acc_norm_stderr': 0.011724350518105891}, 'hendrycksTest-virology': {'acc': 0.37349397590361444, 'acc_stderr': 0.03765845117168862, 'acc_norm': 0.28313253012048195, 'acc_norm_stderr': 0.03507295431370518}, 'hendrycksTest-machine_learning': {'acc': 0.24107142857142858, 'acc_stderr': 0.04059867246952687, 'acc_norm': 0.25892857142857145, 'acc_norm_stderr': 0.041577515398656284}, 'hendrycksTest-marketing': {'acc': 0.44871794871794873, 'acc_stderr': 0.032583346493868806, 'acc_norm': 0.39316239316239315, 'acc_norm_stderr': 0.03199957924651048}, 'hendrycksTest-us_foreign_policy': {'acc': 0.57, 'acc_stderr': 0.049756985195624284, 'acc_norm': 0.45, 'acc_norm_stderr': 0.049999999999999996}, 'hendrycksTest-high_school_european_history': {'acc': 0.3696969696969697, 'acc_stderr': 0.037694303145125674, 'acc_norm': 0.3515151515151515, 'acc_norm_stderr': 0.037282069986826503}, 'hendrycksTest-high_school_macroeconomics': {'acc': 0.33589743589743587, 'acc_stderr': 0.02394672474156397, 'acc_norm': 0.3076923076923077, 'acc_norm_stderr': 0.023400928918310495}, 'hendrycksTest-logical_fallacies': {'acc': 0.25766871165644173, 'acc_stderr': 0.03436150827846917, 'acc_norm': 0.31901840490797545, 'acc_norm_stderr': 0.03661997551073836}, 'hendrycksTest-abstract_algebra': {'acc': 0.26, 'acc_stderr': 0.04408440022768077, 'acc_norm': 0.24, 'acc_norm_stderr': 0.04292346959909284}, 'hendrycksTest-high_school_government_and_politics': {'acc': 0.38341968911917096, 'acc_stderr': 0.03508984236295342, 'acc_norm': 0.30569948186528495, 'acc_norm_stderr': 0.03324837939758159}, 'hendrycksTest-moral_disputes': {'acc': 0.28901734104046245, 'acc_stderr': 0.024405173935783234, 'acc_norm': 0.3265895953757225, 'acc_norm_stderr': 0.025248264774242822}, 'hendrycksTest-high_school_world_history': {'acc': 0.350210970464135, 'acc_stderr': 0.03105239193758435, 'acc_norm': 0.3291139240506329, 'acc_norm_stderr': 0.03058732629470237}, 'hendrycksTest-high_school_microeconomics': {'acc': 0.3277310924369748, 'acc_stderr': 0.03048991141767323, 'acc_norm': 0.3739495798319328, 'acc_norm_stderr': 0.031429466378837076}, 'hendrycksTest-college_medicine': {'acc': 0.30057803468208094, 'acc_stderr': 0.03496101481191181, 'acc_norm': 0.2138728323699422, 'acc_norm_stderr': 0.03126511206173043}, 'hendrycksTest-security_studies': {'acc': 0.4530612244897959, 'acc_stderr': 0.03186785930004128, 'acc_norm': 0.37142857142857144, 'acc_norm_stderr': 0.030932858792789848}, 'hendrycksTest-high_school_geography': {'acc': 0.3181818181818182, 'acc_stderr': 0.03318477333845331, 'acc_norm': 0.3181818181818182, 'acc_norm_stderr': 0.0331847733384533}, 'hendrycksTest-world_religions': {'acc': 0.47368421052631576, 'acc_stderr': 0.038295098689947286, 'acc_norm': 0.43859649122807015, 'acc_norm_stderr': 0.038057975055904594}, 'hendrycksTest-business_ethics': {'acc': 0.37, 'acc_stderr': 0.048523658709391, 'acc_norm': 0.33, 'acc_norm_stderr': 0.04725815626252604}, 'hendrycksTest-professional_accounting': {'acc': 0.23049645390070922, 'acc_stderr': 0.025123739226872402, 'acc_norm': 0.2553191489361702, 'acc_norm_stderr': 0.026011992930902013}, 'hendrycksTest-global_facts': {'acc': 0.14, 'acc_stderr': 0.03487350880197772, 'acc_norm': 0.1, 'acc_norm_stderr': 0.030151134457776334}, 'hendrycksTest-high_school_us_history': {'acc': 0.3872549019607843, 'acc_stderr': 0.03418931233833343, 'acc_norm': 0.3235294117647059, 'acc_norm_stderr': 0.03283472056108567}, 'hendrycksTest-professional_psychology': {'acc': 0.3055555555555556, 'acc_stderr': 0.018635594034423976, 'acc_norm': 0.2875816993464052, 'acc_norm_stderr': 0.018311653053648222}, 'hendrycksTest-college_computer_science': {'acc': 0.31, 'acc_stderr': 0.04648231987117316, 'acc_norm': 0.25, 'acc_norm_stderr': 0.04351941398892446}, 'hendrycksTest-philosophy': {'acc': 0.2765273311897106, 'acc_stderr': 0.025403832978179615, 'acc_norm': 0.3183279742765273, 'acc_norm_stderr': 0.02645722506781103}, 'hendrycksTest-anatomy': {'acc': 0.3333333333333333, 'acc_stderr': 0.04072314811876837, 'acc_norm': 0.25925925925925924, 'acc_norm_stderr': 0.037857144650666544}, 'hendrycksTest-nutrition': {'acc': 0.39869281045751637, 'acc_stderr': 0.028036092273891755, 'acc_norm': 0.41830065359477125, 'acc_norm_stderr': 0.028245134024387282}, 'hendrycksTest-computer_security': {'acc': 0.35, 'acc_stderr': 0.0479372485441102, 'acc_norm': 0.37, 'acc_norm_stderr': 0.048523658709391}, 'hendrycksTest-econometrics': {'acc': 0.2894736842105263, 'acc_stderr': 0.042663394431593935, 'acc_norm': 0.30701754385964913, 'acc_norm_stderr': 0.04339138322579861}, 'hendrycksTest-clinical_knowledge': {'acc': 0.29056603773584905, 'acc_stderr': 0.02794321998933713, 'acc_norm': 0.3169811320754717, 'acc_norm_stderr': 0.028637235639800935}, 'hendrycksTest-high_school_mathematics': {'acc': 0.26666666666666666, 'acc_stderr': 0.026962424325073828, 'acc_norm': 0.31851851851851853, 'acc_norm_stderr': 0.02840653309060846}, 'hendrycksTest-medical_genetics': {'acc': 0.25, 'acc_stderr': 0.04351941398892446, 'acc_norm': 0.33, 'acc_norm_stderr': 0.047258156262526045}, 'hendrycksTest-public_relations': {'acc': 0.21818181818181817, 'acc_stderr': 0.03955932861795833, 'acc_norm': 0.15454545454545454, 'acc_norm_stderr': 0.03462262571262667}, 'hendrycksTest-miscellaneous': {'acc': 0.40357598978288634, 'acc_stderr': 0.017544332237926414, 'acc_norm': 0.3065134099616858, 'acc_norm_stderr': 0.016486952893041508}, 'hendrycksTest-management': {'acc': 0.3592233009708738, 'acc_stderr': 0.04750458399041694, 'acc_norm': 0.34951456310679613, 'acc_norm_stderr': 0.047211885060971716}, 'hendrycksTest-professional_medicine': {'acc': 0.28308823529411764, 'acc_stderr': 0.02736586113151381, 'acc_norm': 0.27205882352941174, 'acc_norm_stderr': 0.02703304115168146}, 'hendrycksTest-high_school_computer_science': {'acc': 0.34, 'acc_stderr': 0.047609522856952365, 'acc_norm': 0.34, 'acc_norm_stderr': 0.047609522856952365}, 'hendrycksTest-high_school_psychology': {'acc': 0.3339449541284404, 'acc_stderr': 0.0202205541967364, 'acc_norm': 0.23669724770642203, 'acc_norm_stderr': 0.018224078117299078}, 'hendrycksTest-high_school_chemistry': {'acc': 0.2315270935960591, 'acc_stderr': 0.029678333141444455, 'acc_norm': 0.270935960591133, 'acc_norm_stderr': 0.031270907132976984}, 'hendrycksTest-college_mathematics': {'acc': 0.16, 'acc_stderr': 0.03684529491774709, 'acc_norm': 0.25, 'acc_norm_stderr': 0.04351941398892446}, 'hendrycksTest-high_school_statistics': {'acc': 0.35648148148148145, 'acc_stderr': 0.03266478331527272, 'acc_norm': 0.3333333333333333, 'acc_norm_stderr': 0.03214952147802749}}, 'versions': {'hendrycksTest-international_law': 0, 'hendrycksTest-high_school_biology': 0, 'hendrycksTest-college_physics': 0, 'hendrycksTest-college_biology': 0, 'hendrycksTest-electrical_engineering': 0, 'hendrycksTest-high_school_physics': 0, 'hendrycksTest-elementary_mathematics': 0, 'hendrycksTest-formal_logic': 0, 'hendrycksTest-human_aging': 0, 'hendrycksTest-jurisprudence': 0, 'hendrycksTest-moral_scenarios': 0, 'hendrycksTest-astronomy': 0, 'hendrycksTest-conceptual_physics': 0, 'hendrycksTest-college_chemistry': 0, 'hendrycksTest-human_sexuality': 0, 'hendrycksTest-sociology': 0, 'hendrycksTest-prehistory': 0, 'hendrycksTest-professional_law': 0, 'hendrycksTest-virology': 0, 'hendrycksTest-machine_learning': 0, 'hendrycksTest-marketing': 0, 'hendrycksTest-us_foreign_policy': 0, 'hendrycksTest-high_school_european_history': 0, 'hendrycksTest-high_school_macroeconomics': 0, 'hendrycksTest-logical_fallacies': 0, 'hendrycksTest-abstract_algebra': 0, 'hendrycksTest-high_school_government_and_politics': 0, 'hendrycksTest-moral_disputes': 0, 'hendrycksTest-high_school_world_history': 0, 'hendrycksTest-high_school_microeconomics': 0, 'hendrycksTest-college_medicine': 0, 'hendrycksTest-security_studies': 0, 'hendrycksTest-high_school_geography': 0, 'hendrycksTest-world_religions': 0, 'hendrycksTest-business_ethics': 0, 'hendrycksTest-professional_accounting': 0, 'hendrycksTest-global_facts': 0, 'hendrycksTest-high_school_us_history': 0, 'hendrycksTest-professional_psychology': 0, 'hendrycksTest-college_computer_science': 0, 'hendrycksTest-philosophy': 0, 'hendrycksTest-anatomy': 0, 'hendrycksTest-nutrition': 0, 'hendrycksTest-computer_security': 0, 'hendrycksTest-econometrics': 0, 'hendrycksTest-clinical_knowledge': 0, 'hendrycksTest-high_school_mathematics': 0, 'hendrycksTest-medical_genetics': 0, 'hendrycksTest-public_relations': 0, 'hendrycksTest-miscellaneous': 0, 'hendrycksTest-management': 0, 'hendrycksTest-professional_medicine': 0, 'hendrycksTest-high_school_computer_science': 0, 'hendrycksTest-high_school_psychology': 0, 'hendrycksTest-high_school_chemistry': 0, 'hendrycksTest-college_mathematics': 0, 'hendrycksTest-high_school_statistics': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-27 14:59:31 root] (main.py 196): INFO Average accuracy 0.2846 - STEM
[2025-03-27 14:59:31 root] (main.py 196): INFO Average accuracy 0.3287 - humanities
[2025-03-27 14:59:31 root] (main.py 196): INFO Average accuracy 0.3569 - social sciences
[2025-03-27 14:59:31 root] (main.py 196): INFO Average accuracy 0.3208 - other (business, health, misc.)
[2025-03-27 14:59:31 root] (main.py 198): INFO Average accuracy: 0.3188
