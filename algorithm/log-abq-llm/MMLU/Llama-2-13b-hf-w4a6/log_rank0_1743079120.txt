[2025-03-27 12:38:40 root] (main.py 265): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-abq-llm/MMLU/Llama-2-13b-hf-w4a6', save_dir=None, resume='./log-abq-llm/Llama-2-13b-hf-w4a6/abq_parameters.pth', real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hendrycksTest*', eval_ppl=False, num_fewshot=0, wbits=4, abits=6, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=0, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None)
[2025-03-27 12:38:45 root] (main.py 332): INFO === start quantization ===
[2025-03-27 12:38:45 root] (main.py 338): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-27 12:38:46 root] (abq_llm.py 62): INFO Starting ...
[2025-03-27 12:39:00 root] (abq_llm.py 212): INFO === Start quantize layer 0 ===
[2025-03-27 12:39:00 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:39:03 root] (abq_llm.py 212): INFO === Start quantize layer 1 ===
[2025-03-27 12:39:11 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:39:11 root] (abq_llm.py 212): INFO === Start quantize layer 2 ===
[2025-03-27 12:39:19 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:39:19 root] (abq_llm.py 212): INFO === Start quantize layer 3 ===
[2025-03-27 12:39:27 root] (abq_llm.py 212): INFO === Start quantize layer 4 ===
[2025-03-27 12:39:35 root] (abq_llm.py 212): INFO === Start quantize layer 5 ===
[2025-03-27 12:39:43 root] (abq_llm.py 212): INFO === Start quantize layer 6 ===
[2025-03-27 12:39:51 root] (abq_llm.py 212): INFO === Start quantize layer 7 ===
[2025-03-27 12:39:58 root] (abq_llm.py 212): INFO === Start quantize layer 8 ===
[2025-03-27 12:40:06 root] (abq_llm.py 212): INFO === Start quantize layer 9 ===
[2025-03-27 12:40:15 root] (abq_llm.py 212): INFO === Start quantize layer 10 ===
[2025-03-27 12:40:22 root] (abq_llm.py 212): INFO === Start quantize layer 11 ===
[2025-03-27 12:40:29 root] (abq_llm.py 212): INFO === Start quantize layer 12 ===
[2025-03-27 12:40:36 root] (abq_llm.py 212): INFO === Start quantize layer 13 ===
[2025-03-27 12:40:44 root] (abq_llm.py 212): INFO === Start quantize layer 14 ===
[2025-03-27 12:40:51 root] (abq_llm.py 212): INFO === Start quantize layer 15 ===
[2025-03-27 12:41:01 root] (abq_llm.py 212): INFO === Start quantize layer 16 ===
[2025-03-27 12:41:11 root] (abq_llm.py 212): INFO === Start quantize layer 17 ===
[2025-03-27 12:41:20 root] (abq_llm.py 212): INFO === Start quantize layer 18 ===
[2025-03-27 12:41:29 root] (abq_llm.py 212): INFO === Start quantize layer 19 ===
[2025-03-27 12:41:37 root] (abq_llm.py 212): INFO === Start quantize layer 20 ===
[2025-03-27 12:41:44 root] (abq_llm.py 212): INFO === Start quantize layer 21 ===
[2025-03-27 12:41:51 root] (abq_llm.py 212): INFO === Start quantize layer 22 ===
[2025-03-27 12:42:01 root] (abq_llm.py 212): INFO === Start quantize layer 23 ===
[2025-03-27 12:42:12 root] (abq_llm.py 212): INFO === Start quantize layer 24 ===
[2025-03-27 12:42:24 root] (abq_llm.py 212): INFO === Start quantize layer 25 ===
[2025-03-27 12:42:35 root] (abq_llm.py 212): INFO === Start quantize layer 26 ===
[2025-03-27 12:42:46 root] (abq_llm.py 212): INFO === Start quantize layer 27 ===
[2025-03-27 12:42:56 root] (abq_llm.py 212): INFO === Start quantize layer 28 ===
[2025-03-27 12:43:04 root] (abq_llm.py 212): INFO === Start quantize layer 29 ===
[2025-03-27 12:43:17 root] (abq_llm.py 212): INFO === Start quantize layer 30 ===
[2025-03-27 12:43:28 root] (abq_llm.py 212): INFO === Start quantize layer 31 ===
[2025-03-27 12:43:41 root] (abq_llm.py 212): INFO === Start quantize layer 32 ===
[2025-03-27 12:43:52 root] (abq_llm.py 212): INFO === Start quantize layer 33 ===
[2025-03-27 12:44:00 root] (abq_llm.py 212): INFO === Start quantize layer 34 ===
[2025-03-27 12:44:10 root] (abq_llm.py 212): INFO === Start quantize layer 35 ===
[2025-03-27 12:44:19 root] (abq_llm.py 212): INFO === Start quantize layer 36 ===
[2025-03-27 12:44:29 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:44:29 root] (abq_llm.py 212): INFO === Start quantize layer 37 ===
[2025-03-27 12:44:36 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:44:37 root] (abq_llm.py 212): INFO === Start quantize layer 38 ===
[2025-03-27 12:44:45 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:44:45 root] (abq_llm.py 212): INFO === Start quantize layer 39 ===
[2025-03-27 12:44:55 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:44:55 root] (main.py 361): INFO 370.1241111755371
[2025-03-27 14:51:21 root] (main.py 169): INFO {'results': {'hendrycksTest-international_law': {'acc': 0.35537190082644626, 'acc_stderr': 0.04369236326573981, 'acc_norm': 0.5867768595041323, 'acc_norm_stderr': 0.04495087843548408}, 'hendrycksTest-high_school_biology': {'acc': 0.36451612903225805, 'acc_stderr': 0.027379871229943255, 'acc_norm': 0.31290322580645163, 'acc_norm_stderr': 0.02637756702864586}, 'hendrycksTest-college_physics': {'acc': 0.27450980392156865, 'acc_stderr': 0.044405219061793275, 'acc_norm': 0.28431372549019607, 'acc_norm_stderr': 0.04488482852329017}, 'hendrycksTest-college_biology': {'acc': 0.3194444444444444, 'acc_stderr': 0.03899073687357336, 'acc_norm': 0.2708333333333333, 'acc_norm_stderr': 0.03716177437566018}, 'hendrycksTest-electrical_engineering': {'acc': 0.3724137931034483, 'acc_stderr': 0.0402873153294756, 'acc_norm': 0.3793103448275862, 'acc_norm_stderr': 0.040434618619167466}, 'hendrycksTest-high_school_physics': {'acc': 0.2913907284768212, 'acc_stderr': 0.03710185726119994, 'acc_norm': 0.304635761589404, 'acc_norm_stderr': 0.03757949922943343}, 'hendrycksTest-elementary_mathematics': {'acc': 0.30423280423280424, 'acc_stderr': 0.023695415009463087, 'acc_norm': 0.31746031746031744, 'acc_norm_stderr': 0.02397386199899208}, 'hendrycksTest-formal_logic': {'acc': 0.30158730158730157, 'acc_stderr': 0.04104947269903394, 'acc_norm': 0.2777777777777778, 'acc_norm_stderr': 0.040061680838488774}, 'hendrycksTest-human_aging': {'acc': 0.2914798206278027, 'acc_stderr': 0.030500283176545906, 'acc_norm': 0.242152466367713, 'acc_norm_stderr': 0.028751392398694755}, 'hendrycksTest-jurisprudence': {'acc': 0.3888888888888889, 'acc_stderr': 0.047128212574267705, 'acc_norm': 0.4351851851851852, 'acc_norm_stderr': 0.04792898170907062}, 'hendrycksTest-moral_scenarios': {'acc': 0.264804469273743, 'acc_stderr': 0.014756906483260659, 'acc_norm': 0.27262569832402234, 'acc_norm_stderr': 0.014893391735249588}, 'hendrycksTest-astronomy': {'acc': 0.40789473684210525, 'acc_stderr': 0.039993097127774706, 'acc_norm': 0.4473684210526316, 'acc_norm_stderr': 0.04046336883978251}, 'hendrycksTest-conceptual_physics': {'acc': 0.251063829787234, 'acc_stderr': 0.028346963777162466, 'acc_norm': 0.2127659574468085, 'acc_norm_stderr': 0.026754391348039766}, 'hendrycksTest-college_chemistry': {'acc': 0.25, 'acc_stderr': 0.04351941398892446, 'acc_norm': 0.28, 'acc_norm_stderr': 0.045126085985421276}, 'hendrycksTest-human_sexuality': {'acc': 0.4732824427480916, 'acc_stderr': 0.04379024936553894, 'acc_norm': 0.3435114503816794, 'acc_norm_stderr': 0.04164976071944878}, 'hendrycksTest-sociology': {'acc': 0.3681592039800995, 'acc_stderr': 0.03410410565495301, 'acc_norm': 0.3582089552238806, 'acc_norm_stderr': 0.03390393042268815}, 'hendrycksTest-prehistory': {'acc': 0.39197530864197533, 'acc_stderr': 0.027163686038271236, 'acc_norm': 0.2932098765432099, 'acc_norm_stderr': 0.025329888171900926}, 'hendrycksTest-professional_law': {'acc': 0.2620599739243807, 'acc_stderr': 0.011231552795890396, 'acc_norm': 0.303129074315515, 'acc_norm_stderr': 0.011738669951254305}, 'hendrycksTest-virology': {'acc': 0.39759036144578314, 'acc_stderr': 0.038099730845402184, 'acc_norm': 0.3373493975903614, 'acc_norm_stderr': 0.0368078369072758}, 'hendrycksTest-machine_learning': {'acc': 0.25, 'acc_stderr': 0.04109974682633932, 'acc_norm': 0.23214285714285715, 'acc_norm_stderr': 0.04007341809755806}, 'hendrycksTest-marketing': {'acc': 0.5042735042735043, 'acc_stderr': 0.03275489264382132, 'acc_norm': 0.41452991452991456, 'acc_norm_stderr': 0.03227396567623778}, 'hendrycksTest-us_foreign_policy': {'acc': 0.53, 'acc_stderr': 0.05016135580465919, 'acc_norm': 0.46, 'acc_norm_stderr': 0.05009082659620333}, 'hendrycksTest-high_school_european_history': {'acc': 0.4, 'acc_stderr': 0.03825460278380026, 'acc_norm': 0.40606060606060607, 'acc_norm_stderr': 0.03834816355401181}, 'hendrycksTest-high_school_macroeconomics': {'acc': 0.3717948717948718, 'acc_stderr': 0.024503472557110946, 'acc_norm': 0.34102564102564104, 'acc_norm_stderr': 0.024035489676335054}, 'hendrycksTest-logical_fallacies': {'acc': 0.294478527607362, 'acc_stderr': 0.03581165790474082, 'acc_norm': 0.34355828220858897, 'acc_norm_stderr': 0.03731133519673893}, 'hendrycksTest-abstract_algebra': {'acc': 0.25, 'acc_stderr': 0.04351941398892446, 'acc_norm': 0.19, 'acc_norm_stderr': 0.039427724440366234}, 'hendrycksTest-high_school_government_and_politics': {'acc': 0.44559585492227977, 'acc_stderr': 0.03587014986075659, 'acc_norm': 0.36787564766839376, 'acc_norm_stderr': 0.03480175668466036}, 'hendrycksTest-moral_disputes': {'acc': 0.3468208092485549, 'acc_stderr': 0.025624723994030457, 'acc_norm': 0.3352601156069364, 'acc_norm_stderr': 0.025416003773165555}, 'hendrycksTest-high_school_world_history': {'acc': 0.4050632911392405, 'acc_stderr': 0.03195514741370673, 'acc_norm': 0.3459915611814346, 'acc_norm_stderr': 0.03096481058878671}, 'hendrycksTest-high_school_microeconomics': {'acc': 0.35294117647058826, 'acc_stderr': 0.031041941304059274, 'acc_norm': 0.3907563025210084, 'acc_norm_stderr': 0.031693802357129965}, 'hendrycksTest-college_medicine': {'acc': 0.31213872832369943, 'acc_stderr': 0.03533133389323657, 'acc_norm': 0.2947976878612717, 'acc_norm_stderr': 0.034765996075164785}, 'hendrycksTest-security_studies': {'acc': 0.4448979591836735, 'acc_stderr': 0.031814251181977865, 'acc_norm': 0.35918367346938773, 'acc_norm_stderr': 0.030713560455108493}, 'hendrycksTest-high_school_geography': {'acc': 0.40404040404040403, 'acc_stderr': 0.03496130972056129, 'acc_norm': 0.3333333333333333, 'acc_norm_stderr': 0.03358618145732523}, 'hendrycksTest-world_religions': {'acc': 0.5672514619883041, 'acc_stderr': 0.03799978644370608, 'acc_norm': 0.4502923976608187, 'acc_norm_stderr': 0.03815827365913235}, 'hendrycksTest-business_ethics': {'acc': 0.44, 'acc_stderr': 0.04988876515698589, 'acc_norm': 0.37, 'acc_norm_stderr': 0.048523658709391}, 'hendrycksTest-professional_accounting': {'acc': 0.3191489361702128, 'acc_stderr': 0.027807990141320207, 'acc_norm': 0.32269503546099293, 'acc_norm_stderr': 0.027889139300534778}, 'hendrycksTest-global_facts': {'acc': 0.25, 'acc_stderr': 0.04351941398892446, 'acc_norm': 0.23, 'acc_norm_stderr': 0.04229525846816506}, 'hendrycksTest-high_school_us_history': {'acc': 0.3872549019607843, 'acc_stderr': 0.03418931233833344, 'acc_norm': 0.3480392156862745, 'acc_norm_stderr': 0.03343311240488419}, 'hendrycksTest-professional_psychology': {'acc': 0.33986928104575165, 'acc_stderr': 0.019162418588623553, 'acc_norm': 0.29411764705882354, 'acc_norm_stderr': 0.018433427649401896}, 'hendrycksTest-college_computer_science': {'acc': 0.33, 'acc_stderr': 0.04725815626252605, 'acc_norm': 0.31, 'acc_norm_stderr': 0.04648231987117316}, 'hendrycksTest-philosophy': {'acc': 0.3504823151125402, 'acc_stderr': 0.027098652621301747, 'acc_norm': 0.33762057877813506, 'acc_norm_stderr': 0.026858825879488547}, 'hendrycksTest-anatomy': {'acc': 0.35555555555555557, 'acc_stderr': 0.04135176749720386, 'acc_norm': 0.24444444444444444, 'acc_norm_stderr': 0.037125378336148665}, 'hendrycksTest-nutrition': {'acc': 0.4215686274509804, 'acc_stderr': 0.02827549015679143, 'acc_norm': 0.42483660130718953, 'acc_norm_stderr': 0.028304576673141117}, 'hendrycksTest-computer_security': {'acc': 0.42, 'acc_stderr': 0.049604496374885836, 'acc_norm': 0.4, 'acc_norm_stderr': 0.049236596391733084}, 'hendrycksTest-econometrics': {'acc': 0.21929824561403508, 'acc_stderr': 0.03892431106518754, 'acc_norm': 0.2631578947368421, 'acc_norm_stderr': 0.04142439719489362}, 'hendrycksTest-clinical_knowledge': {'acc': 0.3886792452830189, 'acc_stderr': 0.030000485448675986, 'acc_norm': 0.3622641509433962, 'acc_norm_stderr': 0.029582245128384303}, 'hendrycksTest-high_school_mathematics': {'acc': 0.22962962962962963, 'acc_stderr': 0.025644108639267638, 'acc_norm': 0.3, 'acc_norm_stderr': 0.02794045713622841}, 'hendrycksTest-medical_genetics': {'acc': 0.34, 'acc_stderr': 0.047609522856952344, 'acc_norm': 0.42, 'acc_norm_stderr': 0.049604496374885836}, 'hendrycksTest-public_relations': {'acc': 0.36363636363636365, 'acc_stderr': 0.04607582090719976, 'acc_norm': 0.20909090909090908, 'acc_norm_stderr': 0.03895091015724137}, 'hendrycksTest-miscellaneous': {'acc': 0.454661558109834, 'acc_stderr': 0.017806304585052602, 'acc_norm': 0.3231162196679438, 'acc_norm_stderr': 0.016723726512343048}, 'hendrycksTest-management': {'acc': 0.4854368932038835, 'acc_stderr': 0.04948637324026637, 'acc_norm': 0.3883495145631068, 'acc_norm_stderr': 0.04825729337356388}, 'hendrycksTest-professional_medicine': {'acc': 0.34191176470588236, 'acc_stderr': 0.028814722422254194, 'acc_norm': 0.3125, 'acc_norm_stderr': 0.02815637344037142}, 'hendrycksTest-high_school_computer_science': {'acc': 0.37, 'acc_stderr': 0.048523658709391, 'acc_norm': 0.37, 'acc_norm_stderr': 0.048523658709391}, 'hendrycksTest-high_school_psychology': {'acc': 0.3651376146788991, 'acc_stderr': 0.020642801454384005, 'acc_norm': 0.28623853211009176, 'acc_norm_stderr': 0.019379436628919975}, 'hendrycksTest-high_school_chemistry': {'acc': 0.24630541871921183, 'acc_stderr': 0.030315099285617736, 'acc_norm': 0.3251231527093596, 'acc_norm_stderr': 0.032957975663112704}, 'hendrycksTest-college_mathematics': {'acc': 0.22, 'acc_stderr': 0.0416333199893227, 'acc_norm': 0.28, 'acc_norm_stderr': 0.045126085985421276}, 'hendrycksTest-high_school_statistics': {'acc': 0.28703703703703703, 'acc_stderr': 0.030851992993257013, 'acc_norm': 0.3287037037037037, 'acc_norm_stderr': 0.032036140846700596}}, 'versions': {'hendrycksTest-international_law': 0, 'hendrycksTest-high_school_biology': 0, 'hendrycksTest-college_physics': 0, 'hendrycksTest-college_biology': 0, 'hendrycksTest-electrical_engineering': 0, 'hendrycksTest-high_school_physics': 0, 'hendrycksTest-elementary_mathematics': 0, 'hendrycksTest-formal_logic': 0, 'hendrycksTest-human_aging': 0, 'hendrycksTest-jurisprudence': 0, 'hendrycksTest-moral_scenarios': 0, 'hendrycksTest-astronomy': 0, 'hendrycksTest-conceptual_physics': 0, 'hendrycksTest-college_chemistry': 0, 'hendrycksTest-human_sexuality': 0, 'hendrycksTest-sociology': 0, 'hendrycksTest-prehistory': 0, 'hendrycksTest-professional_law': 0, 'hendrycksTest-virology': 0, 'hendrycksTest-machine_learning': 0, 'hendrycksTest-marketing': 0, 'hendrycksTest-us_foreign_policy': 0, 'hendrycksTest-high_school_european_history': 0, 'hendrycksTest-high_school_macroeconomics': 0, 'hendrycksTest-logical_fallacies': 0, 'hendrycksTest-abstract_algebra': 0, 'hendrycksTest-high_school_government_and_politics': 0, 'hendrycksTest-moral_disputes': 0, 'hendrycksTest-high_school_world_history': 0, 'hendrycksTest-high_school_microeconomics': 0, 'hendrycksTest-college_medicine': 0, 'hendrycksTest-security_studies': 0, 'hendrycksTest-high_school_geography': 0, 'hendrycksTest-world_religions': 0, 'hendrycksTest-business_ethics': 0, 'hendrycksTest-professional_accounting': 0, 'hendrycksTest-global_facts': 0, 'hendrycksTest-high_school_us_history': 0, 'hendrycksTest-professional_psychology': 0, 'hendrycksTest-college_computer_science': 0, 'hendrycksTest-philosophy': 0, 'hendrycksTest-anatomy': 0, 'hendrycksTest-nutrition': 0, 'hendrycksTest-computer_security': 0, 'hendrycksTest-econometrics': 0, 'hendrycksTest-clinical_knowledge': 0, 'hendrycksTest-high_school_mathematics': 0, 'hendrycksTest-medical_genetics': 0, 'hendrycksTest-public_relations': 0, 'hendrycksTest-miscellaneous': 0, 'hendrycksTest-management': 0, 'hendrycksTest-professional_medicine': 0, 'hendrycksTest-high_school_computer_science': 0, 'hendrycksTest-high_school_psychology': 0, 'hendrycksTest-high_school_chemistry': 0, 'hendrycksTest-college_mathematics': 0, 'hendrycksTest-high_school_statistics': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-27 14:51:21 root] (main.py 196): INFO Average accuracy 0.3021 - STEM
[2025-03-27 14:51:21 root] (main.py 196): INFO Average accuracy 0.3628 - humanities
[2025-03-27 14:51:21 root] (main.py 196): INFO Average accuracy 0.3899 - social sciences
[2025-03-27 14:51:21 root] (main.py 196): INFO Average accuracy 0.3787 - other (business, health, misc.)
[2025-03-27 14:51:21 root] (main.py 198): INFO Average accuracy: 0.3533
