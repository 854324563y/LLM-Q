[2025-03-27 12:39:59 root] (main.py 265): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-abq-llm/MMLU/llama-7b-hf-w4a7', save_dir=None, resume='./log-abq-llm/llama-7b-hf-w4a7/abq_parameters.pth', real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hendrycksTest*', eval_ppl=False, num_fewshot=0, wbits=4, abits=7, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=0, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None)
[2025-03-27 12:42:50 root] (main.py 332): INFO === start quantization ===
[2025-03-27 12:42:51 root] (main.py 338): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-27 12:42:51 root] (abq_llm.py 62): INFO Starting ...
[2025-03-27 12:42:54 root] (abq_llm.py 212): INFO === Start quantize layer 0 ===
[2025-03-27 12:42:54 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:42:56 root] (abq_llm.py 212): INFO === Start quantize layer 1 ===
[2025-03-27 12:42:56 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:42:56 root] (abq_llm.py 212): INFO === Start quantize layer 2 ===
[2025-03-27 12:42:57 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:42:57 root] (abq_llm.py 212): INFO === Start quantize layer 3 ===
[2025-03-27 12:42:57 root] (abq_llm.py 212): INFO === Start quantize layer 4 ===
[2025-03-27 12:42:58 root] (abq_llm.py 212): INFO === Start quantize layer 5 ===
[2025-03-27 12:42:58 root] (abq_llm.py 212): INFO === Start quantize layer 6 ===
[2025-03-27 12:42:59 root] (abq_llm.py 212): INFO === Start quantize layer 7 ===
[2025-03-27 12:42:59 root] (abq_llm.py 212): INFO === Start quantize layer 8 ===
[2025-03-27 12:43:00 root] (abq_llm.py 212): INFO === Start quantize layer 9 ===
[2025-03-27 12:43:00 root] (abq_llm.py 212): INFO === Start quantize layer 10 ===
[2025-03-27 12:43:00 root] (abq_llm.py 212): INFO === Start quantize layer 11 ===
[2025-03-27 12:43:01 root] (abq_llm.py 212): INFO === Start quantize layer 12 ===
[2025-03-27 12:43:01 root] (abq_llm.py 212): INFO === Start quantize layer 13 ===
[2025-03-27 12:43:02 root] (abq_llm.py 212): INFO === Start quantize layer 14 ===
[2025-03-27 12:43:02 root] (abq_llm.py 212): INFO === Start quantize layer 15 ===
[2025-03-27 12:43:03 root] (abq_llm.py 212): INFO === Start quantize layer 16 ===
[2025-03-27 12:43:03 root] (abq_llm.py 212): INFO === Start quantize layer 17 ===
[2025-03-27 12:43:04 root] (abq_llm.py 212): INFO === Start quantize layer 18 ===
[2025-03-27 12:43:04 root] (abq_llm.py 212): INFO === Start quantize layer 19 ===
[2025-03-27 12:43:04 root] (abq_llm.py 212): INFO === Start quantize layer 20 ===
[2025-03-27 12:43:05 root] (abq_llm.py 212): INFO === Start quantize layer 21 ===
[2025-03-27 12:43:05 root] (abq_llm.py 212): INFO === Start quantize layer 22 ===
[2025-03-27 12:43:06 root] (abq_llm.py 212): INFO === Start quantize layer 23 ===
[2025-03-27 12:43:06 root] (abq_llm.py 212): INFO === Start quantize layer 24 ===
[2025-03-27 12:43:08 root] (abq_llm.py 212): INFO === Start quantize layer 25 ===
[2025-03-27 12:43:08 root] (abq_llm.py 212): INFO === Start quantize layer 26 ===
[2025-03-27 12:43:09 root] (abq_llm.py 212): INFO === Start quantize layer 27 ===
[2025-03-27 12:43:09 root] (abq_llm.py 212): INFO === Start quantize layer 28 ===
[2025-03-27 12:43:09 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:43:09 root] (abq_llm.py 212): INFO === Start quantize layer 29 ===
[2025-03-27 12:43:10 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:43:10 root] (abq_llm.py 212): INFO === Start quantize layer 30 ===
[2025-03-27 12:43:10 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:43:10 root] (abq_llm.py 212): INFO === Start quantize layer 31 ===
[2025-03-27 12:43:10 root] (abq_llm.py 268): INFO use compensation vector
[2025-03-27 12:43:11 root] (main.py 361): INFO 20.563775300979614
[2025-03-27 14:23:28 root] (main.py 169): INFO {'results': {'hendrycksTest-international_law': {'acc': 0.30578512396694213, 'acc_stderr': 0.04205953933884124, 'acc_norm': 0.5206611570247934, 'acc_norm_stderr': 0.04560456086387235}, 'hendrycksTest-high_school_biology': {'acc': 0.27419354838709675, 'acc_stderr': 0.025378139970885203, 'acc_norm': 0.31290322580645163, 'acc_norm_stderr': 0.02637756702864586}, 'hendrycksTest-college_physics': {'acc': 0.2549019607843137, 'acc_stderr': 0.04336432707993177, 'acc_norm': 0.3431372549019608, 'acc_norm_stderr': 0.04724007352383889}, 'hendrycksTest-college_biology': {'acc': 0.24305555555555555, 'acc_stderr': 0.03586879280080341, 'acc_norm': 0.2361111111111111, 'acc_norm_stderr': 0.03551446610810826}, 'hendrycksTest-electrical_engineering': {'acc': 0.3724137931034483, 'acc_stderr': 0.040287315329475604, 'acc_norm': 0.32413793103448274, 'acc_norm_stderr': 0.03900432069185554}, 'hendrycksTest-high_school_physics': {'acc': 0.2847682119205298, 'acc_stderr': 0.03684881521389024, 'acc_norm': 0.2781456953642384, 'acc_norm_stderr': 0.03658603262763743}, 'hendrycksTest-elementary_mathematics': {'acc': 0.29365079365079366, 'acc_stderr': 0.023456037383982033, 'acc_norm': 0.29894179894179895, 'acc_norm_stderr': 0.023577604791655812}, 'hendrycksTest-formal_logic': {'acc': 0.30158730158730157, 'acc_stderr': 0.04104947269903394, 'acc_norm': 0.30158730158730157, 'acc_norm_stderr': 0.04104947269903394}, 'hendrycksTest-human_aging': {'acc': 0.2825112107623318, 'acc_stderr': 0.030216831011508773, 'acc_norm': 0.20179372197309417, 'acc_norm_stderr': 0.02693611191280227}, 'hendrycksTest-jurisprudence': {'acc': 0.3333333333333333, 'acc_stderr': 0.04557239513497751, 'acc_norm': 0.42592592592592593, 'acc_norm_stderr': 0.0478034362693679}, 'hendrycksTest-moral_scenarios': {'acc': 0.23575418994413408, 'acc_stderr': 0.014196375686290804, 'acc_norm': 0.27262569832402234, 'acc_norm_stderr': 0.014893391735249588}, 'hendrycksTest-astronomy': {'acc': 0.3355263157894737, 'acc_stderr': 0.038424985593952694, 'acc_norm': 0.40131578947368424, 'acc_norm_stderr': 0.03988903703336284}, 'hendrycksTest-conceptual_physics': {'acc': 0.2936170212765957, 'acc_stderr': 0.02977164271249123, 'acc_norm': 0.225531914893617, 'acc_norm_stderr': 0.02732107841738754}, 'hendrycksTest-college_chemistry': {'acc': 0.15, 'acc_stderr': 0.035887028128263714, 'acc_norm': 0.26, 'acc_norm_stderr': 0.0440844002276808}, 'hendrycksTest-human_sexuality': {'acc': 0.3969465648854962, 'acc_stderr': 0.04291135671009225, 'acc_norm': 0.31297709923664124, 'acc_norm_stderr': 0.04066962905677698}, 'hendrycksTest-sociology': {'acc': 0.2935323383084577, 'acc_stderr': 0.03220024104534205, 'acc_norm': 0.27860696517412936, 'acc_norm_stderr': 0.031700561834973086}, 'hendrycksTest-prehistory': {'acc': 0.28703703703703703, 'acc_stderr': 0.02517104191530968, 'acc_norm': 0.25308641975308643, 'acc_norm_stderr': 0.024191808600713}, 'hendrycksTest-professional_law': {'acc': 0.2770534550195567, 'acc_stderr': 0.011430462443719676, 'acc_norm': 0.2966101694915254, 'acc_norm_stderr': 0.011665946586082864}, 'hendrycksTest-virology': {'acc': 0.3493975903614458, 'acc_stderr': 0.0371172519074075, 'acc_norm': 0.30120481927710846, 'acc_norm_stderr': 0.035716092300534796}, 'hendrycksTest-machine_learning': {'acc': 0.2857142857142857, 'acc_stderr': 0.042878587513404544, 'acc_norm': 0.25, 'acc_norm_stderr': 0.04109974682633932}, 'hendrycksTest-marketing': {'acc': 0.48717948717948717, 'acc_stderr': 0.032745319388423504, 'acc_norm': 0.405982905982906, 'acc_norm_stderr': 0.03217180182641087}, 'hendrycksTest-us_foreign_policy': {'acc': 0.46, 'acc_stderr': 0.05009082659620332, 'acc_norm': 0.39, 'acc_norm_stderr': 0.04902071300001974}, 'hendrycksTest-high_school_european_history': {'acc': 0.3575757575757576, 'acc_stderr': 0.03742597043806586, 'acc_norm': 0.36363636363636365, 'acc_norm_stderr': 0.03756335775187896}, 'hendrycksTest-high_school_macroeconomics': {'acc': 0.32564102564102565, 'acc_stderr': 0.02375966576741229, 'acc_norm': 0.2846153846153846, 'acc_norm_stderr': 0.02287832279970628}, 'hendrycksTest-logical_fallacies': {'acc': 0.26380368098159507, 'acc_stderr': 0.03462419931615623, 'acc_norm': 0.3067484662576687, 'acc_norm_stderr': 0.036230899157241474}, 'hendrycksTest-abstract_algebra': {'acc': 0.22, 'acc_stderr': 0.041633319989322695, 'acc_norm': 0.27, 'acc_norm_stderr': 0.04461960433384741}, 'hendrycksTest-high_school_government_and_politics': {'acc': 0.27461139896373055, 'acc_stderr': 0.03221024508041156, 'acc_norm': 0.31088082901554404, 'acc_norm_stderr': 0.03340361906276588}, 'hendrycksTest-moral_disputes': {'acc': 0.2630057803468208, 'acc_stderr': 0.023703099525258172, 'acc_norm': 0.30346820809248554, 'acc_norm_stderr': 0.02475241196091722}, 'hendrycksTest-high_school_world_history': {'acc': 0.3206751054852321, 'acc_stderr': 0.03038193194999041, 'acc_norm': 0.32489451476793246, 'acc_norm_stderr': 0.030486039389105296}, 'hendrycksTest-high_school_microeconomics': {'acc': 0.29411764705882354, 'acc_stderr': 0.02959732973097808, 'acc_norm': 0.3487394957983193, 'acc_norm_stderr': 0.030956636328566545}, 'hendrycksTest-college_medicine': {'acc': 0.28901734104046245, 'acc_stderr': 0.03456425745086999, 'acc_norm': 0.31213872832369943, 'acc_norm_stderr': 0.03533133389323657}, 'hendrycksTest-security_studies': {'acc': 0.44081632653061226, 'acc_stderr': 0.03178419114175363, 'acc_norm': 0.33877551020408164, 'acc_norm_stderr': 0.03029950656215418}, 'hendrycksTest-high_school_geography': {'acc': 0.2878787878787879, 'acc_stderr': 0.03225883512300992, 'acc_norm': 0.30303030303030304, 'acc_norm_stderr': 0.03274287914026867}, 'hendrycksTest-world_religions': {'acc': 0.4327485380116959, 'acc_stderr': 0.03799978644370607, 'acc_norm': 0.4269005847953216, 'acc_norm_stderr': 0.03793620616529916}, 'hendrycksTest-business_ethics': {'acc': 0.34, 'acc_stderr': 0.04760952285695235, 'acc_norm': 0.31, 'acc_norm_stderr': 0.04648231987117316}, 'hendrycksTest-professional_accounting': {'acc': 0.1950354609929078, 'acc_stderr': 0.02363698094391562, 'acc_norm': 0.28368794326241137, 'acc_norm_stderr': 0.026891709428343957}, 'hendrycksTest-global_facts': {'acc': 0.29, 'acc_stderr': 0.04560480215720683, 'acc_norm': 0.25, 'acc_norm_stderr': 0.04351941398892446}, 'hendrycksTest-high_school_us_history': {'acc': 0.3235294117647059, 'acc_stderr': 0.03283472056108566, 'acc_norm': 0.3235294117647059, 'acc_norm_stderr': 0.03283472056108567}, 'hendrycksTest-professional_psychology': {'acc': 0.2973856209150327, 'acc_stderr': 0.01849259653639695, 'acc_norm': 0.2696078431372549, 'acc_norm_stderr': 0.017952449196987862}, 'hendrycksTest-college_computer_science': {'acc': 0.35, 'acc_stderr': 0.047937248544110196, 'acc_norm': 0.33, 'acc_norm_stderr': 0.047258156262526045}, 'hendrycksTest-philosophy': {'acc': 0.3215434083601286, 'acc_stderr': 0.026527724079528872, 'acc_norm': 0.31511254019292606, 'acc_norm_stderr': 0.02638527370346449}, 'hendrycksTest-anatomy': {'acc': 0.2962962962962963, 'acc_stderr': 0.03944624162501117, 'acc_norm': 0.2074074074074074, 'acc_norm_stderr': 0.03502553170678315}, 'hendrycksTest-nutrition': {'acc': 0.38562091503267976, 'acc_stderr': 0.027870745278290306, 'acc_norm': 0.42810457516339867, 'acc_norm_stderr': 0.028332397483664264}, 'hendrycksTest-computer_security': {'acc': 0.31, 'acc_stderr': 0.04648231987117316, 'acc_norm': 0.37, 'acc_norm_stderr': 0.04852365870939099}, 'hendrycksTest-econometrics': {'acc': 0.2894736842105263, 'acc_stderr': 0.04266339443159394, 'acc_norm': 0.19298245614035087, 'acc_norm_stderr': 0.037124548537213684}, 'hendrycksTest-clinical_knowledge': {'acc': 0.3018867924528302, 'acc_stderr': 0.02825420034443866, 'acc_norm': 0.35471698113207545, 'acc_norm_stderr': 0.0294451753281996}, 'hendrycksTest-high_school_mathematics': {'acc': 0.23703703703703705, 'acc_stderr': 0.02592887613276612, 'acc_norm': 0.31851851851851853, 'acc_norm_stderr': 0.02840653309060846}, 'hendrycksTest-medical_genetics': {'acc': 0.27, 'acc_stderr': 0.044619604333847394, 'acc_norm': 0.33, 'acc_norm_stderr': 0.047258156262526045}, 'hendrycksTest-public_relations': {'acc': 0.32727272727272727, 'acc_stderr': 0.04494290866252088, 'acc_norm': 0.16363636363636364, 'acc_norm_stderr': 0.03543433054298678}, 'hendrycksTest-miscellaneous': {'acc': 0.33205619412515963, 'acc_stderr': 0.01684117465529572, 'acc_norm': 0.2886334610472541, 'acc_norm_stderr': 0.016203792703197793}, 'hendrycksTest-management': {'acc': 0.2815533980582524, 'acc_stderr': 0.044532548363264673, 'acc_norm': 0.3106796116504854, 'acc_norm_stderr': 0.04582124160161551}, 'hendrycksTest-professional_medicine': {'acc': 0.27205882352941174, 'acc_stderr': 0.02703304115168146, 'acc_norm': 0.27205882352941174, 'acc_norm_stderr': 0.02703304115168146}, 'hendrycksTest-high_school_computer_science': {'acc': 0.29, 'acc_stderr': 0.04560480215720683, 'acc_norm': 0.32, 'acc_norm_stderr': 0.04688261722621503}, 'hendrycksTest-high_school_psychology': {'acc': 0.3192660550458716, 'acc_stderr': 0.019987829069750013, 'acc_norm': 0.26788990825688075, 'acc_norm_stderr': 0.018987462257978652}, 'hendrycksTest-high_school_chemistry': {'acc': 0.27586206896551724, 'acc_stderr': 0.03144712581678242, 'acc_norm': 0.2660098522167488, 'acc_norm_stderr': 0.03108982600293752}, 'hendrycksTest-college_mathematics': {'acc': 0.23, 'acc_stderr': 0.04229525846816503, 'acc_norm': 0.32, 'acc_norm_stderr': 0.04688261722621505}, 'hendrycksTest-high_school_statistics': {'acc': 0.3101851851851852, 'acc_stderr': 0.03154696285656627, 'acc_norm': 0.28703703703703703, 'acc_norm_stderr': 0.03085199299325701}}, 'versions': {'hendrycksTest-international_law': 0, 'hendrycksTest-high_school_biology': 0, 'hendrycksTest-college_physics': 0, 'hendrycksTest-college_biology': 0, 'hendrycksTest-electrical_engineering': 0, 'hendrycksTest-high_school_physics': 0, 'hendrycksTest-elementary_mathematics': 0, 'hendrycksTest-formal_logic': 0, 'hendrycksTest-human_aging': 0, 'hendrycksTest-jurisprudence': 0, 'hendrycksTest-moral_scenarios': 0, 'hendrycksTest-astronomy': 0, 'hendrycksTest-conceptual_physics': 0, 'hendrycksTest-college_chemistry': 0, 'hendrycksTest-human_sexuality': 0, 'hendrycksTest-sociology': 0, 'hendrycksTest-prehistory': 0, 'hendrycksTest-professional_law': 0, 'hendrycksTest-virology': 0, 'hendrycksTest-machine_learning': 0, 'hendrycksTest-marketing': 0, 'hendrycksTest-us_foreign_policy': 0, 'hendrycksTest-high_school_european_history': 0, 'hendrycksTest-high_school_macroeconomics': 0, 'hendrycksTest-logical_fallacies': 0, 'hendrycksTest-abstract_algebra': 0, 'hendrycksTest-high_school_government_and_politics': 0, 'hendrycksTest-moral_disputes': 0, 'hendrycksTest-high_school_world_history': 0, 'hendrycksTest-high_school_microeconomics': 0, 'hendrycksTest-college_medicine': 0, 'hendrycksTest-security_studies': 0, 'hendrycksTest-high_school_geography': 0, 'hendrycksTest-world_religions': 0, 'hendrycksTest-business_ethics': 0, 'hendrycksTest-professional_accounting': 0, 'hendrycksTest-global_facts': 0, 'hendrycksTest-high_school_us_history': 0, 'hendrycksTest-professional_psychology': 0, 'hendrycksTest-college_computer_science': 0, 'hendrycksTest-philosophy': 0, 'hendrycksTest-anatomy': 0, 'hendrycksTest-nutrition': 0, 'hendrycksTest-computer_security': 0, 'hendrycksTest-econometrics': 0, 'hendrycksTest-clinical_knowledge': 0, 'hendrycksTest-high_school_mathematics': 0, 'hendrycksTest-medical_genetics': 0, 'hendrycksTest-public_relations': 0, 'hendrycksTest-miscellaneous': 0, 'hendrycksTest-management': 0, 'hendrycksTest-professional_medicine': 0, 'hendrycksTest-high_school_computer_science': 0, 'hendrycksTest-high_school_psychology': 0, 'hendrycksTest-high_school_chemistry': 0, 'hendrycksTest-college_mathematics': 0, 'hendrycksTest-high_school_statistics': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-27 14:23:28 root] (main.py 196): INFO Average accuracy 0.2784 - STEM
[2025-03-27 14:23:28 root] (main.py 196): INFO Average accuracy 0.3095 - humanities
[2025-03-27 14:23:28 root] (main.py 196): INFO Average accuracy 0.3339 - social sciences
[2025-03-27 14:23:28 root] (main.py 196): INFO Average accuracy 0.3123 - other (business, health, misc.)
[2025-03-27 14:23:28 root] (main.py 198): INFO Average accuracy: 0.3055
